#|title|abstract|venue|year|doi|authors|relative_citation_index|relevance_score
1|Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling|In computer architecture, significant innovation frequently comes from industry. However, the simulation tools used by industry are often not released for open use, and even when they are, the exact details of industrial designs are not disclosed. As a result, research in the architecture space must ensure that assumptions about contemporary processor design remain true.To help bridge the gap between opaque industrial innovation and public research, we introduce three mechanisms that make it much easier for GPU simulators to keep up with industry. First, we introduce a new GPU simulator frontend that minimizes the effort required to simulate different machine ISAs through trace-driven simulation of NVIDIA’s native machine ISA, while still supporting execution-driven simulation of the virtual ISA. Second, we extensively update GPGPU-Sim’s performance model to increase its level of detail, configurability and accuracy. Finally, surrounding the new frontend and flexible performance model is an infrastructure that enables quick, detailed validation. A comprehensive set of microbenchmarks and automated correlation plotting ease the modeling process.We use these three new mechanisms to build Accel-Sim, a detailed simulation framework that decreases cycle error 79 percentage points, over a wide range of 80 workloads, consisting of 1,945 kernel instances. We further demonstrate that Accel-Sim is able to simulate benchmark suites that no other open-source simulator can. In particular, we use Accel-sim to simulate an additional 60 workloads, comprised of 11,440 kernel instances, from the machine learning benchmark suite Deepbench. Deepbench makes use of closed-source, hand-tuned kernels with no virtual ISA implementation. Using a rigorous counter-by-counter analysis, we validate Accel-Sim against contemporary GPUs.Finally, to highlight the effects of falling behind industry, this paper presents two case-studies that demonstrate how incorrect baseline assumptions can hide new areas of opportunity and lead to potentially incorrect design decisions.|International Symposium on Computer Architecture|2018|10.1109/ISCA45697.2020.00047|Mahmoud Khairy, Timothy G. Rogers, Tor M. Aamodt, Zhesheng Shen|21.285714285714285|11
2|PPT-GPU: Scalable GPU Performance Modeling|Performance modeling is a challenging problem due to the complexities of hardware architectures. In this paper, we present PPT-GPU, a scalable and accurate simulation framework that enables GPU code developers and architects to predict the performance of applications in a fast, and accurate manner on different GPU architectures. PPT-GPU is part of the open source project, Performance Prediction Toolkit (PPT) developed at the Los Alamos National Laboratory. We extend the old GPU model in PPT that predict the runtimes of computational physics codes to offer better prediction accuracy, for which, we add models for different memory hierarchies found in GPUs and latencies for different instructions. To further show the utility of PPT-GPU, we compare our model against real GPU device(s) and the widely used cycle-accurate simulator, GPGPU-Sim using different workloads from RODINIA and Parboil benchmarks. The results indicate that the predicted performance of PPT-GPU is within a 10 percent error compared to the real device(s). In addition, PPT-GPU is highly scalable, where it is up to 450x faster than GPGPU-Sim with more accurate results.|IEEE computer architecture letters|2019|10.1109/LCA.2019.2904497|Yehia Arafa, S. Eidenbenz, Gopinath Chennupati, N. Santhi, Abdel-Hameed A. Badawy|4.333333333333333|8
12|MGPUSim: Enabling Multi-GPU Performance Modeling and Optimization|The rapidly growing popularity and scale of data-parallel workloads demand a corresponding increase in raw computational power of Graphics Processing Units (GPUs). As single-GPU platforms struggle to satisfy these performance demands, multi-GPU platforms have started to dominate the high-performance computing world. The advent of such systems raises a number of design challenges, including the GPU microarchitecture, multi-GPU interconnect fabric, runtime libraries, and associated programming models. The research community currently lacks a publicly available and comprehensive multi-GPU simulation framework to evaluate next- generation multi-GPU system designs. In this work, we present MGPUSim, a cycle-accurate, extensively validated, multi-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set architecture. MGPUSim comes with in-built support for multi-threaded execution to enable fast, parallelized, and accurate simulation. In terms of performance accuracy, MGPUSim differs by only 5.5% on average from the actual GPU hardware. We also achieve a 3.5x and a 2.5x average speedup running functional emulation and detailed timing simulation, respectively, on a 4-core CPU, while delivering the same accuracy as serial simulation. We illustrate the flexibility and capability of the simulator through two concrete design studies. In the first, we propose the Locality API, an API extension that allows the GPU programmer to both avoid the complexity of multi-GPU programming, while precisely controlling data placement in the multi-GPU memory. In the second design study, we propose Progressive Page Splitting Migration (PASI), a customized multi-GPU memory management system enabling the hardware to progressively improve data placement. For a discrete 4-GPU system, we observe that the Locality API can speed up the system by 1.6x (geometric mean), and PASI can improve the system performance by 2.6x (geometric mean) across all benchmarks, compared to a unified 4-GPU platform.|International Symposium on Computer Architecture|2019|10.1145/3307650.3322230|Shane Treadway, Shi Dong, Yifan Sun, Trinayan Baruah, A. Joshi, Yuhui Bao, Harrison Barclay, Vincent Zhao, R. Ubal, Xiang Gong, Spencer Hance, Saiful A. Mojumder, Carter McCardwell, Zhongliang Chen, D. Kaeli, José L. Abellán, Amir Kavyan Ziabari, John Kim|9.666666666666666|6
9|Demystifying the Nvidia Ampere Architecture through Microbenchmarking and Instruction-level Analysis|Graphics Processing Units (GPUs) are now considered the leading hardware to accelerate general-purpose workloads such as AI, data analytics, and HPC. Over the last decade, researchers have focused on demystifying and evaluating the microarchitecture features of various GPU architectures beyond what vendors reveal. This line of work is necessary to understand the hardware better and build more efficient workloads and applications. Many works have studied the recent Nvidia architectures, such as Volta and Turing, comparing them to their successor, Ampere. However, some microarchitecture features, such as the clock cycles for the different instructions, have not been extensively studied for the Ampere architecture. In this paper, we study the clock cycles per instruction with various data types found in the instruction-set architecture (ISA) of Nvidia GPUs. We measure the clock cycles for PTX ISA instructions and their SASS ISA counterparts using microbench-marks. We further calculate the clock cycles needed to access each memory unit. Moreover, we demystify the new version of the tensor core unit found in the Ampere architecture using the WMMA API and measuring its clock cycles per instruction and throughput for the different data types and input shapes. Our results should guide software developers and hardware architects. Furthermore, the clock cycles per instructions are widely used by performance modeling simulators and tools to model and predict the performance of the hardware.|IEEE Conference on High Performance Extreme Computing|2022|10.1109/HPEC55821.2022.9926299|Yehia Arafa, N. Santhi, Abdel-Hameed A. Badawy, H. Abdelkhalik|2.0|6
5|NaviSim: A Highly Accurate GPU Simulator for AMD RDNA GPUs|As GPUs continue to grow in popularity for accelerating demanding applications, such as high-performance computing and machine learning, GPU architects need to deliver more powerful devices with updated instruction set architectures (ISAs) and new microarchitectural features. The introduction of the AMD RDNA architecture is one example where the GPU architecture was dramatically changed, modifying the underlying programming model, the core architecture, and the cache hierarchy. To date, no publicly-available simulator infrastructure can model the AMD RDNA GPU, preventing researchers from exploring new GPU designs based on the state-of-the-art RDNA architecture. In this paper, we present the NaviSim simulator, the first cycle-level GPU simulator framework that models AMD RDNA GPUs. NaviSim faithfully emulates the new RDNA ISA. We extensively tune and validate NaviSim using several microbenchmarks and 10 full workloads. Our evaluation shows that NaviSim can accurately model the GPU's kernel execution time, achieving similar performance to hardware execution within 9.92% (on average), as measured on an AMD RX 5500 XT GPU and an AMD Radeon Pro W6800 GPU. To demonstrate the full utility of the NaviSim simulator, we carry out a performance study of the impact of individual RDNA features, attempting to understand better the design decisions behind these features. We carry out a number of experiments to isolate each RDNA feature and evaluate its impact on overall performance, as well as demonstrate the usability and flexibility of NaviSim.|International Conference on Parallel Architectures and Compilation Techniques|2022|10.1145/3559009.3569666|D. Kaeli, Yifan Sun, Trinayan Baruah, Yuhui Bao, M. Shen, Micah Weston, Ajay Joshi, José L. Abellán, Z. Feric, John Kim|1.3333333333333333|6
6|A Detailed Model for Contemporary GPU Memory Systems|This paper explores the impact of simulator accuracy on architecture design decisions in the general-purpose graphics processing unit (GPGPU) space. We enhance the most popular publicly available GPU simulator, GPGPU-Sim, by performing a rigorous correlation of the simulator with a contemporary GPU. Our enhanced GPU model is able to describe the NVIDIA Volta architecture in sufficient detail to reduce error in memory system counters by as much as 66×. The reduced error in the memory system further reduces execution time error by 2.5×. To demonstrate the accuracy of our enhanced model against a real machine, we perform a counter-by-counter validation against an NVIDIA TITAN V Volta GPU, demonstrating the relative accuracy of the new simulator versus the previous model. We go on to demonstrate that the simpler model discounts the importance of advanced memory system designs such as out-of-order memory access scheduling. Our results demonstrate that it is important for the academic community to enhance the level of detail in architecture simulators as system complexity continues to grow.|IEEE International Symposium on Performance Analysis of Systems and Software|2019|10.1109/ISPASS.2019.00023|Akshay Jain, Mahmoud Khairy, Timothy G. Rogers, Tor M. Aamodt|0.6666666666666666|6
8|Exploring Modern GPU Memory System Design Challenges through Accurate Modeling|This paper explores the impact of simulator accuracy on architecture design decisions in the general-purpose graphics processing unit (GPGPU) space. We perform a detailed, quantitative analysis of the most popular publicly available GPU simulator, GPGPU-Sim, against our enhanced version of the simulator, updated to model the memory system of modern GPUs in more detail. Our enhanced GPU model is able to describe the NVIDIA Volta architecture in sufficient detail to reduce error in memory system even counters by as much as 66X. The reduced error in the memory system further reduces execution time error versus real hardware by 2.5X. To demonstrate the accuracy of our enhanced model against a real machine, we perform a counter-by-counter validation against an NVIDIA TITAN V Volta GPU, demonstrating the relative accuracy of the new simulator versus the publicly available model. We go on to demonstrate that the simpler model discounts the importance of advanced memory system designs such as out-of-order memory access scheduling, while overstating the impact of more heavily researched areas like L1 cache bypassing. Our results demonstrate that it is important for the academic community to enhance the level of detail in architecture simulators as system complexity continues to grow. As part of this detailed correlation and modeling effort, we developed a new Correlator toolset that includes a consolidation of applications from a variety of popular GPGPU benchmark suites, designed to run in reasonable simulation times. The Correlator also includes a database of hardware profiling results for all these applications on NVIDIA cards ranging from Fermi to Volta and a toolchain that enables users to gather correlation statistics and create detailed counter-by-counter hardware correlation plots with minimal effort.|arXiv.org|2018|10.1145/3224430|Mahmoud Khairy, Timothy G. Rogers, Tor M. Aamodt, Akshay Jain|0.5714285714285714|6
3|Path Forward Beyond Simulators: Fast and Accurate GPU Execution Time Prediction for DNN Workloads|Today, DNNs’ high computational complexity and sub-optimal device utilization present a major roadblock to democratizing DNNs. To reduce the execution time and improve device utilization, researchers have been proposing new system design solutions, which require performance models (especially GPU models) to help them with pre-product concept validation. Currently, researchers have been utilizing simulators to predict execution time, which provides high flexibility and acceptable accuracy, but at the cost of a long simulation time. Simulators are becoming increasingly impractical to model today’s large-scale systems and DNNs, urging us to find alternative lightweight solutions. To solve this problem, we propose using a data-driven method for modeling DNNs system performance. We first build a dataset that includes the execution time of numerous networks/layers/kernels. After identifying the relationships of directly known information (e.g., network structure, hardware theoretical computing capabilities), we discuss how to build a simple, yet accurate, performance model for DNNs execution time. Our observations on the dataset demonstrate prevalent linear relationships between the GPU kernel execution times, operation counts, and input/output parameters of DNNs layers. Guided by our observations, we develop a fast, linear-regression-based DNNs execution time predictor. Our evaluation using various image classification models suggests our method can predict new DNNs performance with a 7% error and new GPU performance with a 15.2% error. Our case studies also demonstrate how the performance model can facilitate future DNNs system research.|Micro|2023|10.1145/3613424.3614277|Ying Li, Yifan Sun, Adwait Jog|0.5|6
11|Parallel GPU Architecture Simulation Framework Exploiting Architectural-Level Parallelism with Timing Error Prediction|The performance analysis and study of large-scale many-core processor architectures require fast and highly accurate simulation techniques in order to reduce time consumption. State-of-the-art graphics processing units (GPUs), which are used extensively as coprocessors in the high-performance-computing area, also require fast simulation techniques because they have massively complex microarchitectures with thousands of processing elements. At present, however, GPU simulators do not have sufficient simulation speed for advanced software and architecture studies. In this study, we propose a new parallel simulation framework and a new parallel simulation technique for improving the simulation speed of GPUs. The proposed framework facilitates multithreaded simulation by exploiting the architectural-level parallelism and execution model parallelism of GPUs. In addition, an error predictive synchronization scheme based on a timing error prediction mechanism is used to minimize the cycle errors and simulator slowdown during parallel simulations. The experimental results obtained using a simulator with the proposed framework showed that the proposed technique provided a speedup of up to 8.9 times compared with an existing single-thread-based GPU simulator on a 16-core machine.|IEEE transactions on computers|2016|10.1109/TC.2015.2444848|Sangpil Lee, W. Ro|0.4444444444444444|6
4|Balar: A SST GPU Component for Performance Modeling and Profiling.|Programmable accelerators have become commonplace in modern computing systems. Advances in programming models and the availability of unprecedented amounts of data have created a space for massively parallel accelerators capable of maintaining context for thousands of concurrent threads resident on-chip. These threads are grouped and interleaved on a cycle-by-cycle basis among several massively parallel computing cores. One path for the design of future supercomputers relies on an ability to model the performance of these massively parallel cores at scale. The SST framework has been proven to scale up to run simulations containing tens of thousands of nodes. A previous report described the initial integration of the open-source, execution-driven GPU simulator, GPGPU-Sim, into the SST framework. This report discusses the results of the integration and how to use the new GPU component in SST. It also provides examples of what it can be used to analyze and a correlation study showing how closely the execution matches that of a Nvidia V100 GPU when running kernels and mini-apps.||2019|10.2172/1560919|S. Hammond, Mengchi Zhang, C. Hughes, Mahmoud Khairy, R. Hoekstra, Roland N. Green, Timothy G. Rogers|0.0|6
7|Efficient L2 Cache Management to Boost GPGPU Performance|In recent years, the growing need for computing capacity has become a challenge that has led the industry to look for alternative architectures to conventional out-of-order superscalar processors, with the goal of enabling an increase of computing power while achieving higher energy efficiency. GPU architectures, which just a decade ago were applied to accelerate computer graphics exclusively, have been one of the most employed alternatives for several years to reach the mentioned goal. A particular characteristic of GPUs is their high main memory bandwidth, which allows executing a large number of threads in a very efficient way. This feature, as well as their high computational power regarding floating-point operations, have caused the emergence of the GPGPU computing paradigm, where GPU architectures perform general purpose computations. The aforementioned characteristics make GPU devices very appropriate for the execution of massively parallel applications that have been traditionally executed in conventional high-performance processors. The work performed in this thesis aims to help improve the performance of GPUs in the execution of GPGPU applications. To this end, as a first step, a characterization study is carried out. In this study, the most important features of GPGPU applications, with respect to the memory hierarchy and its impact on performance, are identified. For this purpose, a detailed cycle-accurate simulator is used to model the architecture of a recent GPU. The study reveals that it is necessary to model with more detail some critical components of the GPU memory hierarchy in order to obtain accurate results. In addition, it shows that the achieved benefits can vary up to a factor of 3× depending on how these critical components are modeled. Due to this reason, as a second step before realizing a novel proposal, the work in this thesis focuses on determining which components of the GPU memory hierarchy must be modeled with more detail to increase the accuracy of simulator results and improving the existing simulator models of these components. Moreover, a validation study is performed comparing the results obtained with the improved GPU models against those from a real commercial GPU. The implemented simulator improvements reduce the deviation of the results obtained with the simulator from results obtained with the real GPU by about 96%. Finally, once simulation accuracy is increased, this thesis proposes a novel approach, called FRC (Fetch and Replacement Cache), which highly improves the GPU computational power by enhancing main memory-level parallelism. The proposal increases the number of parallel accesses to main memory by accelerating the management of fetch and replacement actions corresponding to those cache accesses that miss in the cache. The FRC approach is based on a small auxiliary cache structure that efficiently unclogs the memory subsystem, enhancing the GPU performance up to 118% on average compared to the studied baseline. In addition, the FRC approach reduces the energy consumption of the memory hierarchy by a 57%.||2019|10.4995/thesis/10251/125477|Francisco Candel Margaix|0.0|6
10|Analyzing and Improving Hardware Modeling of Accel-Sim|GPU architectures have become popular for executing general-purpose programs. Their many-core architecture supports a large number of threads that run concurrently to hide the latency among dependent instructions. In modern GPU architectures, each SM/core is typically composed of several sub-cores, where each sub-core has its own independent pipeline. Simulators are a key tool for investigating novel concepts in computer architecture. They must be performance-accurate and have a proper model related to the target hardware to explore the different bottlenecks properly. This paper presents a wide analysis of different parts of Accel-sim, a popular GPGPU simulator, and some improvements of its model. First, we focus on the front-end and developed a more realistic model. Then, we analyze the way the result bus works and develop a more realistic one. Next, we describe the current memory pipeline model and propose a model for a more cost-effective design. Finally, we discuss other areas of improvement of the simulator.||2024|10.11606/issn.2316-9141.rh.2020.217365|Mojtaba Abaie Shoushtary, Rodrigo Huerta, Antonio Gonz'alez|0.0|6
20|Understanding GPU Power|Modern graphics processing units (GPUs) have complex architectures that admit exceptional performance and energy efficiency for high-throughput applications. Although GPUs consume large amounts of power, their use for high-throughput applications facilitate state-of-the-art energy efficiency and performance. Consequently, continued development relies on understanding their power consumption. This work is a survey of GPU power modeling and profiling methods with increased detail on noteworthy efforts. As direct measurement of GPU power is necessary for model evaluation and parameter initiation, internal and external power sensors are discussed. Hardware counters, which are low-level tallies of hardware events, share strong correlation to power use and performance. Statistical correlation between power and performance counters has yielded worthwhile GPU power models, yet the complexity inherent to GPU architectures presents new hurdles for power modeling. Developments and challenges of counter-based GPU power modeling are discussed. Often building on the counter-based models, research efforts for GPU power simulation, which make power predictions from input code and hardware knowledge, provide opportunities for optimization in programming or architectural design. Noteworthy strides in power simulations for GPUs are included along with their performance or functional simulator counterparts when appropriate. Last, possible directions for future research are discussed.|ACM Computing Surveys|2016|10.1145/2962131|N. Imam, T. Mintz, R. A. Bridges|9.444444444444445|5
13|GPGPU-MiniBench: Accelerating GPGPU Micro-Architecture Simulation|Graphics processing units (GPU), due to their massive computational power with up to thousands of concurrent threads and general-purpose GPU (GPGPU) programming models such as CUDA and OpenCL, have opened up new opportunities for speeding up general-purpose parallel applications. Unfortunately, pre-silicon architectural simulation of modern-day GPGPU architectures and workloads is extremely time-consuming. This paper addresses the GPGPU simulation challenge by proposing a framework, called GPGPU-MiniBench, for generating miniature, yet representative GPGPU workloads. GPGPU-MiniBench first summarizes the inherent execution behavior of existing GPGPU workloads in a profile. The central component in the profile is the Divergence Flow Statistics Graph (DFSG), which characterizes the dynamic control flow behavior including loops and branches of a GPGPU kernel. GPGPU-MiniBench generates a synthetic miniature GPGPU kernel that exhibits similar execution characteristics as the original workload, yet its execution time is much shorter thereby dramatically speeding up architectural simulation. Our experimental results show that GPGPU-MiniBench can speed up GPGPU architectural simulation by a factor of 49× on average and up to 589×, with an average IPC error of 4.7 percent across a broad set of GPGPU benchmarks from the CUDA SDK, Rodinia and Parboil benchmark suites. We also demonstrate the usefulness of GPGPU-MiniBench for driving GPU architecture exploration.|IEEE transactions on computers|2015|10.1109/TC.2015.2395427|Junmin Wu, L. Eeckhout, Hai Jin, Zhibin Yu, L. John, Chengzhong Xu, Nilanjan Goswami, Tao Li|1.9|5
18|A Quantitative Evaluation of Contemporary GPU Simulation Methodology|Contemporary Graphics Processing Units (GPUs) are used to accelerate highly parallel compute workloads. For the last decade, researchers in academia and industry have used cycle-level GPU architecture simulators to evaluate future designs. This paper performs an in-depth analysis of commonly accepted GPU simulation methodology, examining the effect both the workload and the choice of instruction set architecture have on the accuracy of a widely-used simulation infrastructure, GPGPU-Sim. We analyze numerous aspects of the architecture, validating the simulation results against real hardware. Based on a characterized set of over 1700 GPU kernels, we demonstrate that while the relative accuracy of compute-intensive workloads is high, inaccuracies in modeling the memory system result in much higher error when memory performance is critical. We then perform a case study using a recently proposed GPU architecture modification, demonstrating that the cross-product of workload characteristics and instruction set architecture choice can have an affect on the predicted efficacy of the technique.|Measurement and Modeling of Computer Systems|2018|10.1145/3219617.3219658|Akshay Jain, Mahmoud Khairy, Timothy G. Rogers|1.1428571428571428|5
14|ParaCells: A GPU Architecture for Cell-Centered Models in Computational Biology|In computational biology, the hierarchy of biological systems requires the development of flexible and powerful computational tools. Graphics processing unit (GPU) architecture has been a suitable device for parallel computing in simulating multi-cellular systems. However, in modeling complex biological systems, scientists often face two tasks, mathematical formulation and skillful programming. In particular, specific programming skills are needed for GPU programming. Therefore, the development of an easy-to-use computational architecture, which utilizes GPU for parallel computing and provides intuitive interfaces for simple implementation, is needed so that general scientists can perform GPU simulations without knowing much about the GPU architecture. Here, we introduce ParaCells, a cell-centered GPU simulation architecture for NVIDIA compute unified device architecture (CUDA). ParaCells was designed as a versatile architecture that connects the user logic (in C++) with NVIDIA CUDA runtime and is specific to the modeling of multi-cellular systems. An advantage of ParaCells is its object-oriented model declaration, which allows it to be widely applied to many biological systems through the combination of basic biological concepts. We test ParaCells with two applications. Both applications are significantly faster when compared with sequential as well as parallel OpenMP and OpenACC implementations. Moreover, the simulation programs based on ParaCells are cleaner and more readable than other versions.|IEEE/ACM Transactions on Computational Biology & Bioinformatics|2019|10.1109/TCBB.2018.2814570|You Song, Siyu Yang, J. Lei|0.8333333333333334|5
16|Optimization of Heterogeneous NoC for Fused CPU-GPU Architecture|"Author(s): Alhubail, Lulwah | Advisor(s): Bagherzadeh, Nader | Abstract: Heterogeneous computing architectures that utilize both CPU and GPU have been the trend nowadays. Several products from AMD, Intel, and NVIDIA have emerged that fused CPU and GPU on the same chip. In such architectures, different processing elements (PEs), including many CPU cores, GPU cores, memory controllers (MCs), and caches, are connected through a common interconnection. CPU and GPU exhibit different network behaviors; CPU tends to be latency-sensitive and GPU, with its high thread level parallelism (TLP), tends to be throughput hungry. Using homogeneous interconnect for such heterogeneous processors can result in performance degradation and power increase. This dissertation focused on designing a heterogeneous mesh-style network-on-chip (NoC) to connect heterogeneous CPU-GPU processors while considering their diametric network demands. There are many aspects to consider when designing a 2D mesh NoC. Firstly, the placement of the PEs within the mesh. Secondly, setting the NoC parameters: the size of the router's buffer, the number of virtual channels, and the bandwidth of the links. This dissertation tackled all these problems simultaneously. Moreover, to design a heterogeneous NoC, heterogeneity was explored at the router's port and link level, where each port of each router can have different buffer size and number of virtual channels, and each link can have different bandwidth. This explodes the design space and makes exploring all possible design combinations using simulation very difficult. In this dissertation, heuristic-based optimization methods were proposed to obtain a near-optimal heterogeneous NoC design. Firstly, a method based on Genetic Algorithm (GA) to get a design with optimal performance in terms of the average network latency. An analytical model based on queueing theory that supports virtual channels was proposed to get a performance measure of the design. Secondly, a multi-objective method based on the Strength Pareto Evolutionary Algorithm 2 (SPEA2) to get an optimal design in terms of the performance and the power of NoC. Also, an activity based power model was proposed to get the power of the design. The optimal designs were validated using a full-system simulator."||2019|10.23919/date.2019.8714769|Lulwah Alhubail|0.8333333333333334|5
19|A Model-Based Software Solution for Simultaneous Multiple Kernels on GPUs|As a critical computing resource in multiuser systems such as supercomputers, data centers, and cloud services, a GPU contains multiple compute units (CUs). GPU Multitasking is an intuitive solution to underutilization in GPGPU computing. Recently proposed solutions of multitasking GPUs can be classified into two categories: (1) spatially partitioned sharing (SPS), which coexecutes different kernels on disjointed sets of compute units (CU), and (2) simultaneous multikernel (SMK), which runs multiple kernels simultaneously within a CU. Compared to SPS, SMK can improve resource utilization even further due to the interleaving of instructions from kernels with low dynamic resource contentions. However, it is hard to implement SMK on current GPU architecture, because (1) techniques for applying SMK on top of GPU hardware scheduling policy are scarce and (2) finding an efficient SMK scheme is difficult due to the complex interferences of concurrently executed kernels. In this article, we propose a lightweight and effective performance model to evaluate the complex interferences of SMK. Based on the probability of independent events, our performance model is built from a totally new angle and contains limited parameters. Then, we propose a metric, symbiotic factor, which can evaluate an SMK scheme so that kernels with complementary resource utilization can corun within a CU. Also, we analyze the advantages and disadvantages of kernel slicing and kernel stretching techniques and integrate them to apply SMK on GPUs instead of simulators. We validate our model on 18 benchmarks. Compared to the optimized hardware-based concurrent kernel execution whose kernel launching order brings fast execution time, the results of corunning kernel pairs show 11%, 18%, and 12% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average. Compared to the Warped-Slicer, the results show 29%, 18%, and 51% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average.|ACM Transactions on Architecture and Code Optimization (TACO)|2020|10.1145/3377138|Weizhi Liu, Hao Wu, Huanxin Lin, Cho-Li Wang|0.6|5
17|A Quantitative Evaluation of Contemporary GPU Simulation Methodology|Contemporary Graphics Processing Units (GPUs) are used to accelerate highly parallel compute workloads. For the last decade, researchers in academia and industry have used cycle-level GPU architecture simulators to evaluate future designs. This paper performs an in-depth analysis of commonly accepted GPU simulation methodology, examining the effect both the workload and the choice of instruction set architecture have on the accuracy of a widely-used simulation infrastructure, GPGPU-Sim. We analyze numerous aspects of the architecture, validating the simulation results against real hardware. Based on a characterized set of over 1700 GPU kernels, we demonstrate that while the relative accuracy of compute-intensive workloads is high, inaccuracies in modeling the memory system result in much higher error when memory performance is critical. We then perform a case study using a recently proposed GPU architecture modification, Cache-Conscious Wavefront Scheduling. The case study demonstrates that the cross-product of workload characteristics and instruction set architecture choice can affect the predicted efficacy of the technique.|Proceedings of the ACM on Measurement and Analysis of Computing Systems|2018|10.1145/3224430|Akshay Jain, Mahmoud Khairy, Timothy G. Rogers|0.5714285714285714|5
15|Modeling the Energy Efficiency of GEMM using Optical Random Access Memory|General matrix-matrix multiplication (GEMM) is the key computation kernel in many applications. GEMM has been supported on various hardware platforms, including CPU, GPU, FPGA. To optimize the performance of GEMM, developers use on-chip electrical static random access memory (E-SRAM) to exploit the data locality of GEMM. However, intensively accessing E-SRAM for GEMM can lead to significant energy consumption, which is not energy-efficient for commercial data centers. In this paper, we evaluate the optical static random access memory (O-SRAM) for GEMM. O-SRAM is a promising tech-nology that has extremely low access latency and low energy consumption compared with the traditional E-SRAM. First, we propose an O-SRAM based wafer-scale system for GEMM and a baseline E-SRAM based system. Second, we build the theoretical performance models of the two systems to analyze their energy consumption of on-chip memory accesses. Then, we conduct simulation-based experiments to evaluate the energy consumption of the two system. The evaluation results show that O-SRAM based system is 7 x more energy efficient than the baseline E-SRAM based system.|IEEE Conference on High Performance Extreme Computing|2022|10.1109/HPEC55821.2022.9926291|R. T. Lakkireddy, Bingyi Zhang, Ajey P. Jacob, Akhilesh R. Jaiswal, V. Prasanna, Clynn Mathew, Sasindu Wijeratne|0.3333333333333333|5
21|A Survey of Power Consumption Modeling for GPU Architecture|GPUs are of increasing interests in the multi-core era due to their high computing power. However, the power consumption caused by the rising performance of GPUs has been a general concern. As a consequence, it is becoming an imperative demand to optimize the GPU power consumption, among which the power consumption estimation is one of the important and useful solutions. In this work, we give a survey of the power modeling for GPU. We first introduce the current development of heterogeneous architectures and then summarize the existing modeling techniques for GPU power consumption. The main two types of power modeling could be classified as simulator-based methods and real machine-based methods.||2016|10.23977/CPCS.2016.11006|Ning Li, Li Shen, Qiong Wang, Zhiying Wang|0.0|5
24|gem5-gpu: A Heterogeneous CPU-GPU Simulator|gem5-gpu is a new simulator that models tightly integrated CPU-GPU systems. It builds on gem5, a modular full-system CPU simulator, and GPGPUSim, a detailed GPGPU simulator. gem5-gpu routes most memory accesses through Ruby, which is a highly configurable memory system in gem5. By doing this, it is able to simulate many system configurations, ranging from a system with coherent caches and a single virtual address space across the CPU and GPU to a system that maintains separate GPU and CPU physical address spaces. gem5gpu can run most unmodified CUDA 3.2 source code. Applications can launch non-blocking kernels, allowing the CPU and GPU to execute simultaneously. We present gem5-gpu's software architecture and a brief performance validation. We also discuss possible extensions to the simulator. gem5-gpu is open source and available at gem5-gpu.cs.wisc.edu.|IEEE computer architecture letters|2015|10.1109/LCA.2014.2299539|Marc S. Orr, M. Hill, Joel Hestness, Jason Power, D. Wood|15.2|4
114|Automatic Selection of Sparse Matrix Representation on GPUs|Sparse matrix-vector multiplication (SpMV) is a core kernel in numerous applications, ranging from physics simulation and large-scale solvers to data analytics. Many GPU implementations of SpMV have been proposed, targeting several sparse representations and aiming at maximizing overall performance. No single sparse matrix representation is uniformly superior, and the best performing representation varies for sparse matrices with different sparsity patterns. In this paper, we study the inter-relation between GPU architecture, sparse matrix representation and the sparse dataset. We perform extensive characterization of pertinent sparsity features of around 700 sparse matrices, and their SpMV performance with a number of sparse representations implemented in the NVIDIA CUSP and cuSPARSE libraries. We then build a decision model using machine learning to automatically select the best representation to use for a given sparse matrix on a given target platform, based on the sparse matrix features. Experimental results on three GPUs demonstrate that the approach is very effective in selecting the best representation.|International Conference on Supercomputing|2015|10.1145/2751205.2751244|S. Parthasarathy, P. Sadayappan, L. Pouchet, N. Sedaghati, Te Mu|9.7|4
69|Neural acceleration for GPU throughput processors|Graphics Processing Units (GPUs) can accelerate diverse classes of applications, such as recognition, gaming, data analytics, weather prediction, and multimedia. Many of these applications are amenable to approximate execution. This application characteristic provides an opportunity to improve GPU performance and efficiency. Among approximation techniques, neural accelerators have been shown to provide significant performance and efficiency gains when augmenting CPU processors. However, the integration of neural accelerators within a GPU processor has remained unexplored. GPUs are, in a sense, many-core accelerators that exploit large degrees of data-level parallelism in the applications through the SIMT execution model. This paper aims to harmoniously bring neural and GPU accelerators together without hindering SIMT execution or adding excessive hardware overhead. We introduce a low overhead neurally accelerated architecture for GPUs, called NGPU, that enables scalable integration of neural accelerators for large number of GPU cores. This work also devises a mechanism that controls the tradeoff between the quality of results and the benefits from neural acceleration. Compared to the baseline GPU architecture, cycle-accurate simulation results for NGPU show a 2.4× average speedup and a 2.8× average energy reduction within 10% quality loss margin across a diverse set of benchmarks. The proposed quality control mechanism retains a 1.9 ×s average speedup and a 2.1 × energy reduction while reducing the degradation in the quality of results to 2.5%. These benefits are achieved by less than 1% area overhead.|Micro|2015|10.1145/2830772.2830810|Hardik Sharma, H. Esmaeilzadeh, Jongse Park, P. Lotfi-Kamran, A. Yazdanbakhsh|7.6|4
63|Anatomy of GPU Memory System for Multi-Application Execution|As GPUs make headway in the computing landscape spanning mobile platforms, supercomputers, cloud and virtual desktop platforms, supporting concurrent execution of multiple applications in GPUs becomes essential for unlocking their full potential. However, unlike CPUs, multi-application execution in GPUs is little explored. In this paper, we study the memory system of GPUs in a concurrently executing multi-application environment. We first present an analytical performance model for many-threaded architectures and show that the common use of misses-per-kilo-instruction (MPKI) as a proxy for performance is not accurate without considering the bandwidth usage of applications. We characterize the memory interference of applications and discuss the limitations of existing memory schedulers in mitigating this interference. We extend the analytical model to multiple applications and identify the key metrics to control various performance metrics. We conduct extensive simulations using an enhanced version of GPGPU-Sim targeted for concurrently executing multiple applications, and show that memory scheduling decisions based on MPKI and bandwidth information are more effective in enhancing throughput compared to the traditional FR-FCFS and the recently proposed RR FR-FCFS policies.|International Symposium on Memory Systems|2015|10.1145/2818950.2818979|Tuba Kesten, Niladrish Chatterjee, C. Das, Evgeny Bolotin, M. Kandemir, Ashutosh Pattnaik, Onur Kayiran, Adwait Jog, S. Keckler|7.2|4
96|A large-scale study of soft-errors on GPUs in the field|Parallelism provided by the GPU architecture has enabled domain scientists to simulate physical phenomena at a much faster rate and finer granularity than what was previously possible by CPU-based large-scale clusters. Architecture researchers have been investigating reliability characteristics of GPUs and innovating techniques to increase the reliability of these emerging computing devices. Such efforts are often guided by technology projections and simplistic scientific kernels, and performed using architectural simulators and modeling tools. Lack of large-scale field data impedes the effectiveness of such efforts. This study attempts to bridge this gap by presenting a large-scale field data analysis of GPU reliability. We characterize and quantify different kinds of soft-errors on the Titan supercomputer's GPU nodes. Our study uncovers several interesting and previously unknown insights about the characteristics and impact of soft-errors.|International Symposium on High-Performance Computer Architecture|2016|10.1109/HPCA.2016.7446091|James H. Rogers, Saurabh Gupta, E. Smirni, Bin Nie, Devesh Tiwari|6.333333333333333|4
128|GPGPU Performance Estimation With Core and Memory Frequency Scaling|Contemporary graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, accurate and straightforward performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is essential to determine the best frequency configuration for energy saving. In this article, we reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Compared to the cycle-level simulators, which are too slow to apply on real hardware, our model only needs simple and one-off micro-benchmarks to extract a set of hardware parameters and kernel performance counters without any source code analysis. Our experimental results show that the proposed performance model can capture the kernel performance scaling behaviors under different frequency settings and achieve decent accuracy (average errors of 3.85, 8.6, 8.82, and 8.83 percent on a set of 20 GPU kernels with four modern Nvidia GPUs).|IEEE Transactions on Parallel and Distributed Systems|2020|10.1109/TPDS.2020.3004623|X. Chu, Qiang Wang|6.2|4
137|GPU-Accelerated Sparse LU Factorization for Circuit Simulation with Performance Modeling|The sparse matrix solver by LU factorization is a serious bottleneck in Simulation Program with Integrated Circuit Emphasis (SPICE)-based circuit simulators. The state-of-the-art Graphics Processing Units (GPU) have numerous cores sharing the same memory, provide attractive memory bandwidth and compute capability, and support massive thread-level parallelism, so GPUs can potentially accelerate the sparse solver in circuit simulators. In this paper, an efficient GPU-based sparse solver for circuit problems is proposed. We develop a hybrid parallel LU factorization approach combining task-level and data-level parallelism on GPUs. Work partitioning, number of active thread groups, and memory access patterns are optimized based on the GPU architecture. Experiments show that the proposed LU factorization approach on NVIDIA GTX580 attains an average speedup of 7.02× (geometric mean) compared with sequential PARDISO, and 1.55× compared with 16-threaded PARDISO. We also investigate bottlenecks of the proposed approach by a parametric performance model. The performance of the sparse LU factorization on GPUs is constrained by the global memory bandwidth, so the performance can be further improved by future GPUs with larger memory bandwidth.|IEEE Transactions on Parallel and Distributed Systems|2015|10.1109/TPDS.2014.2312199|Xiaoming Chen, Ling Ren, Huazhong Yang, Yu Wang|5.4|4
25|Performance analysis of CUDA, OpenACC and OpenMP programming models on TESLA V100 GPU|Graphics processors are widely utilized in modern supercomputers as accelerators. Ability to perform efficient parallelization and low-level allow scientists to greatly boost performance of their codes. Modern Nvidia GPUs feature low-level approaches, such as CUDA, along with high-level approaches: OpenACC and OpenMP. While the low-level approach aims to explore all possible abilities of SIMT GPU architecture by writing low-level C/C++ code, it takes significant effort from programmer. OpenACC and OpenMP programming models are opposite to CUDA. Using these models the programmer only have to identify the blocks of code to be parallelized using pragmas. We compare the performance of CUDA, OpenMP and OpenACC on state-of-the-art Nvidia Tesla V100 GPU in various typical scenarios that arise in scientific programming, such as matrix multiplication, regular memory access patterns and evaluate performance of physical simulation codes implemented using these programming models. Moreover, we study the performance matrix multiplication implemented in vendor-optimized BLAS libraries for Nvidia Tesla V100 GPU and modern Intel Xeon processor.||2021|10.1088/1742-6596/1740/1/012056|Alexey Timoveev, M. Khalilov|5.25|4
71|A GPU-accelerated Monte Carlo dose calculation platform and its application toward validating an MRI-guided radiation therapy beam model.|PURPOSE\nThe clinical commissioning of IMRT subject to a magnetic field is challenging. The purpose of this work is to develop a GPU-accelerated Monte Carlo dose calculation platform based on penelope and then use the platform to validate a vendor-provided MRIdian head model toward quality assurance of clinical IMRT treatment plans subject to a 0.35 T magnetic field.\n\n\nMETHODS\npenelope was first translated from fortran to c++ and the result was confirmed to produce equivalent results to the original code. The c++ code was then adapted to cuda in a workflow optimized for GPU architecture. The original code was expanded to include voxelized transport with Woodcock tracking, faster electron/positron propagation in a magnetic field, and several features that make gpenelope highly user-friendly. Moreover, the vendor-provided MRIdian head model was incorporated into the code in an effort to apply gpenelope as both an accurate and rapid dose validation system. A set of experimental measurements were performed on the MRIdian system to examine the accuracy of both the head model and gpenelope. Ultimately, gpenelope was applied toward independent validation of patient doses calculated by MRIdian's kmc.\n\n\nRESULTS\nAn acceleration factor of 152 was achieved in comparison to the original single-thread fortran implementation with the original accuracy being preserved. For 16 treatment plans including stomach (4), lung (2), liver (3), adrenal gland (2), pancreas (2), spleen(1), mediastinum (1), and breast (1), the MRIdian dose calculation engine agrees with gpenelope with a mean gamma passing rate of 99.1% ± 0.6% (2%/2 mm).\n\n\nCONCLUSIONS\nA Monte Carlo simulation platform was developed based on a GPU- accelerated version of penelope. This platform was used to validate that both the vendor-provided head model and fast Monte Carlo engine used by the MRIdian system are accurate in modeling radiation transport in a patient using 2%/2 mm gamma criteria. Future applications of this platform will include dose validation and accumulation, IMRT optimization, and dosimetry system modeling for next generation MR-IGRT systems.|Medical Physics (Lancaster)|2016|10.1118/1.4953198|T. Zhao, Hua Li, T. Mazur, O. Green, Deshan Yang, V. Rodriguez, Yuhe Wang, Yanle Hu, H. Wooten, H. Li, S. Mutic|5.0|4
78|Parallelized combined finite‐discrete element (FDEM) procedure using multi‐GPU with CUDA|This paper focuses on the efficiency of finite discrete element method (FDEM) algorithmic procedures in massive computers and analyzes the time‐consuming part of contact detection and interaction computations in the numerical solution. A detailed operable GPU parallel procedure was designed for the element node force calculation, contact detection, and contact interaction with thread allocation and data access based on the CUDA computing. The emphasis is on the parallel optimization of time‐consuming contact detection based on load balance and GPU architecture. A CUDA FDEM parallel program was developed with the overall speedup ratio over 53 times after the fracture from the efficiency and fidelity performance test of models of in situ stress, UCS, and BD simulations in Intel i7‐7700K CPU and the NVIDIA TITAN Z GPU. The CUDA FDEM parallel computing improves the computational efficiency significantly compared with the CPU‐based ones with the same reliability, providing conditions for achieving larger‐scale simulations of fracture.|International journal for numerical and analytical methods in geomechanics (Print)|2019|10.1002/nag.3011|Weiqin Wang, Quansheng Liu, Hao Ma|4.666666666666667|4
27|Need for Speed: Experiences Building a Trustworthy System-Level GPU Simulator|The demands of high-performance computing (HPC) and machine learning (ML) workloads have resulted in the rapid architectural evolution of GPUs over the last decade. The growing memory footprint and diversity of data types in these workloads has required GPUs to embrace micro-architectural heterogeneity and increased memory system sophistication to scale performance. Effective simulation of new architectural features early in the design cycle enables quick and effective exploration of design trade-offs across this increasingly diverse set of workloads. This work provides a retrospective on the design and development of NVArchSim (NVAS), an architectural simulator used within NVIDIA to design and evaluate features that are difficult to appraise using other methodologies due to workload type, size, complexity, or lack of modeling flexibility. We argue that overly precise and/or overly slow architectural models hamper an architect’s ability to evaluate new features within a reasonable time frame, hurting productivity. Because of its speed, NVAS is being used to trace and evaluate hundreds of HPC and state-of-the-art ML workloads on single-GPU or multi-GPU systems. By adding component fidelity only when necessary to improve system-level modeling accuracy, NVAS delivers simulation speed orders of magnitude higher than most publicly available GPU simulators while retaining high levels of accuracy and simulation flexibility. Building trustworthy high-level simulation platforms is a difficult exercise in balance and compromise; we share our experiences to help and encourage those in academia who take on the challenge of building GPU simulation platforms.|International Symposium on High-Performance Computer Architecture|2021|10.1109/HPCA51647.2021.00077|Niladrish Chatterjee, Evgeny Bolotin, D. Nellans, Zi Yan, Yaosheng Fu, Nan Jiang, Daniel Lustig, Oreste Villa|4.5|4
59|Bridging Data Center AI Systems with Edge Computing for Actionable Information Retrieval|Extremely high data rates at modern synchrotron and X-ray free-electron laser light source beamlines motivate the use of machine learning methods for data reduction, feature detection, and other purposes. Regardless of the application, the basic concept is the same: data collected in early stages of an experiment, data from past similar experiments, and/or data simulated for the upcoming experiment are used to train machine learning models that, in effect, learn specific characteristics of those data; these models are then used to process subsequent data more efficiently than would general-purpose models that lack knowledge of the specific dataset or data class. Thus, a key challenge is to be able to train models with sufficient rapidity that they can be deployed and used within useful timescales. We describe here how specialized data center AI (DCAI) systems can be used for this purpose through a geographically distributed workflow. Experiments show that although there are data movement cost and service overhead to use remote DCAI systems for DNN training, the turnaround time is still less than 1/30 of using a locally deploy-able GPU.|Annual Workshop on Large-scale Experiment-in-the-Loop Computing|2021|10.1109/xloop54565.2021.00008|Dennis Trujillo, Ian T. Foster, Hemant Sharma, Chun Hong Yoon, Zhengchun Liu, Jana Thayer, P. Kenesei, N. Schwarz, Ahsan Ali, H. Yoo, R. Coffee, R. Herbst, A. Miceli|4.25|4
149|Fast , Interactive Origami Simulation using GPU Computation|We present an explicit method for simulating origami that can be rapidly computed on a Graphics Processing Unit (GPU). Previous work on origami simulation methods model the geometric or structural behaviors of origami with a focus on physical realism; in this paper we introduce a compliant, explicit numerical simulation method that emphasizes computational speed and interactivity. We do this by reformulating existing techniques for simulating origami so that they can be computed on highly parallel GPU architectures. We implement this method in an open-source, GPU-accelerated WebGL app that runs in any modern browser. We evaluate our method’s performance, stability, and scalability to existing methods (Freeform Origami, MERLIN) and demonstrate its capacity for real-time interaction through a traditional GUI and immersive virtual reality.||2018|10.1109/tro.2018.2862882|N. Gershenfeld, E. Demaine, Amanda Ghassaei|4.142857142857143|4
67|A Survey of Performance Modeling and Simulation Techniques for Accelerator-Based Computing|The high performance computing landscape is shifting from collections of homogeneous nodes towards heterogeneous systems, in which nodes consist of a combination of traditional out-of-order execution cores and accelerator devices. Accelerators, built around GPUs, many-core chips, FPGAs or DSPs, are used to offload compute-intensive tasks. The advent of this type of systems has brought about a wide and diverse ecosystem of development platforms, optimization tools and performance analysis frameworks. This is a review of the state-of-the-art in performance tools for heterogeneous computing, focusing on the most popular families of accelerators: GPUs and Intel's Xeon Phi. We describe current heterogeneous systems and the development frameworks and tools that can be used for developing for them. The core of this survey is a review of the performance models and tools, including simulators, proposed in the literature for these platforms.|IEEE Transactions on Parallel and Distributed Systems|2015|10.1109/TPDS.2014.2308216|J. Miguel-Alonso, A. Mendiburu, Unai López-Novoa|4.1|4
141|A Survey of GPU Multitasking Methods Supported by Hardware Architecture|The ability to support multitasking becomes more and more important in the development of graphic processing unit (GPU). GPU multitasking methods are classified into three types: temporal multitasking, spatial multitasking, and simultaneous multitasking (SMK). This article first introduces the features of some commercial GPU architectures to support multitasking and the common metrics used for evaluating the performance of GPU multitasking methods, and then reviews the GPU multitasking methods supported by hardware architecture (i.e., hardware GPU multitasking methods). The main problems of each type of hardware GPU multitasking methods to be solved are illustrated. Meanwhile, the key idea of each previous hardware GPU multitasking method is introduced. In addition, the characteristics of hardware GPU multitasking methods belonging to the same type are compared. This article also gives some valuable suggestions for the future research. An enhanced GPU simulator is needed to bridge the gap between academia and industry. In addition, it is promising to expand the research space with machine learning technologies, advanced GPU architectural innovations, 3D stacked memory, etc. Because most previous GPU multitasking methods are based on NVIDIA GPUs, this article focuses on NVIDIA GPU architecture, and uses NVIDIA's terminology. To our knowledge, this article is the first survey about hardware GPU multitasking methods. We believe that our survey can help the readers gain insights into the research field of hardware GPU multitasking methods.|IEEE Transactions on Parallel and Distributed Systems|2022|10.1109/TPDS.2021.3115630|F. Nie, W. Gao, Huiyang Zhou, Chen Zhao|4.0|4
144|MGSim + MGMark: A Framework for Multi-GPU System Research|The rapidly growing popularity and scale of data-parallel workloads demand a corresponding increase in raw computational power of GPUs (Graphics Processing Units). As single-GPU systems struggle to satisfy the performance demands, multi-GPU systems have begun to dominate the high-performance computing world. The advent of such systems raises a number of design challenges, including the GPU microarchitecture, multi-GPU interconnect fabrics, runtime libraries and associated programming models. The research community currently lacks a publically available and comprehensive multi-GPU simulation framework and benchmark suite to evaluate multi-GPU system design solutions. \nIn this work, we present MGSim, a cycle-accurate, extensively validated, multi-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set architecture. We complement MGSim with MGMark, a suite of multi-GPU workloads that explores multi-GPU collaborative execution patterns. Our simulator is scalable and comes with in-built support for multi-threaded execution to enable fast and efficient simulations. In terms of performance accuracy, MGSim differs $5.5\%$ on average when compared against actual GPU hardware. We also achieve a $3.5\times$ and a $2.5\times$ average speedup in function emulation and architectural simulation with 4 CPU cores, while delivering the same accuracy as the serial simulation. \nWe illustrate the novel simulation capabilities provided by our simulator through a case study exploring programming models based on a unified multi-GPU system~(U-MGPU) and a discrete multi-GPU system~(D-MGPU) that both utilize unified memory space and cross-GPU memory access. We evaluate the design implications from our case study, suggesting that D-MGPU is an attractive programming model for future multi-GPU systems.|arXiv.org|2018|10.1109/iiswc.2018.8573521|R. Ubal, Xiang Gong, Saiful A. Mojumder, D. Kaeli, José L. Abellán, John Kim, Shane Treadway, Shi Dong, Yifan Sun, Trinayan Baruah, A. Joshi, Yuhui Bao, Vincent Zhao|3.857142857142857|4
108|CUDAAdvisor: LLVM-based runtime profiling for modern GPUs|General-purpose GPUs have been widely utilized to accelerate parallel applications. Given a relatively complex programming model and fast architecture evolution, producing efficient GPU code is nontrivial. A variety of simulation and profiling tools have been developed to aid GPU application optimization and architecture design. However, existing tools are either limited by insufficient insights or lacking in support across different GPU architectures, runtime and driver versions. This paper presents CUDAAdvisor, a profiling framework to guide code optimization in modern NVIDIA GPUs. CUDAAdvisor performs various fine-grained analyses based on the profiling results from GPU kernels, such as memory-level analysis (e.g., reuse distance and memory divergence), control flow analysis (e.g., branch divergence) and code-/data-centric debugging. Unlike prior tools, CUDAAdvisor supports GPU profiling across different CUDA versions and architectures, including CUDA 8.0 and Pascal architecture. We demonstrate several case studies that derive significant insights to guide GPU code optimization for performance improvement.|IEEE/ACM International Symposium on Code Generation and Optimization|2018|10.1145/3168831|Ang Li, Du Shen, Xu Liu, S. Song|3.7142857142857144|4
38|DIRECT: Distributed Cross-Domain Resource Orchestration in Cellular Edge Computing|Network slicing and edge computing are key technologies to enable compute-intensive applications for vertical industries in 5G. We define cellular networks with edge computing capabilities as cellular edge computing. In this paper, we study the cross-domain resource orchestration solution for dynamic network slicing in cellular edge computing. The fundamental research challenge is from the difficulty in modeling the relationship between the slice performance and resources from multiple technical domains across the network with many base stations and distributed edge servers. To address this challenge, we develop a distributed cross-domain resource orchestration (DIRECT) protocol which optimizes the cross-domain resource orchestration while providing the performance and functional isolations among network slices. The main component of DIRECT is a distributed cross-domain resource orchestration algorithm which is designed by integrating the ADMM method and a new learning-assisted optimization approach. The proposed resource orchestration algorithm efficiently orchestrates multi-domain resources without requiring the performance model of the network slices. We develop and implement the DIRECT protocol in a small-scale prototype of cellular edge computing which is designed based on OpenAirInterface LTE and CUDA GPU computing platforms. The performance of DIRECT is validated through both experiments and network simulations.|ACM Interational Symposium on Mobile Ad Hoc Networking and Computing|2019|10.1145/3323679.3326516|T. Han, Qiang Liu|3.3333333333333335|4
87|FPGA-Based Scalable and Power-Efficient Fluid Simulation using Floating-Point DSP Blocks|High-performance and low-power computation is required for large-scale fluid dynamics simulation. Due to the inefficient architecture and structure of CPUs and GPUs, they now have a difficulty in improving power efficiency for the target application. Although FPGAs become promising alternatives for power-efficient and high-performance computation due to their new architecture having floating-point (FP) DSP blocks, their relatively narrow memory bandwidth requires an appropriate way to fully exploit the advantage. This paper presents an architecture and design for scalable fluid simulation based on data-flow computing with a state-of-the-art FPGA. To exploit available hardware resources including FP DSPs, we introduce spatial and temporal parallelism to further scale the performance by adding more stream processing elements (SPEs) in an array. Performance modeling and prototype implementation allow us to explore the design space for both the existing Altera Arria10 and the upcoming Intel Stratix10 FPGAs. We demonstrate that Arria10 10AX115 FPGA achieves 519 GFlops at 9.67 GFlops/W only with a stream bandwidth of 9.0 GB/s, which is 97.9 percent of the peak performance of 18 implemented SPEs. We also estimate that Stratix10 FPGA can scale up to 6844 GFlops by combining spatial and temporal parallelism adequately.|IEEE Transactions on Parallel and Distributed Systems|2017|10.1109/TPDS.2017.2691770|S. Yamamoto, K. Sano|3.25|4
53|Energy Efficient Architecture for Graph Analytics Accelerators|Specialized hardware accelerators can significantly improve the performance and power efficiency of compute systems. In this paper, we focus on hardware accelerators for graph analytics applications and propose a configurable architecture template that is specifically optimized for iterative vertex-centric graph applications with irregular access patterns and asymmetric convergence. The proposed architecture addresses the limitations of the existing multi-core CPU and GPU architectures for these types of applications. The SystemC-based template we provide can be customized easily for different vertex-centric applications by inserting application-level data structures and functions. After that, a cycle-accurate simulator and RTL can be generated to model the target hardware accelerators. In our experiments, we study several graph-parallel applications, and show that the hardware accelerators generated by our template can outperform a 24 core high end server CPU system by up to 3x in terms of performance. We also estimate the area requirement and power consumption of these hardware accelerators through physical-aware logic synthesis, and show up to 65x better power consumption with significantly smaller area.|International Symposium on Computer Architecture|2016|10.1145/3007787.3001155|John Greth, S. Burns, Serif Yesil, Özcan Özturk, A. Ayupov, Taemin Kim, Muhammet Mustafa Ozdal|3.111111111111111|4
152|Hybrid, Scalable, Trace-Driven Performance Modeling of GPGPUs|In this paper, we present PPT-GPU, a scalable performance prediction toolkit for GPUs. PPT-GPU achieves scalability through a hybrid high-level modeling approach where some computations are extrapolated and multiple parts of the model are parallelized. The tool primary prediction models use pre-collected memory and instructions traces of the workloads to accurately capture the dynamic behavior of the kernels. PPT-GPU reports an extensive array of GPU performance metrics accurately while being easily extensible. We use a broad set of benchmarks to verify predictions accuracy. We compare the results against hardware metrics collected using vendor profiling tools and cycle-accurate simulators. The results show that the performance predictions are highly correlated to the actual hardware (MAPE: < 16% and Correlation: > 0.98). Moreover, PPT-GPU is orders of magnitude faster than cycle-accurate simulators. This comprehensiveness of the collected metrics can guide architects and developers to perform design space explorations. Moreover, the scalability of the tool enables conducting efficient and fast sensitivity analyses for performance-critical applications.|International Conference for High Performance Computing, Networking, Storage and Analysis|2021|10.1145/3458817.3476221|Yehia Arafa, Atanu Barai, Ali Eker, S. Eidenbenz, Gopinath Chennupati, N. Santhi, Abdel-Hameed A. Badawy, Ammar Elwazir|3.0|4
90|Preprint High-Throughput Logic Timing Simulation on GPGPUs|This article may be used for research, teaching and private study purposes. Any substantial or systematic reproduction, redistribution , reselling , loan or sub-licensing, systematic supply or distribution in any form to anyone is expressly forbidden. granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Many EDA tasks like test set characterization or the precise estimation of power consumption, power droop and temperature development, require a very large number of time-aware gate-level logic simulations. Until now, such characterizations have been feasible only for rather small designs or with reduced precision due to the high computational demands. The new simulation system presented here is able to accelerate such tasks by more than two orders of magnitude and provides for the first time fast and comprehensive timing simulations for industrial-sized designs. Hazards, pulse-filtering, and pin-to-pin delay are supported for the first time in a GPGPU accelerated simulator, and the system can easily be extended to even more realistic delay models and further applications. A sophisticated mapping with efficient memory utilization and access patterns as well as minimal synchro-nizations and control flow divergence is able to use the full potential of GPGPU architectures. To provide such a mapping, we combine for the first time the versatility of event-based timing simulation and multi-dimensional parallelism used in GPU-based gate-level simulators. The result is a throughput-optimized timing simulation algorithm, which runs many simulation instances in parallel and at the same time fully exploits gate-parallelism within the circuit. A preliminary version of this work has been published at the Asian Test Symposium (ATS) 2012. The manuscript at hand has a more general focus and contains significantly more material as detailed in the supporting documents. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this …||2015|10.1145/2714564|M. Imhof, S. Holst, H. Wunderlich|2.7|4
95|GPU advancements reduce simulation times for 25 GHz automotive radar models|The high frequencies utilized by automotive radar sensors, coupled with the electrically large fascia in front of the sensors, have posed a challenge for simulation software in the form of long simulation times. Advances in NVIDIA graphics processing units (GPUs) have alleviated this problem, increasing the productivity of RF engineers who need high fidelity simulations of a sensor behind fascia. Using a 25 GHz short-range radar (SRR) model, this paper compares multiple GPU architectures released in 2007 through 2013 to look at the downward trend in simulation times. A larger than 65% decrease in simulation times is observed, which allows a fully detailed sensor simulation to run in less than 4 hours on modern GPUs.|European Conference on Antennas and Propagation|2015|10.1140/epjc/s10052-015-3533-3|M. Barney|2.7|4
112|High-Throughput Logic Timing Simulation on GPGPUs|Many EDA tasks such as test set characterization or the precise estimation of power consumption, power droop and temperature development, require a very large number of time-aware gate-level logic simulations. Until now, such characterizations have been feasible only for rather small designs or with reduced precision due to the high computational demands.\n The new simulation system presented here is able to accelerate such tasks by more than two orders of magnitude and provides for the first time fast and comprehensive timing simulations for industrial-sized designs. Hazards, pulse-filtering, and pin-to-pin delay are supported for the first time in a GPGPU accelerated simulator, and the system can easily be extended to even more realistic delay models and further applications.\n A sophisticated mapping with efficient memory utilization and access patterns as well as minimal synchronizations and control flow divergence is able to use the full potential of GPGPU architectures. To provide such a mapping, we combine for the first time the versatility of event-based timing simulation and multi-dimensional parallelism used in GPU-based gate-level simulators. The result is a throughput-optimized timing simulation algorithm, which runs many simulation instances in parallel and at the same time fully exploits gate-parallelism within the circuit.|TODE|2015|10.1145/2714564|M. Imhof, S. Holst, H. Wunderlich|2.7|4
36|Porting the COSMO Weather Model to Manycore CPUs|Weather and climate simulations are a major application driver in high-performance computing (HPC). With the end of Dennard scaling and Moore's law, the HPC industry increasingly employs specialized computation accelerators to increase computational throughput. Manycore architectures, such as Intel's Knights Landing (KNL), are a representative example of future processing devices. However, software has to be modified to use these devices efficiently. In this work, we demonstrate how an existing domain-specific language that has been designed for CPUs and GPUs can be extended to Manycore architectures such as KNL. We achieve comparable performance to the NVIDIA Tesla P100 GPU architecture on hand-tuned representative stencils of the dynamical core of the COSMO weather model and its radiation code. Further, we present performance within a factor of two of the P100 of the full DSL-based GPU-optimized COSMO dycore code. We find that optimizing code to full performance on modern manycore architectures requires similar effort and hardware knowledge as for GPUs. Further, we show limitations of the present approaches, and outline our lessons learned and possible principles for design of future DSLs for accelerators in the weather and climate domain.|Platform for Advanced Scientific Computing Conference|2019|10.1145/3324989.3325723|L. Mosimann, H. Vogt, T. Hoefler, C. Osuna, Felix Thaler, O. Fuhrer, A. Afanasyev, Stefan Moosbrugger, T. Schulthess, Mauro Bianco|2.6666666666666665|4
37|gpuFI-4: A Microarchitecture-Level Framework for Assessing the Cross-Layer Resilience of Nvidia GPUs|Pre-silicon reliability evaluation of processors is usually performed at the microarchitecture or at the software level. Recent studies on CPUs have, however, shown that software level approaches can mislead the soft error vulnerability assessment process and drive designers towards wrong error protection decisions. To avoid such pitfalls in the GPUs domain, the availability of microarchitecture level reliability assessment tools is of paramount importance. Although there are several publicly available frameworks for the reliability assessment of GPUs, they only operate at the software level, and do not consider the microarchitecture. This paper aims at accurate microarchitecture level GPU soft error vulnerability assessment. We introduce gpuFI-4: a detailed microarchitecture-level fault injection framework to assess the cross-layer vulnerability of hardware structures and entire GPU chips for single and multiple bit faults, built on top of the state-of-the-art simulator GPGPU-Sim 4.0. We employ gpuFI-4 for fault injection of soft errors on CUDA-enabled Nvidia GPU architectures. The target hardware structures that our framework analyzes are the register file, the shared memory, the LI data and texture caches and the L2 cache, altogether accounting for tens of MBs of on-chip GPU storage. We showcase the features of the tool reporting the vulnerability of three Nvidia GPU chip models: two different modem GPU architectures – RTX 2060 (Turing) and Quadro GV100 (Volta) – and an older generation – GTX Titan (Kepler), for both single-bit and triple-bit fault injections and for twelve different CUDA benchmarks that are simulated on the actual physical instruction set (SASS). Our experiments report the Architectural Vulnerability Factor (AVF) of the GPU chips (which can be only measured at the microarchitecture level) as well as their predicted Failures in Time (FIT) rate when technology information is incorporated in the assessment.|IEEE International Symposium on Performance Analysis of Systems and Software|2022|10.1109/ISPASS55109.2022.00004|D. Gizopoulos, Dimitris Sartzetakis, G. Papadimitriou|2.6666666666666665|4
89|Vulkan-Sim: A GPU Architecture Simulator for Ray Tracing|Ray tracing can generate photorealistic images with more convincing visual effects compared to rasterization. Recent hardware advances have enabled ray tracing to be applied in real-time. Current GPUs feature a dedicated ray tracing acceleration unit, and game developers have started to make use of ray tracing APIs to bring more realistic graphics to their players. Industry cooperatively contributed to Vulkan, which recently introduced an open-standard API for ray tracing. However, little has been disclosed about the mapping of this API to hardware. In this paper, we introduce Vulkan-Sim, a detailed cycle-level simulator for enabling architecture research for ray tracing. We extend GPGPU-Sim, integrating it with Mesa, an open-source graphics library to support the Vulkan API, and add dedicated ray traversal and intersection units. We also demonstrate an explicit mapping of the Vulkan ray tracing pipeline to a modern GPU using a technique we call delayed intersection and any-hit execution. Additionally we evaluate several ray tracing workloads with Vulkan-Sim, identifying bottlenecks and inefficiencies of the ray tracing hardware we model. To demonstrate the utility of Vulkan-Sim we conduct two case studies evaluating techniques recently proposed or deployed by industry targeting enhanced ray tracing performance.|Micro|2022|10.1109/MICRO56248.2022.00027|Tor M. Aamodt, Tyler Nowicki, Mohammadreza Saed, Yuan Chou, Lufei Liu|2.6666666666666665|4
127|GPU-I-TASSER: a GPU accelerated I-TASSER protein structure prediction tool|MOTIVATION\nAccurate and efficient predictions of protein structures play an important role in understanding their functions. I-TASSER (Iterative Threading Assembly Refinement) is one of the most successful and widely used protein structure prediction methods in the recent community-wide CASP experiments. Yet, the computational efficiency of I-TASSER is one of the limiting factors that prevent its application for large-scale structure modelling.\n\n\nRESULTS\nWe present GPU-I-TASSER, a GPU accelerated I-TASSER protein structure prediction tool for fast and accurate protein structure prediction. Our implementation is based on OpenACC parallelization of the replica-exchange Monte Carlo simulations to enhance the speed of I-TASSER by extending its capabilities to the GPU architecture. On a benchmark dataset of 71 protein structures, GPU-I-TASSER achieves on average a 10x speedup with comparable structure prediction accuracy compared to the CPU version of the I-TASSER.\n\n\nAVAILABILITY\nThe complete source code for GPU-I-TASSER can be downloaded and used without restriction from https://zhanggroup.org/GPU-I-TASSER/.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.|Bioinform.|2022|10.1093/bioinformatics/btab871|Chengxin Zhang, Yang Zhang, Elijah MacCarthy, K. Dukka|2.6666666666666665|4
48|A Compaction Method for STLs for GPU in-field test|Nowadays, Graphics Processing Units (GPUs) are effective platforms for implementing complex algorithms (e.g., for Artificial Intelligence) in different domains (e.g., automotive and robotics), where massive parallelism and high computational effort are required. In some domains, strict safety-critical requirements exist, mandating the adoption of mechanisms to detect faults during the operational phases of a device. An effective test solution is based on Self-Test Libraries (STLs) aiming at testing devices functionally. This solution is frequently adopted for CPUs, but can also be used with GPUs. Nevertheless, the in-field constraints restrict the size and duration of acceptable STLs. This work proposes a method to automatically compact the test programs of a given STL targeting GPUs. The proposed method combines a multi-level abstraction analysis resorting to logic simulation to extract the microarchitectural operations triggered by the test program and the information about the thread-level activity of each instruction and to fault simulation to know its ability to propagate faults to an observable point. The main advantage of the proposed method is that it requires a single fault simulation to perform the compaction. The effectiveness of the proposed approach was evaluated, resorting to several test programs developed for an open-source GPU model (FlexGripPlus) compatible with NVIDIA GPUs. The results show that the method can compact test programs by up to 98.64% in code size and by up to 98.42% in terms of duration, with minimum effects on the achieved fault coverage.|Design, Automation and Test in Europe|2022|10.23919/DATE54114.2022.9774597|Juan-David Guerrero-Balaguera, M. Reorda, J. E. R. Condia|2.3333333333333335|4
61|Titan: A Parallel Asynchronous Library for Multi-Agent and Soft-Body Robotics using NVIDIA CUDA|While most robotics simulation libraries are built for low-dimensional and intrinsically serial tasks, soft-body and multi-agent robotics have created a demand for simulation environments that can model many interacting bodies in parallel. Despite the increasing interest in these fields, no existing simulation library addresses the challenge of providing a unified, highly-parallelized, GPU-accelerated interface for simulating large robotic systems. Titan is a versatile CUDA-based C++ robotics simulation library that employs a novel asynchronous computing model for GPU-accelerated simulations of robotics primitives. The innovative GPU architecture design permits simultaneous optimization and control on the CPU while the GPU runs asynchronously, enabling rapid topology optimization and reinforcement learning iterations. Kinematics are solved with a massively parallel integration scheme that incorporates constraints and environmental forces. We report dramatically improved performance over CPU baselines, simulating as many as 300 million primitive updates per second, while allowing flexibility for a wide range of research applications. We present several applications of Titan to high-performance simulations of soft-body and multi-agent robots.|IEEE International Conference on Robotics and Automation|2019|10.1109/ICRA40945.2020.9196808|H. Lipson, J. Austin, S. Wyetzner, Rafael Corrales-Fatou|2.3333333333333335|4
110|Core tunneling: Variation-aware voltage noise mitigation in GPUs|Voltage noise and manufacturing process variation represent significant reliability challenges for modern microprocessors. Voltage noise is caused by rapid changes in processor activity that can lead to timing violations and errors. Process variation is caused by manufacturing challenges in low-nanometer technologies and can lead to significant heterogeneity in performance and reliability across the chip. To ensure correct execution under worst-case conditions, chip designers generally add operating margins that are often unnecessarily conservative for most use cases, which results in wasted energy. This paper investigates the combined effects of process variation and voltage noise on modern GPU architectures. A distributed power delivery and process variation model at functional unit granularity was developed and used to simulate supply voltage behavior in a multicore GPU system. We observed that, just like in CPUs, large changes in power demand can lead to significant voltage droops. We also note that process variation makes some cores much more vulnerable to noise than others in the same GPU. Therefore, protecting the chip against large voltage droops by using fixed and uniform voltage guardbands is costly and inefficient. This paper presents core tunneling, a variation-aware solution for dynamically reducing voltage margins. The system relies on hardware critical path monitors to detect voltage noise conditions and quickly reacts by clock-gating vulnerable cores to prevent timing violations. This allows a substantial reduction in voltage margins. Since clock gating is enabled infrequently and only on the most vulnerable cores, the performance impact of core tunneling is very low. On average, core tunneling reduces energy consumption by 15%.|International Symposium on High-Performance Computer Architecture|2016|10.1109/HPCA.2016.7446061|Renji Thomas, R. Teodorescu, N. Sedaghati, Li Zhou, Kristin Barber|2.2222222222222223|4
92|Advanced load balancing for SPH simulations on multi-GPU architectures|Smoothed Particle Hydrodynamics (SPH) is a numerical method for fluid flow modeling, in which the fluid is discretized by a set of particles. SPH allows to model complex scenarios, which are difficult or costly to measure in the real world. This method has several advantages compared to other approaches, but suffers from a huge numerical complexity. In order to simulate real life phenomena, up to several hundred millions of particles have to be considered. Hence, HPC methods need to be leveraged to make SPH applicable for industrial applications. Distributing the respective computations among different GPUs to exploit massive parallelism is thereby particularly suited. However, certain characteristics of SPH make it a non-trivial task to properly distribute the respective workload. In this work, we present a load balancing method for a CUDA-based industrial SPH implementation on multi-GPU architectures. To that end, dedicated memory handling schemes are introduced, which reduce the synchronization overhead. Experimental evaluations confirm the scalability and efficiency of the proposed methods.|IEEE Conference on High Performance Extreme Computing|2017|10.1109/HPEC.2017.8091093|K. Szewc, R. Wille, Kevin Verma|2.125|4
33|A Cloud Gaming System Based on User-Level Virtualization and Its Resource Scheduling|Many believe the future of gaming lies in the cloud, namely Cloud Gaming, which renders an interactive gaming application in the cloud and streams the scenes as a video sequence to the player over Internet. This paper proposes GCloud, a GPU/CPU hybrid cluster for cloud gaming based on the user-level virtualization technology. Specially, we present a performance model to analyze the server-capacity and games' resource-consumptions, which categorizes games into two types: CPU-critical and memory-of-critical. Consequently, several scheduling strategies have been proposed to improve the resource-utilization and compared with others. Simulation tests show that both of the First-Fit-like and the Best-Fit-like strategies outperform the other(s); especially they are near optimal in the batch processing mode. Other test results indicate that GCloud is efficient: An off-the-shelf PC can support five high-end video-games run at the same time. In addition, the average per-frame processing delay is 8~19 ms under different image-resolutions, which outperforms other similar solutions.|IEEE Transactions on Parallel and Distributed Systems|2016|10.1109/TPDS.2015.2433916|Cihang Jiang, Weimin Zheng, Youhui Zhang, Peng Qu|2.0|4
130|Parallel Application Performance Prediction Using Analysis Based Models and HPC Simulations|Parallel application performance models provide valuable insight about the performance in real systems. Capable tools providing fast, accurate, and comprehensive prediction and evaluation of high-performance computing (HPC) applications and system architectures have important value. This paper presents PyPassT, an analysis based modeling framework built on static program analysis and integrated simulation of the target HPC architectures. More specifically, the framework analyzes application source code written in C with OpenACC directives and transforms it into an application model describing its computation and communication behavior (including CPU and GPU workloads, memory accesses, and message-passing transactions). The application model is then executed on a simulated HPC architecture for performance analysis. Preliminary experiments demonstrate that the proposed framework can represent the runtime behavior of benchmark applications with good accuracy.|SIGSIM Principles of Advanced Discrete Simulation|2018|10.1145/3200921.3200937|S. Eidenbenz, Gopinath Chennupati, N. Santhi, Jason Liu, M. Obaida|2.0|4
116|FLAME GPU: Complex System Simulation Framework|FLAME GPU is an agent based simulation frame-work that utilises the parallel architecture of Graphic Processing Unit (GPU) to enable real time model interaction and visualisation. In this paper, we provide an overview of the features of FLAME GPU and demonstrate its efficiency as a parallel agent based simulation platform. FLAME GPU abstracts the complexity of the GPU architecture from the users by offering a high level modelling syntax based on a formal state machine representation. A flocking model is presented showing how a simple multi-agent system is modelled.|International Symposium on High Performance Computing Systems and Applications|2017|10.1109/HPCS.2017.12|M. Chimeh, P. Richmond|1.875|4
129|A Massively Parallel Reservoir Simulator on the GPU Architecture|\n Reservoir simulation computational costs have been continuously growing due to high-resolution reservoir characterization, increasing model complexity, and uncertainty analysis workflows. Reducing simulation costs by upscaling is often necessary for operational requirements. Fast evolving HPC technologies offer opportunities to reduce cost without compromising fidelity.\n This work presents a novel in-house massively parallel full-physics reservoir simulator running on the emerging GPU architecture. Almost all the simulation kernels have been designed and implemented to honor the GPU SIMD programming paradigm. These kernels include physical property calculations, phase equilibrium computations, Jacobian construction, linear and nonlinear solvers, and wells. Novel techniques are devised in various kernels to expose enough parallelism to ensure that the control and data-flow patterns are well suited for the GPU environment. Mixed-precision computation is also employed when appropriate (e.g., in derivative calculation) to reduce computational costs without compromising the solution accuracy.\n The GPU implementation of the simulator is tested and benchmarked using various reservoir models, ranging from the synthetic SPE10 Benchmark (Christie & Blunt, 2001) to several industrial-scale models. These real field models range in size from tens of millions of cells to more than billion cells with black-oil and multicomponent compositional fluid. The GPU simulator is benchmarked on the IBM AC922 massively parallel architecture having tens of NVidia Volta V100 GPUs. To compare performance with CPU architectures, an optimized CPU implementation of the simulator is benchmarked on the IBM AC922 CPUs and on a cluster consisting of thousands of Intel's Haswell-EP Xeon® CPU E5-2680 v3. Detailed analysis of several numerical experiments comparing the simulator performance on the GPU and the CPU architectures is presented. In almost all of the cases, the analysis shows that the use of hardware acceleration offers substantial benefits in terms of wall time and power consumption.\n This novel in-house full-physics, black-oil and compositional reservoir simulator employs several novel techniques in various simulation kernels to ensure full utilization of the GPU resources. Detailed analysis is presented to highlight the simulator performance in terms of runtime reduction, parallel scalability and power savings.|Day 1 Tue, October 26, 2021|2021|10.2118/203918-ms|A. Dogru, Alhubail Maitham Makki, Todd R. Ferguson, Usuf Middya, T. Byer, A. Manea|1.75|4
43|SnuHPL: high performance LINPACK for heterogeneous GPUs|These days, it is typical for a large-scale cluster system to have different kinds of GPUs. However, HPL (High-Performance LINPACK), the de-facto standard LINPACK implementation for evaluating the performance of a cluster system, is originally designed to work only for homogeneous CPU-only systems. In this paper, we develop SnuHPL, an optimized HPL for clusters of modern heterogeneous GPUs. To optimize SnuHPL for the heterogeneous GPUs, we design a performance model, a SnuHPL simulator based on the model, and a greedy heuristic algorithm based on the simulator. The algorithm generates the best data distribution for a given cluster configuration by considering computing power, memory capacity, and network performance altogether. We also present a simple technique to increase the energy efficiency of HPL by adjusting the core clock frequency of the GPUs. The evaluation of the data distribution algorithm on small clusters of different GPU combinations shows that it outperforms well-known other data distribution strategies. We show the effectiveness of SnuHPL on a cluster of 1,760 NVIDIA A100-80GB GPUs and 440 A100-40GB GPUs. We also show the effectiveness of the proposed energy optimization technique on a cluster of 144 A100-80GB GPUs.|International Conference on Supercomputing|2022|10.1145/3524059.3532370|Jihwan Park, Jinpyo Kim, Seungwook Lee, Jintaek Kang, Hyungdal Kwon, Jaejin Lee|1.6666666666666667|4
52|A GPU-Based Solution for Ray Tracing 3-D Radiative Transfer Model for Optical and Thermal Images|Three-dimensional (3-D) radiative transfer (RT) models are frequently recognized as a prerequisite when using high spatial resolution remote sensing (RS) data in heterogeneous surfaces. However, most studies of 3-D RT models have been restricted to limited applications due to the low computational efficiency. Therefore, this study proposed a graphic processing unit (GPU)-based solution for the ray tracing 3-D RT model. A state-of-the-art graphics and compute application programming interface (API), Vulkan, were introduced to implement the RT process. A bounding box method was adopted for the computation acceleration. By comparison with a central processing unit (CPU)-based solution, the performance efficiency of the proposed solution is significantly better: the simulation time of a GPU model is significantly reduced by more than 99% when facing a large-scale simulation mission. The simulation accuracy of the two solutions is similar, with root mean squared errors (RMSEs) lower than 0.005, 0.032, and 0.31 K for the red, near-infrared (NIR), and brightness temperature (BT) images, respectively. An evaluation based on airborne multiangle measurements also indicated that the accuracy of the proposed solution was satisfactory for simulating the red and NIR bidirectional reflectance factor (BRF) and BT directional anisotropies (DAs), with RMSEs lower than 0.003, 0.020, and 0.20 K, respectively, when treating the whole scene as a pixel. Considering the simulation accuracy and efficiency, a GPU-based model will be an important supplement to the CPU model.|IEEE Geoscience and Remote Sensing Letters|2022|10.1109/LGRS.2022.3206312|Yongming Du, J. Gastellu-Etchegorry, J. Roujean, Qinhuo Liu, Qing Xiao, Jianbo Qi, Biao Cao, Z. Bian, Hua Li|1.6666666666666667|4
81|Performance Characterisation and Simulation of Intel's Integrated GPU Architecture|Integrated GPUs (iGPUs) are ubiquitous in today's client devices such as laptops and desktops. Examples include Intel's HD or Iris Graphics and AMD's APUs. An iGPU resides on the same chip as the CPU, which is in contrast to a conventional discrete GPU that would typically be connected over the PCI-E bus. Much like discrete GPUs, iGPUs are also capable of general purpose computation in addition to traditional graphics roles. Further, iGPUs have some interesting differences compared to traditional GPUs such as a cache-coherent memory hierarchy and a shared last level cache with the CPU. Despite their wide spread use, they are not studied very extensively. To the best of our knowledge, this paper introduces the first open source trace generation and microarchitectural simulation framework for Intel's integrated GPUs. We characterise the performance of Intel's Skylake and Kabylake GPUs through detailed microbenchmarks, and use the performance evaluations to guide our models and validate the simulator.|IEEE International Symposium on Performance Analysis of Systems and Software|2018|10.1109/ISPASS.2018.00027|Sunpyo Hong, Prasun Gera, Hyesoon Kim, Hyojong Kim, V. George, C. Luk|1.5714285714285714|4
146|EmerGPU: Understanding and mitigating resonance-induced voltage noise in GPU architectures|This paper characterizes voltage noise in GPU architectures running general purpose workloads. In particular, it focuses on resonance-induced voltage noise, which is caused by workload-induced fluctuations in power demand that occur at the resonance frequency of the chip's power delivery network. A distributed power delivery model at functional unit granularity was developed and used to simulate supply voltage behavior in a GPU system. We observe that resonance noise can lead to very large voltage droops and protecting against these droops by using voltage guardbands is costly and inefficient. We propose EmerGPU, a solution that detects and mitigates resonance noise in GPUs. EmerGPU monitors workload activity levels and detects oscillations in power demand that approach resonance frequencies. When such conditions are detected, EmerGPU deploys a mitigation mechanism implemented in the warp scheduler that disrupts the resonance activity pattern. EmerGPU has no impact on performance and a small power cost. Reducing voltage noise improves system reliability and allows for smaller voltage margins to be used, reducing overall energy consumption by an average of 21%.|IEEE International Symposium on Performance Analysis of Systems and Software|2016|10.1109/ISPASS.2016.7482076|Renji Thomas, R. Teodorescu, N. Sedaghati|1.5555555555555556|4
155|Assessment of Efficiency and Performance in Tsunami Numerical Modeling with GPU|Non-linear shallow water equations (NSWE) are used to solve the propagation and coastal amplification of long waves and tsunamis. Leap Frog scheme of finite difference technique is one of the satisfactory numerical methods which is widely used in these problems. Tsunami numerical models are necessary for not only academic but also operational purposes which need faster and accurate solutions. Recent developments in information technology provide considerably faster numerical solutions in this respect and are becoming one of the crucial requirements. Tsunami numerical code NAMI DANCE uses finite difference numerical method to solve linear and non-linear forms of shallow water equations for long wave problems, specifically for tsunamis. In this study, the new code is structured for Graphical Processing Unit (GPU) using CUDA API. The new code is applied to different (analytical, experimental and field) benchmark problems of tsunamis for tests. One of those applications is 2011 Great East Japan tsunami which was instrumentally recorded on various types of gauges including tide and wave gauges and offshore GPS buoys cabled Ocean Bottom Pressure (OBP) gauges and DART buoys. The accuracy of the results are compared with the measurements and fairly well agreements are obtained. The efficiency and performance of the code is also compared with the version using multi-core Central Processing Unit (CPU). Dependence of simulation speed with GPU on linear or non-linear solutions is also investigated. One of the results is that the simulation speed is increased up to ∼75 times comparing to the process time in the computer using single 4/8 thread multi-core CPU. The results are presented with comparisons and discussions. Furthermore how multi-dimensional finite difference problems fits towards GPU architecture is also discussed.||2017|10.1007/s11069-017-3082-1|B. Yalçiner, A. Zaytsev|1.5|4
45|A PCIe Congestion-Aware Performance Model for Densely Populated Accelerator Servers|MeteoSwiss, the Swiss national weather forecast institute, has selected densely populated accelerator servers as their primary system to compute weather forecast simulation. Servers with multiple accelerator devices that are primarily connected by a PCI-Express (PCIe) network achieve a significantly higher energy efficiency. Memory transfers between accelerators in such a system are subjected to PCIe arbitration policies. In this paper, we study the impact of PCIe topology and develop a congestion-aware performance model for PCIe communication. We present an algorithm for computing congestion factors of every communication in a congestion graph that characterizes the dynamic usage of network resources by an application. Our model applies to any PCIe tree topology. Our validation results on two different topologies of 8 GPU devices demonstrate that our model achieves an accuracy of over 97% within the PCIe network. We demonstrate the model on a weather forecast application to identify the best algorithms for its communication patterns among GPUs.|International Conference for High Performance Computing, Networking, Storage and Analysis|2016|10.1109/SC.2016.62|Maxime Martinasso, T. Schulthess, T. Hoefler, S. Alam, Grzegorz Kwasniewski|1.4444444444444444|4
145|End-to-end performance modeling of distributed GPU applications|With the growing number of GPU-based supercomputing platforms and GPU-enabled applications, the ability to accurately model the performance of such applications is becoming increasingly important. Most current performance models for GPU-enabled applications are limited to single node performance. In this work, we propose a methodology for end-to-end performance modeling of distributed GPU applications. Our work strives to create performance models that are both accurate and easily applicable to any distributed GPU application. We combine trace-driven simulation of MPI communication using the TraceR-CODES framework with a profiling-based roofline model for GPU kernels. We make substantial modifications to these models to capture the complex effects of both on-node and off-node networks in today's multi-GPU supercomputers. We validate our model against empirical data from GPU platforms and also vary tunable parameters of our model to observe how they might affect application performance.|International Conference on Supercomputing|2020|10.1145/3392717.3392737|A. Bhatele, D. Richards, Jaemin Choi, L. Kalé|1.4|4
23|Performance Portability of Sparse Block Diagonal Matrix Multiple Vector Multiplications on GPUs|The emergence of accelerator-based computer architectures and programming models makes it challenging to achieve performance portability for large-scale scientific simulation software. In this paper, we focus on a sparse block diagonal matrix multiple vector (SpMM) computational kernel and discuss techniques that can be used to achieve performance portability on NVIDIA and AMD based accelerators using CUDA, HIP, OpenACC, Kokkos. We show that performance portability can vary significantly across programming models, GPU architectures, and problem settings, by up to 52× in the explored problems. Our study visits the performance portability aggregation techniques to guide the development and the selection of performance portable algorithmic variants.|International Workshop on Performance, Portability and Productivity in HPC|2022|10.1109/P3HPC56579.2022.00011|Pieter Maris, K. Ibrahim, Chao Yang|1.3333333333333333|4
100|optimizations in CuSNP Simulator for Spiking Neural P Systems on CUDA GPUs|Spiking Neural P systems (in short, SNP systems) are computing models based on living neurons. SNP systems are non-deterministic and parallel, hence making use of a parallel processor such as a graphics processing unit (in short, GPU) is a natural candidate for simulations. Matrix representations and algorithms were previously developed for simulating SNP systems. In this work, our two results extend previous works in simulating SNP systems in the GPU: (a) the number of neurons the simulator can handle is now arbitrary; (b) SNP systems are now represented in a dense instead of sparse way. The impact in terms of time and space of these extensions to the GPU simulator are analysed. As expected, SNP systems with more neurons need more simulation time, although the simulator performance can scale (i.e. perform better) with larger GPUs. The dense representation helps in the simulation of larger systems.|International Symposium on High Performance Computing Systems and Applications|2019|10.1109/HPCS48598.2019.9188174|J. P. Carandang, F. Cabarle, H. Adorna, R. T. Cruz, Miguel A. Martínez-del-Amor, Edward James A. Bariring, Blaine Corwyn D. Aboy|1.3333333333333333|4
30|OO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future NUMA-Based Multi-GPU Systems|With the strong computation capability, NUMA-based multi-GPU system is a promising candidate to provide sustainable and scalable performance for Virtual Reality (VR) applications and deliver the excellent user experience. However, the entire multi-GPU system is viewed as a single GPU under the single programming model which greatly ignores the data locality among VR rendering tasks during the workload distribution, leading to tremendous remote memory accesses among GPU models (GPMs). The limited inter- GPM link bandwidth (e.g., 64GB/s for NVlink) becomes the major obstacle when executing VR applications in the multi-GPU system. By conducting comprehensive characterizations on different kinds of parallel rendering frameworks, we observe that distributing the rendering object along with its required data per GPM can reduce the inter-GPM memory accesses. However, this object-level rendering still faces two major challenges in NUMA-based multi- GPU system: (1) the large data locality between the left and right views of the same object and the data sharing among different objects and (2) the unbalanced workloads induced by the software- level distribution and composition mechanisms. To tackle these challenges, we propose object-oriented VR rendering framework (OO-VR) that conducts the software and hardware co-optimization to provide a NUMA friendly solution for VR multi-view rendering in NUMA-based multi-GPU systems. We first propose an object-oriented VR programming model to exploit the data sharing between two views of the same object and group objects into batches based on their texture sharing levels. Then, we design an object aware runtime batch distribution engine and distributed hardware composition unit to achieve the balanced workloads among GPMs and further improve the performance of VR rendering. Finally, evaluations on our VR featured simulator show that OO-VR provides 1.58x overall performance improvement and 76% inter-GPM memory traffic reduction over the state-of- the-art multi-GPU systems. In addition, OO-VR provides NUMA friendly performance scalability for the future larger multi-GPU scenarios with ever increasing asymmetric bandwidth between local and remote memory.|International Symposium on Computer Architecture|2019|10.1145/3307650.3322247|Xin Fu, S. Song, Chenhao Xie, Mingsong Chen|1.1666666666666667|4
98|Real-time end-to-end AO simulations at ELT scale on multiple GPUs with the COMPASS platform|The COMPASS platform was designed to meet the need for high-performance for the simulation of AO systems. Taking advantage of the specific hardware architecture of the GPU, the COMPASS tool allows the AO scientist to obtain adequate execution speeds and to conduct large simulation campaigns scaled to the E-ELT dimensioning for a variety of AO flavors from SCAO to MOAO. On the latest GPU architecture (NVIDA Volta), execution speeds of several hundreds of short exposure PSF per second can be achieved for a SCAO system on the E-ELT, making the COMPASS platform a real-time end-to-end simulation tool. In this paper, we provide a full description of the critical physical models used in the simulation pipeline, review a range AO system configurations that can be addressed with COMPASS and report on the time to solution obtained for this systems. Scalability over multiple GPUs and multiple generations of GPUs is also discussed.|Astronomical Telescopes + Instrumentation|2018|10.1117/12.2312593|F. Vidal, N. Doucet, E. Gendron, F. Ferreira, D. Gratadour, A. Sevin, V. Deo|1.1428571428571428|4
26|Characterization of Transmission Lines in Microelectronic Circuits Using the ARTEMIS Solver|Modeling and characterization of electromagnetic wave interactions with microelectronic devices to derive network parameters has been a widely used practice in the electronic industry. However, as these devices become increasingly miniaturized with finer-scale geometric features, computational tools must make use of manycore/GPU architectures to efficiently resolve length and time scales of interest. This has been the focus of our open-source solver, ARTEMIS (Adaptive mesh Refinement Time-domain ElectrodynaMIcs Solver), which is performant on modern GPU-based supercomputing architectures while being amenable to additional physics coupling. This work demonstrates its use for characterizing network parameters of transmission lines using established techniques. A rigorous verification and validation of the workflow is carried out, followed by its application for analyzing a transmission line on a CMOS chip designed for a photon-detector application. Simulations are performed for millions of timesteps on state-of-the-art GPU resources to resolve nanoscale features at gigahertz frequencies. The network parameters are used to obtain phase delay and characteristic impedance that serve as inputs to SPICE models. The code is demonstrated to exhibit ideal weak scaling efficiency up to 1024 GPUs and 84% efficiency for 2048 GPUs, which underscores its use for network analysis of larger, more complex circuit devices in the future.|IEEE Journal on Multiscale and Multiphysics Computational Techniques|2022|10.1109/JMMCT.2022.3228281|A. Nonaka, Saurabh S. Sawant, R. Jambunathan, Z. Yao|1.0|4
74|Probabilistically Certified Management of Data Centers Using Predictive Control|Data centers are facilities with large number of servers providing cloud services. The increasing number of data centers in the last years has generated environmental concern due to the large amount of energy consumed by them. This also includes some auxiliary services such as the cooling equipment which is known to be very costly. For that reason, efficient data center strategies are needed in order to provide an acceptable quality of service (QoS) and suitable temperature for every server while using the least amount of resources possible. This article presents some strategies to deal with the unified workload and temperature problem that appears in the data center. As the system is modeled as a queue and the control variables have an hybrid nature, some highly parallelizable particle-based optimization algorithms are proposed to solve the optimization problem. Numerical simulations are provided in order to illustrate the effectiveness of the strategy. These simulations also show the improvements obtained from the GPU computing. Finally, a probabilistic evaluation approach is developed in order to provide certificates on the probability of constraint satisfaction without increasing the computational burden of the online problem. Note to Practitioners—This article addresses the problem of deciding in real-time the number of active servers in a data center that is required to meet the quality of service (QoS) demands while keeping energy consumption at a minimum. The temperature set point of the cooling equipment must also be taken into account, as it is advisable to use the minimum cooling that keeps the servers running in safe conditions. The management strategy proposed is based on predictive control. In this way, the number of active servers and temperature set point will be chosen so that the future energy consumption is minimized while guaranteeing that QoS and safety demands are met under different possible operating conditions. Furthermore, the proposed management strategy can be tuned depending on the QoS that it is desirable to provide. The proposed strategy will lead to energy-consumption improvements while having guarantees on the data center performance.|IEEE Transactions on Automation Science and Engineering|2022|10.1109/TASE.2021.3093699|A. D. Carnerero, D. Ramírez, T. Alamo, D. Limón|1.0|4
88|Parallelizing Hines Matrix Solver in Neuron Simulations on GPU|Hines matrices arise in the simulations of mathematical models describing initiation and propagation of action potentials in a neuron. In this work, we exploit the structural properties of Hines matrices and design a scalable, linear work, recursive parallel algorithm for solving a system of linear equations where the underlying matrix is a Hines matrix, using the Exact Domain Decomposition Method (EDD). We give a general form for representing a Hines matrix and use the general form to prove that the intermediate matrix obtained via the EDD has the same structural properties as that of a Hines matrix. Using the above observation, we propose a novel decomposition strategy called fine decomposition which is suitable for a GPU architecture. Our algorithmic approach R-FINE-TPT based on fine decomposition outperforms the previously known approach in all the cases and gives a speedup of 2.5x on average for a variety of input neuron morphologies. We further perform experiments to understand the behaviour of R-FINE-TPT approach and show its robustness. We also employ a machine learning technique called linear regression to effectively guide recursion in our algorithm.|International Conference on High Performance Computing|2017|10.1109/HiPC.2017.00051|Kishore Kothapalli, U. Bhalla, Dharma Teja Vooturi|1.0|4
143|Computing and Compressing Electron Repulsion Integrals on FPGAs|The computation of electron repulsion integrals (ERIs) over Gaussian-type orbitals (GTOs) is a challenging problem in quantum-mechanics-based atomistic simulations. In practical simulations, several trillions of ERIs may have to be computed for every time step. In this work, we investigate FPGAs as accelerators for the ERI computation. We use template parameters, here within the Intel oneAPI tool flow, to create customized designs for 256 different ERI quartet classes, based on their orbitals. To maximize data reuse, all intermediates are buffered in FPGA on-chip memory with customized layouts. The pre-calculation of intermediates also helps to overcome data dependencies caused by multi-dimensional recurrence relations. The involved loop structures are partially or even fully unrolled for high throughput of FPGA kernels. Furthermore, a lossy compression algorithm utilizing arbitrary bitwidth integers is integrated in the FPGA kernels. To our best knowledge, this is the first work on ERI computation on FPGAs that supports more than just the single most basic quartet class. Also, the integration of ERI computation and compression is a novelty that is not even covered by CPU or GPU libraries so far. Our evaluation shows that using 16-bit integer for the ERI compression, the fastest FPGA kernels exceed the performance of 10 GERIS ($10\times 10^{9}$ ERIs per second) on one Intel Stratix 10 GX 2800 FPGA, with maximum absolute errors around 10−7 - 10−5 Hartree. The measured throughput can be accurately explained by a performance model. The FPGA kernels deployed on 2 FPGAs outperform similar computations using the widely used libint reference on a two-socket server with 40 Xeon Gold 6148 CPU cores of the same process technology by factors up to 6.0x and on a new two-socket server with 128 EPYC 7713 CPU cores by up to 1.9x.|IEEE Symposium on Field-Programmable Custom Computing Machines|2023|10.1109/FCCM57271.2023.00026|Tobias Kenter, Robert Schade, Xin-Chuan Wu, Christian Plessl, T. Kühne|1.0|4
76|CPU/GPU Heterogeneous Parallel CFD Solver and Optimizations|Graphics Processing Units (GPU) has been widely used in the area of general computing. Nowadays, CFD has dramatically benefited from the strong abilities of floating-point operation and memory bandwidth of GPU architecture. Lots of GPU-based CFD solvers have already shown that GPU has capacities in accelerating numerical simulations. Compute unified device architecture (CUDA), a general parallel computing platform and programming model, reduces program complication, brings the great opportunities to CFD, and has been successfully used to the parallel solution of compressible Navier-Stokes equations. In this paper, we present a CPU/GPU heterogeneous parallel CFD solver which established on NVIDIA GTX 1070 GPU, and the serial code executes on the host, the parallel code performs on the device. Three optimization methods are discussed: maximize utilization, reducing global memory access and minimize data transfer between host and device, which further improve the performance of the solver. Two cases, including flow over sphere at Reynolds number 118 and flow over double ellipsoid, are presented to demonstrate the solver's capacities for compressible flow. Numerical results agree well with experimental data, which illustrate that the solver has a high computational precision for compressible flow.|International Conference Service Robotics Technologies|2018|10.1145/3208833.3208847|Zhengyu Tian, Hua Li, Jianqi Lai|0.8571428571428571|4
151|AMulti-GPU PCISPH Implementation with Efficient Memory Transfers|Smoothed Particle Hydrodynamics (SPH) is a particle-based method for fluid flow modeling. One promising variant of SPH is Predictive-Corrective Incompressible SPH (PCISPH), which employs a dedicate prediction-correction scheme and, by this, outperforms other SPH variants by almost one order of magnitude. However, similar to other particle-based methods, it suffers from a huge numerical complexity. In order to simulate real world phenomena, several millions of particles need to be considered. To make SPH applicable to real world engineering problems, it is hence common to exploit massive parallelism of multi-GPU architectures. However, certain algorithmic characteristics of PCISPH make it a non-trivial task to efficiently parallelize this method on multi-GPUs. In this work, we are, for the first time, proposing a multi-GPU implementation for PCISPH. To this end, we are proposing a scheme which allows to overlap the memory transfers between GPUs by actual computations and, by this, avoids the drawbacks caused by the mentioned algorithmic characteristics of PCISPH. Experimental evaluations confirm the efficiency of the proposed methods.|IEEE Conference on High Performance Extreme Computing|2018|10.1109/HPEC.2018.8547542|R. Wille, Chong Peng, Kevin Verma, K. Szewc|0.8571428571428571|4
39|Power and Performance Optimal NoC Design for CPU-GPU Architecture Using Formal Models|Heterogeneous computing architectures that fuse both CPU and GPU on the same chip are common nowadays. Using homogeneous interconnect for such heterogeneous processors each with different network demands can result in performance degradation. In this paper, we focused on designing a heterogeneous mesh-style network-on-chip (NoC) to connect heterogeneous CPU-GPU processors. We tackled three problems at once; mapping Processing Elements (PEs) to the routers of the mesh, assigning the number of virtual channels (VC), and assigning the buffer size (BS) for each port of each router in the NoC. By relying on formal models, we developed a method based on Strength Pareto Evolutionary Algorithm2 (SPEA2) to obtain the Pareto optimal set that optimizes communication performance and power consumption of the NoC. By validating our method on a full-system simulator, results show that the NoC performance can be improved by 17% while minimizing the power consumption by at least 2.3x and maintaining the overall system performance.|Design, Automation and Test in Europe|2019|10.23919/DATE.2019.8714769|N. Bagherzadeh, Lulwah Alhubail|0.8333333333333334|4
70|Modeling Emerging Memory-Divergent GPU Applications|Analytical performance models yield valuable architectural insight without incurring the excessive runtime overheads of simulation. In this work, we study contemporary GPU applications and find that the key performance-related behavior of such applications is distinct from traditional GPU applications. The key issue is that these GPU applications are memory-intensive and have poor spatial locality, which implies that the loads of different threads commonly access different cache blocks. Such memory-divergent applications quickly exhaust the number of misses the L1 cache can process concurrently, and thereby cripple the GPU's ability to use Memory-Level Parallelism (MLP) and Thread-Level Parallelism (TLP) to hide memory latencies. Our Memory Divergence Model (MDM) is able to accurately represent this behavior and thereby reduces average performance prediction error by 14× compared to the state-of-the-art GPUMech approach across our memory-divergent applications.|IEEE computer architecture letters|2019|10.1109/LCA.2019.2923618|Lu Wang, L. Eeckhout, Almutaz Adileh, Zhiying Wang, Magnus Jahre|0.8333333333333334|4
60|A Performance Model for GPU-Accelerated FDTD Applications|In this work we develop, validate and use a performance model for a Finite-Difference Time-Domain (FDTD) application which is parallelized on multiple GPUs. FDTD is a method for simulating electrodynamic interaction and is applied in a number of research and engineering areas. In this work we focus on a particular implementation called B-CALM (Belgium-California Light Machine). We adopt a simple, semi-empirical modelling approach to design a model which we validate for different hardware architectures. Using the model allows making implementation decisions and exploring the architectural design space with the goal of optimizing HPC systems for this application.|International Conference on High Performance Computing|2015|10.1109/HiPC.2015.24|D. Pleiter, P. Wahl, J. Kraus, T. Hater, P. Baumeister|0.8|4
117|Task-Based Crowd Simulation for Heterogeneous Architectures|Industry trends in the coming years imply the availability of cluster computing with hundreds to thousands of cores per chip, as well as the use of accelerators. Programming presents a challenge due to this heterogeneous architecture; thus, using novel programming models that facilitate this process is necessary. In this chapter, the case of simulation and visualization of crowds is presented. The authors analyze and compare the use of two programming models: OmpSs and CUDA. OmpSs allows to take advantage of all the resources available per node by combining the CPU and GPU while automatically taking care of memory management, scheduling, communications and synchronization. Experimental results obtained from Fermi, Kepler and Maxwell GPU architectures are presented, and the different modes used for visualizing the results are described, as well.||2016|10.4018/978-1-5225-0287-6.CH008|E. Ayguadé, Benjamín Hernández, Hugo Perez, Isaac Rudomín|0.7777777777777778|4
138|Parallel Massive-Thread Electromagnetic Transient Simulation on GPU|The electromagnetic transient (EMT) simulation of a large-scale power system consumes so much computational power that parallel programming techniques are urgently needed in this area. For example, realistic-sized power systems include thousands of buses, generators, and transmission lines. Massive-thread computing is one of the key developments that can increase the EMT computational capabilities substantially when the processing unit has enough hardware cores. Compared to the traditional CPU, the graphic-processing unit (GPU) has many more cores with distributed memory which can offer higher data throughput. This paper proposes a massive-thread EMT program (MT-EMTP) and develops massive-thread parallel modules for linear passive elements, the universal line model, and the universal machine model for offline EMT simulation. An efficient node-mapping structure is proposed to transform the original power system admittance matrix into a block-node diagonal sparse format to exploit the massive-thread parallel GPU architecture. The developed MT-EMTP program has been tested on large-scale power systems of up to 2458 three-phase buses with detailed component modeling. The simulation results and execution times are compared with mainstream commercial software, EMTP-RV, to show the improvement in performance with equivalent accuracy.|IEEE Transactions on Power Delivery|2015|10.1109/PESGM.2015.7285591|V. Dinavahi, Zhiyin Zhou|0.7|4
22|A Parallel Mode Optimized GPU Accelerated Monte Carlo Model for Light Propagation in 3-D Voxelized Bio-Tissues|Monte Carlo simulation is a precise method to model light propagation in bio-tissues and has been considered the golden standard to estimate the result of other computation methods. But the huge computation burden limited the application. In this paper, we propose a parallel computing model using graphic card to accelerate the Monte Carlo simulation in 3-D voxelized media with the consideration of internal refraction. Optimization of the parallel mode is made by using segmentations and offered an extra boost of simulation speed. The acceleration efficiency affecting factors are investigated and the acceleration rate of the five segmented model is 32.6 times higher than non-GPU model and 1.66 times higher than non-optimized model for a real human head 3-D structure simulation.|IEEE Access|2019|10.1109/ACCESS.2019.2923320|Xiang Fang, Ting Li, Yingxin Li, Weichao Liu, Hao Li|0.6666666666666666|4
86|NoMali: Simulating a realistic graphics driver stack using a stub GPU|Since the advent of the smartphone, all high-end mobile devices have required graphics acceleration in the form of a GPU. Today, even low-power devices such as smartwatches use GPUs for rendering and composition. However, the computer architecture community has largely ignored these developments when evaluating new architecture proposals. A common approach when evaluating CPU designs for the mobile space has been to use software rendering instead of a GPU model. However, due to the ubiquity of GPUs in mobile devices, they are used in both 3D applications and 2D applications. For example, when running a 2D application such as the web browser in Android with a software renderer instead of a GPU, the CPU ends up executing twice as many instructions. Both the CPU characteristics and the memory system characteristics differ significantly between the browser and the software renderer. The software renderer typically executes tight loops of vector instructions, while the browser predominantly consists of integer instructions and complex control flow with hard-to-predict branches. Including software rendering results in unrepresentative benchmark performance. In this paper, we use gem5 to quantify the effects of software rendering on a set of common mobile workloads. We also introduce the NoMali stub GPU model that can be used as a drop-in replacement for a real Mali GPU model. This model behaves like a normal GPU, but does not render anything. Using this stub GPU, we demonstrate how most of the problems associated with software rendering can be avoided, while at the same time simulating a representative graphics stack.|IEEE International Symposium on Performance Analysis of Systems and Software|2016|10.1109/ISPASS.2016.7482100|R. D. Jong, Andreas Sandberg|0.6666666666666666|4
94|Towards Virtual Certification of Gas Turbine Engines With Performance-Portable Simulations|We present the large-scale, computational fluid dy-namics (CFD) simulation of a full gas-turbine engine compressor, demonstrating capability towards overcoming current limitations for virtual certification of aero-engine design. The simulation is carried out through a performance portable code-base on multi-core/many-core HPC clusters with a CFD-to-CFD coupled execution, combining an industrial CFD solver linked using custom coupler software. The application innovates in its design for performance portability through the OP2 domain specific library for the CFD components, allowing the automatic generation of highly optimized platform-specific parallelizations for both multi-core (CPU) and many-core (GPU) clusters from a single high-level source. The code is used for the simulation of a 4.58B node, full-annulus 10-row production-grade test compressor (DLR's Rig250), using a coupled sliding-plane setup on the ARCHER2 and Cirrus supercomputers at EPCC. The OP2 generated multiple parallelizations, together with optimized coupler configurations on heterogeneous/hybrid settings achieve, for the first time, execution of 1 revolution in less than 6 hours on 512 nodes of ARCHER2 (65k cores), with a parallel scaling efficiency of over 80 % compared to a 107 node run. Results indicate a speed up of the CFD suite by an order of a magnitude (≈30 x) relative to current production capability. Benchmarking and performance modelling project a time-to-solution of less than 5 hours on a cluster of 488xNVIDIA V100 GPUs, about 3x-4 x speedup over CPU clusters. The work demonstrates a step-change towards achieving virtual certification of aircraft engines with the requisite fidelity and tractable time-to-solution that was previously out of reach under production settings.|IEEE International Conference on Cluster Computing|2022|10.1109/CLUSTER51413.2022.00034|D. Amirante, S. Jarvis, G. Mudalige, I. Reguly, L. Lapworth, A. Prabhakar|0.6666666666666666|4
73|Solving incompressible Navier-Stokes equations on heterogeneous parallel architectures. (Résolution des équations de Navier-Stokes incompressibles sur architectures parallèles hétérogènes)|In this PhD thesis, we present our research in the domain of high performance software for computational fluid dynamics (CFD). With the increasing demand of high-resolution simulations, there is a need of numerical solvers that can fully take advantage of current manycore accelerated parallel architectures. In this thesis we focus more specifically on developing an efficient parallel solver for 3D incompressible Navier-Stokes (NS) equations on heterogeneous CPU/GPU architectures. We first present an overview of the CFD domain along with the NS equations for incompressible fluid flows and existing numerical methods. We describe the mathematical model and the numerical method that we chose, based on an incremental prediction-projection method.A balanced distribution of the computational workload is obtained by using a domain decomposition method. A two-level parallelization combined with SIMD vectorization is used in our implementation to take advantage of the current distributed multicore machines. Numerical experiments on various parallel architectures show that this solver provides satisfying performance and good scalability.In order to further improve the performance of the NS solver, we integrate GPU computing to accelerate the most time-consuming tasks. The resulting solver can be configured for running on various heterogeneous architectures by specifying explicitly the numbers of MPI processes, threads and GPUs. This thesis manuscript also includes simulation results for two benchmarks designed from real physical cases. The computed solutions are compared with existing reference results. The code developed in this work will be the base for a future CFD library for parallel CPU/GPU computations.||2015|10.1002/fld.4019|Yushan Wang|0.6|4
93|Benchmarking multi-GPU communication using the shallow water equations|The shallow water model equations provide a simple yet realistic benchmark problem in computational fluid dynamics (CFD) that can be implemented on a variety of computational platforms. Graphical processing units (GPUs) can be used to accelerate such applications on a single device using a data parallel decompositional scheme or with multiple devices using a domain decompositional approach. A shallow water equation simulation is implemented on a range of modern GPU architectures and multi-GPU systems. The typical performance of these systems using the two main device-device communication methods is reported on for single- and double-precision calculations.|International Journal of Big Data Intelligence|2015|10.1504/IJBDI.2015.070596|D. Playne, K. Hawick|0.6|4
46|Multi-GPU hybrid programming accelerated three-dimensional phase-field model in binary alloy|In the process of dendritic growth simulation, the computational efficiency and the problem scales have extremely important influence on simulation efficiency of three-dimensional phase-field model. Thus, seeking for high performance calculation method to improve the computational efficiency and to expand the problem scales has a great significance to the research of microstructure of the material. A high performance calculation method based on MPI+CUDA hybrid programming model is introduced. Multi-GPU is used to implement quantitative numerical simulations of three-dimensional phase-field model in binary alloy under the condition of multi-physical processes coupling. The acceleration effect of different GPU nodes on different calculation scales is explored. On the foundation of multi-GPU calculation model that has been introduced, two optimization schemes, Non-blocking communication optimization and overlap of MPI and GPU computing optimization, are proposed. The results of two optimization schemes and basic multi-GPU model are compared. The calculation results show that the use of multi-GPU calculation model can improve the computational efficiency of three-dimensional phase-field obviously, which is 13 times to single GPU, and the problem scales have been expanded to 8193. The feasibility of two optimization schemes is shown, and the overlap of MPI and GPU computing optimization has better performance, which is 1.7 times to basic multi-GPU model, when 21 GPUs are used.In the process of dendritic growth simulation, the computational efficiency and the problem scales have extremely important influence on simulation efficiency of three-dimensional phase-field model. Thus, seeking for high performance calculation method to improve the computational efficiency and to expand the problem scales has a great significance to the research of microstructure of the material. A high performance calculation method based on MPI+CUDA hybrid programming model is introduced. Multi-GPU is used to implement quantitative numerical simulations of three-dimensional phase-field model in binary alloy under the condition of multi-physical processes coupling. The acceleration effect of different GPU nodes on different calculation scales is explored. On the foundation of multi-GPU calculation model that has been introduced, two optimization schemes, Non-blocking communication optimization and overlap of MPI and GPU computing optimization, are proposed. The results of two optimization schemes and b...||2018|10.1063/1.5021730|M. Zhu, Li Feng, Changsheng Zhu, Jieqiong Liu|0.5714285714285714|4
40|TUFLOW GPU – Best Practice Advice for Hydrologic and Hydraulic Model Simulations|Graphics Processing Unit (GPU) computing represents a significant advancement in the continued evolution of flood modelling. BMT WBM the developers of TUFLOW are continually undertaking research work focusing on how best to use the software. This paper presents a range of GPU model validation results and summarises best practice recommendations on how to optimise execution to achieve fastest model simulation and most accurate results for hydrologic and hydraulic applications.||2016|10.1038/537587a|Bill Syme, C. Huxley|0.5555555555555556|4
85|GPU Performance Prediction Through Parallel Discrete Event Simulation and Common Sense|We present the GPU Module of a Performance Prediction Toolkit developed at Los Alamos National Laboratory, which enables code developers to efficiently test novel algorithmic ideas particularly for large-scale computational physics codes. The GPU Module is a heavily-parameterized model of the GPU hardware that takes as input a sequence of abstracted instructions that the user provides as a representation of the application or can also be read in from the GPU intermediate representation PTX format. These instructions are then executed in a discrete event simulation framework of the entire computing infrastructure that can include multi-GPU and also multi-node components as typically found in high performance computing applications. Our GPU Module aims at a trade-off between the cycle-accuracy of GPU simulators and the fast execution times of analytical models. This trade-off is achieved by simulating at cycle level only a portion of the computations and using this partial runtime to analytically predict the total execution of the modeled application. We present GPU models that we validate against three different benchmark applications that cover the range from bandwidth- to cycle-limited. Our runtime predictions are within an error of 20%. We then predict performance of a next-generation GPU (Nvidia’s Pascal) for the same benchmark applications.|EAI Endorsed Transactions on Ubiquitous Environments|2016|10.4108/eai.14-12-2015.2262575|S. Eidenbenz, N. Santhi, Guillaume Chapuis|0.5555555555555556|4
135|A review of load flow and network reconfiguration techniques with their enhancement for radial distribution network|Load flow analysis of electrical distribution networks either for providing household electricity or in integrated circuits has always been a topic of great interest for researchers from last few decades. Various novel methods and techniques have been proposed for load flow calculations in the network and simulation tools are being used to determine the various characteristics of the network. The paper mainly focusses on these techniques at a single site and the enhancements made to the already existing techniques throughout the use of GPU architecture using CUDA platform in lieu of enhancing the performance of already existing solution with respect to time and algorithmic complexity. The paper also explains the need of enhancing prevailing solutions for load flow analysis for future generation smart grids or real time systems by comparing the performance with serial version for different topologies of network. Further the paper also throws light on some of the network reconfiguration techniques used to remodel the RDN due to high power losses in the network and floats an idea how parallel processing can be beneficial in enhancing already existing genetic algorithm based network reconfiguration technique to support real time load flow calculations and topology construction in smart grids and future generation systems.|2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC)|2016|10.1109/PDGC.2016.7913188|Rahul Saxena, Monika Jain, Ankit Mundra, D. P. Sharma|0.5555555555555556|4
29|Porting Batched Iterative Solvers onto Intel GPUs with SYCL|Batched linear solvers play a vital role in computational sciences, especially in the fields of plasma physics and combustion simulations. With the imminent deployment of the Aurora Supercomputer and other upcoming systems equipped with Intel GPUs, there is a compelling demand to expand the capabilities of these solvers for Intel GPU architectures. In this paper, we present our efforts in porting and optimizing the batched iterative solvers on Intel GPUs using the SYCL programming model. These new solvers achieve impressive performance on the Intel GPU Max 1550s (Ponte Vecchio GPUs) which surpass our previous CUDA implementation on NVIDIA H100 GPUs by an average of 2.4x for the PeleLM application inputs. The batched solvers are ready for production use in real-world scientific applications through the Ginkgo library, complementing the performance portability of the batched functionality of Ginkgo.|SC Workshops|2023|10.1145/3624062.3624181|Phuong Nguyen, Pratik Nayak, H. Anzt|0.5|4
58|Attribute-Aware RBFs: Interactive Visualization of Time Series Particle Volumes Using RT Core Range Queries|Smoothed-particle hydrodynamics (SPH) is a mesh-free method used to simulate volumetric media in fluids, astrophysics, and solid mechanics. Visualizing these simulations is problematic because these datasets often contain millions, if not billions of particles carrying physical attributes and moving over time. Radial basis functions (RBFs) are used to model particles, and overlapping particles are interpolated to reconstruct a high-quality volumetric field; however, this interpolation process is expensive and makes interactive visualization difficult. Existing RBF interpolation schemes do not account for color-mapped attributes and are instead constrained to visualizing just the density field. To address these challenges, we exploit ray tracing cores in modern GPU architectures to accelerate scalar field reconstruction. We use a novel RBF interpolation scheme to integrate per-particle colors and densities, and leverage GPU-parallel tree construction and refitting to quickly update the tree as the simulation animates over time or when the user manipulates particle radii. We also propose a Hilbert reordering scheme to cluster particles together at the leaves of the tree to reduce tree memory consumption. Finally, we reduce the noise of volumetric shadows by adopting a spatially temporal blue noise sampling scheme. Our method can provide a more detailed and interactive view of these large, volumetric, time-series particle datasets than traditional methods, leading to new insights into these physics simulations.|IEEE Transactions on Visualization and Computer Graphics|2023|10.1109/TVCG.2023.3327366|Patrick Shriwise, Valerio Pascucci, Stefan Zellmann, Alper Sahistan, N. Morrical|0.5|4
84|GATSim: Abstract timing simulation of GPUs|General-Purpose Graphic Processing Units (GPUs) have become an integral part of heterogeneous system architectures. Ever increasing complexities have made rapid, early performance evaluation of GPU-based architectures and applications a primary design concern. Traditional cycle-accurate GPU simulators are too slow, while existing analytical or source-level estimation approaches are often inaccurate. This paper proposes a novel abstract GPU performance simulation approach that is based on flexible separation of functional and timing models, combining a fast functional execution either on existing simulators or native GPU hardware with a light, fast and accurate abstract timing model. Micro-architecture timing of individual GPU cores is abstracted through static, one-time pre-characterization of code, and only the dynamic scheduling effects are simulated. Using a native GPU for functional execution and excluding pre-characterization, our GPU simulation achieves a throughput of more than 80 MIPS. This is on average 400x faster with 4% error compared to a cycle-accurate GPU simulator for standard GPU benchmarks. Moreover, our simple timing model provides flexibility to target different GPU configurations with little or no extra effort.|Design, Automation and Test in Europe|2017|10.23919/DATE.2017.7926956|Behzad Boroujerdian, A. Gerstlauer, Kishore Punniyamurthy|0.5|4
109|Characterizing and Optimizing Irregular Applications on Graphics Processing Units|In recent years, GPGPUs have experienced tremendous growth as general-purpose and high-throughput computing devices. Applications from various domains achieve significant speedups using GPGPUs. However, irregular applications do not perform well due to the mismatches between irregular problem structures and SIMD-like GPU architectures. The lack of in-depth characterization and quantifying the ways in which irregular applications differ from regular ones on GPGPUs has prevented users from effectively making use of the hardware resource. To characterize the performance aspects and analyze the bottlenecks, a suite of representative irregular applications are examined on a cycle-accurate GPU simulator as well as a real GPU. The experimental results identify control-flow divergences,||2015|10.1016/j.compfluid.2014.11.017|Tao Zhang|0.5|4
154|Efficient parallelization of SPH algorithm on modern multi-core CPUs and massively parallel GPUs|Smoothed Particle Hydrodynamics (SPH) is fast emerging as a practically useful computational simulation tool for a wide variety of engineering problems. SPH is also gaining popularity as the back bone for fast and realistic animations in graphics and video games. The Lagrangian and mesh-free nature of the method facilitates fast and accurate simulation of material deformation, interface capture, etc. Typically, particle-based methods would necessitate particle search and locate algorithms to be implemented efficiently, as continuous creation of neighbor particle lists is a computationally expensive step. Hence, it is advantageous to implement SPH, on modern multi-core platforms with the help of High-Performance Computing (HPC) tools. In this work, the computational performance of an SPH algorithm is assessed on multi-core Central Processing Unit (CPU) as well as massively parallel General Purpose Graphical Processing Units (GP-GPU). Parallelizing SPH faces several challenges such as, scalability of the neighbor search process, force calculations, minimizing thread divergence, achieving coalesced memory access patterns, balancing workload, ensuring optimum use of computational resources, etc. While addressing some of these challenges, detailed analysis of performance metrics such as speedup, global load efficiency, global store efficiency, warp execution efficiency, occupancy, etc. is evaluated. The OpenMP and Compute Unified Device Architecture[Formula: see text] parallel programming models have been used for parallel computing on Intel Xeon[Formula: see text] E5-[Formula: see text] multi-core CPU and NVIDIA Quadro M[Formula: see text] and NVIDIA Tesla p[Formula: see text] massively parallel GPU architectures. Standard benchmark problems from the Computational Fluid Dynamics (CFD) literature are chosen for the validation. The key concern of how to identify a suitable architecture for mesh-less methods which essentially require heavy workload of neighbor search and evaluation of local force fields from neighbor interactions is addressed.|Advances in Complex Systems|2021|10.1142/s1793962321500549|V. Sanapala, R. Nasre, B. Patnaik, Pravin Jagtap|0.5|4
107|Using colored petri nets for GPGPU performance modeling|Performance analysis and modeling of applications running on GPUs is still a challenge for most designers and developers. State-of-the-art solutions are dominated by two classic approaches: statistical models that require a lot of training and profiling on existing hardware, and analytical models that require in-depth knowledge of the hardware platform and significant calibration. Both these classes separate the application from the hardware and attempt a high-level combination of the two models for performance prediction. In this work, we propose an orthogonal approach, based on high-level simulation. Specifically, we use Colored Petri Nets (CPN) to model both the hardware and the application. Using this model, the execution of the application is a simulation of the CPN model using warps as tokens. Our prototype implementation of this modeling approach demonstrates promising results on a few case studies on two different GPU architectures: both reasonably accurate predictions and detailed execution information are obtained. We conclude that CPN-based GPU performance modeling is an elegant solution for systematic performance prediction, and we focus further on optimizing the models to improve the execution time of the symbolic simulation.|Conf. Computing Frontiers|2016|10.1145/2903150.2903167|A. Varbanescu, C. D. Laat, S. Madougou|0.4444444444444444|4
125|Simulating PCI-Express Interconnect for Future System Exploration|The PCI-Express interconnect is the dominant interconnection technology within a single computer node that is used for connecting off-chip devices such as network interface cards (NICs) and GPUs to the processor chip. The PCI-Express bandwidth and latency are often the bottleneck in the processor, memory and device interactions and impacts the overall performance of the connected devices. Architecture simulators often focus on modeling the performance of processor and memory and lack a performance model for the I/O devices and interconnections. In this work, we implement a flexible and detailed model for the PCI-Express interconnect in a widely known architecture simulator. We also implement a PCI-Express device model that is configured by a PCI-Express device driver. We validate our PCI-Express interconnect performance against a physical Gen 2 PCI-Express link. Our evaluation results show that the PCI-Express model bandwidth is within 19.0% of the physical setup. We use our model to evaluate different PCI-Express link widths and latency and show its impact on the overall I/O performance of an I/O intensive application.|IEEE International Symposium on Workload Characterization|2018|10.1109/IISWC.2018.8573496|Mohammad Alian, N. Kim, K. Srinivasan|0.42857142857142855|4
80|Developing Efficient Discrete Simulations on Multicore and GPU Architectures|In this paper we show how to efficiently implement parallel discrete simulations on multicore and GPU architectures through a real example of an application: a cellular automata model of laser dynamics. We describe the techniques employed to build and optimize the implementations using OpenMP and CUDA frameworks. We have evaluated the performance on two different hardware platforms that represent different target market segments: high-end platforms for scientific computing, using an Intel Xeon Platinum 8259CL server with 48 cores, and also an NVIDIA Tesla V100 GPU, both running on Amazon Web Server (AWS) Cloud; and on a consumer-oriented platform, using an Intel Core i9 9900k CPU and an NVIDIA GeForce GTX 1050 TI GPU. Performance results were compared and analyzed in detail. We show that excellent performance and scalability can be obtained in both platforms, and we extract some important issues that imply a performance degradation for them. We also found that current multicore CPUs with large core numbers can bring a performance very near to that of GPUs, and even identical in some cases.||2020|10.3390/electronics9010189|M. López-Torres, D. Cagigas-Muñiz, J. Guisado, F. Díaz-del-Río, F. Jiménez-Morales|0.4|4
82|LISFLOOD-FP 8.0: the new discontinuous Galerkin shallow-water solver for multi-core CPUs and GPUs|Abstract. LISFLOOD-FP 8.0 includes second-order discontinuous Galerkin (DG2) and first-order finite-volume (FV1) solvers of the two-dimensional shallow-water equations for modelling a wide range of flows, including rapidly propagating, supercritical flows, shock waves or flows over very smooth surfaces.\nThe solvers are parallelised on multi-core CPU and Nvidia GPU architectures and run existing LISFLOOD-FP modelling scenarios without modification.\nThese new, fully two-dimensional solvers are available alongside the existing local inertia solver (called ACC), which is optimised for multi-core CPUs and integrates with the LISFLOOD-FP sub-grid channel model.\nThe predictive capabilities and computational scalability of the new DG2 and FV1 solvers are studied for two Environment Agency benchmark tests and a real-world fluvial flood simulation driven by rainfall across a 2500 km2 catchment.\nDG2's second-order-accurate, piecewise-planar representation of topography and flow variables enables predictions on coarse grids that are competitive with FV1 and ACC predictions on 2–4 times finer grids, particularly where river channels are wider than half the grid spacing.\nDespite the simplified formulation of the local inertia solver, ACC is shown to be spatially second-order-accurate and yields predictions that are close to DG2. The DG2-CPU and FV1-CPU solvers achieve near-optimal scalability up to 16 CPU cores and achieve greater efficiency on grids with fewer than 0.1 million elements. The DG2-GPU and FV1-GPU solvers are most efficient on grids with more than 1 million elements, where the GPU solvers are 2.5–4 times faster than the corresponding 16-core CPU solvers. LISFLOOD-FP 8.0 therefore marks a new step towards operational DG2 flood inundation modelling at the catchment scale.\nLISFLOOD-FP 8.0 is freely available under the GPL v3 license, with additional documentation and case studies at https://www.seamlesswave.com/LISFLOOD8.0 (last access: 2 June 2021).\n|Geoscientific Model Development|2020|10.5194/gmd-2020-340|M. Sharifian, G. Kesserwani, P. Bates, James Shaw, J. Neal|0.4|4
119|SLATE: Managing Heterogeneous Cloud Functions|This paper presents SLATE, a fully-managed, heterogeneous Function-as-a-Service (FaaS) system for deploying serverless functions onto heterogeneous cloud infrastructures. We extend the traditional homogeneous FaaS execution model to support heterogeneous functions, automating and abstracting runtime management of heterogeneous compute resources in order to improve cloud tenant accessibility to specialised, accelerator resources, such as FPGAs and GPUs. In particular, we focus on the mechanisms required for heterogeneous scaling of deployed function instances to guarantee latency objectives while minimising cost. We develop a simulator to validate and evaluate our approach, considering case-study functions in three application domains: machine learning, bio-informatics, and physics. We incorporate empirically derived performance models for each function implementation targeting a hardware platform with combined computational capacity of 24 FPGAs and 12 CPU cores. Compared to homogeneous CPU and homogeneous FPGA functions, simulation results achieve respectively a cost improvement for non-uniform task traffic of up to 8.7 times and 1.7 times, while maintaining specified latency objectives.|IEEE International Conference on Application-Specific Systems, Architectures, and Processors|2020|10.1109/ASAP49362.2020.00032|Eriko Nurvitadhi, Mishali Naik, W. Luk, J. Coutinho, Jessica Vandebon|0.4|4
142|PAQSIM: Fast Performance Model for Graphics Workload on Mobile GPUs|As the popularity of GPU in embedded systems keeps increasing, there is a growing demand for performance models for rapid estimation and tuning. One major challenge of developing a GPU performance model is the balance between accuracy and speed. The analytical model and the architectural model, two prevailing performance models, both have their weaknesses. The analytical model is fast to execute and simple to implement but usually suffers from low simulation accuracy. On the other hand, the cycle-level architectural model can offer high accuracy, but often at the expense of the execution time. In this work, we present a hybrid performance model for core-level performance studies. Our model takes advantage of the speed of the analytical model and the accuracy of the cycle-level architectural model. We model the resource contention as in traditional architectural models but reduce the pipeline stages when no contention is expected. The graphics workloads have shown uniform characteristics, which allows us to replace some detailed simulation with analytical models for latency estimation in key events such as memory accesses, texture fetches, and synchronizations. Such design greatly reduces the simulation time while maintains decent simulation accuracy. We evaluate our performance model against commercial mobile GPUs. The experiments using graphics workloads from popular games show great simulation speed and high accuracy in predicting the GPU performance. For simulations using the aggressive mode, the simulator can achieve an average 4.1x slowdown, with an average error rate at 6% and the peak error rate at 27.9%.|ACM SIGPLAN Conference on Languages, Compilers, and Tools for Embedded Systems|2020|10.1145/3372799.3394359|C. Lim, Xiang Gong, Chunling Hu|0.4|4
148|Massive Scaling of MASSIF: Algorithm Development and Analysis for Simulation on GPUs|Micromechanical Analysis of Stress-Strain Inhomogeneities with Fourier transforms (MASSIF) is a large-scale Fortran-based differential equation solver used to study local stresses and strains in materials. Due to its prohibitive memory requirements, it is extremely difficult to port the code to GPUs with small on-device memory. In this work, we present an algorithm design that uses domain decomposition with approximate convolution, which reduces memory footprint to make the MASSIF simulation feasible on distributed GPU systems. A first-order performance model of our method estimates that compression and multi-resolution sampling strategies can enable domain computation within GPU memory constraints for 3D grids larger than those simulated by the current state-of-the-art Fortran MPI implementation. The model analysis also provides an insight into design requirements for further scalability. Lastly, we discuss the extension of our method to irregular domain decomposition and challenges to be tackled in the future.|Platform for Advanced Scientific Computing Conference|2020|10.1145/3394277.3401857|Jelena Kovacevic, Anuva Kulkarni, F. Franchetti|0.4|4
62|Real-Time Simulation and Optimization of Elastic Aircraft Vehicle Based on Multi-GPU Workstation|Modern aircraft such as missile and rocket, due to the large slenderness ratio of slender body vehicles, the influence of elastic deformation and vibration on navigation, guidance, and engine modules in simulation can not be ignored. For the problems of slow calculation speed and incapability of real-time simulation for time-domain simulation, by analyzing the time proportion of each calculation step under different computing scale, the dynamic parallel construction of octree is used to represent the aerodynamic parameter table under the environment of single and multi GPU. Meanwhile, an innovative parallel algorithm of element stiffness matrix based on finite element model is designed in GPU architecture. Accordingly, the optimized performance is enhanced through the adaptive hardware resources and rational use of shared memory. Furthermore, A multi-threaded asynchronous framework based on task queue and thread pool is proposed to realize the parallel task calculation with different granularities. The numerical result shows that the acceleration ratio of about 20 times in the single GPU condition can be obtained, and the acceleration ratio of at least 30 times can be obtained by the parallel computing of dual GPUs, enabling the real-time simulation of the flexible aircraft with 1200 elements within 20ms.|IEEE Access|2019|10.1109/ACCESS.2019.2946684|Liu Xingguo, Binxing Hu|0.3333333333333333|4
104|GPU accelerated Monte-Carlo simulation of SEM images for metrology|In this work we address the computation times of numerical studies in dimensional metrology. In particular, full Monte-Carlo simulation programs for scanning electron microscopy (SEM) image acquisition are known to be notoriously slow. Our quest in reducing the computation time of SEM image simulation has led us to investigate the use of graphics processing units (GPUs) for metrology. We have succeeded in creating a full Monte-Carlo simulation program for SEM images, which runs entirely on a GPU. The physical scattering models of this GPU simulator are identical to a previous CPU-based simulator, which includes the dielectric function model for inelastic scattering and also refinements for low-voltage SEM applications. As a case study for the performance, we considered the simulated exposure of a complex feature: an isolated silicon line with rough sidewalls located on a at silicon substrate. The surface of the rough feature is decomposed into 408 012 triangles. We have used an exposure dose of 6 mC/cm2, which corresponds to 6 553 600 primary electrons on average (Poisson distributed). We repeat the simulation for various primary electron energies, 300 eV, 500 eV, 800 eV, 1 keV, 3 keV and 5 keV. At first we run the simulation on a GeForce GTX480 from NVIDIA. The very same simulation is duplicated on our CPU-based program, for which we have used an Intel Xeon X5650. Apart from statistics in the simulation, no difference is found between the CPU and GPU simulated results. The GTX480 generates the images (depending on the primary electron energy) 350 to 425 times faster than a single threaded Intel X5650 CPU. Although this is a tremendous speedup, we actually have not reached the maximum throughput because of the limited amount of available memory on the GTX480. Nevertheless, the speedup enables the fast acquisition of simulated SEM images for metrology. We now have the potential to investigate case studies in CD-SEM metrology, which otherwise would take unreasonable amounts of computation time.|SPIE Advanced Lithography|2016|10.1117/12.2219160|T. Verduin, S. Lokhorst, C. W. Hagen|0.3333333333333333|4
122|Validated Thermal Air Management Simulations of Data Centers Using Remote Graphics Processing Units|Simulation tools for thermal management of data centers help to improve layout of new builds or analyse thermal problems in existing data centers. The development of LBM on remote GPUs as an approach for such simulations is discussed making use of VirtualGL and prioritised multi-threaded implementations of an existing LBM code. The simulation is configured to model an existing and highly monitored test data center. Steady-state root mean square averages of measured and simulated temperatures are compared showing good agreement. The full capability of this simulation approach is demonstrated when comparing rack temperatures against a time varying workload, which employs time-dependent boundary conditions.|Annual Conference of the IEEE Industrial Electronics Society|2018|10.1109/IECON.2018.8591192|N. Delbosc, Mattias Vesterlund, J. Summers, Johannes Sjolund, Amirul Khan|0.2857142857142857|4
72|VLAG: A very fast locality approximation model for GPU kernels with regular access patterns|Performance modeling plays an important role for optimal hardware design and optimized application implementation. This paper presents a very low overhead performance model, called VLAG, to approximate the data localities exploited by GPU kernels. VLAG receives source code-level information to estimate per memory-access instruction, per data array, and per kernel localities within GPU kernels. VLAG is only applicable to kernels with regular memory access patterns. VLAG was experimentally evaluated using an NVIDIA Maxwell GPU. For two different Matrix Multiplication kernels, the average errors of 7.68% and 6.29%, was resulted, respectively. The slowdown of VLAG for MM was measured 1.4X which, comparing with other approaches such as trace-driven simulation, is negligible.|International Conference on Computer and Knowledge Engineering|2017|10.1109/ICCKE.2017.8167887|Mohsen Kiani, Amir Rajabzadeh|0.25|4
123|M2S-CGM: A Detailed Architectural Simulator for Coherent CPU-GPU Systems|We introduce M2S-CGM a detailed architectural simulator that models the interactions between CPUs and GPUs operating in coherent heterogeneous compute environments. M2S-CGM extends an existing and established x86 CPU model and Southern Islands GPU model, adds a new custom-built memory system model and switching fabric called CGM, and incorporates a well-known SDRAM model. The CGM memory system simulator provides configurable entire system simulation and can support a range of non-coherent and coherent CPU-GPU configurations. M2S-CGM supports the runtime for OpenCL-based benchmarks in addition to traditional multithreaded CPU benchmarks and can run benchmarks from established heterogeneous benchmark collections. This allows us to experiment with different coherent CPU-GPU configurations and propose effective future improvements in these systems. We present the makeup of M2S-CGM's software architectural design, provide a validation of the simulator, and provide coherent CPU-GPU execution results. Our validation results show average differences between our physical test system and M2S-CGM, of 10.4%, 22%, and 6.4% for 2 threaded, 4 threaded, and heterogeneous benchmark runs respectively. Our coherent CPU-GPU experimental results show an average speedup of 2.8 for our benchmarks over the baseline noncoherent system.|ICCD|2017|10.1109/ICCD.2017.84|Christopher E. Giles, Mark A. Heinrich|0.25|4
103|Implementation of a pre-calculated database approach for scatter correction in SPECT|In single photon emission computed tomography (SPECT), attenuation and scatter introduce important artefacts in the reconstructed images leading to mis diagnosis for patient’s follow-up. Furthermore, by using Monte Carlo simulation (MCS), physical effects undergone by photons during the SPECT exam can be precisely modeled and accounted for during iterative reconstruction, which improves the quality of the image. However, MCS are time consuming and therefore inappropriate for the rate of daily exams performed in clinical routine. Our work is based on the assumption that patients are composed of identical biological tissues and that photon propagation in an element volume of a given tissue is similar and reproducible from one subject to another. We hence propose to accelerate modeling of the physical effects occurring in emission tomography making it adequate for daily exam by using the approach of scatter pre-calculated database. The developed efficient patient-dependent attenuation and scatter correction were implemented on a GPU architecture on a state-of-art single-processor workstation and yielded to a speed-up factor in time computing of four orders of magnitude. Results presented in this proof of concept study are in good agreement with full MCS.||2016|10.1088/2057-1976/2/5/055014|Benjamin Auer, Z. E. Bitar, Clement Rey, V. Bekaert, J. Gallone|0.2222222222222222|4
49|Green High Performance Simulation for AMB models of Aedes aegypti|The increase in temperature caused by the climate change has resulted in the rapid dissemination of infectious diseases. Given the alert for the current situation, the World Health Organization (WHO) has declared a state of health emergency, highlighting the severity of the situation in some countries. For this reason, coming up with knowledge and tools that can help control and eradicate the vectors propagating these diseases is of the utmost importance. High-performance modeling and simulation can be used to produce knowledge and strategies that allow predicting infections, guiding actions and/or training health/civil protection agents. The model developed as part of this research work is aimed at assisting the decision-making process for disease prevention and control, as well as evaluating the reproduction and predicting the evolution of the Aedes aegypti mosquito, which is the transmitting vector of the dengue, Zika and chikungunya diseases. Since a large number of simulation runs are required to achieve results with statistical variability, GPU has been used. This platform has enough computational power to reduce execution time while maintaining a lower energy consumption. Different scenarios and experiments are proposed to corroborate the benefits of the architecture proposed.|Journal of Computational Science and Technology|2020|10.24215/16666038.20.e02|R. Suppi, Laura Cristina De Gisuti, M. Naiouf, Erica Soledad Montes de Oca|0.2|4
106|Efficient simulations of spiking neurons on parallel and distributed platforms: Towards large-scale modeling in computational neuroscience|Human brain communicates information by means of electro-chemical reactions and processes it in a parallel, distributed manner. Computational models of neurons at different levels of details are used in order to make predictions for physiological dysfunctions. Advances in the field of brain simulations and brain computer interfaces have increased the complexity of this modeling process. With a focus to build large-scale detailed networks, we used high performance computing techniques to model and simulate the granular layer of the cerebellum. Neuronal firing patterns of cerebellar granule neurons were modeled using two mathematical models Hodgkin-Huxley (HH) and Adaptive Exponential Leaky Integrate and Fire (AdEx). The performance efficiency of these modeled neurons was tested against a detailed multi-compartmental model of the granule cell. We compared different schemes suitable for large scale simulations of cerebellar networks. Large networks of neurons were constructed and simulated. Graphic Processing Units (GPU) was employed in the pleasantly parallel implementation while Message Passing Interface (MPI) was used in the distributed computing approach. This allowed to explore constraints of different parallel architectures and to efficiently load balance the tasks by maximally utilizing the available resources. For small scale networks, the observed absolute speedup was 6X in an MPI based approach with 32 processors while GPUs gave 10X performance gain compared to a single CPU implementation. In large networks, GPUs gave approximately 5X performance gain in processing time compared to the MPI implementation. The results enabled us to choose parallelization schemes suitable for large-scale simulations of cerebellar circuits. We are currently extending the network model based on large scale simulations evaluated in this paper and using a hybrid - heterogeneous MPI based multi-GPU architecture for incorporating millions of cerebellar neurons for assessing physiological disorders in such circuits.|IEEE Recent Advances in Intelligent Computational Systems|2015|10.1109/RAICS.2015.7488425|Revathy S. Kumar, Manjusha Nair, Shyam Diwakar, B. Nair, S. Surya|0.2|4
124|Improving accuracy of source level timing simulation for GPUs using a probabilistic resource model|After their success in the high performance and desktop market, Graphic Processing Units (GPUs), that can be used for general purpose computing are introduced for embedded systems on a chip (SOCs). Due to some advanced architectural features, like massive simultaneous multithreading, static performance analysis and high-level timing simulation are difficult to apply to code running on these systems. This paper extends a method for performance simulation of GPUs. The method uses automated performance annotations in the application's OpenCL C source code, and an extended performance model for derivation of a kernels runtime from metrics produced by the execution of annotated kernels. The final results are then generated using a probabilistic resource conflict model. The model reaches an accuracy of 90% on most test cases and delivers a higher average accuracy than previous methods.|International Conference / Workshop on Embedded Computer Systems: Architectures, Modeling and Simulation|2015|10.1109/SAMOS.2015.7363655|Christoph Gerum, W. Rosenstiel, O. Bringmann|0.2|4
56|Parallel computing algorithm for real-time mapping between large-scale networks|In this paper, we propose a scalable massively-parallel algorithm to solve the general mapping problem in large-scale networks in real-time. The proposed parallel algorithm takes advantage of GPU architecture and launches millions of workers to calculate values on a target network simultaneously. Threads are managed through the SIMT execution model and target values are updated through atomic operations. Our experiments show the proposed algorithm can accomplish network mapping (find importance weights for links in a real-world large-scale shared-mobility network) with more than 2 million weights within 1.82 µs (microsecond-level), which is truly real-time. The algorithm performance suggests that mapping computations may no longer be the bottleneck in highly dynamic network-centered problems, as the computations can be completed faster than the solid state drive (SSD) read access latency. Compared to serial algorithms, the speedup is more than 12,000 times. The proposed algorithm is also scalable. Results on simulated data show that even when the network size grows exponentially, microsecond-level computing performance can still be obtained, and even more than 190,000 times speedup can be achieved. The proposed algorithm can serve as a cornerstone for ultra-fast processing of highly dynamic large-scale networks.|International Conference on Intelligent Transportation Systems|2019|10.1109/ITSC.2019.8917463|Ethan Zhang, Amirmahdi Tafreshian, Neda Masoud|0.16666666666666666|4
68|Projections of achievable performance for Weather & Climate Dwarfs, and for entire NWP applications, on hybrid architectures|This document is one of the deliverable reports created for the ESCAPE project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather Prediction at Exascale. The project develops world-class, extreme-scale computing capabilities for European operational numerical weather prediction and future climate models. This is done by identifying Weather & Climate dwarfs which are key patterns in terms of computation and communication (in the spirit of the Berkeley dwarfs). These dwarfs are then optimised for different hardware architectures (single and multi-node) and alternative algorithms are explored. Performance portability is addressed through the use of domain specific languages. \nThis deliverable contains the description of the performance and energy models for the selected Weather & Climate dwarfs for different hardware architectures, multinode with GPU accelerators in particular. Presented performance models are extension to model provided in Deliverable 3.2. With some further enhancements, they are incorporated in the DCworms simulator. In particular, extended models allow to predict computational and energy performance on different architectures: single and multinodes, equipped with CPUs and GPUs accelerators. This allows to provide feasible performance projection at system scale.|arXiv.org|2019|10.2139/ssrn.3432809|Marek Blazewicz, Sebastian Ciesielski, M. Kulczewski|0.16666666666666666|4
140|Gravitational Octree Code Performance Evaluation on Volta GPU|In this study, the gravitational octree code originally optimized for the Fermi, Kepler, and Maxwell GPU architectures is adapted to the Volta architecture. The Volta architecture introduces independent thread scheduling requiring either the insertion of the explicit synchronizations at appropriate locations or the enforcement of the same implicit synchronizations as do the Pascal or earlier architectures by specifying -gencode arch=compute_60,code=sm_70. The performance measurements on Tesla V100, the current flagship GPU by NVIDIA, revealed that the N-body simulations of the Andromeda galaxy model with 223 = 8 388 608 particles took 3.8 × 10-2 s or 3.3 × 10-2 s per step for cases without or with the implicit synchronizations, respectively. Tesla V100 achieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the flagship GPU in the previous generation. The observed speed-up of 2.2 is greater than 1.5, which is the ratio of the theoretical peak performance of the two GPUs. The independence of the units for integer operations from those for floating-point number operations enables the overlapped execution of integer and floating-point number operations. It hides the execution time of the integer operations leading to the speed-up rate above the theoretical peak performance ratio. Tesla V100 can execute N-body simulation with up to 25 × 220 = 26 214 400 particles, and it took 2.0 × 10-1 s per step. It corresponds to 3.5 TFlop/s, which is 22% of the single-precision theoretical peak performance.|International Conference on Parallel Processing|2018|10.1145/3337821.3337845|Yohei Miki|0.14285714285714285|4
32|Algorithms and Software for High-Performance Fracture Simulation on GPU Architectures|"Author(s): Lim, Rone Kwei | Advisor(s): Petzold, Linda R | Abstract: Computer simulation of fracture in materials with nonlinear mechanical response can be computationally expensive. These simulations often require a large number of degrees of freedom, and the nonlinearity in the problem can pose difficulties when computing solutions. This work focuses on two material models. The first model consists of rigid bricks interacting through nonlinear cohesive springs. Fracture in the material occurs through the rupture of the cohesive springs. The second, more complicated, model consists of deformable elements interacting through nonlinear cohesive springs. In the first model, we assume the bricks are under a quasi-static loading scenario. With this assumption, the problem can be solved using a global Monte Carlo minimization algorithm to minimize the energy of the system. The energy in the system comes from the deformation and rupture of the nonlinear cohesive springs. Since these simulations have a high computational cost, we have developed a GPU-based (Graphics Processing Unit) Monte Carlo minimization algorithm that offers a significant speedup compared to a conventional multithreaded CPU-based algorithm. With the second model, we have dynamic simulations with explicit time discretization. In this case we compute the force, acceleration, velocity, and position explicitly. The force in the system comes from both the deformation of the elements as well as the deformation of the nonlinear cohesive springs. We have developed explicit, CPU-based methods and implicit-explict methods on both CPUs and GPUs. Our implicit-explict GPU-based method achieves substantial performance improvement compared to the explicit, CPU-based method. We present our GPU-based implementation of AES (Advanced Encryption Standard), which is used in the Monte Carlo minimization algorithm to generate random numbers. Our implementation is substantially faster than CPU-based implementation of AES. It is also faster than previous GPU implementations of AES."||2017|10.5977/jkasne.2017.23.3.330|R. K. Lim|0.125|4
41|An Out-of-Core Method for Physical Simulations on a Multi-GPU Architecture Using Lattice Boltzmann Method|Simulating complex physical phenomena implies the manipulation of an important amount of data. In order to simulate very large simulation domains on a limited computing architecture, such as industrial infrastructures, solutions have to be proposed. In this paper, a new out-of-core method is introduced in order to perform fast physical simulations using a complex Lattice Boltzmann model (LBM) on a single-node multi-GPU (CUDA) architecture. GPU global memory generally is far lower than the CPU main memory, can be problematic for a large simulation domain. The objective of this paper is to propose an efficient method of data exchanges between GPUs, the CPU main memory, which allows to perform fast complex simulations on large installations. The combination of this method with the massive parallelism of GPUs allows to keep good simulation performance. A complex simulation involving two physical components (water + air) is used in order to validate this method.|2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)|2016|10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0099|J. Duchateau, C. Renaud, G. Roussel, N. Maquignon, F. Rousselle|0.1111111111111111|4
120|Energy and Performance Prediction of CUDA Applications using Dynamic Regression Models|Many emerging supercomputers and future exa-scale computing machines require accelerator-based GPU computing architectures for boosting their computing performances. CUDA is one of the widely applied GPGPU parallel computing platform for those architectures owing to its better performance for certain scientific applications. However, the emerging rise in the development of CUDA applications from various scientific domains, such as, bioinformatics, HEP, and so forth, has urged the need for tools that identify optimal application parameters and the other GPGPU architecture metrics, including work group size, work item, memory utilization, and so forth. In fact, the tuning process might end up with several executions of various possible code variants. This paper proposed Dynamic Regression models, namely, Dynamic Random Forests (DynRFM), Dynamic Support Vector Machines (DynSVM), and Dynamic Linear Regression Models (Dyn LRM) for the energy/performance prediction of the code variants of CUDA applications. The prediction was based on the application parameters and the performance metrics of applications, such as, number of instructions, memory issues, and so forth. In order to obtain energy/performance measurements for CUDA applications, EACudaLib (a monitoring library implemented in EnergyAnalyzer tool) was developed. In addition, the proposed Dynamic Regression models were compared to the classical regression models, such as, RFM, SVM, and LRM. The validation results of the proposed dynamic regression models, when tested with the different problem sizes of Nbody and Particle CUDA simulations, manifested the energy/performance prediction improvement of over 50.26 to 61.23 percentages.|International Symposium on Electronic Commerce|2016|10.1145/2856636.2856643|S. Benedict, R. Rejitha, Suja A. Alex|0.1111111111111111|4
147|Fast Race Detection and Profiling Framework for Heterogeneous System|Heterogeneous computing is a growing trend in recent computer architecture design and is often used to improve the performance and power efficiency for computing applications by utilizing the special-purpose processors or accelerators, such as the Graphic Computing Unit (GPU), Field Programmable Gate Array (FPGA) and Digital Signal Processor (DSP). With the increase of complexity, the interaction among accelerators and processors could be deadfall if a race condition happens. However, the existing tools for such task are either too slow or hard to extend the race condition detection mechanism. Therefore, tools for application profiling with approximate timing model are important to the design of such heterogeneous systems in a timing manner. In this paper, we proposed a pluggable GPU interface on an existing timing approximate CPU simulator based on QEMU for analyzing the memory behavior of heterogeneous systems. Monitoring the memory behavior, the pluggable interface could be extended to any kinds of accelerators, such as GPU, DSP and FPGA, for race condition detection. Taking the GPU as an example, we integrated the detailed GPU simulator from Multi2Sim with the existing timing approximate CPU simulator, VPA, to showcase the efficiency of the proposed work. The experimental results showed that the emulation speed of the proposed framework reached at most 9x faster than Multi2Sim with acceptable timing results accuracy which is less than 20% error rate from our previous work. In addition, the race condition detection mechanism further indicates the problematic memory accesses to user.|International Conference on Supercomputing|2016|10.1109/ICS.2016.0110|Chenggang Lai, Shih-Hao Hung, C. Yeh|0.1111111111111111|4
83|Communication efficient work distributions in stencil operation based applications|In recent years, the use of accelerators in conjunction with CPUs, known as heterogeneous computing, has brought about significant performance increases for scientific applications. One of the best examples of this is lattice quantum chromodynamics (QCD), a stencil operation based simulation. These simulations have a large memory footprint necessitating the use of many graphics processing units (GPUs) in parallel. This requires the use of a heterogeneous cluster with one or more GPUs per node. In order to obtain optimal performance, it is necessary to determine an efficient communication pattern between GPUs on the same node and between nodes. In this paper, we present a performance model based method for minimizing the communication time of applications with stencil operations, such as lattice QCD, on heterogeneous computing systems with a non‐blocking InfiniBand interconnection network. The proposed method is able to increase the performance of the most computationally intensive kernel of lattice QCD by 25% due to improved overlapping of communication and computation. We also demonstrate that the aforementioned performance model and efficient communication patterns can be used to determine a cost efficient heterogeneous system design for stencil operation based applications. Copyright © 2014 John Wiley & Sons, Ltd.|Concurrency and Computation|2015|10.1002/cpe.3210|T. El-Ghazawi, L. Ríha, J. Schneible, A. Alexandru, Maria Malik|0.1|4
139|Asynchronous OpenCL/MPI numerical simulations of conservation laws|Hyperbolic conservation laws are important mathematical models for describing many phenomena in physics or engineering. The Finite Volume (FV) method and the Discontinuous Galerkin (DG) methods are two popular methods for solving conservation laws on computers. Those two methods are good candidates for parallel computing: • they require a large amount of uniform and simple computations, • they rely on explicit time-integration • they present regular and local data access pattern. In this paper, we present several FV and DG numerical sim- ulations that we have realized with the OpenCL and MPI paradigms. First, we compare two optimized implementations of the FV method on a regular grid: an OpenCL implementation and a more traditional OpenMP implementation. We compare the efficiency of the approach on several CPU and GPU architectures of different brands. Then we give a short presentation of the DG method. Finally, we present how we have implemented this DG method in the OpenCL/MPI framework in order to achieve high efficiency. The implementation relies on a splitting of the DG mesh into sub-domains and sub-zones. Different kernels are compiled according to the zones properties. In addition, we rely on the OpenCL asynchronous task graph in order to overlap OpenCL computations, memory transfers and MPI communications.|Software for Exascale Computing|2015|10.1145/2791321.2791325|T. Strub, M. Roberts, P. Helluy, M. Massaro|0.1|4
150|Finite element numerical integration for first order approximations on multi-core architectures|The paper presents investigations on the implementation and performance of the finite element numerical integration algorithm for first order approximations and three processor architectures, popular in scientific computing, classical CPU, Intel Xeon Phi and NVIDIA Kepler GPU. A unifying programming model and portable OpenCL implementation is considered for all architectures. Variations of the algorithm due to different problems solved and different element types are investigated and several optimizations aimed at proper optimization and mapping of the algorithm to computer architectures are demonstrated. Performance models of execution are developed for different processors and tested in practical experiments. The results show the varying levels of performance for different architectures, but indicate that the algorithm can be effectively ported to all of them. The general conclusion is that the finite element numerical integration can achieve sufficient performance on different multiand many-core architectures and should not become a performance bottleneck for finite element simulation codes.|arXiv.org|2015|10.1115/omae2015-42101|K. Banas, J. Bielanski, Filip Kruzel|0.1|4
28|Transient-simulation guided graph sparsification approach to scalable Harmonic Balance (HB) analysis of post-layout RF circuits leveraging heterogeneous CPU-GPU computing systems|Harmonic Balance (HB) analysis is key to efficient verification of large post-layout RF and microwave integrated circuits (ICs). This paper introduces a novel transient-simulation guided graph sparsification technique, as well as an efficient runtime performance modeling approach tailored for heterogeneous manycore CPU-GPU computing system to build nearly-optimal subgraph preconditioners that can lead to minimum HB simulation runtime. Additionally, we propose a novel heterogeneous parallel sparse block matrix algorithm by taking advantages of the structure of HB Jacobian matrices as well as GPU's streaming multiprocessors to achieve optimal work load balancing during the preconditioning phase of HB analysis. We also show how the proposed preconditioned iterative algorithm can efficiently adapt to heterogeneous computing systems with different CPU and GPU computing capabilities. Extensive experimental results show that our HB solver can achieve up to 20X speedups and 5X memory reduction when compared with the state-of-the-art direct solver highly optimized for eight-core CPUs.|Design Automation Conference|2015|10.1145/2744769.2744920|Zhuo Feng, Lengfei Han|0.0|4
31|A GPU-Accelerated Hydrodynamics Solver For Atmosphere-Fire Interactions|A fundamental process to understand fire spread is the atmospheric flow. Building computational tools to simulate this complex flow has several challenges including boundary layer effects, resolving vegetation and the forest canopies, conserving fluid mass, and incorporating fire-induced flows. We develop a two-dimensional hydrodynamic solver that models fire-induced flow as a convective sink that converts the two-dimensional horizontal flow into a vertical flow through the buoyant plume. The resulting equations are the two-dimensional Navier-Stokes equations, but with point source delta functions appearing in the conservation of mass equation. We develop a projection method to solve these equations and implement them on a GPU architecture. The ultimate goalis to simulate wildfire spread faster than real-time, and with the ability for users to introduce real-time updates in an augmented reality sandbox.|SIGGRAPH Posters|2022|10.1145/3532719.3543263|B. Quaife, K. Speer, Jhamieka Greenwood|0.0|4
34|Reliability issues in GPGPUs|"The present work discusses approaches for implementing software redundancy schemes using the open source GPGPU model FlexGrip to increase the reliability of a GPGPU. ?? ??Most GPUs do not feature hardware support for error detection, and a device such as GPGPU a corrupt result could be unacceptable, as applications such as machine vision rely on the correctness of the processed image. A fault could occur at any time during the operation of the device, and it's critical that it is either masked or detected. Therefore improving the fidelity of GPGPU using software redundancy seems to be the only way to avoid errors. ?? ??In this work of thesis several approaches for matrix multiplication were produced, recording the performance of each; The three approaches differ in the method by which they guarantee the correct result. The first case is double comparison (DWC) which implies repeatedly performing operations and comparing the results, in case they are equal the correct result is stored in memory. ??The second method is the TMR. It is based on the triplication of resources and a voter who establishes by a majority which element is the correct one. The last method studied is ABFT which through comparisons identifies in which cell the error occurred and corrects it. ?? ??Each code was tested on the FlexGrip model after the injection of static faults inside the register file of each streaming multiprocessor. The expected result of each program obtained in simulation - the ""golden output"" - was compared to the same result in presence of injected static faults. ?? ??Results were finally collected and the fault coverage analysed, along with the time required and memory space. Future tests may be performed with different fault models, such as transient or delay faults, since the behaviour of the circuit would vary unpredictably."||2019|10.4324/9780429276316-4|Chiara Penaglia|0.0|4
35|BcBench: Exploring Throughput Processor Designs based on Blockchain Benchmarking|Benchmark suites have become the most important wheel to drive the research of GPU architecture designs. The most popular GPU benchmark suites, such as Rodinia and Parboil, cover a wide range of real-world applications, which provide a good reference for the researchers to estimate the performance behaviors of the GPU devices in production. However, the existing GPU benchmarks were initially proposed a decade ago, which unfortunately cannot reflect the unique features of the latest GPU applications accurately. Therefore, it is inappropriate to employ the stale benchmarks as the reference to evaluate the state-of-the-art GPU architectures. Tackling this challenge, we propose BcBench, a new set of GPU workloads collected from the emerging blockchain applications. BcBench is designed for estimating the performance behaviors of the future GPU architectures in accelerating the execution of the emerging applications. To this end, we have ported BcBench to the popular GPU simulators (e.g., GPGPU-Sim) for in-depth performance analysis. We first characterize the blockchain workloads at the micro-architecture level of GPUs fully by leveraging the adequate statistics generated by the GPU simulator. We then conclude five key observations from the workload characterization. We further explore five future GPU architecture designs, which target for the blockchain applications.|ACM Symposium on Applied Computing|2023|10.1145/3555776.3577701|Yue Chen, Xiurui Pan, Shushu Yi, Jie Zhang|0.0|4
42|TH-AB-BRA-07: PENELOPE-Based GPU-Accelerated Dose Calculation System Applied to MRI-Guided Radiation Therapy.|PURPOSE\nThe clinical commissioning of IMRT subject to a magnetic field is challenging. The purpose of this work is to develop a GPU-accelerated Monte Carlo dose calculation platform based on PENELOPE and then use the platform to validate a vendor-provided MRIdian head model toward quality assurance of clinical IMRT treatment plans subject to a 0.35 T magnetic field.\n\n\nMETHODS\nWe first translated PENELOPE from FORTRAN to C++ and validated that the translation produced equivalent results. Then we adapted the C++ code to CUDA in a workflow optimized for GPU architecture. We expanded upon the original code to include voxelized transport boosted by Woodcock tracking, faster electron/positron propagation in a magnetic field, and several features that make gPENELOPE highly user-friendly. Moreover, we incorporated the vendor-provided MRIdian head model into the code. We performed a set of experimental measurements on MRIdian to examine the accuracy of both the head model and gPENELOPE, and then applied gPENELOPE toward independent validation of patient doses calculated by MRIdian's KMC.\n\n\nRESULTS\nWe achieve an average acceleration factor of 152 compared to the original single-thread FORTRAN implementation with the original accuracy preserved. For 16 treatment plans including stomach (4), lung (2), liver (3), adrenal gland (2), pancreas (2), spleen (1), mediastinum (1) and breast (1), the MRIdian dose calculation engine agrees with gPENELOPE with a mean gamma passing rate of 99.1% ± 0.6% (2%/2 mm).\n\n\nCONCLUSIONS\nWe developed a Monte Carlo simulation platform based on a GPU-accelerated version of PENELOPE. We validated that both the vendor provided head model and fast Monte Carlo engine used by the MRIdian system are accurate in modeling radiation transport in a patient using 2%/2 mm gamma criteria. Future applications of this platform will include dose validation and accumulation, IMRT optimization, and dosimetry system modeling for next generation MR-IGRT systems.|Medical Physics (Lancaster)|2016|10.1118/1.4958058|Y. Hu, V. Rodriguez, Y. Wang, H. Wooten, H. Li, S. Mutic, T. Zhao, D. Yang, T. Mazur, O. Green|0.0|4
44|Exploration of the Human Purkinje Network in Virtual Populations|This thesis investigates the Purkinje network (PN) and its dependency on the heart shape (HS) through cardiac simulation on virtual populations (VPs). The heart is a complex organ and essential to the wellbeing of humans; its dysfunction is responsible for more than 27% of all deaths in the UK. The PN delivers the activation impulse to the ventricles of the heart and ensures their synchronous activation. Thus, the morphology of the PN is important, but it varies between species and in vivo imaging is not feasible. However, computer simulation could provide an alternative experimental tool. \nIn simulation of the cardiac electrophysiology, the PN is often replaced by stimulus points on the HS that are ﬁtted to physiological measurements (heart activation times, ECG). Thus, not allowing the study of the PN morphology, nor studies of arrhythmia involving re-entry into the PN. In this thesis, three studies involving explicit models of PNs have been conducted. \nFirst, an eﬃcient algorithm for solving electrophysiology models for the PN is introduced. These allow performing simulations of physiological activations. To minimise the time for simulations, parallelisation with CPU and GPU architectures are investigated, which is of interest for VP studies. \nIn the second study, false tendons (FTs) are studied, which provide an additional connection from the left bundle branch (LBB) and are potentially beneﬁcial in case of LBB block. Therefore, the reduction in activation times by FT is studied as a function of the HS. \nIn the third study, an automatically generated VP is used to explore uncertainty in the PN morphology. The conjecture is that the PN structure adapts to the HS. The coverage of the septum and the minimum distance of the PN to the base are varied. The features of the resulting ECG are used to ﬁnd the PN that gives maximally synchronised contraction.||2016|10.1007/978-3-319-28712-6_10|M. Lange|0.0|4
47|A study of recent contributions on performance and simulation techniques for accelerator devices|High performance computing platform is moving from homogeneous individual unites to heterogeneous systems. Where each unit is a combination of homogeneous cores and accelerator devices. Accelerator s uch as GPUs, FPGAs, DSPs, these devices usually designed for the specific and intensive type of computing tasks. The presence of these devices have created fresh and attractive development platforms for developers and designers, brand new performance analysis frameworks and optimization tools. This is the cutting edge in the performance of some accelerator devices like GPUs and Intel's Xeon Phi. We outline some of the existing heterogeneous systems and their development frameworks. The core of this study is a review of performance modeling of these devices. In this paper, we address the emerging issues that affect the performance of these devices and associated techniques employed for simulation and evaluation.|International Conference on E-Learning and E-Technologies in Education|2017|10.1109/ICEEE2.2017.7935829|Hussein Ajam, Michael Opoku Agyeman|0.0|4
50|Techniques for Managing Irregular Control Flow on GPUs|GPGPU is a highly multithreaded throughput architecture that can deliver high speed-up for regular applications while remaining energy efficient. In recent years, there has been much focus on tuning irregular applications and/or the GPU architecture to achieve similar benefits for irregular applications as well as efforts to extract data parallelism from task parallel applications. In this work we tackle both problems.The first part of this work tackles the problem of Control divergence in GPUs. GPGPUs’ SIMT execution model is ineffective for workloads with irregular control-flow because GPGPUs serialize the execution of divergent paths which lead to thread-level parallelism (TLP) loss. Previous works focused on creating new warps based on the control path threads follow, or created different warps for the different paths, or ran multiple narrower warps in parallel. While all previous solutions showed speedup for irregular workloads, they imposed some performance loss on regular workloads. In this work we propose a more fine-grained approach to exploit intra-warpconvergence: rather than threads executing the same code path, opcode-convergent threadsexecute the same instruction, but with potentially different operands. Based on this new definition we find that divergent control blocks within a warp exhibit substantial opcode convergence. We build a compiler that analyzes divergent blocks and identifies the common streams of opcodes. We modify the GPU architecture so that these common instructions are executed as convergent instructions. Using software simulation, we achieve a 17% speedup over baseline GPGPU for irregular workloads and do not incur any performance loss on regular workloads.In the second part we suggest techniques for extracting data parallelism from irregular, task parallel applications in order to take advantage of the massive parallelism provided by the GPU. Our technique involves dividing each task into multiple sub-tasks each performing less work and touching a smaller memory footprint. Our framework performs a locality-aware scheduling that works on minimizing the memory footprint of each warp (a set of threads performing in lock-step). We evaluate our framework with 3 task-parallel benchmarks and show that we can achieve significant speedups over optimized GPU code.||2020|10.25394/PGS.8041376.V1|Jad Hbeika|0.0|4
51|Multi-agent System with Multiple Group Modelling for Bird Flocking on GPU|Birds flocking is an interesting natural phenomenon to study as proven by numerous papers in this field. In this paper, we present a GPGPU model for birds flocking simulation using NVIDIA's CUDA framework. This technology has been widely adopted in computational science and have dramatically increased computation performances. Using the autonomous agent approach with multi-agents and multiple groups for birds flocking modeling, we present the ACIADDRI model both aggregate motion of a large number of birds in virtual environment and other species or predators avoidance in the plane as well. From these experiments we gained significant performance improvements in the terms of speedup. In conclusion, the work shows that the use of the CUDA technology can be effective to cut computational costs also in multi-agent modeling.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2016|10.1109/PDP.2016.112|D. D'Ambrosio, W. Spataro, Rahmat Hidayat, E. D. Giorgio, D. Spataro|0.0|4
54|X-Aevol: GPU implementation of an evolutionary experimentation simulator|X-Aevol is the GPU port of the Aevol model, a bio-inspired genetic algorithm designed to study the evolution of micro-organisms and its effects on their genome structure. This model is used for in-silico experimental evolution that requires the computation of populations of thousands of individuals during tens of millions of generations. As the model is extended with new features and experiments are conducted with larger populations, computational time becomes prohibitive. X-Aevol is a response to the need of more computational power. It was designed to leverage the massive parallelization capabilities of GPU. As Aevol exposes an irregular and dynamic computational pattern, it was not a straightforward process to adapt it for massively parallel architectures. In this paper, we present how we have adapted the Aevol underlying algorithms to GPU architectures. We implement our new algorithms with CUDA programming language and test them on a representative benchmark of Aevol workloads. To conclude, we present our performance evaluation on NVIDIA Tesla V100 and A100. We show how we reach a speed-up of 1,000 over a sequential execution on a CPU and the speed-up gain up to 50% from using the newer Ampere micro-architecture in comparison with Volta one.|GECCO Companion|2021|10.1145/3449726.3463195|T. Gautier, Jonathan Rouzaud-Cornabas, L. Turpin|0.0|4
55|Towards Detailed Real-Time Simulations of Cardiac Arrhythmia|Recent advances in personalized arrhythmia risk prediction show that computational models can provide not only safer but also more accurate results than invasive procedures. However, biophysically accurate simulations require solving linear systems over fine meshes and time resolutions, which can take hours or even days. This limits the use of such simulations in the clinic where diagnosis and treatment planning can be time sensitive, even if it is just for the reason of operation schedules. Furthermore, the non-interactive, non-intuitive way of accessing simulations and their results makes it hard to study these collaboratively. Overcoming these limitations requires speeding up computations from hours to seconds, which requires a massive increase in computational capabilities.Fortunately, the cost of computing has fallen dramatically in the past decade. A prominent reason for this is the recent introduction of manycore processors such as GPUs, which by now power the majority of the world’s leading supercomputers. These devices owe their success to the fact that they are optimized for massively parallel workloads, such as applying similar ODE kernel computations to millions of mesh elements in scientific computing applications. Unlike CPUs, which are typically optimized for sequential performance, this allows GPU architectures to dedicate more transistors to performing computations, thereby increasing parallel speed and energy efficiency.|2019 Computing in Cardiology (CinC)|2019|10.23919/CinC49843.2019.9005849|Xing Cai, Kristian Gregorius Hustad, H. Arevalo, J. Langguth|0.0|4
57|A comparison of Algebraic Multigrid Bidomain solvers on hybrid CPU-GPU architectures|The numerical simulation of cardiac electrophysiology is a highly challenging problem in scientific computing. The Bidomain system is the most complete mathematical model of cardiac bioelectrical activity. It consists of an elliptic and a parabolic partial differential equation (PDE), of reaction-diffusion type, describing the spread of electrical excitation in the cardiac tissue. The two PDEs are coupled with a stiff system of ordinary differential equations (ODEs), representing ionic currents through the cardiac membrane. Developing efficient and scalable preconditioners for the linear systems arising from the discretization of such computationally challenging model is crucial in order to reduce the computational costs required by the numerical simulations of cardiac electrophysiology. In this work, focusing on the Bidomain system as a model problem, we have benchmarked two popular implementations of the Algebraic Multigrid (AMG) preconditioner embedded in the PETSc library and we have studied the performance on the calibration of specific parameters. We have conducted our analysis on modern HPC architectures, performing scalability tests on multi-core and multi-GPUs setttings. The results have shown that, for our problem, although scalability is verified on CPUs, GPUs are the optimal choice, since they yield the best performance in terms of solution time.|arXiv.org|2023|10.48550/arXiv.2311.13914|Simone Scacchi, Edoardo Centofanti|0.0|4
64|LAWS: Large-Scale Accelerated Wave Simulations on FPGAs|Computing numerical solution to large-scale scientific computing problems described by partial differential equations is a common task in high-performance computing. Improving their performance and efficiency is critical to exa-scale computing. Application-specific hardware design is a well-known solution, but the wide range of kernels makes it infeasible to provision supercomputers with accelerators for all applications. This makes reconfigurable platforms a promising direction. In this work, we focus on wave simulations using discontinuous Galerkin solvers, as an important class of applications. Existing work using FPGAs is limited to accelerating specific kernels or small problems that fit into FPGA BRAM. We present LAWS, a generic and configurable architecture for large-scale accelerated wave simulation problems running on FPGAs out of DRAM. LAWS exploits fine- and coarse-grain parallelism using a scalable array of application-specific cores, and incorporates novel dataflow optimizations, including prefetching, kernel fusion, and memory layout optimizations to minimize data transfers and maximize DRAM bandwidth utilization. We further accompany LAWS with an analytical performance model that allows for scaling across technology trends and architecture configurations. We demonstrate LAWS on the simulation of elastic wave equations. Results show that a single FPGA core achieves 69% higher performance than 24 Xeon cores with 13.27x better energy efficiency, when given 1.94x less peak DRAM bandwidth. Scaling to the same peak DRAM bandwidth shows that an FPGA is 3.27x and 1.5x faster than 24 CPU cores and an Nvidia P100 GPU, with 22.3x and 4.53x better efficiency, respectively.|Symposium on Field Programmable Gate Arrays|2023|10.1145/3543622.3573160|Dimitrios Gourounas, A. Fathi, Dimitar Trenev, L. John, Bagus Hanindhito, A. Gerstlauer|0.0|4
65|Exploration of GPU Cache Architectures Targeting Machine Learning Applications|The computation power from graphics processing units (GPUs) has become prevalent in many fields of computer engineering. Massively parallel workloads and large data set capabilities make GPUs an essential asset in tackling today’s computationally intensive problems. One field that benefited greatly with the introduction of GPUs is machine learning. Many applications of machine learning use algorithms that show a significant speedup on a GPU compared to other processors due to the massively parallel nature of the problem set. The existing cache architecture, however, may not be ideal for these applications. The goal of this thesis is to determine if a cache architecture for the GPU can be redesigned to better fit the needs of this increasingly popular field of computer engineering. This work uses a cycle accurate GPU simulator, Multi2Sim, to analyze NVIDIA GPU architectures. The architectures are based on the Kepler series, but the flexibility of the simulator allows for emulation of newer features. Changes have been made to source code to expand on the metrics recorded to further the understanding of the cache architecture. Two suites of benchmarks were used: one for general purpose algorithms and another for machine learning. Running the benchmarks with various cache configurations led to insight into the effects the cache architecture had on each of them. Analysis of the results shows that the cache architecture, while beneficial to the general purpose algorithms, does not need to be as complex for machine learning algorithms. A large contributor to the complexity is the cache coherence protocol used by GPUs. Due to the high spacial locality associated with machine learning problems, the overhead needed by implementing the coherence protocol has little benefit, and simplifying the architecture can lead to smaller, cheaper, and more efficient designs.||2019|10.1145/3343031.3350545|Gerald Kotas|0.0|4
66|Photon: A Fine-grained Sampled Simulation Methodology for GPU Workloads|GPUs, due to their massively-parallel computing architectures, provide high performance for data-parallel applications. However, existing GPU simulators are too slow to enable architects to quickly evaluate their hardware designs and software analysis studies. Sampled simulation methodologies are one common way to speed up CPU simulation. However, GPUs apply drastically different execution models that challenge the sampled simulation methods designed for CPU simulations. Recent GPU sampled simulation methodologies do not fully take advantage of the GPU’s special architecture features, such as limited types of basic blocks or warps. Moreover, these methods depend on up-front analysis via profiling tools or functional simulation, making them difficult to use. To address this, we extensively studied the execution patterns of a variety of GPU workloads and propose Photon, a sampled simulation methodology tailored to GPUs. Photon incorporates methodologies that automatically consider different levels of GPU execution, such as kernels, warps, and basic blocks. Photon does not require up-front profiling of GPU workloads and utilizes a light-weight online analysis method based on the identification of highly repetitive software behavior. We evaluate Photon using a variety of GPU workloads, including real-world applications like VGG and ResNet. The final result shows that Photon reduces the simulation time needed to perform one inference of ResNet-152 with batch size 1 from 7.05 days to just 1.7 hours with a low sampling error of 10.7%.|Micro|2023|10.1145/3613424.3623773|Yifan Sun, Trevor E. Carlson, Changxi Liu|0.0|4
75|Characterizing the Performance Bottlenecks of Irregular GPU Kernels|Graphics processing units (GPUs) are increasingly being used to accelerate general-purpose applications, including applications with data-dependent, irregular memory access patterns and control flow. However, relatively little is known about the behavior of irregular GPU codes, and there has been minimal effort to quantify the ways in which they differ from regular general-purpose GPU applications. I examine the behavior of a suite of optimized irregular GPU applications written in CUDA on a cycle-level GPU simulator. I characterize the performance bottlenecks in each program and connect source code to microarchitectural performance characteristics. I also assess the performance impact of modifying hardware parameters such as the cache and DRAM bandwidths and latencies, data cache sizes, coalescing behavior, and warp scheduling policy, and I discuss the implications for future GPU architecture design. I find that, while irregular graph codes exhibit significantly more underutilized execution cycles due to branch divergence, load imbalance, and synchronization overhead than regular programs, overall, code optimizations are often able to effectively address these performance hurdles. Insufficient bandwidth, long memory latency, and poor cache effectiveness are the biggest limiters of performance. Applications with irregular memory access patterns are more sensitive to changes in L2 latency and bandwidth than DRAM latency and bandwidth. Additionally, greedy-then-oldest scheduling is the best simple warp scheduler for irregular codes, and two-level scheduling does not significantly improve the performance of such codes.||2015|10.1109/sspd.2015.7288505|M. A. O'Neil|0.0|4
77|Optimizing Complex Spatially-Variant Coefficient Stencils for Seismic Modeling on GPU|The Explicit Time Evolution (ETE) method is an innovative Finite-Difference (FD) type method to simulate the wave propagation in acoustic media with higher spatial and temporal accuracy. However, different from FD, it is difficult to achieve an efficient GPU design because of the poor memory access patterns caused by the off-axis points and spatially-variant coefficients. In this paper, we present a set of new optimization strategies for ETE stencils according to the memory hierarchy of NVIDIA GPU. To handle the problem caused by the complexity of the stencil shapes, we design a one-to-multi updating scheme for shared memory usage. To alleviate the performance damage resulted from the poor memory access pattern of reading spatially-variant coefficients, we propose a stencil decomposition method to reduce un-coalesced global memory access. Based on the state-of-the-art GPU architecture, combining with existing spatial and temporal stencil blocking schemes, we manage to achieve 9.6x and 9.9x speedups compared with a well-tuned 12-core CPUs version for 37-point and 73-point ETE stencils, respectively. Compared with a well-tuned MIC version, the best speedups for the 2 type stencils are 3.7x and 4.7x. Our designs leads to an ETE method that is 31.2x faster than conventional CPU-FD method and make it a practical seismic imaging technology.|International Conference on Parallel and Distributed Systems|2015|10.1109/ICPADS.2015.86|L. Gan, Guangwen Yang, H. Fu, N. Dai, Wei Wu, He Zhang, Jiarui Fang|0.0|4
79|Performance evaluation using GEM 5-GPU simulator|With introduction of heterogeneous computing for developing and manufacturing next generation of computer it is necessary to have a simulator which would evaluate and simulate designed heterogeneous system showing how it would behave in actual application. The heterogeneous system refers to a system which uses more than one type of processor or cores. For the purpose of analyzing heterogeneous systems (CPU-GPU) behavior Gem5-gpu simulator was developed. The Gem5-gpu simulator was build combining two different simulators Gem5 simulator and GPGPU-sim simulator. The GPGPU-sim is used for simulating GPUs while Gem5 simulator is used for modelling CPUs. The Gem5-gpu combines best characteristics of both simulators.|International Conference Computing Methodologies and Communication|2017|10.1109/ICCMC.2017.8282713|Sagar Namdev Sawal, Nitesh B. Guinde|0.0|4
91|Suitability of Shallow Water Solving Methods for GPU Acceleration|In the past 15 years the field of general purpose computing on graphics processing units, or GPUs, has become well developed and the practice is becoming more mainstream. Computational demands of simulation software are continuously increasing. As such for many applications traditionally computed on the central processing unit the question arises of whether moving to GPU computing is a possible cost effective way of meeting these demands. The fundamental nature of GPU architecture that makes it so cost effective at doing bulk computation also poses restrictions on which applications are suitable for it. The shallow water equations are a simplified form of the Navier-Stokes equations and describe water levels and flow currents in suitably shallow water such as rivers, estuaries and the North sea. The main research goal of this thesis project was to determine whether the shallow water equations are suitable for implementation on a GPU. Two options exist, the equations may be solved with either an explicit or implicit time integration method. First, a literature study was conducted to familiarize with the tools required to build explicit and implicit shallow water models on a GPU. Then both an explicit and implicit shallow water solver were developed first in the MATLAB programming language and later in CUDA C++ on both CPU and GPU. The main findings are that both explicit and implicit methods are well suited for GPU implementation. Both methods proved to be compatible with a wetting and drying mechanism of numerical cells. The Cuda C++ implementation was in the order of 10 times as fast as a MATLAB implementation for both CPU and GPU. For the benchmark cases tested, the Cuda C++ GPU implementation was in the order of 50 times faster than the equivalent multithreaded CPU implementation. The implicit implementation was benchmarked using the conjugate gradient method to solve the linear system. Various preconditioners were tested and a Repeated Red Black preconditioner was found to be the most effective. The computation time of the RRB preconditioned implicit method was compared with the explicit method and it was found that the two methods reached parity in computation time when the implicit time step was taken roughly 50 times as large as the explicit time step. For implicit time steps smaller than that the explicit method was faster and when the implicit time step was larger the implicit method was faster. For the benchmark cases tested, the implicit method using a time step 50 times larger than the explicit method was found to be less accurate and less stable than the explicit method. The conclusion is that for cases similar to the benchmark cases an explicit method is the fastest, most stable and most accurate method and thus the preferred choice.||2020|10.21125/edulearn.2020.1231|Floris Buwalda|0.0|4
97|UISS-GPU: Accelerated In-Silico Tuberculosis Vaccine Trials Using FLAME GPU|The Universal Immune System Simulator (UISS) is a computational framework based on agent-based modelling (ABM) paradigm that has been specifically developed for simulating the immune system behaviour in presence of diseases and treatments. It has a long history of development, ranging from its initial applications into the field of tumor immunology and then moving towards wide disease modelling scenarios such as influenza, Multiple Sclerosis and atherosclerosis. Recently, inside the STriTuVaD H2020 EU project, it has been specialized to simulate tuberculosis dynamics and its interaction with the immune system, including the efficacy of the combined action of various treatments such as isoniazid and novel vaccines. TB simulation entitles large scale (e.g., tissue to organ scale) simulations over a wide digital population cohort. The computational costs of running large scale simulations are prohibitive using traditional forms of CPU simulation. This paper considers the use of parallel to gpu-based computing approaches via an agent-based domain independent complex systems simulator, FLAME GPU. Integration of FLAME GPU with UISS enables the simulation of larger, more complex problem domains. The combined UISS-FLAMEGPU simulator provides vastly increased performance characteristics for large problems, with a speedup of 4.22x for a typical tuberculosis model simulating 128 microlitres. FLAME GPU abstracts away a significant portion of the normal programming that would be required to effectively parallelise a model of this complexity. Adaptations were made to increase performance, such as message mutation and parallelisation of certain algorithms.|IEEE International Conference on Bioinformatics and Biomedicine|2022|10.1109/BIBM55620.2022.9995159|P. Richmond, G. Russo, M. Pennisi, F. Pappalardo, M. Leach, Peter Heywood|0.0|4
99|Chrono DEM-Engine: A Discrete Element Method dual-GPU simulator with customizable contact forces and element shape|"This paper introduces DEM-Engine, a new submodule of Project Chrono, that is designed to carry out Discrete Element Method (DEM) simulations. Based on spherical primitive shapes, DEM-Engine can simulate polydisperse granular materials and handle complex shapes generated as assemblies of primitives, referred to as clumps. DEM-Engine has a multi-tier parallelized structure that is optimized to operate simultaneously on two GPUs. The code uses custom-defined data types to reduce memory footprint and increase bandwidth. A novel""delayed contact detection""algorithm allows the decoupling of the contact detection and force computation, thus splitting the workload into two asynchronous GPU streams. DEM-Engine uses just-in-time compilation to support user-defined contact force models. This paper discusses its C++ and Python interfaces and presents a variety of numerical tests, in which impact forces, complex-shaped particle flows, and a custom force model are validated considering well-known benchmark cases. Additionally, the full potential of the simulator is demonstrated for the investigation of extraterrestrial rover mobility on granular terrain. The chosen case study demonstrates that large-scale co-simulations (comprising 11 million elements) spanning 15 seconds, in conjunction with an external multi-body dynamics system, can be efficiently executed within a day. Lastly, a performance test suggests that DEM-Engine displays linear scaling up to 150 million elements on two NVIDIA A100 GPUs."|arXiv.org|2023|10.48550/arXiv.2311.04648|D. Negrut, B. Tagliafierro, Shlok Sabarwal, R. Serban, Ruochun Zhang, Yulong Yue, Luning Bakke, Xin Wei, Colin Vanden Heuvel|0.0|4
101|Optimizations for energy efficiency in GPGPU architectures|"Author(s): Sankaranarayanan, Alamelu | Advisor(s): Renau, Jose; Briz, Jose L | Abstract: It is commonplace for graphics processing units or GPUs today to render extremely complex 3D scenes and textures, in real time, both in the traditional and mobile computing spaces. The computational power required to do this makes them a valuable resource to exploit for general purpose computation. In order to map programs originally designed for sequential CPUs onto massively parallel GPU architectures, it would be necessary to justify the transition with huge performance benefits. Over the last couple of years, there have been numerous proposals to improve the performance of GPUs used for general purpose computing (GPGPUs), but without much consideration for energy efficiency. In my dissertation, I evaluate the feasibility of GPGPUs from an energy perspective and propose some optimizations based on the unique programming model used by GPGPUs. First, I describe the simulation infrastructure, one of the few available to model GPGPUs today, both individually and as part of a heterogeneous system. Next, I propose a design using a shared translation lookaside buffer (TLB) to eliminate chronic memory copies between the CPU and GPU addressing spaces, making heterogeneous CPU-GPU designs energy efficient. Furthermore, to improve the energy efficiency of the on-chip memory hierarchy, I propose adding tiny incoherent caches per processing element, which can filter out frequent accesses to large shared and energy-inefficient cache structures. Finally, I evaluate a design which moves away from the underlying SIMD architecture of GPUs towards a more MIMD-like architecture, enabling the execution of both CPU and GPGPU workloads without negatively affecting the energy efficiency availed by traditional workloads on GPGPUs."||2016|10.15740/has/tajh/11.1/176-179|Alamelu Sankaranarayanan|0.0|4
102|Parallel Simulation of PDP Systems: Updates and Roadmap|PDP systems are a type of multienvironment P systems, which serve as a formal modeling framework for Population Dynamics. The accurate simulation of these probabilistic models entails large run times. Hence, parallel platforms such as GPUs has been employed to speedup the simulation. In 2012 [14], the first GPU simulator of PDP systems was presented. In this paper, we present current updates made on this simulator, and future developments to consider.||2015|10.1007/978-3-319-28475-0_12|M. J. P. Jiménez, L. F. Ramos, M. A. Amor|0.0|4
105|Programming Perspectives for Pre-exascale Systems|Reservoir simulation of large scale projects is becoming increasingly complex, requiring more than simple black oil models and vertical well models to capture the behaviour of unconventional, fractured and highly heterogeneous production zones. Nvidia provides an array of accelerated linear algebra libraries to deal with the equations that must be solved in these situations. Accelerating sparse linear algebra on the latest GPU architectures has real potential for performance gains of hundreds of percent over carefully tuned multi-core CPU-only implementations, but at what cost in complexity? This talk will address the programming approaches needed to utilize GPUs at scale for today’s most challenging problems, and give a glimpse of the path forward to pre-exascale applications.|International Conference on High Performance Computing|2015|10.3997/2214-4609.201414023|F. Courteille, J. Eaton|0.0|4
111|KPU-SQL: Kernel Processing Unit for High-Performance SQL Acceleration|Application-specific accelerator is a prominent way for analytic query processing. To achieve a substantial improvement over the state-of-the-art in performance while maintaining programmability, we propose a kernel processing unit (KPU) framework and apply it to SQL acceleration. Kernel customization and data transmission are two critical bottlenecks, we separately optimize them in the key core and shadow core with a self-designed data management system. A software stack named RACE with a performance model and function simulator is also introduced. The experiments demonstrate that KPU-SQL outperforms the CPU and GPU by 24.5x and 8.75x on average, respectively.|ACM Great Lakes Symposium on VLSI|2023|10.1145/3583781.3590268|Wenyan Lu, Guihai Yan, Hao Kong, Liyun Cheng, Jingya Wu, Haishuang Fan, Xiaowei Li, Yan Chen|0.0|4
113|A Low-Latency Communication Design for Brain Simulations|Brain simulation, as one of the latest advances in artificial intelligence, facilitates better understanding about how information is represented and processed in the brain. The extreme complexity of the human brain makes brain simulations only feasible on high-performance computing platforms. Supercomputers with a large number of interconnected graphical processing units (GPUs) are currently employed for supporting brain simulations. Therefore, high-throughput low-latency inter-GPU communications in super-computers play a crucial role in meeting the performance requirements of brain simulation as a highly time-sensitive application. In this article, we first provide an overview of the current parallelizing technologies for brain simulations using multi-GPU architectures. Then we analyze the challenges to communications for brain simulation and summarize guidelines for communication design to address such challenges. Furthermore, we propose a partitioning algorithm and a two-level routing method to achieve efficient low-latency communications in multi-GPU architecture for brain simulation. We report experiment results obtained on a supercomputer with 2000 GPUs for simulating a brain model with 10 billion neurons (digital twin brain, DTB) to show that our approach can significantly improve communication performance. We also discuss open issues and identify some research directions for low-latency communication design for brain simulations.|IEEE Network|2022|10.48550/arXiv.2205.07125|Xin Du|0.0|4
115|Performance Portable Solid Mechanics via Matrix-Free p-Multigrid|—Finite element analysis of solid mechanics is a foundational tool of modern engineering, with low-order ﬁnite element methods and assembled sparse matrices representing the industry standard for implicit analysis. We use performance models and numerical experiments to demonstrate that high- order methods greatly reduce the costs to reach engineering tolerances while enabling effective use of GPUs. We demonstrate the reliability, efﬁciency, and scalability of matrix-free p -multigrid methods with algebraic multigrid coarse solvers through large deformation hyperelastic simulations of multiscale structures. We investigate accuracy, cost, and execution time on multi-node CPU and GPU systems for moderate to large models using AMD MI250X (OLCF Crusher), NVIDIA A100 (NERSC Perlmutter), and V100 (LLNL Lassen and OLCF Summit), resulting in order of magnitude efﬁciency improvements over a broad range of model properties and scales. We discuss efﬁcient matrix-free representation of Jacobians and demonstrate how automatic differentiation enables rapid development of nonlinear material models without impacting debuggability and workﬂows targeting GPUs.|arXiv.org|2022|10.48550/arXiv.2204.01722|W. Moses, J. Thompson, M. Knepley, Rezgar Shakeri, Jed Brown, Leila Ghaffari, Junchao Zhang, V. Barra, Karen Stengel, Natalie N. Beams|0.0|4
118|Modelling and simulation of GPU processing in the MERPSYS environment|In this work, we evaluate an analytical GPU performance model based on Little's law, that expresses the kernel execution time in terms of latency bound, throughput bound, and achieved occupancy.We then combine it with the results of several research papers, introduce equations for data transfer time estimation, and finally incorporate it into the MERPSYS framework, which is a general-purpose simulator for parallel and distributed systems.The resulting solution enables the user to express a CUDA application in a MERPSYS editor using an extended Java language and then conveniently evaluate its performance for various launch configurations using different hardware units.We also provide a systematic methodology for extracting kernel characteristics, that are used as input parameters of the model.The model was evaluated using kernels representing different traits and for a large variety of launch configurations.We found it to be very accurate for computation bound kernels and realistic workloads, whilst for memory throughput bound kernels and uncommon scenarios the results were still within acceptable limits.We have also proven its portability between two devices of the same hardware architecture but different processing power.Consequently, MERPSYS with the theoretical models embedded in it can be used for evaluationof application performance on various GPUs and used for performance prediction and e.g. purchase decision making.|Scalable Computing : Practice and Experience|2018|10.12694/scpe.v19i4.1439|P. Czarnul, Tomasz Gajger|0.0|4
121|Predictive Analysis of Large-Scale Coupled CFD Simulations with the CPX Mini-App|As the complexity of multi-physics simulations increases, there is a need for efficient flow of information between components. Discrete ‘coupler’ codes can abstract away this process, improving solver interoperability. One such multi-physics problem is modelling the high pressure compressor of turbofan engines, where instances of rotor/stator CFD simulations are coupled. Configuring couplers and allocating resources correctly can be challenging for such problems due to the sliding interfaces between codes. In this research, we present CPX, a mini-coupler designed to model the performance behaviour of a production coupler framework at Rolls-Royce plc., used for coupling rotor/stator simulations. CPX, the first mini-coupler framework of its kind, is combined with a CFD mini-app to predict the run-time and scaling behaviour of large scale coupled CFD simulations. We demonstrate high qualitative and quantitative predictive accuracy with a less than 17 % mean error. A performance model is developed to predict the ‘optimum’ configuration of resources, and is tested to show the high accuracy of these predictions. The model is also used to project the ‘optimum’ configuration for a 6 Billion cell test case, a problem size representative of current leading-edge production workloads, on a 100,000 core cluster and a 400 GPU cluster. Further testing reveals that the ‘optimum’ configuration is unstable if not set up correctly, and therefore a trade-off needs to be made with a marginally less-than-optimal setup to ensure stability. The work illustrates the significant utility of CPX to carry out such rapid design space and run-time setup exploration studies to obtain the best performance from production CFD coupled simulations.|International Conference on High Performance Computing|2021|10.1109/HiPC53243.2021.00028|I. Reguly, K. Choudry, S. Jarvis, Arvind Prabhakar, D. Amirante, A. Powell, G. Mudalige|0.0|4
126|Energy efficiency and performance modeling of stencil applications on manycore and GPU computing resources|One of the most critical application areas for many operational High-Performance Computing systems worldwide is a numerical weather prediction. Such complex predictions and corresponding computing models have to solve a large number of Partial Differential Equations using stencil computations on structured grids within tight production schedules. A stencil kernel within simulations is often the most demanding computing part and naturally may impact the energy consumption of the whole HPC system. In this paper, we introduce new functional extensions developed for DCworms and GSSIM simulators to predict the energy efficiency of extreme-scale stencil applications on heterogeneous computing resources, in particular consisting of a large number of many-core and GPUs. New energy-efficiency metrics for modeling of heterogeneous computing resources have been successfully added to the simulators thanks to their pluggable and extensible architectures. Our recent improvements related to application-specific performance models in simulators help users to take into account many relevant application parameters, in particular, those related to detailed energy consumption on heterogeneous HPC resources. We show in this paper how to extract stencil application-specific parameters based on real experiments. Moreover, we demonstrate how those parameters can be applied for modeling and simulation experiments to evaluate the overall performance and energy consumption of stencil computations on manycore CPUs and GPUs. Finally, we discuss new DCworms simulator capabilities and demonstrate added-values of performance analysis tools.|IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing|2020|10.1109/CCGrid49817.2020.00-70|M. Ciznicki, J. Weglarz, K. Kurowski|0.0|4
131|Cache Memory Access Patterns in the GPU Architecture|Data exchange between a Central Processing Unit (CPU) and a Graphic Processing Unit (GPU) can be very expensive in terms of performance. The characterization of data and cache memory access patterns differ between a CPU and a GPU. The motivation of this research is to analyze the cache memory access patterns of GPU architectures and to potentially improve data exchange between a CPU and GPU. The methodology of this work uses Multi2Sim GPU simulator for AMD Radeon and NVIDIA Kepler GPU architectures. This simulator, used to emulate the GPU architecture in software, enables certain code modifications for the L1 and L2 cache memory blocks. Multi2Sim was configured to run multiple benchmarks to analyze and record how the benchmarks access GPU cache memory. The recorded results were used to study three main metrics: (1) Most Recently Used (MRU) and Least Recently Used (LRU) accesses for L1 and L2 caches, (2) Inter-warp and Intra-warp cache memory accesses in the GPU architecture for different sets of workloads, and (3) To record and compare the GPU cache access patterns for certain machine learning benchmarks with its general purpose counterparts.||2018|10.1109/igcc.2018.8752137|Yash Nimkar|0.0|4
132|ANT-MOC: Scalable Neutral Particle Transport Using 3D Method of Characteristics on Multi-GPU Systems|The Method Of Characteristic (MOC) to solve the Neutron Transport Equation (NTE) is the core of full-core simulation for reactors. High resolution is enabled by discretizing the NTE through massive tracks to traverse the 3D reactor geometry. However, the 3D full-core simulation is prohibitively expensive because of the high memory consumption and the severe load imbalance. To deal with these challenges, we develop ANT-MOC1. Specifically, we build a performance model for memory footprint, computation and communication, based on which a track management strategy is proposed to overcome the resolution bottlenecks caused by limited GPU memory. Furthermore, we implement a novel multi-level load mapping strategy to ensure load balancing among nodes, GPUs, and CUs. ANT-MOC enables a 3D full-core reactor simulation with 100 billion tracks on 16,000 GPUs, with 70.69% and 89.38% parallel efficiency for strong scalability and weak scalability, respectively.|International Conference on Software Composition|2023|10.1145/3581784.3607063|X. Chi, Zhikuang Xin, Shigang Li, Yangang Wang, Shunde Li, Jue Wang, Zongguo Wang, Yangde Feng, Lingkun Bu, Peng Shi, Yun Hu|0.0|4
133|CPU-GPU Heterogeneous Code Acceleration of a Finite Volume Computational Fluid Dynamics Solver|This work deals with the CPU-GPU heterogeneous code acceleration of a finite-volume CFD solver utilizing multiple CPUs and GPUs at the same time. First, a high-level description of the CFD solver called SENSEI, the discretization of SENSEI, and the CPU-GPU heterogeneous computing workflow in SENSEI leveraging MPI and OpenACC are given. Then, a performance model for CPU-GPU heterogeneous computing requiring ghost cell exchange is proposed to help estimate the performance of the heterogeneous implementation. The scaling performance of the CPU-GPU heterogeneous computing and its comparison with the pure multi-CPU/GPU performance for a supersonic inlet test case is presented to display the advantages of leveraging the computational power of both the CPU and the GPU. Using CPUs and GPUs as workers together, the performance can be improved further compared to using pure CPUs or GPUs, and the advantages can be fairly estimated by the performance model proposed in this work. Finally, conclusions are drawn to provide 1) suggestions for application users who have an interest to leverage the computational power of the CPU and GPU to accelerate their own scientific computing simulations and 2) feedback for hardware architects who have an interest to design a better CPU-GPU heterogeneous system for heterogeneous computing.|arXiv.org|2023|10.48550/arXiv.2305.18057|Hongyu Wang, Weicheng Xue, Christopher J. Roy|0.0|4
134|Exploiting Nested Parallelism on Heterogeneous Processors|Title of Thesis: EXPLOTING NESTED PARALLELISM ON HETEROGENEOUS PROCESSORS Michael Jeffrey Zuzak, Master of Science, 2016 Thesis Directed By: Associate Professor and Director of Computer Engineering Education, Doctor Donald Yeung, Department of Electrical and Computer Engineering Heterogeneous computing systems have become common in modern processor architectures. These systems, such as those released by AMD, Intel, and Nvidia, include both CPU and GPU cores on a single die available with reduced communication overhead compared to their discrete predecessors. Currently, discrete CPU/GPU systems are limited, requiring larger, regular, highly-parallel workloads to overcome the communication costs of the system. Without the traditional communication delay assumed between GPUs and CPUs, we believe non-traditional workloads could be targeted for GPU execution. Specifically, this thesis focuses on the execution model of nested parallel workloads on heterogeneous systems. We have designed a simulation flow which utilizes widely used CPU and GPU simulators to model heterogeneous computing architectures. We then applied this simulator to nontraditional GPU workloads using different execution models. We also have proposed a new execution model for nested parallelism allowing users to exploit these heterogeneous systems to reduce execution time. EXPLOITING NESTED PARALLELISM IN HETEROGENEOUS COMPUTING SYSTEMS||2016|10.13016/M28B6V|Michael Zuzak|0.0|4
136|Parallel Q-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation|Reinforcement learning is time-consuming for complex tasks due to the need for large amounts of training data. Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works have used on-policy methods like PPO due to their simplicity and easy-to-scale nature. Off-policy meth-ods are more sample-efficient, but challenging to scale, resulting in a longer wall-clock training time. This paper presents a novel Parallel Q -Learning (PQL) scheme that outperforms PPO in terms of wall-clock time and maintains superior sample efficiency. The driving force lies in the parallelization of data collection, policy function learning, and value function learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate the capability of scaling up Q -learning methods to tens of thousands of parallel environments and investigate important factors that can affect learning speed, including the number of parallel environments, exploration strategies, batch size, GPU models, etc. The code is available at https://github.com/Improbable-AI/pql.|International Conference on Machine Learning|2023|10.48550/arXiv.2307.12983|Pulkit Agrawal, Tao Chen, Zechu Li, Zhang-Wei Hong, Anurag Ajay|0.0|4
153|SCALABLE INTEGRATED CIRCUIT SIMULATION ALGORITHMS FOR ENERGY-EFFICIENT TERAFLOP HETEROGENEOUS PARALLEL COMPUTING PLATFORMS|Integrated circuit technology has gone through several decades of aggressive scaling. It is increasingly challenging to analyze growing design complexity. Post-layout SPICE simulation can be computationally prohibitive due to the huge amount of parasitic elements, which can easily boost the computation and memory cost. As the decrease in device size, the circuits become more vulnerable to process variations. Designers need to statistically simulate the probability that a circuit does not meet the performance metric, which requires millions times of simulations to capture rare failure events. Recent, multiprocessors with heterogeneous architecture have emerged as mainstream computing platforms. The heterogeneous computing platform can achieve highthroughput energy efficient computing. However, the application of such platform is not trivial and needs to reinvent existing algorithms to fully utilize the computing resources. This dissertation presents several new algorithms to address those aforementioned two significant and challenging issues on the heterogeneous platform. Harmonic Balance (HB) analysis is essential for efficient verification of large postlayout RF and microwave integrated circuits (ICs). However, existing methods either suffer from excessively long simulation time and prohibitively large memory consumption or exhibit poor stability. This dissertation introduces a novel transient-simulation guided graph sparsification technique, as well as an efficient runtime performance modeling approach tailored for heterogeneous manycore CPU-GPU computing system to build nearly-optimal subgraph preconditioners that can lead to minimum HB simulation runtime. Additionally, we propose a novel heterogeneous parallel sparse block matrix algorithm by taking advantages of the structure of HB Jacobian matrices as well as GPU’s streaming multiprocessors to achieve optimal workload balancing||2016|10.37099/mtu.dc.etdr/86|Lengfei Han|0.0|4
162|Understanding the Future of Energy Efficiency in Multi-Module GPUs|As Moore’s law slows down, GPUs must pivot towards multi-module designs to continue scaling performance at historical rates. Prior work on multi-module GPUs has focused on performance, while largely ignoring the issue of energy efficiency. In this work, we propose a new metric for GPU efficiency called EDP Scaling Efficiency that quantifies the effects of both strong performance scaling and overall energy efficiency in these designs. To enable this analysis, we develop a novel top-down GPU energy estimation framework that is accurate within 10% of a recent GPU design. Being decoupled from granular GPU microarchitectural details, the framework is appropriate for energy efficiency studies in future GPUs. Using this model in conjunction with performance simulation, we show that the dominating factor influencing the energy efficiency of GPUs over the next decade is GPUmodule (GPM) idle time. Furthermore, neither inter-module interconnect energy, nor GPM microarchitectural design is expected to play a key role in this regard. We demonstrate that multi-module GPUs are on a trajectory to become 2⇥ less energy efficient than current monolithic designs; a significant issue for data centers which are already energy constrained. Finally, we show that architects must be willing to spend more (not less) energy to enable higher bandwidth inter-GPM connections, because counter-intuitively, this additional energy expenditure can reduce total GPU energy consumption by as much as 45%, providing a path to energy efficient strong scaling in the future.|International Symposium on High-Performance Computer Architecture|2019|10.1109/HPCA.2019.00063|Evgeny Bolotin, D. Nellans, A. Arunkumar, Carole-Jean Wu|4.0|3
177|Blind Identification of Thermal Models and Power Sources From Thermal Measurements|The ability to sense the temperatures and power consumption of various key components of a chip is central to the operation of modern integrated circuits, such as processors. While modern chips often include a number of embedded thermal sensors, they lack the ability to sense power at fine granularity. This paper proposes a new direction to simultaneously identify the thermal models and the fine-grain power consumption of a chip from just the measurements of the thermal sensors and the total power consumption. Our identification technique is blind as it does not require design knowledge of the thermal-power model to identify the power sources. We investigate the main challenges in blind identification, which are the permutation and scaling ambiguities, and propose novel techniques to resolve these ambiguities. We implement our technique and apply it in three contexts. First, we implement it within a controlled simulation environment, which enables us to verify its accuracy and analyze its sensitivity to relevant issues, such as measurement noise and number of available training samples. Second, we apply it on a real multi-core CPU + GPU processor-based system, where we show the ability to identify the runtime power consumption of the individual cores using just the total power measurement and the measurements of the embedded thermal sensors under different workloads. Third, we apply it for non-invasive power sensing of chips by inverting the temperatures measured using an external infrared imaging camera. We show that our technique consistently improves the modeling and sensing accuracy of integrated circuits.|IEEE Sensors Journal|2018|10.1109/JSEN.2017.2774704|S. Reda, K. Dev, A. Belouchrani|2.4285714285714284|3
172|Graphics-Processing-Unit-Based Acceleration of Electromagnetic Transients Simulation|This paper presents a novel approach to speed up electromagnetic-transients (EMT) simulation, using graphics-processing-unit (GPU)-based computing. This paper extends earlier published works in the area, by exploiting additional parallelism inside EMT simulation. A 2D-parallel matrix-vector multiplication is used that is faster than previous 1D-methods. Also, this paper implements a GPU-specific sparsity technique to further speed up the simulations, as the available CPU-based sparsity techniques are not suitable for GPUs. In addition, as an extension to previous works, this paper demonstrates modelling a power-electronic subsystem. The efficacy of the approach is demonstrated using two different scalable test systems. A low granularity system, that is, one with a large cluster of buses connected to others with a few transmission lines is considered, as is also a high granularity where a small cluster of buses is connected to other clusters, thereby requiring more interconnecting transmission lines. Computation times for GPU-based computing are compared with the computation times for sequential implementations on the CPU. This paper shows two surprising differences of GPU simulation in comparison with CPU simulation. First, the inclusion of sparsity only makes minor reductions in the GPU-based simulation time. Second, excessive granularity, even though it appears to increase the number of parallel-computable subsystems, significantly slows down the GPU-based simulation.|IEEE Transactions on Power Delivery|2016|10.1109/TPWRD.2015.2492983|W. Fung, A. Gole, J. Debnath|2.3333333333333335|3
175|Machine Learning for Performance and Power Modeling of Heterogeneous Systems|Modern processing systems with heterogeneous components (e.g., CPUs, GPUs) have numerous configuration and design options such as the number and types of cores, frequency, and memory bandwidth. Hardware architects must perform design space explorations in order to accurately target markets of interest under tight time-to-market constraints. This need highlights the importance of rapid performance and power estimation mechanisms. This work describes the use of machine learning (ML) techniques within a methodology for the estimating performance and power of heterogeneous systems. In particular, we measure the power and performance of a large collection of test applications running on real hardware across numerous hardware configurations. We use these measurements to train a ML model; the model learns how the applications scale with the system's key design parameters. Later, new applications of interest are executed on a single configuration, and we gather hardware performance counter values which describe how the application used the hardware. These values are fed into our ML model's inference algorithm, which quickly identify how this application will scale across various design points. In this way, we can rapidly predict the performance and power of the new application across a wide range of system configurations. Once the initial run of the program is complete, our ML algorithm can predict the application's performance and power at many hardware points faster than running it at each of those points and with a level of accuracy comparable to cycle-level simulators.|2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)|2018|10.1145/3240765.3243484|J. Greathouse, G. Loh|1.8571428571428572|3
167|Granular layEr Simulator: Design and Multi-GPU Simulation of the Cerebellar Granular Layer|In modern computational modeling, neuroscientists need to reproduce long-lasting activity of large-scale networks, where neurons are described by highly complex mathematical models. These aspects strongly increase the computational load of the simulations, which can be efficiently performed by exploiting parallel systems to reduce the processing times. Graphics Processing Unit (GPU) devices meet this need providing on desktop High Performance Computing. In this work, authors describe a novel Granular layEr Simulator development implemented on a multi-GPU system capable of reconstructing the cerebellar granular layer in a 3D space and reproducing its neuronal activity. The reconstruction is characterized by a high level of novelty and realism considering axonal/dendritic field geometries, oriented in the 3D space, and following convergence/divergence rates provided in literature. Neurons are modeled using Hodgkin and Huxley representations. The network is validated by reproducing typical behaviors which are well-documented in the literature, such as the center-surround organization. The reconstruction of a network, whose volume is 600 × 150 × 1,200 μm3 with 432,000 granules, 972 Golgi cells, 32,399 glomeruli, and 4,051 mossy fibers, takes 235 s on an Intel i9 processor. The 10 s activity reproduction takes only 4.34 and 3.37 h exploiting a single and multi-GPU desktop system (with one or two NVIDIA RTX 2080 GPU, respectively). Moreover, the code takes only 3.52 and 2.44 h if run on one or two NVIDIA V100 GPU, respectively. The relevant speedups reached (up to ~38× in the single-GPU version, and ~55× in the multi-GPU) clearly demonstrate that the GPU technology is highly suitable for realistic large network simulations.|Frontiers in Computational Neuroscience|2021|10.3389/fncom.2021.630795|F. Leporati, Stefano Masoli, Giordana Florimbi, E. D’Angelo, E. Torti|1.5|3
173|Online Power Estimation of Graphics Processing Units|Accurate power estimation at runtime is essential for the efficient functioning of a power management system. While years of research have yielded accurate power models for the online prediction of instantaneous power for CPUs, such power models for graphics processing units (GPUs) are lacking. GPUs rely on low-resolution power meters that only nominally support basic power management. To address this, we propose an instantaneous power model, and in turn, a power estimator, that uses performance counters in a novel way so as to deliver accurate power estimation at runtime. Our power estimator runs on two real NVIDIA GPUs to show that accurate runtime estimation is possible without the need for the high-fidelity details that are assumed on simulation-based power models. To construct our power model, we first use correlation analysis to identify a concise set of performance counters that work well despite GPU device limitations. Next, we explore several statistical regression techniques and identify the best one. Then, to improve the prediction accuracy, we propose a novel application-dependent modeling technique, where the model is constructed online at runtime, based on the readings from a low-resolution, built-in GPU power meter. Our quantitative results show that a multi-linear model, which produces a mean absolute error of 6%, works the best in practice. An application-specific quadratic model reduces the error to nearly 1%. We show that this model can be constructed with low overhead and high accuracy at runtime. To the best of our knowledge, this is the first work attempting to model the instantaneous power of a real GPU system, earlier related work focused on average power.|IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing|2016|10.1109/CCGrid.2016.93|Vignesh Adhinarayanan, Balaji Subramaniam, Wu-chun Feng|1.2222222222222223|3
156|Parallel High-Fidelity Electromagnetic Transient Simulation of Large-Scale Multi-Terminal DC Grids|Electromagnetic transient (EMT) simulation of power electronics conducted on the CPU slows down as the system scales up. Thus, the massively parallelism of the graphics processing unit (GPU) is utilized to expedite the simulation of the multi-terminal DC (MTDC) grid, where detailed models of the semiconductor switches are adopted to provide comprehensive device-level information. As the large number of nodes leads to an inefficient solution of the DC grid, three levels of circuit partitioning are applied, i.e., the transmission line-based natural separation of converter stations, splitting of the apparatus inside the station, and the coupled voltage-current sources for fine-grained partitioning. Components of similar attributes are written as one CUDA C function and computed in massive parallelism by means of single-instruction multi-threading. The GPU’s potential as a new EMT simulation platform for the analysis of large-scale MTDC grids is demonstrated by a remarkable speedup of up to 270 times for the Greater CIGRÉ DC grid with time-steps of 50 ns and  $1~\mu \text{s}$  for device-level and system-level simulation over the CPU implementation. Finally, the accuracy of GPU simulation is validated by the commercial tools SaberRD and PSCAD/EMTDC.|IEEE Power and Energy Technology Systems Journal|2019|10.1109/JPETS.2018.2881483|Ning Lin, V. Dinavahi|1.1666666666666667|3
157|Know Your Enemy To Save Cloud Energy: Energy-Performance Characterization of Machine Learning Serving|The proportion of machine learning (ML) inference in modern cloud workloads is rapidly increasing, and graphic processing units (GPUs) are the most preferred computational accelerators for it. The massively parallel computing capability of GPUs is well-suited to the inference workloads but consumes more power than conventional CPUs. Therefore, GPU servers contribute significantly to the total power consumption of a data center. However, despite their heavy power consumption, GPU power management in cloud-scale has not yet been actively researched. In this paper, we reveal three findings about energy efficiency of ML inference clusters in the cloud. ❶ GPUs of different architectures have comparative advantages in energy efficiency to each other for a set of ML models. ❷ The energy efficiency of a GPU set may significantly vary depending on the number of active GPUs and their clock frequencies even when producing the same level of throughput. ❸ The service level objective(SLO)-blind dynamic voltage and frequency scaling (DVFS) driver of commercial GPUs maintain an immoderately high clock frequency. Based on these implications, we propose a hierarchical GPU resource management approach for cloud-scale inference services. The proposed approach consists of energy-aware cluster allocation, intra-cluster node scaling, intra-node GPU scaling and GPU clock scaling schemes considering the inference service architecture hierarchy. We evaluated our approach with its prototype implementation and cloud-scale simulation. The evaluation with real-world traces showed that the proposed schemes can save up to 28.3% of the cloud-scale energy consumption when serving five ML models with 105 servers having three different kinds of GPUs.|International Symposium on High-Performance Computer Architecture|2023|10.1109/HPCA56546.2023.10070943|Junyeol Yu, Jongseok Kim, Euiseong Seo|1.0|3
158|Runtime Construction of Large-Scale Spiking Neuronal Network Models on GPU Devices|Simulation speed matters for neuroscientific research: this includes not only how quickly the simulated model time of a large-scale spiking neuronal network progresses but also how long it takes to instantiate the network model in computer memory. On the hardware side, acceleration via highly parallel GPUs is being increasingly utilized. On the software side, code generation approaches ensure highly optimized code at the expense of repeated code regeneration and recompilation after modifications to the network model. Aiming for a greater flexibility with respect to iterative model changes, here we propose a new method for creating network connections interactively, dynamically, and directly in GPU memory through a set of commonly used high-level connection rules. We validate the simulation performance with both consumer and data center GPUs on two neuroscientifically relevant models: a cortical microcircuit of about 77,000 leaky-integrate-and-fire neuron models and 300 million static synapses, and a two-population network recurrently connected using a variety of connection rules. With our proposed ad hoc network instantiation, both network construction and simulation times are comparable or shorter than those obtained with other state-of-the-art simulation technologies while still meeting the flexibility demands of explorative network modeling.|Applied Sciences|2023|10.3390/app13179598|B. Golosio, Johanna Senk, E. Pastorelli, A. Morrison, V. Fanti, P. Paolucci, Gianmarco Tiddia, Jose Villamar, J. Stapmanns|1.0|3
171|Getting the Ball Rolling: Learning a Dexterous Policy for a Biomimetic Tendon-Driven Hand with Rolling Contact Joints|Biomimetic, dexterous robotic hands have the potential to replicate much of the tasks that a human can do, and to achieve status as a general manipulation platform. Recent advances in reinforcement learning (RL) frameworks have achieved remarkable performance in quadrupedal locomotion and dexterous manipulation tasks. Combined with GPU-based highly parallelized simulations capable of simulating thousands of robots in parallel, RL-based controllers have become more scalable and approachable. However, in order to bring RL-trained policies to the real world, we require training frameworks that output policies that can work with physical actuators and sensors as well as a hardware platform that can be manufactured with accessible materials yet is robust enough to run interactive policies. This work introduces the biomimetic tendon-driven Faive Hand and its system architecture, which uses tendon-driven rolling contact joints to achieve a 3D printable, robust high-DoF hand design. We model each element of the hand and integrate it into a GPU simulation environment to train a policy with RL, and achieve zero-shot transfer of a dexterous in-hand sphere rotation skill to the physical robot hand.11https://srl-ethz.github.io/get-ball-rolling/video:https://youtu.be/YahsMhqNU8o|IEEE-RAS International Conference on Humanoid Robots|2023|10.1109/Humanoids57100.2023.10375231|U. Steger, Barnabas Gavin Cangan, Manuel Knecht, Stefan Weirich, B. Forrai, Robert K. Katzschmann, Yasunori Toshimitsu|1.0|3
178|Efficient Performance Estimation and Work-Group Size Pruning for OpenCL Kernels on GPUs|Graphic Processing Units (GPUs) play a vital role in state-of-the-art high-performance scientific computing realm and research work towards its performance analysis is crucial but nontrivial. Extant GPU performance models are far from practical use, while fine-grained GPU simulation requires a considerably large time cost. Moreover, massive amounts of designs with various program inputs and parameter settings pose a challenge for efficient performance estimation and tuning of parallel GPU applications. To this end, this article presents a hybrid framework for the efficient performance estimation and work-group size pruning of OpenCL workloads on GPUs. The framework contains a static module used to extract the kernel execution trace from the high-level source code and a dynamical module used to mimic the kernel execution flow to estimate the runtime performance. For the design space pruning, an extra analysis is performed to filter out the redundant work-group sizes with duplicated execution traces and inferior pipelines. The proposed framework does not require any program runs to estimate the performance and find the optimal or near-optimal designs. Experiments on four Commercial Off-The-Shelf (COTS) Nvidia GPUs show that the framework can predict the runtime performance with an average error of 17.04 percent and reduce the program design space by an average of 78.47 percent.|IEEE Transactions on Parallel and Distributed Systems|2020|10.1109/TPDS.2019.2958343|Kai Huang, Xuehai Qian, A. Knoll, Xiebing Wang|0.6|3
168|Fast GPU simulations of the FRAP experiment|In this thesis we study several models of the Fluorescence Recovery After Photobleaching (FRAP) experiment. FRAP is a technique used to estimate the diﬀusion coefficient of ﬂuids based on Confocal Laser Scanning Microscopy (CLSM). A ﬂuorescent sample is ﬁrst photobleached on a user deﬁned region. Then, by studying the recovery of ﬂuorescence in the bleaching region, one can retrieve important parameters of the ﬂuid such as the diffusion coefficient and binding constants by ﬁtting a model to the data. We implemented and compared three models of the FRAP experiment. The ﬁrst model assumes bleaching and image acquisition is an instantaneous process. The second model, based on the ﬁrst one, introduces multiple bleach frames. The ﬁnal model takes into account the scanning movement of the CLSM and is computationally much more complex. For the instantaneous models, two schemes are introduced and compared against each other to ensure correct implementation of the algorithms. The ﬁrst scheme uses the spectral method to solve the diﬀusion-reaction equations and the second uses a stochastic formulation of the problem. The last model, due to its complexity, has only been implemented stochasticaly. All three models have been implemented on Graphical Processing Units (GPUs) using the OpenCL API in C++. The GPU has a massively parallel architecture that can be exploited for scientiﬁc computing. These schemes are ”embarrassingly parallel” and thus suitable for a GPU implementation. By comparing the diﬀerent models, we see that a good compromise between precision and computing resource is given by the instantaneous bleaching with multiple bleach frames model. Because of the scanning nature of the CLSM, we would expect the last model to reveal some asymmetry in the results. These were only found for extreme and unrealistic parameters and it is thus not necessary to simulate the FRAP experiment with such complexity.||2018|10.23919/eusipco.2018.8553615|Leander Lacroix|0.5714285714285714|3
179|Synchronous speculative simulation of tightly coupled agents in continuous time on CPUs and GPUs|Traditionally, parallel discrete-event simulations of agent-based models in continuous time are organized around logical processes exchanging time-stamped events, which clashes with the properties of models in which tightly coupled agents frequently and instantaneously access each other’s states. To illustrate the challenges of such models and to derive a solution, we consider the domain-specific modeling language ML3, which allows modelers to succinctly express transitions and interactions of linked agents based on a continuous-time Markov chain (CTMC) semantics. We propose synchronous optimistic synchronization algorithms tailored toward simulations of fine-grained interactions among tightly coupled agents in highly dynamic topologies and present implementations targeting multicore central processing units (CPUs) as well as many-core graphics processing units (GPUs). By dynamically restricting the temporal progress per round to ensure that at most one transition or state access per agent, the synchronization algorithms enable efficient direct agent interaction and limit the required agent state history to only a single current and projected state. To maintain concurrency given actions that depend on dynamically updated macro-level properties, we introduce a simple relaxation scheme with guaranteed error bounds. Using an extended variant of the classical susceptible-infected-recovered network model, we benchmark and profile the performance of the different algorithms running on CPUs and on a data center GPU.|International Conference on Advances in System Simulation|2023|10.1177/00375497231158930|Philipp Andelfinger, A. Uhrmacher|0.5|3
159|ALUPower: Data Dependent Power Consumption in GPUs|Existing architectural power models for GPUs count activities such as executing floating point or integer instructions, but do not consider the data values processed. While data value dependent power consumption can often be neglected when performing architectural simulations of high performance Out-of-Order (OoO) CPUs, we show that this approach is invalid for estimating the power consumption of GPUs. The throughput processing approach of GPUs reduces the amount of control logic and shifts the area and power budget towards functional units and register files. This makes accurate estimations of the power consumption of functional units even more crucial than in OoO CPUs. Using measurements from actual GPUs, we show that the processed data values influence the energy consumption of GPUs significantly. For example, the power consumption of one kernel varies between 155 and 257 Watt depending on the processed values. Existing architectural simulators are not able to model the influence of the data values on power consumption. RTL and gate level simulators usually consider data values in their power estimates but require detailed modeling of the employed units and are extremely slow. We first describe how the power consumption of GPU functional units can be measured and characterized using microbenchmarks. Then measurement results are presented and several opportunities for energy reduction by software developers or compilers are described. Finally, we demonstrate a simple and fast power macro model to estimate the power consumption of functional units and provide a significant improvement in accuracy compared to previously used constant energy per instruction models.|IEEE/ACM International Symposium on Modeling, Analysis, and Simulation On Computer and Telecommunication Systems|2016|10.1109/MASCOTS.2016.21|J. Lucas, B. Juurlink|0.4444444444444444|3
163|Special Issue on Recent Trends and Future of Fog and Edge Computing, Services and Enabling Technologies Editorial File|Recent Trends and Future of Fog and Edge Computing, Services, and Enabling Technologies \nCloud computing has been established as the most popular as well as suitable computing infrastructure providing on-demand, scalable and pay-as-you-go computing resources and services for the state-of-the-art ICT applications which generate a massive amount of data. Though Cloud is certainly the most fitting solution for most of the applications with respect to processing capability and storage, it may not be so for the real-time applications. The main problem with Cloud is the latency as the Cloud data centres typically are very far from the data sources as well as the data consumers. This latency is ok with the application domains such as enterprise or web applications, but not for the modern Internet of Things (IoT)-based pervasive and ubiquitous application domains such as autonomous vehicle, smart and pervasive healthcare, real-time traffic monitoring, unmanned aerial vehicles, smart building, smart city, smart manufacturing, cognitive IoT, and so on. The prerequisite for these types of application is that the latency between the data generation and consumption should be minimal. For that, the generated data need to be processed locally, instead of sending to the Cloud. This approach is known as Edge computing where the data processing is done at the network edge in the edge devices such as set-top boxes, access points, routers, switches, base stations etc. which are typically located at the edge of the network. These devices are increasingly being incorporated with significant computing and storage capacity to cater to the need for local Big Data processing. The enabling of Edge computing can be attributed to the Emerging network technologies, such as 4G and cognitive radios, high-speed wireless networks, and energy-efficient sophisticated sensors. \nDifferent Edge computing architectures are proposed (e.g., Fog computing, mobile edge computing (MEC), cloudlets, etc.). All of these enable the IoT and sensor data to be processed closer to the data sources. But, among them, Fog computing, a Cisco initiative, has attracted the most attention of people from both academia and corporate and has been emerged as a new computing-infrastructural paradigm in recent years. Though Fog computing has been proposed as a different computing architecture than Cloud, it is not meant to replace the Cloud. Rather, Fog computing extends the Cloud services to network edges for providing computation, networking, and storage services between end devices and data centres. Ideally, Fog nodes (edge devices) are supposed to pre-process the data, serve the need of the associated applications preliminarily, and forward the data to the Cloud if the data are needed to be stored and analysed further. \nFog computing enhances the benefits from smart devices operational not only in network perimeter but also under cloud servers. Fog-enabled services can be deployed anywhere in the network, and with these services provisioning and management, huge potential can be visualized to enhance intelligence within computing networks to realize context-awareness, high response time, and network traffic offloading. Several possibilities of Fog computing are already established. For example, sustainable smart cities, smart grid, smart logistics, environment monitoring, video surveillance, etc. \nTo design and implementation of Fog computing systems, various challenges concerning system design and implementation, computing and communication, system architecture and integration, application-based implementations, fault tolerance, designing efficient algorithms and protocols, availability and reliability, security and privacy, energy-efficiency and sustainability, etc. are needed to be addressed. Also, to make Fog compatible with Cloud several factors such as Fog and Cloud system integration, service collaboration between Fog and Cloud, workload balance between Fog and Cloud, and so on need to be taken care of. \nIt is our great privilege to present before you Volume 20, Issue 2 of the Scalable Computing: Practice and Experience. We had received 20 Research Papers and out of which 14 Papers are selected for Publication. The aim of this special issue is to highlight Recent Trends and Future of Fog and Edge Computing, Services and Enabling technologies. The special issue will present new dimensions of research to researchers and industry professionals with regard to Fog Computing, Cloud Computing and Edge Computing. \nSujata Dash et al. contributed a paper titled “Edge and Fog Computing in Healthcare- A Review” in which an in-depth review of fog and mist computing in the area of health care informatics is analysed, classified and discussed. The review presented in this paper is primarily focussed on three main aspects: The requirements of IoT based healthcare model and the description of services provided by fog computing to address then. The architecture of an IoT based health care system embedding fog computing layer and implementation of fog computing layer services along with performance and advantages. In addition to this, the researchers have highlighted the trade-off when allocating computational task to the level of network and also elaborated various challenges and security issues of fog and edge computing related to healthcare applications. \nParminder Singh et al. in the paper titled “Triangulation Resource Provisioning for Web Applications in Cloud Computing: A Profit-Aware” proposed a novel triangulation resource provisioning (TRP) technique with a profit-aware surplus VM selection policy to ensure fair resource utilization in hourly billing cycle while giving the quality of service to end-users. The proposed technique use time series workload forecasting, CPU utilization and response time in the analysis phase. The proposed technique is tested using CloudSim simulator and R language is used to implement prediction model on ClarkNet weblog. The proposed approach is compared with two baseline approaches i.e. Cost-aware (LRM) and (ARMA). The response time, CPU utilization and predicted request are applied in the analysis and planning phase for scaling decisions. The profit-aware surplus VM selection policy used in the execution phase for select the appropriate VM for scale-down. The result shows that the proposed model for web applications provides fair utilization of resources with minimum cost, thus provides maximum profit to application provider and QoE to the end users. \n  \nAkshi kumar and Abhilasha Sharma in the paper titled “Ontology driven Social Big Data Analytics for Fog enabled Sentic-Social Governance” utilized a semantic knowledge model for investigating public opinion towards adaption of fog enabled services for governance and comprehending the significance of two s-components (sentic and social) in aforesaid structure that specifically visualize fog enabled Sentic-Social Governance. The results using conventional TF-IDF (Term Frequency-Inverse Document Frequency) feature extraction are empirically compared with ontology driven TF-IDF feature extraction to find the best opinion mining model with optimal accuracy. The results concluded that implementation of ontology driven opinion mining for feature extraction in polarity classification outperforms the traditional TF-IDF method validated over baseline supervised learning algorithms with an average of 7.3% improvement in accuracy and approximately 38% reduction in features has been reported. \n  \n  \nAvinash Kaur and Pooja Gupta in the paper titled “Hybrid Balanced Task Clustering Algorithm for Scientific workflows in Cloud Computing” proposed novel hybrid balanced task clustering algorithm using the parameter of impact factor of workflows along with the structure of workflow and using this technique, tasks can be considered for clustering either vertically or horizontally based on value of impact factor. The testing of the algorithm proposed is done on Workflowsim- an extension of CloudSim and DAG model of workflow was executed. The Algorithm was tested on variables- Execution time of workflow and Performance Gain and compared with four clustering methods: Horizontal Runtime Balancing (HRB), Horizontal Clustering (HC), Horizontal Distance Balancing (HDB) and Horizontal Impact Factor Balancing (HIFB) and results stated that proposed algorithm is almost 5-10% better in makespan time of workflow depending on the workflow used. \nPijush Kanti Dutta Pramanik et al. in the paper titled “Green and Sustainable High-Performance Computing with Smartphone Crowd Computing: Benefits, Enablers and Challenges” presented a comprehensive statistical survey of the various commercial CPUs, GPUs, SoCs for smartphones confirming the capability of the SCC as an alternative to HPC. An exhaustive survey is presented on the present and optimistic future of the continuous improvement and research on different aspects of smartphone battery and other alternative power sources which will allow users to use their smartphones for SCC without worrying about the battery running out. \nDhanapal and P. Nithyanandam in the paper titled “The Slow HTTP Distributed Denial of Service (DDOS) Attack Detection in Cloud” proposed a novel method to detect slow HTTP DDoS attacks in cloud to overcome the issue of consuming all available server resources and making it unavailable to the real users. The proposed method is implemented using OpenStack cloud platform with slowHTTPTest tool. The results stated that proposed technique detects the attack in efficient manner. \nMandeep Kaur and Rajni Mohana in the paper titled “Static Load Balancing Technique for Geographically partitioned Public Cloud” proposed a novel approach focused upon load balancing in the partitioned public cloud by combining centralized and decentralized approaches, assuming the presence of fog layer. A load balancer entity is used for decentralized load balancing at partitions and a co|Scalable Computing : Practice and Experience|2019|10.12694/SCPE.V20I2.1558|P. K. Pramanik, A. Nayyar, Rudra Rameshwar|0.3333333333333333|3
169|Mobile AP GPU power distribution network simulation and analysis based on chip power model|These days, mobile devices require low-power consumption. To meet these requirements, IC's operating voltage is continuously lowered. As a result, power noise margin decreases which require more precise Power Distribution Network (PDN) design. In this work, we simulated and analyzed the mobile Application Processor (AP) GPU system based on Chip Power Model (CPM). We applied GPU's current model to simulate simultaneous switching noise and power noise in the chip PDN. To verify the model and simulation set-up, we measured voltage ripple and compared with simulation. We concluded that our simulation setup is reliable and conducted power integrity case studies for the future PDN design.|2016 IEEE International Symposium on Electromagnetic Compatibility (EMC)|2016|10.1109/ISEMC.2016.7571668|Taisik Yang, Heegon Kim, Jonghyun Cho, Youngwoo Kim, Joungho Kim, Yun Ra, Kibum Kang, Woohyun Paik|0.3333333333333333|3
160|Surrogate Modelling for Efficient Discovery of Emergent Population Dynamics|Outcomes of simulating complex systems models, such as emergent properties and desirable system level behaviours, can be discovered via heuristic techniques such as Genetic Algorithms (GAs). Using simulation as the cost function evaluation for a GA (i.e. simulation guided search) is computationally expensive. Additionally the GA search process may require many generations before high quality solutions can be discovered. As such, simulation guided search can be considered high latency with respect to discovery of a range of high quality solutions. In this paper we experimentally demonstrate that the time to discovery of high quality solutions can be reduced through a low latency, hybrid GA search using a machine learning surrogate model trained to approximate simulation via large amounts of batched parallel simulation data generated in a HPC environment. Using a common population dynamics model optimised for GPU simulation by the FLAME GPU framework, we directly compare the hybrid approach with simulation guided search to understand the relationship between computational cost and quality of prediction. Our results indicate that given equivalent levels of simulation investment, results of equivalent quality can be obtained. The hybrid approach is however able to reduce the latency of the GA search process by shifting the computational cost of simulation to a highly parallel pre-search step used to train surrogate models.|International Symposium on High Performance Computing Systems and Applications|2019|10.1109/HPCS48598.2019.9188208|M. Chimeh, P. Richmond, James Pyle|0.16666666666666666|3
161|Accelerating Large Scale Artificial Society Simulation with CPU/GPU Based Heterogeneous Parallel Method|Artificial society is an effective way for social science research. However, in order to meet real-time and super real-time requirement of computational experiment, the execution efficiency of large-scale artificial society then becomes the burning question. The emergence of heterogeneous parallel system offers opportunities and challenges for accelerating large scale artificial society simulation. How to fully utilize heterogeneous computational resources in large scale agent based simulation becomes the key issue. The paper proposes a CPU/GPU-based accelerating computational method, in which GPU is fully utilized in two different ways at the same time. Firstly GPU is treated as host processor, and a GPU based simulation kernel is designed to execute the models collaboratively with CPU simulation kernel. Secondly, in order to accelerate the domain-specific models, a specific domain-oriented GPU simulation computational service component is proposed, and GPU is used as a co-processor to offer domain-specific parallel optimization. A SPMT (Single Process Multi Threads) based conservative parallel simulation framework is proposed to integrate the GPU simulation kernel and computational service component. At last, an experiment is designed to test the efficiency of GPU based simulation kernel, and argues about the application mode of GPU.|IEEE International Symposium on Distributed Simulation and Real-Time Applications|2015|10.1109/DS-RT.2015.11|Chen Bin, Qiu Xiaogang, Guo Gang, Li Zhen|0.1|3
164|FAcET: Fast and accurate power/energy estimation tool for CPU-GPU platforms at architectural-level|This paper proposes a novel fast and accurate architectural-level tool to estimate power and energy (FAcET) for heterogeneous (CPU-GPU) system architecture based platforms. FAcET consists of two components. The first is a set of generic parametrizable power models generated by characterizing the functional-level activities for different blocks of the chosen platforms. The second is a simulation-based architectural-level prototype that uses SystemC (JIT) simulators to accurately evaluate the parameters of the corresponding power models of the first component. The combination of the two components leads to a novel power and energy estimation methodology at the architectural level that provides a better balance between speed and accuracy. The efficacy of the FAcET tool is verified against measurements taken on real board platforms, which consist of low-power ARM quad-core processors (Cortex-A7, -A9 and -A15), NVIDIA GPUs (Quadro 1000M, Quadro FX5600, Tegra K1, and GTX480) and heterogeneous platforms (NVIDIA Tegra3 and NVIDIA Jetson TK1). Power and energy estimation results obtained with FAcET deviate in less than 3.6% for quad-core processors, 6.5% for GPU, 10% for heterogeneous multiprocessor based systems from the measurements and estimation is 15x faster than state-of-the-art tools.|ACM Symposium on Cloud Computing|2015|10.1109/SOCC.2015.7406947|A. Cristal, J. Moreno, Oscar Palomar, S. Rethinagiri, O. Unsal|0.0|3
165|Accelerated GPU simulation of the gaseous detonation cell structure|The aim of the present paper is to report on our recent results for GPU accelerated simulations of the gaseous detonation structure. Reactive Euler equations with a one-step Arrhenius chemistry model have been used for numerical simulation. And the NND space discretization scheme combined with Steger-Warming split method has been used. For time discretization we have applied the explicit third order Runge-Kutta method. We have obtained a speedup of 8 times ( in comparison to 30 threads openmp program) for the gaseous detonation simulation on a structure grid of 320 million points.|Journal of Physics: Conference Series|2021|10.1088/1742-6596/2012/1/012079|Rui Q. Yang, Chun Wang|0.0|3
166|Decentralized Training of Foundation Models in Heterogeneous Environments|"Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational""tasklets""in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8X faster than prior state-of-the-art training systems (Megatron)."|Neural Information Processing Systems|2022|10.48550/arXiv.2206.01288|Yongjun He, Tianyi Zhang, Beidi Chen, Percy Liang, Tri Dao, Jared Davis, Ce Zhang, Christopher Ré, Binhang Yuan|0.0|3
170|Fundamentals of a numerical cloud computing for applied sciences|This paper is supposed to be used as a contribution for the `Consultation on Cloud Computing Research Innovation Challenges for WP 2018-2020 ́ as called for by the European Commission (DG CONNECT, unit `Cloud and software ́). We propose to encourage and support fundamental interdisciplinary research for making the benefits generated by cloud computing accessible to the applied science community. Introduction: Why cloud computing and high performance computing are contradicting The basic idea of cloud computing (CC) is to abstract from an IT infrastructure including compute-, memory-, networkingand software resources by virtualization. These resources are made accessible to the user in a dynamic and adaptive way. The major resulting advantages compared to a specially tailored `in-house solution ́ can be found in a transparent and simple usage, enhanced flexibility due to scalability and adaptivity to a specific need and finally in the increased efficiency due to savings in energy and money spent. The latter is due to scaling effects, operational efficiency, consolidation of resources and reduction of risks. The application is literally independent from any (local) data and compute resources as these can be concentrated effectively. All together, these advantages may some day supersede the traditional local / regional data center approach which can be found on the level of modern universities and research centers. From the point of view of data center management and operations, CC leads to a higher occupancy and therefore efficiency: The inevitable granularity effects that occur with medium or large workloads can be tackled with a backfilling of many small jobs. In addition, due to the fact that a specific application run's need for resources may vary from time to time, left-over capacities can be provided in a profitable `pay per use ́ style. In High Performance Computing (HPC) on the other hand, virtualization and abstraction concepts contradict the usual approaches especially in the simulation of technical processes: Here, the focus is put on enhancing the performance of an application by explicitly optimize for a certain type of hardware. This requires an a priori knowledge of the hardware which usually is given by the fact, that universities and regional research facilities have their own local or regional compute centers with comparatively static hardware components. This point of view can in some cases generate several orders of magnitude of performance gains and we call this concept hardware-oriented numerics. This paradigm comprises the simultaneous optimization for hardware, numerical and energy efficiency on all levels of application development [1,2,3,4]. One effort in hardware-oriented numerics is to optimize code and develop or choose numerical methods with respect to a heterogeneous hardware ecosystem: Multicore CPUs are as straight-forward as hardware accelerators like GPUs, FPGAs, Xeon Phi processors and system on a chip designs such as ARM-based CPUs with integrated GPUs. In addition, there are non-uniform memory architectures on the device level as well as heterogeneous communication infrastructures on the cluster level. The usual design pattern however is to optimize code for a (single) given hardware configuration where the simulation code is then optimized in a comparatively expensive way due to this proximity to hardware details. This development process is therefore the complete opposite of relying on a virtualization approach. Today's scientific cloud computing is not feasible for numerical simulation Up to today all efforts to make use of CC techniques in the science community can be characterized by what we call scientific cloud computing (SCC), which basically has been very successful for a specific type of application: In the scope of Big Data often a direct projection of a problem to a bag of tasks programming model can be found. Also other problems that are constituted by smaller independent tasks, where the coupling and therefore communication is minimal or zero can be coped with easily in a cloud environment. In numerical simulation on the other hand a strong coupling of the very computationally intense subproblems is the standard case. This induces a comparatively high synchronization need, requiring low communication latencies. The execution models of CC are literally blind for this type of strong coupling because the virtualization shuts down any attempt to optimize inter process communication. We believe, that the development of numerical simulation software should be characterized by the synthesis of hardware, numerical, and energy efficiency. Hence for this type of application a CC concept which takes into account the heterogeneity of compute hardware would be most feasible: According to our vision in future scenarios the user of such codes might want to choose for run time optimization in different metrics: Flexibility in the selection in which way a specific run should be allocated to a certain type(s) of compute node(s) are required. This flexibility has not been accounted for in the development of numerical code frameworks yet. A direct result of the service providers internalizing the concept of hardware-oriented numerics would be that the user of the service would be able to make an a priori choice for the core requirements for the run. For instance it could be decided whether an allocation of hardware should be made in order to minimize wall clock time or minimize energy to solution. Other hardware specifics could be made allocateable such as the type and properties of the communication links between nodes. The service would then return a number of allocations based upon available hardware. After selection, a complex optimization problem then has to be solved: The simulation software has to be able to select numerical algorithmic components that fit to this allocation and finally, a load balancing has to be performed for the individual problem to be solved. Towards a numerical cloud computing In order to realize this vision, there are two fundamental problems to solve: (1) Specially tailored numerics as well as load balancing strategies as well as (2) mapping, scheduling and operation strategies for numerical simulation have to be developed. In (1) numerical components in a code framework have to be revisited or developed from scratch with respect to (2) by adjusting them to the respective strategies. Such numerical alternatives range from preconditioners in linear solvers to whole discretization approaches on the model level. Different hardware specific implementations have to be provided and tuned in order to enable the optimizer in (2) to succeed, which is closely related to performance engineering. This has to be undergone with respect to all levels of parallelism in modern hardware architectures and on all levels of an application. On the other hand, the systems / strategies developed in (2) have to be sensitive for the effects of specific numerics on specific hardware. This problem is often closely related to numerical scaling, convergence and complexity theories. These theories and related skills are usually not addressed as an integral part of the training in computer science or service providers / operators. Here an automatic tuning system has to be developed that is capable of deciding what type of numerics is to be used for a given hardware allocation and which parts of the data are distributed to which part of the hardware by a static or even dynamic load balancing. The latter is an even more complex problem keeping in mind the heterogeneity even within one specific allocation, where CPUs are for instance to be saturated alongside GPUs. This optimization problem is very similar to how compilers schedule instructions on the processor level. It is also multi-dimensional, as not only raw performance has to be optimized for but also energy to solution as stated in the previous section. Hence we emphasize that these two components, (1) and (2), cannot be brought up independently: Specialists from the domain of applied mathematics, performance engineers and application specialists are required for the former, whereas the latter is to be coped with by computer sciences and service providers / specialists.||2017|10.17877/DE290R-17825|M. Geveler, S. Turek|0.0|3
174|GPU-Parallelized Simulation of Optical Forces on Nanoparticles in a Fluid Medium|Experimental research in physics can be a costly and time-consuming venture, requiring simulation-based approaches to effectively narrow down the scope of experiments to only the most promising cases. Our multidisciplinary research in this paper demonstrates how the simulation of light-driven nanoparticles can substantially benefit from GPU-based parallelism. We develop a novel ray-tracing strategy and we implement it in C++/CUDA and extend it with a parallel differential equation solver. Our implementation relies on a custom memory layout optimization to tackle the computational challenges in the field and provide accurate solutions in near real time. We evaluate our approach on a variety of popular GPU architectures, including advanced data-center GPUs like the Nvidia V100, as well as consumer-grade hardware like the Nvidia RTX 2080 Ti and Nvidia GTX 1080. Our GPU-based approach achieves a speedup of up to $20\times$ compared to a parallel CPU-based prototype implementation.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2023|10.1109/IPDPSW59300.2023.00114|Florian Fey, Alexander Gerwing, S. Gorlatch|0.0|3
176|Power modeling and architectural techniques for energy-efficient GPUs|Graphics Processing Units (GPUs) have evolved from fixed function graphics processors to programmable general-purpose compute accelerators in a short time. The high theoretical performance and energy efficiency of GPUs compared to CPUs have made them indispensable for mainstream computing. However, their high power consumption and limited energy efficiency under low utilization is a challenge that still needs to be tackled. This thesis investigates bottlenecks that cause low performance and low energy efficiency in GPUs and proposes architectural techniques to address them. To conduct energy efficiency research for GPUs, we first develop a flexible and accurate power simulator called GPUSimPow. We use a hybrid approach for power modeling that improves flexibility and accuracy compared to previous approaches. Our evaluation shows an average relative error of 11.7% and 10.8% between simulated and measured power for the NVIDIA GT240 and GTX580, respectively. We then use GPUSimPow to study the energy efficiency of a wide range of kernels and categorize them into high performance and low performance. We further investigate the bottlenecks of low-performance kernels by analyzing their occupancy. We quantify the gain in performance and energy efficiency when occupancy is increased. For instance, the average increase in instructions per cycle, the average reduction in energy consumption and energy-delay-product is 11%, 9%, and 23%, respectively, when occupancy is increased for a sub-category of low occupancy kernels. The full occupancy kernels have low performance despite having the maximum number of threads. Further investigation shows that several of these kernels are memory-bound and can gain significantly from an increase in memory bandwidth. The traditional ways of increasing memory bandwidth by widening interfaces and increasing frequency have issues such as high power consumption, large form factor, and difficulty in the scaling of pin count. Memory compression is a promising alternative to increase the effective memory bandwidth of GPUs, however, we find that the existing memory compression techniques for GPUs exploit simple patterns for compression and trade low compression ratios for low decompression latency. Based on the evidence that GPUs are less sensitive to latency than CPUs, we propose the more complex Entropy Encoding Based Memory Compression (E2MC) technique for GPUs. On average, E2MC delivers 53% higher compression ratio and 8% higher speedup than the state of the art. E2MC reduces energy consumption and energy-delay-product by 13% and 27%, respectively. While designing E2MC, we observe that lossless memory compression techniques including E2MC often have a low effective compression ratio due to the large memory access granularity (MAG) exhibited by GPUs. Our study of the distribution of compressed blocks reveals that a significant percentage of compressed blocks have only a few||2019|10.14279/DEPOSITONCE-9156|S. Lal|0.0|3
523|DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames|"We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever ""stale""), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. \n\nThis massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially ""solves"" the task -- near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ""ImageNet pre-training + task-specific fine-tuning"" for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available)."|International Conference on Learning Representations|2019|10.1007/s11263-019-01228-7|Ari S. Morcos, Devi Parikh, Stefan Lee, Erik Wijmans, M. Savva, Irfan Essa, Dhruv Batra, Abhishek Kadian|507.6666666666667|2
566|New ways to boost molecular dynamics simulations|We describe a set of algorithms that allow to simulate dihydrofolate reductase (DHFR, a common benchmark) with the AMBER all‐atom force field at 160 nanoseconds/day on a single Intel Core i7 5960X CPU (no graphics processing unit (GPU), 23,786 atoms, particle mesh Ewald (PME), 8.0 Å cutoff, correct atom masses, reproducible trajectory, CPU with 3.6 GHz, no turbo boost, 8 AVX registers). The new features include a mixed multiple time‐step algorithm (reaching 5 fs), a tuned version of LINCS to constrain bond angles, the fusion of pair list creation and force calculation, pressure coupling with a “densostat,” and exploitation of new CPU instruction sets like AVX2. The impact of Intel's new transactional memory, atomic instructions, and sloppy pair lists is also analyzed. The algorithms map well to GPUs and can automatically handle most Protein Data Bank (PDB) files including ligands. An implementation is available as part of the YASARA molecular modeling and simulation program from www.YASARA.org. © 2015 The Authors Journal of Computational Chemistry Published by Wiley Periodicals, Inc.|Journal of Computational Chemistry|2015|10.1002/jcc.23899|E. Krieger, G. Vriend|78.1|2
220|Assessing the Current State of Amber Force Field Modifications for DNA|The utility of molecular dynamics (MD) simulations to model biomolecular structure, dynamics, and interactions has witnessed enormous advances in recent years due to the availability of optimized MD software and access to significant computational power, including GPU multicore computing engines and other specialized hardware. This has led researchers to routinely extend conformational sampling times to the microsecond level and beyond. The extended sampling time has allowed the community not only to converge conformational ensembles through complete sampling but also to discover deficiencies and overcome problems with the force fields. Accuracy of the force fields is a key component, along with sampling, toward being able to generate accurate and stable structures of biopolymers. The Amber force field for nucleic acids has been used extensively since the 1990s, and multiple artifacts have been discovered, corrected, and reassessed by different research groups. We present a direct comparison of two of the most recent and state-of-the-art Amber force field modifications, bsc1 and OL15, that focus on accurate modeling of double-stranded DNA. After extensive MD simulations with five test cases and two different water models, we conclude that both modifications are a remarkable improvement over the previous bsc0 force field. Both force field modifications show better agreement when compared to experimental structures. To ensure convergence, the Drew–Dickerson dodecamer (DDD) system was simulated using 100 independent MD simulations, each extended to at least 10 μs, and the independent MD simulations were concatenated into a single 1 ms long trajectory for each combination of force field and water model. This is significantly beyond the time scale needed to converge the conformational ensemble of the internal portions of a DNA helix absent internal base pair opening. Considering all of the simulations discussed in the current work, the MD simulations performed to assess and validate the current force fields and water models aggregate over 14 ms of simulation time. The results suggest that both the bsc1 and OL15 force fields render average structures that deviate significantly less than 1 Å from the average experimental structures. This can be compared to similar but less exhaustive simulations with the CHARMM 36 force field that aggregate to the ∼90 μs time scale and also perform well but do not produce structures as close to the DDD NMR average structures (with root-mean-square deviations of 1.3 Å) as the newer Amber force fields. On the basis of these analyses, any future research involving double-stranded DNA simulations using the Amber force fields should employ the bsc1 or OL15 modification.|Journal of Chemical Theory and Computation|2016|10.1021/acs.jctc.6b00186|P. Jurečka, M. Otyepka, T. Cheatham, J. Šponer, R. Galindo-Murillo, Marie Zgarbová, James C Robertson|45.888888888888886|2
892|GPUMD: A package for constructing accurate machine-learned potentials and performing highly efficient atomistic simulations.|We present our latest advancements of machine-learned potentials (MLPs) based on the neuroevolution potential (NEP) framework introduced in Fan et al. [Phys. Rev. B 104, 104309 (2021)] and their implementation in the open-source package gpumd. We increase the accuracy of NEP models both by improving the radial functions in the atomic-environment descriptor using a linear combination of Chebyshev basis functions and by extending the angular descriptor with some four-body and five-body contributions as in the atomic cluster expansion approach. We also detail our efficient implementation of the NEP approach in graphics processing units as well as our workflow for the construction of NEP models and demonstrate their application in large-scale atomistic simulations. By comparing to state-of-the-art MLPs, we show that the NEP approach not only achieves above-average accuracy but also is far more computationally efficient. These results demonstrate that the gpumd package is a promising tool for solving challenging problems requiring highly accurate, large-scale atomistic simulations. To enable the construction of MLPs using a minimal training set, we propose an active-learning scheme based on the latent space of a pre-trained NEP model. Finally, we introduce three separate Python packages, viz., gpyumd, calorine, and pynep, that enable the integration of gpumd into Python workflows.|Journal of Chemical Physics|2022|10.1063/5.0106617|Jianyang Wu, P. Erhart, Z. Fan, Haikuan Dong, Keke Song, E. Lindgren, Ke Xu, Junjie Wang, A. Gabourie, Jiahui Liu, J. Rahm, Zezhu Zeng, Yue Chen, T. Ala‐Nissila, Yanzhou Wang, Yong Wang, Penghua Ying, Y. Su, Zheng Zhong, Jian Sun|41.666666666666664|2
888|ASE|The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.|ACM Transactions on Graphics|2022|10.1145/3528223.3530110|Yunrong Guo, S. Fidler, S. Levine, L. Halper, X. B. Peng|31.0|2
446|A practical light transport system model for chemiluminescence distribution reconstruction|Plenoptic cameras and other integral photography instruments capture richer angular information from a scene than traditional 2D cameras. This extra information is used to estimate depth, perform superresolution or reconstruct 3D information from the scene. Many of these applications involve solving a large-scale numerical optimization problem. Most published approaches model the camera(s) using pre-computed matrices that require large amounts of memory and are not well-suited to modern many-core processors. We propose a flexible camera model based on light transport and use it to model plenoptic and traditional cameras. We implement the proposed model on a GPU and use it to reconstruct simulated and real 3D chemiluminescence distributions (flames) from images taken by traditional and plenoptic cameras.||2018|10.1080/09542299.2018.1520050|Madison G. McGaffin, V. Sick, Hao Chen, J. Fessler|23.714285714285715|2
265|GPU-based simulation of cloth wrinkles at submillimeter levels|In this paper, we study physics-based cloth simulation in a very high resolution setting, presumably at submillimeter levels with millions of vertices, to meet perceptual precision of our human eyes. State-of-the-art simulation techniques, mostly developed for unstructured triangular meshes, can hardly meet this demand due to their large computational costs and memory footprints. We argue that in a very high resolution, it is more plausible to use regular meshes with an underlying grid structure, which can be highly compatible with GPU acceleration like high-resolution images. Based on this idea, we formulate and solve the nonlinear optimization problem for simulating high-resolution wrinkles, by a fast block-based descent method with reduced memory accesses. We also investigate the development of the collision handling component in our system, whose performance benefits greatly from the grid structure. Finally, we explore various issues related to the applications of our system, including initialization for fast convergence and temporal coherence, gathering effects, inflation and stuffing models, and mesh simplification. We can treat our system as a quasistatic wrinkle synthesis tool, run it as a standalone dynamic simulator, or integrate it into a multi-resolution solver as an additional component. The experiment demonstrates the capability, efficiency and flexibility of our system in producing a variety of high-resolution wrinkles effects.|ACM Transactions on Graphics|2021|10.1145/3450626.3459787|Huamin Wang|22.5|2
1078|QoE Fairness Resource Allocation in Digital Twin-Enabled Wireless Virtual Reality Systems|Wireless virtual reality (VR) is expected to be a technology that revolutionizes human interaction and perceived media, where the quality of experience (QoE) is an important indicator to measure user service perception. However, existing schemes only consider general and time-invariant QoE optimization, which may suffer performance degradation. Moreover, it is also necessary to ensure the fairness of the individual user’s performance in wireless VR. To address these challenges, we employ digital twin technology to investigate a max-min QoE-optimal problem for wireless VR systems in this paper. Specifically, we maximize the QoE of the worst-case head-mounted displays (HDMs) client, where the QoE model is the linear weighting combination of video quality, service delay, and energy efficiency. The formulated optimization problem is characterized by multidimensional control, which jointly optimizes model selection, transmit power, computation time, and GPU-cycle frequency. Due to the mixed combinatorial features of the optimization problem, we give a low-complexity algorithm design by decoupling the optimization variables. Notably, we first obtain the allocation of the transmit power by employing the generalized fractional programming theory and the Lagrangian dual decomposition, followed by attaining the optimal allocation of GPU-cycle frequency in VR mode is derived by the proposed adaptive modified harmony search algorithm, and finally achieve the computation time by the barrier method. Meanwhile, we devise a greedy-style heuristic algorithm for mode selection. In the simulation, three baseline schemes are established as comparisons to assess the effectiveness of the proposed scheme. Meanwhile, the simulation results manifest that the proposed algorithms have good convergence performance and better increase the QoE of the DT-enabled wireless VR system compared to benchmark solutions.|IEEE Journal on Selected Areas in Communications|2023|10.1109/JSAC.2023.3313195|Celimuge Wu, Xiangwang Hou, Jie Feng, Qingqi Pei, Lei Liu|22.0|2
834|Limitations of Linear Cross-Entropy as a Measure for Quantum Advantage|"Demonstrating quantum advantage requires experimental implementation of a computational task that is hard to achieve using state-of-the-art classical systems. One approach is to perform sampling from a probability distribution associated with a class of highly entangled many-body wavefunctions. It has been suggested that this approach can be certified with the Linear Cross-Entropy Benchmark (XEB). We critically examine this notion. First, in a""benign""setting where an honest implementation of noisy quantum circuits is assumed, we characterize the conditions under which the XEB approximates the fidelity. Second, in an""adversarial""setting where all possible classical algorithms are considered for comparison, we show that achieving relatively high XEB values does not imply faithful simulation of quantum dynamics. We present an efficient classical algorithm that, with 1 GPU within 2s, yields high XEB values, namely 2-12% of those obtained in experiments. By identifying and exploiting several vulnerabilities of the XEB, we achieve high XEB values without full simulation of quantum circuits. Remarkably, our algorithm features better scaling with the system size than noisy quantum devices for commonly studied random circuit ensembles. To quantitatively explain the success of our algorithm and the limitations of the XEB, we use a theoretical framework in which the average XEB and fidelity are mapped to statistical models. We illustrate the relation between the XEB and the fidelity for quantum circuits in various architectures, with different gate choices, and in the presence of noise. Our results show that XEB's utility as a proxy for fidelity hinges on several conditions, which must be checked in the benign setting but cannot be assumed in the adversarial setting. Thus, the XEB alone has limited utility as a benchmark for quantum advantage. We discuss ways to overcome these limitations."|arXiv.org|2021|10.1016/j.aop.2021.168618|B. Barak, M. Kalinowski, M. Lukin, Xun Gao, Chi-Ning Chou, Soonwon Choi|21.5|2
1491|Accelerated 3D visualization of mock galaxy catalogues for the Dark Energy Survey|We present a novel implementation of the visualization code Splotch1, exploiting optimized atomic operations of modern GPU hardware found in today's heterogeneous supercomputers, and show its application to the visualization of mock galaxy catalogues for the Dark Energy Survey (DES)2. These mock catalogues are generated by numerical simulation codes producing results which can then be directly compared with observations or validated against other models. As use case we compare a catalogue based on the L-PICOLA3 code with the MICE4 catalogue for DES. We present and discuss the potentiality of our optimized visualization algorithm as a debugging tool for L-PICOLA, in particular focusing on gaining rapidly an insight into potential anomalies on large scales and aiding in planning of appropriate remediation measures.||2015|10.1093/mnras/stu2693|M. Krokos, M. Manera, C. Gheller, Timothy Dykes, M. Rivi|20.5|2
871|DUNE Software and High Performance Computing|DUNE, like other HEP experiments, faces a challenge related to matching execution patterns of our production simulation and data processing software to the limitations imposed by modern high-performance computing facilities. In order to efficiently exploit these new architectures, particularly those with high CPU core counts and GPU accelerators, our existing software execution models require adaptation. In addition, the large size of individual units of raw data from the far detector modules pose an additional challenge somewhat unique to DUNE. Here we describe some of these problems and how we begin to solve them today with existing software frameworks and toolkits. We also describe ways we may leverage these existing software architectures to attack remaining problems going forward. This whitepaper is a contribution to the Computational Frontier of Snowmass21.||2022|10.1103/physrevresearch.4.013231|X. Qian, K. Knoepfel, Hanyu Wei, Yihui Ren, B. Viren, Meifeng Lin, Haiwang Yu, B. Fleming, Shinjae Yoo|19.666666666666668|2
733|OxDNA.org: a public webserver for coarse-grained simulations of DNA and RNA nanostructures|Abstract OxDNA and oxRNA are popular coarse-grained models used by the DNA/RNA nanotechnology community to prototype, analyze and rationalize designed DNA and RNA nanostructures. Here, we present oxDNA.org, a graphical web interface for running, visualizing and analyzing oxDNA and oxRNA molecular dynamics simulations on a GPU-enabled high performance computing server. OxDNA.org automatically generates simulation files, including a multi-step relaxation protocol for structures exported in non-physical states from DNA/RNA design tools. Once the simulation is complete, oxDNA.org provides an interactive visualization and analysis interface using the browser-based visualizer oxView to facilitate the understanding of simulation results for a user’s specific structure. This online tool significantly lowers the entry barrier of integrating simulations in the nanostructure design pipeline for users who are not experts in the technical aspects of molecular simulation. The webserver is freely available at oxdna.org.|Nucleic Acids Res.|2021|10.1093/nar/gkab324|P. Šulc, Erik Poppleton, R. Romero, Aatmik Mallya, L. Rovigatti|17.75|2
1477|nbody6++gpu: ready for the gravitational million-body problem|Accurate direct N-body simulations help to obtain detailed information about the dynamical evolution of star clusters. They also enable comparisons with analytical models and Fokker-Planck or Monte-Carlo methods. NBODY6 is a well-known direct N-body code for star clusters, and NBODY6++ is the extended version designed for large particle number simulations by supercomputers. We present NBODY6++GPU, an optimized version of NBODY6++ with hybrid parallelization methods (MPI, GPU, OpenMP, and AVX/SSE) to accelerate large direct N-body simulations, and in particular to solve the million-body problem. We discuss the new features of the NBODY6++GPU code, benchmarks, as well as the first results from a simulation of a realistic||2015|10.1093/mnras/stv817|M. Kouwenhoven, S. Aarseth, T. Naab, P. Berczik, Keigo Nitadori, Long Wang, R. Spurzem|16.6|2
1575|Accelerating Peridynamics Program Using GPU with CUDA and OpenACC|In order to model some materials that may naturally form discontinuities such as cracks as a result of deformation, the peridynamics theory is introduced by Silling (2000). In this theory, the formulation employs integral equations instead of partial differential equations and it is assumed that particles in a continuum body interact with each other at a distance. However, because it is a particle based method, the simulation of 3-D model requires a large number of particles and time steps. This leads to huge runtime requirements. Hence, to find ways to accelerate the peridynamics program becomes important to the research. In this paper, we used two kinds of parallel computing methods to accelerate the peridynamics problems: CUDA (Compute Unified Device Architecture) and OpenACC. CUDA is a parallel language extension and OpenACC is a compiler directive. A bond-based peridynamics model of crack propagation was developed and tested using the two parallel computing methods. The results show that both methods can reach the speedups from 12 to 100 compared to the corresponding sequential implementation of peridynamics code. We also compared the differences between the two methods in speed-up ratio, time-cost of the code modification, multi-platform portability and utilization of GPU. Finally we will provide some suggestions on how to choose these two parallel computing methods.||2017|10.1016/j.ijheatmasstransfer.2017.02.032|J. Li, Y. Liu, Fei Xu, J. M. Zhao|15.125|2
702|Tinker-HP: Accelerating Molecular Dynamics Simulations of Large Complex Systems with Advanced Point Dipole Polarizable Force Fields Using GPUs and Multi-GPU Systems|We present the extension of the Tinker-HP package (Lagardère, Chem. Sci.2018, 9, 956−97229732110) to the use of Graphics Processing Unit (GPU) cards to accelerate molecular dynamics simulations using polarizable many-body force fields. The new high-performance module allows for an efficient use of single- and multiple-GPU architectures ranging from research laboratories to modern supercomputer centers. After detailing an analysis of our general scalable strategy that relies on OpenACC and CUDA, we discuss the various capabilities of the package. Among them, the multiprecision possibilities of the code are discussed. If an efficient double precision implementation is provided to preserve the possibility of fast reference computations, we show that a lower precision arithmetic is preferred providing a similar accuracy for molecular dynamics while exhibiting superior performances. As Tinker-HP is mainly dedicated to accelerate simulations using new generation point dipole polarizable force field, we focus our study on the implementation of the AMOEBA model. Testing various NVIDIA platforms including 2080Ti, 3090, V100, and A100 cards, we provide illustrative benchmarks of the code for single- and multicards simulations on large biosystems encompassing up to millions of atoms. The new code strongly reduces time to solution and offers the best performances to date obtained using the AMOEBA polarizable force field. Perspectives toward the strong-scaling performance of our multinode massive parallelization strategy, unsupervised adaptive sampling and large scale applicability of the Tinker-HP code in biophysics are discussed. The present software has been released in phase advance on GitHub in link with the High Performance Computing community COVID-19 research efforts and is free for Academics (see https://github.com/TinkerTools/tinker-hp).|Journal of Chemical Theory and Computation|2021|10.1021/acs.jctc.0c01164|I. Dupays, Théo Jaffrelot Inizan, Frédéric Célerse, Luc-Henri Jolly, Louis Lagardère, Zhi Wang, Jean‐Philip Piquemal, J. Ponder, O. Adjoua, A. Durocher, Pengyu Y. Ren, Thibaut Very|14.5|2
603|petar: a high-performance N-body code for modelling massive collisional stellar systems|The numerical simulations of massive collisional stellar systems, such as globular clusters (GCs), are very time-consuming. Until now, only a few realistic million-body simulations of GCs with a small fraction of binaries (5%) have been performed by using the NBODY6++GPU code. Such models took half a year computational time on a GPU based super-computer. In this work, we develop a new N-body code, PeTar, by combining the methods of Barnes-Hut tree, Hermite integrator and slow-down algorithmic regularization (SDAR). The code can accurately handle an arbitrary fraction of multiple systems (e.g. binaries, triples) while keeping a high performance by using the hybrid parallelization methods with MPI, OpenMP, SIMD instructions and GPU. A few benchmarks indicate that PeTar and NBODY6++GPU have a very good agreement on the long-term evolution of the global structure, binary orbits and escapers. On a highly configured GPU desktop computer, the performance of a million-body simulation with all stars in binaries by using PeTar is 11 times faster than that of NBODY6++GPU. Moreover, on the Cray XC50 supercomputer, PeTar well scales when number of cores increase. The ten million-body problem, which covers the region of ultra compact dwarfs and nuclearstar clusters, becomes possible to be solved.||2020|10.1093/mnras/staa1915|Keigo Nitadori, Long Wang, J. Makino, M. Iwasawa|14.4|2
475|Data-driven fluid simulations using regression forests|Traditional fluid simulations require large computational resources even for an average sized scene with the main bottleneck being a very small time step size, required to guarantee the stability of the solution. Despite a large progress in parallel computing and efficient algorithms for pressure computation in the recent years, realtime fluid simulations have been possible only under very restricted conditions. In this paper we propose a novel machine learning based approach, that formulates physics-based fluid simulation as a regression problem, estimating the acceleration of every particle for each frame. We designed a feature vector, directly modelling individual forces and constraints from the Navier-Stokes equations, giving the method strong generalization properties to reliably predict positions and velocities of particles in a large time step setting on yet unseen test videos. We used a regression forest to approximate the behaviour of particles observed in the large training set of simulations obtained using a traditional solver. Our GPU implementation led to a speed-up of one to three orders of magnitude compared to the state-of-the-art position-based fluid solver and runs in real-time for systems with up to 2 million particles.|ACM Transactions on Graphics|2015|10.1145/2816795.2818129|Sohyeon Jeong, M. Pollefeys, B. Solenthaler, Lubor Ladicky, M. Gross|14.1|2
1142|HELIOS-K: AN ULTRAFAST, OPEN-SOURCE OPACITY CALCULATOR FOR RADIATIVE TRANSFER|We present an ultrafast opacity calculator that we name HELIOS-K. It takes a line list as an input, computes the shape of each spectral line, and provides an option for grouping an enormous number of lines into a manageable number of bins. We implement a combination of Algorithm 916 and Gauss–Hermite quadrature to compute the Voigt profile, write the code in CUDA, and optimize the computation for graphics processing units (GPUs). We restate the theory of the k-distribution method and use it to reduce ∼ 10 5 ?> –108 lines to ∼10–104 wavenumber bins, which may then be used for radiative transfer, atmospheric retrieval and general circulation models. The choice of line-wing cutoff for the Voigt profile is a significant source of error and affects the value of the computed flux by ∼ 10 % ?> . This is an outstanding physical (rather than computational) problem, due to our incomplete knowledge of pressure broadening of spectral lines in the far line wings. We emphasize that this problem remains regardless of whether one performs line-by-line calculations or uses the k-distribution method and affects all calculations of exoplanetary atmospheres requiring the use of wavelength-dependent opacities. We elucidate the correlated-k approximation and demonstrate that it applies equally to inhomogeneous atmospheres with a single atomic/molecular species or homogeneous atmospheres with multiple species. Using a NVIDIA K20 GPU, HELIOS-K is capable of computing an opacity function with ∼ 10 5 ?> spectral lines in ∼1 s and is publicly available as part of the Exoclimes Simulation Platform (www.exoclime.org).||2015|10.1088/0004-637X/808/2/182|K. Heng, S. Grimm|13.9|2
555|Decentralized Distributed PPO: Mastering PointGoal Navigation|"We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever ""stale""), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially ""solves"" the task -- near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ""ImageNet pre-training + task-specific fine-tuning"" for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models + code will be publicly available)."|International Conference on Learning Representations|2020|10.1007/978-3-030-58604-1_7|Irfan Essa, Dhruv Batra, Erik Wijmans, Abhishek Kadian, Ari S. Morcos, Devi Parikh, Stefan Lee, M. Savva|13.4|2
939|DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality|Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in diverse kinds of hardware and simulator setups, and in our case, with the Allegro Hand and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for researchers to achieve such results with commonly-available, affordable robot hands and cameras. Videos of the resulting policy and supplementary information, including experiments and demos, can be found on the website.|IEEE International Conference on Robotics and Automation|2022|10.1109/ICRA48891.2023.10160216|Jean-Francois Lafleche, Jingzhou Liu, Yashraj S. Narang, Gavriel State, Denys Makoviichuk, Aleksei Petrenko, Karl Van Wyk, D. Fox, Arthur Allshire, Viktor Makoviychuk, Ankur Handa, Ritvik Singh, Balakumar Sundaralingam, Alexander Zhurkevich|13.333333333333334|2
842|GPU-accelerated Faster Mean Shift with euclidean distance metrics|Handling clustering problems are important in data statistics, pattern recognition and image processing. The mean-shift algorithm, a common unsupervised algorithms, is widely used to solve clustering problems. However, the mean-shift algorithm is restricted by its huge computational resource cost. In previous research [1], we proposed a novel GPU-accelerated Faster Mean-shift algorithm, which greatly speed up the cosine-embedding clustering problem. In this study, we extend and improve the previous algorithm to handle Euclidean distance metrics. Different from conventional GPU-based mean-shift algorithms, our algorithm adopts novel Seed Selection & Early Stopping approaches, which greatly increase computing speed and reduce GPU memory consumption. In the simulation testing, when processing a 200K points clustering problem, our algorithm achieved around 3 times speedup compared to the state-of-the-art GPU-based mean-shift algorithms with optimized GPU memory consumption. Moreover, in this study, we implemented a plug-and-play model for faster mean-shift algorithm, which can be easily deployed. (Plug-and-play model is available: https://github.com/masqm/Faster-Mean-Shift-Euc)|Annual International Computer Software and Applications Conference|2021|10.1109/COMPSAC54236.2022.00037|Lingxi Chen, Mengyang Zhao, Han Jiang, Jinyong Hu, Xintong Cui, C. Chang|12.75|2
206|Molecular dynamics simulations using the drude polarizable force field on GPUs with OpenMM: Implementation, validation, and benchmarks|Presented is the implementation of the Drude force field in the open‐source OpenMM simulation package allowing for access to graphical processing unit (GPU) hardware. In the Drude model, electronic degrees of freedom are represented by negatively charged particles attached to their parent atoms via harmonic springs, such that extra computational overhead comes from these additional particles and virtual sites representing lone pairs on electronegative atoms, as well as the associated thermostat and integration algorithms. This leads to an approximately fourfold increase in computational demand over additive force fields. However, by making the Drude model accessible to consumer‐grade desktop GPU hardware it will be possible to perform simulations of one microsecond or more in less than a month, indicating that the barrier to employ polarizable models has largely been removed such that polarizable simulations with the classical Drude model are readily accessible and practical.|Journal of Computational Chemistry|2018|10.1002/jcc.25339|Jing Huang, P. Eastman, Alexander D. MacKerell, Justin A. Lemkul|12.571428571428571|2
775|NVBitFI: Dynamic Fault Injection for GPUs|GPUs have found wide acceptance in domains such as high-performance computing and autonomous vehicles, which require fast processing of large amounts of data along with provisions for reliability, availability, and safety. A key component of these dependability characteristics is the propagation of errors and their eventual effect on system outputs. In addition to analytical and simulation models, fault injection is an important technique that can evaluate the effect of errors on a complete computing system running the full software stack. However, the complexity of modern GPU systems and workloads challenges existing fault injection tools. Some tools require the recompilation of source code that may not be available, struggle to handle dynamic libraries, lack support for modern GPUs, or add unacceptable performance overheads. We introduce the NVBitFI tool for fault injection into GPU programs. In contrast with existing tools, NVBitFI performs instrumentation of code dynamically and selectively to instrument the minimal set of target dynamic kernels; as it requires no access to source code, NVBitFI provides improvements in performance and usability. The NVBitFI tool is publicly available for download and use at https://github.com/NVlabs/nvbitfi.|Dependable Systems and Networks|2021|10.1109/DSN48987.2021.00041|Michael B. Sullivan, Oreste Villa, S. Hari, S. Keckler, Timothy Tsai|12.5|2
846|Finding General Equilibria in Many-Agent Economic Simulations Using Deep Reinforcement Learning|Real economies can be seen as a sequential imperfect-information game with many heterogeneous, interacting strategic agents of various agent types, such as consumers, ﬁrms, and governments. Dynamic general equilibrium models are common economic tools to model the economic activity, interactions, and outcomes in such systems. However, existing analytical and computational methods struggle to ﬁnd explicit equilibria when all agents are strategic and interact, while joint learning is unstable and challenging. Amongst others, a key reason is that the actions of one economic agent may change the reward function of another agent, e.g., a consumer’s expendable income changes when ﬁrms change prices or governments change taxes. We show that multi-agent deep reinforcement learning (RL) can discover stable solutions that are (cid:15) -Nash equilibria for a meta-game over agent types, in economic simulations with many agents, through the use of structured learning curricula and efﬁcient GPU-only simulation and training. Conceptually, our approach is more ﬂexible and does not need unrealistic assumptions, e.g., market clearing, that are commonly used for analytical tractability. Our GPU implementation enables training and analyzing economies with a large number of agents within reasonable time frames, e.g., training completes within a day. We demonstrate our approach in real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 ﬁrms, and a government who taxes and redistributes. We validate the learned meta-game (cid:15) -Nash equilibria through approximate best-response analyses, show that RL policies align with economic intuitions, and that our approach is constructive, e.g., by explicitly learning a spectrum|arXiv.org|2022|10.1126/sciadv.abk2607|Soham R. Phade, Stephan Zheng, Michael J. Curry, Alexander R. Trott, Yunru Bai|12.0|2
869|Modernizing the NEURON Simulator for Sustainability, Portability, and Performance|The need for reproducible, credible, multiscale biological modeling has led to the development of standardized simulation platforms, such as the widely-used NEURON environment for computational neuroscience. Developing and maintaining NEURON over several decades has required attention to the competing needs of backwards compatibility, evolving computer architectures, the addition of new scales and physical processes, accessibility to new users, and efficiency and flexibility for specialists. In order to meet these challenges, we have now substantially modernized NEURON, providing continuous integration, an improved build system and release workflow, and better documentation. With the help of a new source-to-source compiler of the NMODL domain-specific language we have enhanced NEURON’s ability to run efficiently, via the CoreNEURON simulation engine, on a variety of hardware platforms, including GPUs. Through the implementation of an optimized in-memory transfer mechanism this performance optimized backend is made easily accessible to users, providing training and model-development paths from laptop to workstation to supercomputer and cloud platform. Similarly, we have been able to accelerate NEURON’s reaction-diffusion simulation performance through the use of just-in-time compilation. We show that these efforts have led to a growing developer base, a simpler and more robust software distribution, a wider range of supported computer architectures, a better integration of NEURON with other scientific workflows, and substantially improved performance for the simulation of biophysical and biochemical models.|bioRxiv|2022|10.3389/fninf.2022.884046|R. McDougal, Fernando L. Pereira, N. Cornu, Adam John Hunter Newton, O. Awile, F. Schürmann, P. Kumbhar, S. Dura-Bernal, O. Lupton, N. Carnevale, A. Savulescu, J. King, W. Lytton, M. Hines, Ioannis Magkanaris|11.666666666666666|2
1312|CoreNEURON : An Optimized Compute Engine for the NEURON Simulator|The NEURON simulator has been developed over the past three decades and is widely used by neuroscientists to model the electrical activity of neuronal networks. Large network simulation projects using NEURON have supercomputer allocations that individually measure in the millions of core hours. Supercomputer centers are transitioning to next generation architectures and the work accomplished per core hour for these simulations could be improved by an order of magnitude if NEURON was able to better utilize those new hardware capabilities. In order to adapt NEURON to evolving computer architectures, the compute engine of the NEURON simulator has been extracted and has been optimized as a library called CoreNEURON. This paper presents the design, implementation, and optimizations of CoreNEURON. We describe how CoreNEURON can be used as a library with NEURON and then compare performance of different network models on multiple architectures including IBM BlueGene/Q, Intel Skylake, Intel MIC and NVIDIA GPU. We show how CoreNEURON can simulate existing NEURON network models with 4–7x less memory usage and 2–7x less execution time while maintaining binary result compatibility with NEURON.|Front. Neuroinform.|2019|10.3389/fninf.2019.00063|F. Schürmann, Jeremy Fouriaux, P. Kumbhar, F. Delalondre, J. King, M. Hines, A. Ovcharenko|11.5|2
926|PeleC: An adaptive mesh refinement solver for compressible reacting flows|Reacting flow simulations for combustion applications require extensive computing capabilities. Leveraging the AMReX library, the Pele suite of combustion simulation tools targets the largest supercomputers available and future exascale machines. We introduce PeleC, the compressible solver in the Pele suite, and detail its capabilities, including complex geometry representation, chemistry integration, and discretization. We present a comparison of development efforts using both OpenACC and AMReX’s C++ performance portability framework for execution on multiple GPU architectures. We discuss relevant details that have allowed PeleC to achieve high performance and scalability. PeleC’s performance characteristics are measured through relevant simulations on multiple supercomputers. The success of PeleC’s design for exascale is exhibited through demonstration of a 160 billion cell simulation and weak scaling onto 100% of Summit, an NVIDIA-based GPU supercomputer at Oak Ridge National Laboratory. Our results provide confidence that PeleC will enable future combustion science simulations with unprecedented fidelity.|The international journal of high performance computing applications|2022|10.1177/10943420221121151|S. Yellapantula, Weiqun Zhang, H. Sitaraman, M. Day, R. Grout, J. Bell, Bruce A. Perry, M. T. Henry de Frahan, Jonathan S. Rood, A. Almgren, Jacqueline H. Chen|11.333333333333334|2
261|Speedup for quantum optimal control from automatic differentiation based on graphics processing units|We implement a quantum optimal control algorithm based on automatic differentiation and harness the acceleration afforded by graphics processing units (GPUs). Automatic differentiation allows us to specify advanced optimization criteria and incorporate them in the optimization process with ease. We show that the use of GPUs can speedup calculations by more than an order of magnitude. Our strategy facilitates efficient numerical simulations on affordable desktop computers and exploration of a host of optimization constraints and system parameters relevant to real-life experiments. We demonstrate optimization of quantum evolution based on fine-grained evaluation of performance at each intermediate time step, thus enabling more intricate control on the evolution path, suppression of departures from the truncated model subspace, as well as minimization of the physical time needed to perform high-fidelity state preparation and unitary gates.||2016|10.1103/PhysRevA.95.042318|M. Abdelhafez, D. Schuster, J. Koch, N. Leung|11.222222222222221|2
1302|Arbor — A Morphologically-Detailed Neural Network Simulation Library for Contemporary High-Performance Computing Architectures|We introduce Arbor, a performance portable library for simulation of large networks of multi-compartment neurons on HPC systems. Arbor is open source software, developed under the auspices of the HBP. The performance portability is by virtue of back-end specific optimizations for x86 multicore, Intel KNL, and NVIDIA GPUs. When coupled with low memory overheads, these optimizations make Arbor an order of magnitude faster than the most widely-used comparable simulation software. The single-node performance can be scaled out to run very large models at extreme scale with efficient weak scaling.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2019|10.1109/EMPDP.2019.8671560|W. Klijn, A. Peyser, Benjamin Cumming, Stuart Yates, N. A. Akar, V. Karakasis, Anne Küsters|10.5|2
493|An Accurate and Fast Method for Conducted EMI Modeling and Simulation of MMC-Based HVdc Converter Station|The analysis of electromagnetic interference (EMI) noise of power electronic circuits involves both the transient characteristics of power semiconductor devices and the wideband stray parameters of passive equipment. Modular multilevel converters (MMCs) used in high-voltage direct current (HVdc) transmission systems contain thousands of submodules (SMs), which makes it considerably challenging to perform device-level simulation on the traditional commercial software. This article presents an accurate and fast method for wideband modeling and simulation of MMC-HVdc system for the assessment of conducted EMI during the design stage. Physical characteristics of the semiconductor devices, parasitic parameters of the insulated-gate bipolar transistor (IGBT) packages, and stray capacitances of the SMs are all taken into consideration, and massively parallel transient simulation of the wideband MMC model is carried out on the graphics processor (GPU). The accuracy and efficiency of the GPU-based parallel algorithm are validated by the comparison with the experimental measurement of an 11-level full-bridge MMC prototype. Furthermore, the stray capacitance network of the valve tower in HVdc project is extracted, and a matrix partition method based on the shielding plate configuration is utilized to conduct the computation in a fully parallelized manner. The developed GPU program is used to run the large-scale case of a 201-level two-terminal MMC-HVdc system, and the primarily affected frequency range by various factors is analyzed. Execution time test is conducted for different level topology, and it is demonstrated that the GPU can achieve a remarkable speedup over multicore CPUs, especially when the system scale is more substantial.|IEEE transactions on power electronics|2020|10.1109/TPEL.2019.2945931|V. Dinavahi, Ruimin Zhu, Guishu Liang, Ning Lin|10.2|2
762|SiP-ML: high-bandwidth optical network interconnects for machine learning training|This paper proposes optical network interconnects as a key enabler for building high-bandwidth ML training clusters with strong scaling properties. Our design, called SiP-ML, accelerates the training time of popular DNN models using silicon photonics links capable of providing multiple terabits-per-second of bandwidth per GPU. SiP-ML partitions the training job across GPUs with hybrid data and model parallelism while ensuring the communication pattern can be supported efficiently on the network interconnect. We develop task partitioning and device placement methods that take the degree and reconfiguration latency of optical interconnects into account. Simulations using real DNN models show that, compared to the state-of-the-art electrical networks, our approach improves training time by 1.3--9.1x.|Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication|2021|10.1145/3452296.3472900|Mehrdad Khani Shirkoohi, M. Glick, K. Bergman, A. Vahdat, Eiman Ebrahimi, Ziyi Zhu, M. Alizadeh, M. Ghobadi, Benjamin Klenk|10.0|2
905|Molecular Dynamics Simulations and Diversity Selection by Extended Continuous Similarity Indices|Molecular dynamics (MD) is a core methodology of molecular modeling and computational design for the study of the dynamics and temporal evolution of molecular systems. MD simulations have particularly benefited from the rapid increase of computational power that has characterized the past decades of computational chemical research, being the first method to be successfully migrated to the GPU infrastructure. While new-generation MD software is capable of delivering simulations on an ever-increasing scale, relatively less effort is invested in developing postprocessing methods that can keep up with the quickly expanding volumes of data that are being generated. Here, we introduce a new idea for sampling frames from large MD trajectories, based on the recently introduced framework of extended similarity indices. Our approach presents a new, linearly scaling alternative to the traditional approach of applying a clustering algorithm that usually scales as a quadratic function of the number of frames. When showcasing its usage on case studies with different system sizes and simulation lengths, we have registered speedups of up to 2 orders of magnitude, as compared to traditional clustering algorithms. The conformational diversity of the selected frames is also noticeably higher, which is a further advantage for certain applications, such as the selection of structural ensembles for ligand docking. The method is available open-source at https://github.com/ramirandaq/MultipleComparisons.|Journal of Chemical Information and Modeling|2022|10.1021/acs.jcim.2c00433|D. Bajusz, R. Miranda‐Quintana, Anita Rácz, K. Héberger, Levente M. Mihalovits|10.0|2
1007|High-speed turbulent flows towards the exascale: STREAmS-2 porting and performance|Exascale High Performance Computing (HPC) represents a tremendous opportunity to push the boundaries of Computational Fluid Dynamics (CFD), but despite the consolidated trend towards the use of Graphics Processing Units (GPUs), programmability is still an issue. STREAmS-2 (Bernardini et al. Comput. Phys. Commun. 285 (2023) 108644) is a compressible solver for canonical wall-bounded turbulent flows capable of harvesting the potential of NVIDIA GPUs. Here we extend the already available CUDA Fortran backend with a novel HIPFort backend targeting AMD GPU architectures. The main implementation strategies are discussed along with a novel Python tool that can generate the HIPFort and CPU code versions allowing developers to focus their attention only on the CUDA Fortran backend. Single GPU performance is analysed focusing on NVIDIA A100 and AMD MI250x cards which are currently at the core of several HPC clusters. The gap between peak GPU performance and STREAmS-2 performance is found to be generally smaller for NVIDIA cards. Roofline analysis allows tracing this behavior to unexpectedly different computational intensities of the same kernel using the two cards. Parallel performance is measured on the two largest EuroHPC pre-exascale systems, LUMI (AMD GPUs) and Leonardo (NVIDIA GPUs). Strong scalability reveals more than 80% efficiency up to 16 nodes for Leonardo and up to 32 for LUMI. Weak scalability shows an impressive efficiency of over 95% up to the maximum number of nodes tested (256 for LUMI and 512 for Leonardo). This analysis shows that STREAmS-2 is the perfect candidate to fully exploit the power of current pre-exascale HPC systems in Europe, allowing users to simulate flows with over a trillion mesh points, thus reducing the gap between the Reynolds numbers achievable in high-fidelity simulations and those of real engineering applications.||2023|10.1016/j.cpc.2022.108644|D. Modesti, F. Salvadore, Srikanth Sathyanarayana, M. Bernardini, S. Pirozzoli|9.5|2
1303|A Fourier Disparity Layer Representation for Light Fields|In this paper, we present a new Light Field representation for efficient Light Field processing and rendering called Fourier Disparity Layers (FDL). The proposed FDL representation samples the Light Field in the depth (or equivalently the disparity) dimension by decomposing the scene as a discrete sum of layers. The layers can be constructed from various types of Light Field inputs, including a set of sub-aperture images, a focal stack, or even a combination of both. From our derivations in the Fourier domain, the layers are simply obtained by a regularized least square regression performed independently at each spatial frequency, which is efficiently parallelized in a GPU implementation. Our model is also used to derive a gradient descent-based calibration step that estimates the input view positions and an optimal set of disparity values required for the layer construction. Once the layers are known, they can be simply shifted and filtered to produce different viewpoints of the scene while controlling the focus and simulating a camera aperture of arbitrary shape and size. Our implementation in the Fourier domain allows real-time Light Field rendering. Finally, direct applications such as view interpolation or extrapolation and denoising are presented and evaluated.|IEEE Transactions on Image Processing|2019|10.1109/TIP.2019.2922099|C. Guillemot, A. Smolic, Mikael Le Pendu|9.5|2
1396|Learning-Based Proxy Collision Detection for Robot Motion Planning Applications|This article demonstrates that collision detection-intensive applications such as robotic motion planning may be accelerated by performing collision checks with a machine learning model. We propose Fastron, a learning-based algorithm, to model a robot's configuration space to be used as a proxy collision detector in place of standard geometric collision checkers. We demonstrate that leveraging the proxy collision detector results in up to an order of magnitude faster performance in robot simulation and planning than state-of-the-art collision detection libraries. Our results show that Fastron learns a model more than 100 times faster than a competing C-space modeling approach, while also providing theoretical guarantees of learning convergence. Using the open motion planning libraries (OMPLs), we were able to generate initial motion plans across all experiments with varying robot and environment complexities and workspace obstacle locations. With Fastron, we can repeatedly generate new motion plans at a 56 Hz rate, showing its application toward autonomous surgical assistance task in shared environments with human-controlled manipulators. All performance gains were achieved despite using only CPU-based calculations, suggesting further computational gains with a GPU approach that can parallelize tensor algebra. Code is available online.1|IEEE Transactions on robotics|2019|10.1109/tro.2020.2974094|Nikhil Das, Michael C. Yip|9.5|2
1572|Non-smooth Newton Methods for Deformable Multi-body Dynamics|We present a framework for the simulation of rigid and deformable bodies in the presence of contact and friction. Our method is based on a non-smooth Newton iteration that solves the underlying nonlinear complementarity problems (NCPs) directly. This approach allows us to support nonlinear dynamics models, including hyperelastic deformable bodies and articulated rigid mechanisms, coupled through a smooth isotropic friction model. The fixed-point nature of our method means it requires only the solution of a symmetric linear system as a building block. We propose a new complementarity preconditioner for NCP functions that improves convergence, and we develop an efficient GPU-based solver based on the conjugate residual (CR) method that is suitable for interactive simulations. We show how to improve robustness using a new geometric stiffness approximation and evaluate our method’s performance on a number of robotics simulation scenarios, including dexterous manipulation and training using reinforcement learning.|ACM Transactions on Graphics|2019|10.1145/3338695|Kenny Erleben, S. Jeschke, M. Macklin, Matthias Müller, Viktor Makoviychuk, N. Chentanez|9.5|2
656|Modeling the effect of oxygen on the chemical stage of water radiolysis using GPU-based microscopic Monte Carlo simulations, with an application in FLASH radiotherapy|Oxygen plays a critical role in determining the initial DNA damages induced by ionizing radiation. It is important to mechanistically model the oxygen effect in the water radiolysis process. However, due to the computational costs from the many body interaction problem, oxygen is often ignored or treated as a constant continuum radiolysis-scavenger background in the simulations using common microscopic Monte Carlo tools. In this work, we reported our recent progress on the modeling of the chemical stage of the water radiolysis with an explicit consideration of the oxygen effect, based upon our initial development of an open-source graphical processing unit (GPU)-based MC simulation tool, gMicroMC. The inclusion of oxygen mainly reduces the yields of eh and H. chemical radicals, turning them into highly toxic O2.− and HO2. species. To demonstrate the practical value of gMicroMC in large scale simulation problems, we applied the oxygen-simulation-enabled gMicroMC to compute the yields of chemical radicals under a high instantaneous dose rate D˙i to study the oxygen depletion hypothesis in FLASH radiotherapy. A decreased oxygen consumption rate (OCR) was found associated with a reduced initial oxygen concentration level due to reduced probabilities of reactions. With respect to dose rate, for the oxygen concentration of 21% and electron energy of 4.5 keV , OCR remained approximately constant (∼0.22 μM Gy−1 ) for D˙i ’s of 106 , 107 Gys−1 and reduced to 0.19 μM Gy−1 at 108 Gys−1 , because the increased dose rate improved the mutual reaction frequencies among radicals, hence reducing their reactions with oxygen. We computed the time evolution of oxygen concentration under the FLASH irradiation setups. At the dose rate of 107 Gys−1 and initial oxygen concentrations from 0.01% to 21%, the oxygen is unlikely to be fully depleted with an accumulative dose of 30 Gy, which is a typical dose used in FLASH experiments. The computational efficiency of gMicroMC when considering oxygen molecules in the chemical stage was evaluated through benchmark work to GEANT4-DNA with simulating an equivalent number of radicals. With an initial oxygen concentration of 3% (∼105 molecules), a speedup factor of 1228 was achieved for gMicroMC on a single GPU card when comparing with GEANT4-DNA on a single CPU.|Physics in Medicine and Biology|2020|10.1088/1361-6560/abc93b|X. Jia, Y. Lai, Y. Chi|9.4|2
191|Descent methods for elastic body simulation on the GPU|We show that many existing elastic body simulation approaches can be interpreted as descent methods, under a nonlinear optimization framework derived from implicit time integration. The key question is how to find an effective descent direction with a low computational cost. Based on this concept, we propose a new gradient descent method using Jacobi preconditioning and Chebyshev acceleration. The convergence rate of this method is comparable to that of L-BFGS or nonlinear conjugate gradient. But unlike other methods, it requires no dot product operation, making it suitable for GPU implementation. To further improve its convergence and performance, we develop a series of step length adjustment, initialization, and invertible model conversion techniques, all of which are compatible with GPU acceleration. Our experiment shows that the resulting simulator is simple, fast, scalable, memory-efficient, and robust against very large time steps and deformations. It can correctly simulate the deformation behaviors of many elastic materials, as long as their energy functions are second-order differentiable and their Hessian matrices can be quickly evaluated. For additional speedups, the method can also serve as a complement to other techniques, such as multi-grid.|ACM Transactions on Graphics|2016|10.1145/2980179.2980236|Huamin Wang, Y. Yang|9.333333333333334|2
609|NERO: A Near High-Bandwidth Memory Stencil Accelerator for Weather Prediction Modeling|Ongoing climate change calls for fast and accurate weather and climate modeling. However, when solving large-scale weather prediction simulations, state-of-the-art CPU and GPU implementations suffer from limited performance and high energy consumption. These implementations are dominated by complex irregular memory access patterns and low arithmetic intensity that pose fundamental challenges to acceleration. To overcome these challenges, we propose and evaluate the use of near-memory acceleration using a reconfigurable fabric with high-bandwidth memory (HBM). We focus on compound stencils that are fundamental kernels in weather prediction models. By using high-level synthesis techniques, we develop NERO, an FPGA+HBM-based accelerator connected through IBM CAPI2 (Coherent Accelerator Processor Interface) to an IBM POWER9 host system. Our experimental results show that NERO outperforms a 16-core POWER9 system by 4.2x and 8.3x when running two different compound stencil kernels. NERO reduces the energy consumption by 22x and 29x for the same two kernels over the POWER9 system with an energy efficiency of 1.5 GFLOPS/Watt and 17.3 GFLOPS/Watt. We conclude that employing near-memory acceleration solutions for weather prediction modeling is promising as a means to achieve both high performance and high energy efficiency.|International Conference on Field-Programmable Logic and Applications|2020|10.1109/FPL50879.2020.00014|S. Stuijk, O. Mutlu, Juan Gómez-Luna, H. Corporaal, Gagandeep Singh, C. Hagleitner, D. Diamantopoulos|9.2|2
515|DeepSimulator1.5: a more powerful, quicker and lighter simulator for Nanopore sequencing|Abstract Motivation Nanopore sequencing is one of the leading third-generation sequencing technologies. A number of computational tools have been developed to facilitate the processing and analysis of the Nanopore data. Previously, we have developed DeepSimulator1.0 (DS1.0), which is the first simulator for Nanopore sequencing to produce both the raw electrical signals and the reads. However, although DS1.0 can produce high-quality reads, for some sequences, the divergence between the simulated raw signals and the real signals can be large. Furthermore, the Nanopore sequencing technology has evolved greatly since DS1.0 was released. It is thus necessary to update DS1.0 to accommodate those changes. Results We propose DeepSimulator1.5 (DS1.5), all three modules of which have been updated substantially from DS1.0. As for the sequence generator, we updated the sample read length distribution to reflect the newest real reads’ features. In terms of the signal generator, which is the core of DeepSimulator, we added one more pore model, the context-independent pore model, which is much faster than the previous context-dependent one. Furthermore, to make the generated signals more similar to the real ones, we added a low-pass filter to post-process the pore model signals. Regarding the basecaller, we added the support for the newest official basecaller, Guppy, which can support both GPU and CPU. In addition, multiple optimizations, related to multiprocessing control, memory and storage management, have been implemented to make DS1.5 a much more amenable and lighter simulator than DS1.0. Availability and implementation The main program and the data are available at https://github.com/lykaust15/DeepSimulator. Supplementary information Supplementary data are available at Bioinformatics online.|Bioinform.|2020|10.1093/bioinformatics/btz963|C. Bi, Xin Gao, Zhaowen Qiu, Mo Li, Sheng Wang, Yu Li|9.0|2
538|A new joint species distribution model for faster and more accurate inference of species associations from big community data|Joint species distribution models (JSDMs) explain spatial variation in community composition by contributions of the environment, biotic associations and possibly spatially structured residual covariance. They show great promise as a general analytical framework for community ecology and macroecology, but current JSDMs, even when approximated by latent variables, scale poorly on large datasets, limiting their usefulness for currently emerging big (e.g. metabarcoding and metagenomics) community datasets. Here, we present a novel, more scalable JSDM (sjSDM) that circumvents the need to use latent variables by using a Monte Carlo integration of the joint JSDM likelihood together with flexible elastic net regularization on all model components. We implemented sjSDM in PyTorch, a modern machine learning framework, which allows making use of both CPU and GPU calculations. Using simulated communities with known species–species associations and different number of species and sites, we compare sjSDM with state‐of‐the‐art JSDM implementations to determine computational runtimes and accuracy of the inferred species–species and species–environment associations. We find that sjSDM is orders of magnitude faster than existing JSDM algorithms (even when run on the CPU) and can be scaled to very large datasets. Despite the dramatically improved speed, sjSDM produces more accurate estimates of species association structures than alternative JSDM implementations. We demonstrate the applicability of sjSDM to big community data using eDNA case study with thousands of fungi operational taxonomic units (OTU). Our sjSDM approach makes the analysis of JSDMs to large community datasets with hundreds or thousands of species possible, substantially extending the applicability of JSDMs in ecology. We provide our method in an R package to facilitate its applicability for practical data analysis.|Methods in Ecology and Evolution|2020|10.1111/2041-210X.13687|Maximilian Pichler, F. Hartig|9.0|2
627|High-resolution mining of the SARS-CoV-2 main protease conformational space: supercomputer-driven unsupervised adaptive sampling|We provide an unsupervised adaptive sampling strategy capable of producing μs-timescale molecular dynamics (MD) simulations of large biosystems using many-body polarizable force fields (PFFs). The global exploration problem is decomposed into a set of separate MD trajectories that can be restarted within a selective process to achieve sufficient phase-space sampling. Accurate statistical properties can be obtained through reweighting. Within this highly parallel setup, the Tinker-HP package can be powered by an arbitrary large number of GPUs on supercomputers, reducing exploration time from years to days. This approach is used to tackle the urgent modeling problem of the SARS-CoV-2 Main Protease (Mpro) producing more than 38 μs of all-atom simulations of its apo (ligand-free) dimer using the high-resolution AMOEBA PFF. The first 15.14 μs simulation (physiological pH) is compared to available non-PFF long-timescale simulation data. A detailed clustering analysis exhibits striking differences between FFs, with AMOEBA showing a richer conformational space. Focusing on key structural markers related to the oxyanion hole stability, we observe an asymmetry between protomers. One of them appears less structured resembling the experimentally inactive monomer for which a 6 μs simulation was performed as a basis for comparison. Results highlight the plasticity of the Mpro active site. The C-terminal end of its less structured protomer is shown to oscillate between several states, being able to interact with the other protomer, potentially modulating its activity. Active and distal site volumes are found to be larger in the most active protomer within our AMOEBA simulations compared to non-PFFs as additional cryptic pockets are uncovered. A second 17 μs AMOEBA simulation is performed with protonated His172 residues mimicking lower pH. Data show the protonation impact on the destructuring of the oxyanion loop. We finally analyze the solvation patterns around key histidine residues. The confined AMOEBA polarizable water molecules are able to explore a wide range of dipole moments, going beyond bulk values, leading to a water molecule count consistent with experimental data. Results suggest that the use of PFFs could be critical in drug discovery to accurately model the complexity of the molecular interactions structuring Mpro.|Chemical Science|2021|10.1039/d1sc00145k|Chengwen Liu, Frédéric Célerse, Louis Lagardère, Luc-Henri Jolly, Jean‐Philip Piquemal, Nathalie Lagarde, Dina El Ahdab, Pierre Monmarché, O. Adjoua, M. Montès, Pengyu Y. Ren, Théo Jaffrelot Inizan|9.0|2
1042|Greedy Gradient-free Adaptive Variational Quantum Algorithms on a Noisy Intermediate Scale Quantum Computer|Hybrid quantum-classical adaptive Variational Quantum Eigensolvers (VQE) already hold the potential to outperform classical computing for simulating quantum many-body systems. However, their practical implementation on current quantum processing units (QPUs) is very challenging due to the noisy evaluation of a polynomially scaling number of observables, undertaken for operator selection and optimisation of a high-dimensional cost function. To overcome this, we propose new techniques to execute adaptive algorithms on a 25-qubit error-mitigated QPU coupled to a GPU-accelerated HPC simulator. Targeting physics applications, we compute the ground state of a 25-body Ising model using the newly introduced Greedy Gradient-free Adaptive VQE (CGA-VQE) requiring only five circuit measurements per iteration, regardless of the number of qubits and size of the operator pool. Towards chemistry, we combine the GGA-VQE and Overlap-ADAPT-VQE algorithms to approximate a molecular system ground state. We show that the QPU successfully executes the algorithms and yields the correct choice of parametrised unitary operators. While the QPU evaluation of the resulting ansatz wave-function is polluted by hardware noise, a single final evaluation of the sought-after observables on a classical GPU-accelerated/noiseless simulator allows the recovery of the correct approximation of the ground state, thus highlighting the need for hybrid quantum-classical observable measurement.||2023|10.1038/s42005-023-01312-y|Y. Maday, Jean‐Philip Piquemal, Muhammad Hassan, O. Adjoua, Axel Courtat, B. Claudon, C. Feniou|9.0|2
1583|Characterization and Prediction of Performance Interference on Mediated Passthrough GPUs for Interference-aware Scheduler|Sharing GPUs in the cloud is cost effective and can facilitate the adoption of hardware accelerator enabled cloud. But sharing causes interference between co-located VMs and leads to performance degradation. In this paper, we proposed an interference-aware VM scheduler at the cluster level with the goal of minimizing interference. NVIDIA vGPU provides sharing capability and high performance, but it has unique performance characteristics, which have not been studied thoroughly before. Our study reveals several key observations. We leverage our observations to construct models based on machine learning techniques to predict interference between co-located VMs on the same GPU. We proposed a system architecture leveraging our models to schedule VMs to minimize the interference. The experiments show that our observations improves the model accuracy (by 15% ̃ 40%) and the scheduler reduces application run-time overhead by 24.2% in simulated scenarios.|USENIX Workshop on Hot Topics in Cloud Computing|2019|10.1109/jsyst.2019.2910409|N. Zhang, Michael Cui, Ridhi Surana, Michael He, Xin Xu|8.833333333333334|2
565|Scaling Bayesian inference of mixed multinomial logit models to very large datasets|Variational inference methods have been shown to lead to significant improvements in the computational efficiency of approximate Bayesian inference in mixed multinomial logit models when compared to standard Markov-chain Monte Carlo (MCMC) methods without compromising accuracy. However, despite their demonstrated efficiency gains, existing methods still suffer from important limitations that prevent them to scale to very large datasets, while providing the flexibility to allow for rich prior distributions and to capture complex posterior distributions. In this paper, we propose an Amortized Variational Inference approach that leverages stochastic backpropagation, automatic differentiation and GPU-accelerated computation, for effectively scaling Bayesian inference in Mixed Multinomial Logit models to very large datasets. Moreover, we show how normalizing flows can be used to increase the flexibility of the variational posterior approximations. Through an extensive simulation study, we empirically show that the proposed approach is able to achieve computational speedups of multiple orders of magnitude over traditional MSLE and MCMC approaches for large datasets without compromising estimation accuracy.|arXiv.org|2020|10.3390/electronics9071164|Filipe Rodrigues|8.6|2
310|Acuros CTS: A fast, linear Boltzmann transport equation solver for computed tomography scatter - Part I: Core algorithms and validation.|PURPOSE\nTo describe Acuros® CTS, a new software tool for rapidly and accurately estimating scatter in x-ray projection images by deterministically solving the linear Boltzmann transport equation (LBTE).\n\n\nMETHODS\nThe LBTE describes the behavior of particles as they interact with an object across spatial, energy, and directional (propagation) domains. Acuros CTS deterministically solves the LBTE by modeling photon transport associated with an x-ray projection in three main steps: (a) Ray tracing photons from the x-ray source into the object where they experience their first scattering event and form scattering sources. (b) Propagating photons from their first scattering sources across the object in all directions to form second scattering sources, then repeating this process until all high-order scattering sources are computed using the source iteration method. (c) Ray-tracing photons from scattering sources within the object to the detector, accounting for the detector's energy and anti-scatter grid responses. To make this process computationally tractable, a combination of analytical and discrete methods is applied. The three domains are discretized using the Linear Discontinuous Finite Elements, Multigroup, and Discrete Ordinates methods, respectively, which confer the ability to maintain the accuracy of a continuous solution. Furthermore, through the implementation in CUDA, we sought to exploit the parallel computing capabilities of graphics processing units (GPUs) to achieve the speeds required for clinical utilization. Acuros CTS was validated against Geant4 Monte Carlo simulations using two digital phantoms: (a) a water phantom containing lung, air, and bone inserts (WLAB phantom) and (b) a pelvis phantom derived from a clinical CT dataset. For these studies, we modeled the TrueBeam® (Varian Medical Systems, Palo Alto, CA) kV imaging system with a source energy of 125 kVp. The imager comprised a 600 μm-thick Cesium Iodide (CsI) scintillator and a 10:1 one-dimensional anti-scatter grid. For the WLAB studies, the full-fan geometry without a bowtie filter was used (with and without the anti-scatter grid). For the pelvis phantom studies, a half-fan geometry with bowtie was used (with the anti-scatter grid). Scattered and primary photon fluences and energies deposited in the detector were recorded.\n\n\nRESULTS\nThe Acuros CTS and Monte Carlo results demonstrated excellent agreement. For the WLAB studies, the average percent difference between the Monte Carlo- and Acuros-generated scattered photon fluences at the face of the detector was -0.7%. After including the detector response, the average percent differences between the Monte Carlo- and Acuros-generated scatter fractions (SF) were -0.1% without the grid and 0.6% with the grid. For the digital pelvis simulation, the Monte Carlo- and Acuros-generated SFs agreed to within 0.1% on average, despite the scatter-to-primary ratios (SPRs) being as high as 5.5. The Acuros CTS computation time for each scatter image was ~1 s using a single GPU.\n\n\nCONCLUSIONS\nAcuros CTS enables a fast and accurate calculation of scatter images by deterministically solving the LBTE thus offering a computationally attractive alternative to Monte Carlo methods. Part II describes the application of Acuros CTS to scatter correction of CBCT scans on the TrueBeam system.|Medical Physics (Lancaster)|2018|10.1002/mp.12850|T. Wareing, J. Star-Lack, Adam S. Wang, A. Maslowski, I. Davis, Mingshan Sun|8.571428571428571|2
1205|A Modular Approach to Performance, Portability and Productivity for 3D Wave Models|The HPC hardware landscape is growing increasingly complex in order to meet demands in scientific computing for greater performance. In recent years there has been an explosion of parallel devices coming on to the scene: GPUs, Xeon Phis and FPGAs to name but a few examples. As of writing, even the current leading supercomputer, Sunway TianhuLight, uses its own bespoke on-chip accelerators[12]. Available programming models, however, lag behind and are not currently able to provide the necessary tools for running scientific codes across platforms in ways that are performant, portable and productive. This environment creates a plethora of challenges for computational scientists of which we focus on two: first the need for a high level of productivity for codes that still get good performance and second consistently getting good performance across platforms the “performance portability” problem. Existing solutions tend to be either not productive but provide good performance or focus on high-level abstractions requiring heuristics to get good performance (often which are tied to particular platforms). While some current approaches raise the productivity level, they are often trying to solve the same problems over and over or trying to solve too many issues for a niche domain. In addition, many of these approaches have only been tested on simplistic benchmarks, which can lose critical functionality of real-world simulation codes. We instead propose a modular approach using existing frameworks to target these issues separately: a high-level DSL to target the productivity problem compiling into an IR language which addresses the performance portability problem. Our previous research has shown that the development of more productive and performance portable codes for room acoustics simulations is possible. Preliminary results using the intermediary parallel language lift[16] confirm that this framework is capable of handling complex stencils. Further developing lift and targeting existing stencil-focused DSLs will create a simple, modularized approach which harnesses and expands existing functionality instead of trying to reinvent the wheel. This modular approach can then be used as an example to extend to other physical simulations using similar algorithms. WOLFHPC, November 2017, Denver, USA 2017. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn||2017|10.1109/cgo.2017.7863730|A. Gray, Christophe Dubach, Michel Steuwer, S. Bilbao, Larisa Stoltzfus|8.5|2
1586|Subspace neural physics: fast data-driven interactive simulation|Data-driven methods for physical simulation are an attractive option for interactive applications due to their ability to trade precomputation and memory footprint in exchange for improved runtime performance. Yet, existing data-driven methods fall short of the extreme memory and performance constraints imposed by modern interactive applications like AAA games and virtual reality. Here, performance budgets for physics simulation range from tens to hundreds of micro-seconds per frame, per object. We present a data-driven physical simulation method that meets these constraints. Our method combines subspace simulation techniques with machine learning which, when coupled, enables a very efficient subspace-only physics simulation that supports interactions with external objects - a longstanding challenge for existing subspace techniques. We also present an interpretation of our method as a special case of subspace Verlet integration, where we apply machine learning to efficiently approximate the physical forces of the system directly in the subspace. We propose several practical solutions required to make effective use of such a model, including a novel training methodology required for prediction stability, and a GPU-friendly subspace decompression algorithm to accelerate rendering.|Symposium on Computer Animation|2019|10.1145/3309486.3340245|D. Nowrouzezahrai, Daniel Holden, S. Datta, Bang Chi Duong|8.5|2
327|Advanced Potential Energy Surfaces for Molecular Simulation.|Advanced potential energy surfaces are defined as theoretical models that explicitly include many-body effects that transcend the standard fixed-charge, pairwise-additive paradigm typically used in molecular simulation. However, several factors relating to their software implementation have precluded their widespread use in condensed-phase simulations: the computational cost of the theoretical models, a paucity of approximate models and algorithmic improvements that can ameliorate their cost, underdeveloped interfaces and limited dissemination in computational code bases that are widely used in the computational chemistry community, and software implementations that have not kept pace with modern high-performance computing (HPC) architectures, such as multicore CPUs and modern graphics processing units (GPUs). In this Feature Article we review recent progress made in these areas, including well-defined polarization approximations and new multipole electrostatic formulations, novel methods for solving the mutual polarization equations and increasing the MD time step, combining linear-scaling electronic structure methods with new QM/MM methods that account for mutual polarization between the two regions, and the greatly improved software deployment of these models and methods onto GPU and CPU hardware platforms. We have now approached an era where multipole-based polarizable force fields can be routinely used to obtain computational results comparable to state-of-the-art density functional theory while reaching sampling statistics that are acceptable when compared to that obtained from simpler fixed partial charge force fields.|Journal of Physical Chemistry B|2016|10.1021/acs.jpcb.6b06414|T. Head‐Gordon, J. Dziedzic, Omar N. A. Demerdash, Alex Albaugh, I. Todorov, J. Swails, Y. Shao, M. Tuckerman, D. Case, J. Essex, P. Eastman, V. Pande, Richard T. Bradshaw, H. A. Boateng, Daniel T. Margul, Lee‐Ping Wang, Qiao Zeng, Yuezhi Mao, J. Ponder, Chris-Kriton Skylaris, M. Head‐Gordon|8.444444444444445|2
1493|Investigations on turbulence in low pressure turbines based on direct numerical simulations|To date no generally accepted prediction method is available for a low pressure turbine designer. Detailed information about the flow field that can be used to improve turbulence models is difficult to obtain from experiments. Direct numerical simulations (DNS), capable of delivering that information, have been published and showed the ability of this method to accurately predict the flow field. However, in these previous studies detailed turbulence information was not published and to date detailed information about turbulence in LPT flows is missing. In this work compressible DNS were conducted and turbulence was investigated in detail. First a method was developed that enables large scale numerical simulations of low pressure turbines on state of the art high performance computers. Particular emphasis was put on boundary conditions with respect to reflections and an efficient way of generating inlet turbulence. To allow for efficient computation the computational performance was examined and optimized and showed good scaling up to large numbers of cores and GPUs, allowing the exploitation of large parallel computing systems. A grid convergence study is presented showing that while large scales are grid converged at similar resolutions to previously published work, convergence of quantities related to ne scales required a substantially higher resolution than published work. Budgets of the transport equations of Favre averaged momentum, total energy and TKE are presented in the vortex shedding region, the developed wake and the blade boundary layer. These are useful for turbulence modellers to understand the accuracy of a particular turbulence model. Energy transport mechanisms were discussed to provide an understanding of the important mechanisms and their variation with Reynolds number. Finally a local assessment of linear eddy-viscosity models was presented where a turbulence model based on an optimized turbulence viscosity was compared to the standard k- model. It was found that in the vortex shedding region there was scope for improvement of the k- model. However, the remaining modelling error of the optimized model was large highlighting the limitation of linear eddy viscosity models. Further, regions with large errors in the turbulence model were identified close to leading and trailing edge.||2016|10.1115/1.4032435|R. Pichler|8.444444444444445|2
539|A Hybrid Systolic-Dataflow Architecture for Inductive Matrix Algorithms|Dense linear algebra kernels are critical for wireless, and the oncoming proliferation of 5G only amplifies their importance. Due to the inductive nature of many such algorithms, parallelism is difficult to exploit: parallel regions have fine-grain producer/consumer interaction with iteratively changing depen-dence distance, reuse rate, and memory access patterns. This makes multi-threading impractical due to fine-grain synchronization, and vectorization ineffective due to the non-rectangular iteration domain. CPUs, DSPs, and GPUs perform order-of-magnitude below peak. Our insight is that if the nature of inductive dependences and memory accesses were explicit in the hardware/software interface, then a spatial architecture could efficiently execute parallel code regions. To this end, we first develop a novel execution model, inductive dataflow, where inductive dependence patterns and memory access patterns (streams) are first-order primitives. Second, we develop a hybrid spatial architecture combining systolic and tagged dataflow execution to attain high utilization at low energy and area cost. Finally, we create a scalable design through a novel vector-stream control model which amortizes control overhead both in time and spatially across architecture lanes. We evaluate our design, REVEL, with a full stack (compiler, ISA, simulator, RTL). Across a suite of linear algebra kernels, REVEL outperforms equally-provisioned DSPs by 4.6×-37×. Compared to state-of-the-art spatial architectures, REVEL is mean 3× faster. Compared to a set of ASICs, REVEL is only 2× the power and half the area.|International Symposium on High-Performance Computer Architecture|2020|10.1109/HPCA47549.2020.00063|Zhengrong Wang, Sihao Liu, Tony Nowatzki, Vidushi Dadu, Jian Weng|8.4|2
1326|DIFFERENTIAL POLYTROPIC MODEL FOR SIMULATION OF STIRLING ENGINES CONSIDERING VARIOUS REGENERATORS MODELS|ARTICLE INFORMATION ABSTRACT Original Research Paper Received 08 September 2014 Accepted 2 November 2014 Available Online 0 January 2015 differential thermal model for simulation of Stirling engines was presented. In the new model polytropic expansion/compression processes were substituted for traditional isothermal or adiabatic models of previous studies. In addition, the developed polytropic model was corrected for various loss mechanisms of real engines. In this regard, the effect of non-ideal operation as well as heat recovery in the regenerator was considered. In addition, non-ideal heat transfer of heater and cooler were implemented into the model. In pressure analysis and evaluation of work produced or consumed in cylinders, the effect of finite speed motion of piston was considered based on the concept of finite speed thermodynamics. Moreover, the effects of heat leakage in regenerator, leakage effect and shuttle effect were evaluated. Finally, new differential polytropic models were employed on benchmark Stirling engine, so-called GPU-3, and accuracy of models was validated through comparison with experimental results as well as previous models. As thermal performance of Stirling engines is significantly affected by thermohydraulic performance of regeneratorand there are various thermohydraulic models for regenerator, three famous thermohydraulic models of regenerator were integrated into models and through comparison with experimental performance of GPUengine, more accurate thermohydraulic model was introduced.||2015|10.1016/j.apenergy.2014.12.033|Babaelahi Mojtaba, Sayyaadi Hoseyn|8.4|2
300|Atomistic non-adiabatic dynamics of the LH2 complex with a GPU-accelerated ab initio exciton model.|We recently outlined an efficient multi-tiered parallel ab initio excitonic framework that utilizes time dependent density functional theory (TDDFT) to calculate ground and excited state energies and gradients of large supramolecular complexes in atomistic detail - enabling us to undertake non-adiabatic simulations which explicitly account for the coupled anharmonic vibrational motion of all the constituent atoms in a supramolecular system. Here we apply that framework to the 27 coupled bacterio-chlorophyll-a chromophores which make up the LH2 complex, using it to compute an on-the-fly nonadiabatic surface-hopping (SH) trajectory of electronically excited LH2. Part one of this article is focussed on calibrating our ab initio exciton Hamiltonian using two key parameters: a shift δ, which corrects for the error in TDDFT vertical excitation energies; and an effective dielectric constant ε, which describes the average screening of the transition-dipole coupling between chromophores. Using snapshots obtained from equilibrium molecular dynamics simulations (MD) of LH2, we tune the values of both δ and ε through fitting to the thermally broadened experimental absorption spectrum, giving a linear absorption spectrum that agrees reasonably well with experiment. In part two of this article, we construct a time-resolved picture of the coupled vibrational and excitation energy transfer (EET) dynamics in the sub-picosecond regime following photo-excitation. Assuming Franck-Condon excitation of a narrow eigenstate band centred at 800 nm, we use surface hopping to follow a single nonadiabatic dynamics trajectory within the full eigenstate manifold. Consistent with experimental data, this trajectory gives timescales for B800→B850 population transfer (τB800→B850) between 650-1050 fs, and B800 population decay (τ800→) between 10-50 fs. The dynamical picture that emerges is one of rapidly fluctuating LH2 eigenstates that are delocalized over multiple chromophores and undergo frequent crossing on a femtosecond timescale as a result of the atomic vibrations of the constituent chromophores. The eigenstate fluctuations arise from disorder that is driven by vibrational dynamics with multiple characteristic timescales. The scalability of our ab initio excitonic computational framework across massively parallel architectures opens up the possibility of addressing a wide range of questions, including how specific dynamical motions impact both the pathways and efficiency of electronic energy-transfer within large supramolecular systems.|Physical Chemistry, Chemical Physics - PCCP|2017|10.1039/c7cp00492c|Graham T Johnson, Michael B. O'Connor, Simon McIntosh-Smith, F. Manby, Aaron Sisto, M. W. van der Kamp, D. Glowacki, Clement Stross, T. Martínez, E. Hohenstein|8.375|2
1417|Computational Estimation of Microsecond to Second Atomistic Folding Times.|Despite the development of massively parallel computing hardware including inexpensive graphics processing units (GPUs), it has remained infeasible to simulate the folding of atomistic proteins at room temperature using conventional molecular dynamics (MD) beyond the microsecond scale. Here, we report the folding of atomistic, implicitly solvated protein systems with folding times τ ranging from ∼10 μs to ∼100 ms using the weighted ensemble (WE) strategy in combination with GPU computing. Starting from an initial structure or set of structures, WE organizes an ensemble of GPU-accelerated MD trajectory segments via intermittent pruning and replication events to generate statistically unbiased estimates of rate constants for rare events such as folding; no biasing forces are used. Although the variance among atomistic WE folding runs is significant, multiple independent runs are used to reduce and quantify statistical uncertainty. Folding times are estimated directly from WE probability flux and from history-augmented Markov analysis of the WE data. Three systems were examined: NTL9 at low solvent viscosity (yielding τf = 0.8-9 μs), NTL9 at water-like viscosity (τf = 0.2-2 ms), and Protein G at low viscosity (τf = 3-200 ms). In all cases, the folding time, uncertainty, and ensemble properties could be estimated from WE simulation; for Protein G, this characterization required significantly less overall computing than would be required to observe a single folding event with conventional MD simulations. Our results suggest that the use and calibration of force fields and solvent models for precise estimation of kinetic quantities is becoming feasible.|Journal of the American Chemical Society|2019|10.1021/jacs.8b10735|D. Zuckerman, Andrew A. Petersen, S. Subramanian, U. Adhikari, J. Copperman, Barmak Mostofian|8.333333333333334|2
1569|SPARE: SPArse-view REconstruction challenge for 4D cone-beam CT from a one-minute scan.|PURPOSE\nCurrently 4D cone-beam CT (CBCT) requires a 3-4 minute full-fan scan to ensure usable image quality. Recent advancements in sparse-view 4D-CBCT reconstruction have opened the possibility to reduce scan time and dose. The aim of this study is to provide a common framework for systematically evaluating algorithms for 4D-CBCT reconstruction from a one-minute scan. Using this framework, the AAPM-sponsored SPARE Challenge was conducted in 2018 to identify and compare state-of-the-art algorithms.\n\n\nMETHODS\nA clinically realistic CBCT dataset was simulated using patient CT volumes from the 4D-Lung database. The selected patients had multiple 4D-CT sessions, where the first 4D-CT was used as the prior CT, and the rest were used as the ground truth volumes for simulating CBCT projections. A GPU-based Monte Carlo tool was used to simulate the primary, scatter, and quantum noise signals. A total of 32 CBCT scans of 9 patients were generated. Additional qualitative analysis was performed on a clinical Varian and clinical Elekta dataset to validate the simulation study. Participants were blinded from the ground truth, and were given three months to apply their reconstruction algorithms to the projection data. The submitted reconstructions were analyzed in terms of root-mean-squared-error (RMSE) and structural similarity index (SSIM) with the ground truth within four different region-of-interests (ROI) -patient body, lungs, planning target volume (PTV), and bony anatomy. Geometric accuracy was quantified as the alignment error of the PTV.\n\n\nRESULTS\nTwenty teams participated in the challenge, with five teams completing the challenge. Techniques involved in the five methods included iterative optimization, motion-compensation, and deformation of the prior 4D-CT. All five methods rendered significant reduction in noise and streaking artifacts when compared to the conventional Feldkamp-Davis-Kress (FDK) algorithm. The RMS of the 3D target registration error of the five methods ranged from 1.79-3.00 mm. Qualitative observations from the Varian and Elekta datasets mostly concur with those from the simulation dataset. Each of the methods was found to have its own strengths and weaknesses. Overall, the MA-ROOSTER method, which utilizes a 4D-CT motion model for temporal regularization, had the best and most consistent image quality and accuracy.\n\n\nCONCLUSION\nThe SPARE Challenge represents the first framework for systematically evaluating state-of-the-art algorithms for 4D-CBCT reconstruction from a one-minute scan. Results suggest the potential for reducing scan time and dose for 4D-CBCT. The challenge dataset and analysis framework are publicly available for benchmarking future reconstruction algorithms. This article is protected by copyright. All rights reserved.|Medical Physics (Lancaster)|2019|10.1002/mp.13687|C. Mory, Xiaoning Liu, Zhuoran Jiang, X. Jia, Y. Gonzalez, S. Rit, Yawei Zhang, M. Riblett, P. Keall, L. Ren, Geoffrey D. Hugo, C. Shieh, Bin Li|8.0|2
293|Performance and Power Analysis of High-Density Multi-GPGPU Architectures: A Preliminary Case Study|A system architecture with high-density general purpose graphic processing unit (GPGPU) is emerging as a promising solution that can offer high compute performance and performance-per-watt for building cluster supercomputers. The raw compute power of these heterogeneous systems greatly exceeds the current prevailing homogenous systems, motivating their rapid adoption. These heterogeneous systems do however increase the complexity of developing parallel applications and there is a need to investigate the compute performances and associated power consumption of common benchmarks and scientific computing applications. In this paper, we present the performance and power studies through using the Dell C4130 server that integrates up to 4 GPGPU cards and NVIDIA GPGPU K80 is used. The high performance Linpack (HPL) and molecular dynamics (MD) simulators including NAMD, LAMMPS and GROMACS are tested. Through comparing 4-K80 and 2-Xeon E5-2690 v3 systems, we show that: (1) for HPL tests, the 4- GPU server delivers up to 7 TFLOPS that is 9 times faster than the 2-CPU system and its power efficiency is 4 GFLOPS per Watt, (2) for MD tests, NAMD on 4-GPU server achieves 7.8 times speedup and it uses 2.3 times power consumption compared to 2-CPU system, and LAMMPS achieves 16 times speedup and it uses 2.6 times power consumption, and GROMACS achieves 3.3 times speed up and it uses 2.6 times power consumption. These preliminary results demonstrated that the novel high-density multi-GPGPU architecture offers high performances for computing intensive applications and molecular simulators with superior power efficiencies in a space efficient design. In future, such heterogeneous architecture could be a powerful alternative solution for next generation supercomputer systems.|2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems|2015|10.1109/HPCC-CSS-ICESS.2015.68|Yu-Xiang Gao, Peng Zhang, Meikang Qiu, Saeed Iqbal|7.9|2
716|Real-Time Implementation of Randomized Model Predictive Control for Autonomous Driving|Model predictive control (MPC) using randomized optimization is expected to solve different control problems. However, it still faces various challenges for real-world applications. This paper attempts to solve those challenges and demonstrates a successful implementation of randomized MPC on the autonomous driving using a radio-controlled (RC) car. First of all, a sample generation technique in the frequency domain is discussed. This prevents undesirable randomness which affect the smoothness of the steering operation. Second, the proposed randomized MPC is implemented on a Graphics Processing Unit (GPU). The expected GPU acceleration in calculation speed at various problem sizes is also presented. The results show the improved control performance and computational speed that was not achievable using CPU based implementation. Besides, the selection of parameters for randomized MPC is discussed. The usefulness of the proposed scheme is demonstrated by both simulation and experiments. In the experiments, a 1/10 model RC car is used for collision avoidance task by autonomous driving.|IEEE Transactions on Intelligent Vehicles|2021|10.1109/TIV.2021.3062730|H. Okuda, Arun Muraleedharan, Tatsuya Suzuki|7.75|2
530|Multiscale computational models can guide experimentation and targeted measurements for crop improvement.|Computational models of plants have identified gaps in our understanding of biological systems, and have revealed ways to optimize cellular processes or organ-level architecture to increase productivity. Thus, computational models are learning tools that help direct experimentation and measurements. Models are simplifications of complex systems, and often simulate specific processes at single scales (e.g., temporal, spatial, organizational, etc.). Consequently, single-scale models are unable to capture the critical cross-scale interactions that result in emergent properties of the system. In this perspective article, we contend that to accurately predict how a plant will respond in an untested environment, it is necessary to integrate mathematical models across biological scales. Computationally mimicking the flow of biological information from the genome to the phenome is an important step in discovering new experimental strategies to improve crops. A key challenge is to connect models across biological, temporal, and computational (e.g. CPU vs GPU) scales, and then to visualize and interpret integrated model outputs. We address this challenge by describing the efforts of the international Crops in silico consortium.|The Plant Journal|2020|10.1111/tpj.14722|Meagan Lang, S. Long, L. Sweetlove, Bedrich Benes, Amy Marshall-Colón, B. Peng, M. Turk, James c. Schnable, J. Lynch, K. Guan|7.6|2
694|High resolution 3D ultrasonic breast imaging by time-domain full waveform inversion|Ultrasound tomography (UST) scanners allow quantitative images of the human breast’s acoustic properties to be derived with potential applications in screening, diagnosis and therapy planning. Time domain full waveform inversion (TD-FWI) is a promising UST image formation technique that fits the parameter fields of a wave physics model by gradient-based optimization. For high resolution 3D UST, it holds three key challenges: firstly, its central building block, the computation of the gradient for a single US measurement, has a restrictively large memory footprint. Secondly, this building block needs to be computed for each of the 103–104 measurements, resulting in a massive parallel computation usually performed on large computational clusters for days. Lastly, the structure of the underlying optimization problem may result in slow progression of the solver and convergence to a local minimum. In this work, we design and evaluate a comprehensive computational strategy to overcome these challenges: firstly, we exploit a gradient computation based on time reversal that dramatically reduces the memory footprint at the expense of one additional wave simulation per source. Secondly, we break the dependence on the number of measurements by using source encoding (SE) to compute stochastic gradient estimates. Also we describe a more accurate, TD-specific SE technique with a finer variance control and use a state-of-the-art stochastic LBFGS method. Lastly, we design an efficient TD multi-grid scheme together with preconditioning to speed up the convergence while avoiding local minima. All components are evaluated in extensive numerical proof-of-concept studies simulating a bowl-shaped 3D UST breast scanner prototype. Finally, we demonstrate that their combination allows us to obtain an accurate 442 × 442 × 222 voxel image with a resolution of 0.5 mm using Matlab on a single GPU within 24 h.|Inverse Problems|2021|10.1088/1361-6420/ac3b64|M. Pérez-Liva, B. Cox, F. Lucka, B. Treeby|7.5|2
636|Oceananigans.jl: Fast and friendly geophysical fluid dynamics on GPUs|Oceananigans.jl is a fast and friendly software package for the numerical simulation of incompressible, stratified, rotating fluid flows on CPUs and GPUs. Oceananigans.jl is fast and flexible enough for research yet simple enough for students and first-time programmers. Oceananigans.jl is being developed as part of the Climate Modeling Alliance project for the simulation of small-scale ocean physics at high-resolution that affect the evolution of Earth’s climate.|Journal of Open Source Software|2020|10.21105/JOSS.02018|J. Campin, Andre N. Souza, A. Edelman, John Marshall, G. Wagner, R. Ferrari, Tim Besard, A. Ramadhan, C. Hill, Valentin Churavy|7.4|2
856|Affine Body Dynamics: Fast, Stable & Intersection-free Simulation of Stiff Materials|Simulating stiff materials in applications where deformations are either not significant or can safely be ignored is a pivotal task across fields. Rigid body modeling has thus long remained a fundamental tool and is, by far, the most popular simulation strategy currently employed for modeling stiff solids. At the same time, numerical models of a rigid body continue to pose a number of known challenges and trade-offs including intersections, instabilities, inaccuracies, and/or slow performances that grow with contact-problem complexity. In this paper we revisit this problem and present ABD, a simple and highly effective affine body dynamics framework, which significantly improves state-of-the-art stiff simulations. We trace the challenges in the rigid-body IPC (incremental potential contact) method to the necessity of linearizing piecewise-rigid (SE(3)) trajectories and subsequent constraints. ABD instead relaxes the unnecessary (and unrealistic) constraint that each body's motion be exactly rigid with a stiff orthogonality potential, while preserving the rigid body model's key feature of a small coordinate representation. In doing so ABD replaces piecewise linearization with piecewise linear trajectories. This, in turn, combines the best from both parties: compact coordinates ensure small, sparse system solves, while piecewise-linear trajectories enable efficient and accurate constraint (contact and joint) evaluations. Beginning with this simple foundation, ABD preserves all guarantees of the underlying IPC model e.g., solution convergence, guaranteed non-intersection, and accurate frictional contact. Over a wide range and scale of simulation problems we demonstrate that ABD brings orders of magnitude performance gains (two- to three-order on the CPU and an order more utilizing the GPU, which is 10,000x speedups) over prior IPC-based methods with a similar or higher simulation quality.|arXiv.org|2022|10.1145/3528223.3530064|Chenfanfu Jiang, Minchen Li, L. Lan, D. Kaufman, Yin Yang|7.333333333333333|2
912|Affine body dynamics|Simulating stiff materials in applications where deformations are either not significant or else can safely be ignored is a fundamental task across fields. Rigid body modeling has thus long remained a critical tool and is, by far, the most popular simulation strategy currently employed for modeling stiff solids. At the same time, rigid body methods continue to pose a number of well known challenges and trade-offs including intersections, instabilities, inaccuracies, and/or slow performances that grow with contact-problem complexity. In this paper we revisit the stiff body problem and present ABD, a simple and highly effective affine body dynamics framework, which significantly improves state-of-the-art for simulating stiff-body dynamics. We trace the challenges in rigid-body methods to the necessity of linearizing piecewise-rigid trajectories and subsequent constraints. ABD instead relaxes the unnecessary (and unrealistic) constraint that each body's motion be exactly rigid with a stiff orthogonality potential, while preserving the rigid body model's key feature of a small coordinate representation. In doing so ABD replaces piecewise linearization with piecewise linear trajectories. This, in turn, combines the best of both worlds: compact coordinates ensure small, sparse system solves, while piecewise-linear trajectories enable efficient and accurate constraint (contact and joint) evaluations. Beginning with this simple foundation, ABD preserves all guarantees of the underlying IPC model we build it upon, e.g., solution convergence, guaranteed non-intersection, and accurate frictional contact. Over a wide range and scale of simulation problems we demonstrate that ABD brings orders of magnitude performance gains (two- to three-orders on the CPU and an order more when utilizing the GPU, obtaining 10, 000× speedups) over prior IPC-based methods, while maintaining simulation quality and nonintersection of trajectories. At the same time ABD has comparable or faster timings when compared to state-of-the-art rigid body libraries optimized for performance without guarantees, and successfully and efficiently solves challenging simulation problems where both classes of prior rigid body simulation methods fail altogether.|ACM Transactions on Graphics|2022|10.1145/3528223.3530064|Chenfanfu Jiang, Minchen Li, L. Lan, D. Kaufman, Yin Yang|7.333333333333333|2
1236|Interactive design of periodic yarn-level cloth patterns|"We describe an interactive design tool for authoring, simulating, and adjusting yarn-level patterns for knitted and woven cloth. To achieve interactive performance for notoriously slow yarn-level simulations, we propose two acceleration schemes: (a) yarn-level periodic boundary conditions that enable the restricted simulation of only small periodic patches, thereby exploiting the spatial repetition of many cloth patterns in cardinal directions, and (b) a highly parallel GPU solver for efficient yarn-level simulation of the small patch. Our system supports interactive pattern editing and simulation, and runtime modification of parameters. To adjust the amount of material used (yarn take-up) we support ""on the fly"" modification of (a) local yarn rest-length adjustments for pattern specific edits, e.g., to tighten slip stitches, and (b) global yarn length by way of a novel yarn-radius similarity transformation. We demonstrate the tool's ability to support interactive modeling, by novice users, of a wide variety of yarn-level knit and woven patterns. Finally, to validate our approach, we compare dozens of generated patterns against reference images of actual woven or knitted cloth samples, and we release this corpus of digital patterns and simulated models as a public dataset to support future comparisons."|ACM Transactions on Graphics|2018|10.1145/3272127.3275105|Rundong Wu, Jonathan Leaf, Steve Marschner, Doug L. James, Eston Schweickart|7.285714285714286|2
721|Modeling of a chain of three plasma accelerator stages with the WarpX electromagnetic PIC code on GPUs|The fully electromagnetic particle-in-cell code WarpX is being developed by a team of the U.S. DOE Exascale Computing Project (with additional non-U.S. collaborators on part of the code) to enable the modeling of chains of tens to hundreds of plasma accelerator stages on exascale supercomputers, for future collider designs. The code is combining the latest algorithmic advances (e.g., Lorentz boosted frame and pseudo-spectral Maxwell solvers) with mesh refinement and runs on the latest computer processing unit and graphical processing unit (GPU) architectures. In this paper, we summarize the strategy that was adopted to port WarpX to GPUs, report on the weak parallel scaling of the pseudo-spectral electromagnetic solver, and then present solutions for decreasing the time spent in data exchanges from guard regions between subdomains. In Sec. IV, we demonstrate the simulations of a chain of three consecutive multi-GeV laser-driven plasma accelerator stages.|Physics of Plasmas|2021|10.1063/5.0028512|M. Hogan, John B. Bell, R. Lehe, R. Jambunathan, A. Almgren, A. Myers, C. Ng, L. Amorim, David P. Grote, Weiqun Zhang, H. Vincenti, L. Ge, K. Gott, N. Zaïm, O. Shapoval, E. Yang, M. Rowan, J. Vay, M. Thévenet, E. Zoni, L. Fedeli, Y. Zhao, Axel Huebl|7.25|2
786|Assembly of Biomolecular Gigastructures and Visualization with the Vulkan Graphics API|Building and displaying all-atom models of biomolecular structures with millions or billions of atoms, like virus particles or cells, remain a challenge due to the sheer size of the data, the required levels of automated building, and the visualization limits of today’s graphics hardware. Based on concepts introduced with the CellPack program, we report new algorithms to create such large-scale models using an intermediate coarse-grained “pet representation” of biomolecules with 1/10th the normal size. Pet atoms are placed such that they optimally trace the surface of the original molecule with just ∼1/50th the original atom number and are joined with covalent bonds. Molecular dynamics simulations of pet molecules allow for efficient packing optimization, as well as the generation of realistic DNA/RNA conformations. This pet world can be expanded back to the all-atom representation to be explored and visualized with full details. Essential for the efficient interactive visualization of gigastructures is the use of multiple levels of detail (LODs), where distant molecules are drawn with a heavily reduced polygon count. We present a grid-based algorithm to create such LODs for all common molecular graphics styles (including ball-and-sticks, ribbons, and cartoons) that do not require monochrome molecules to hide LOD transitions. As a practical application, we built all-atom models of SARS-CoV-2, HIV, and an entire presynaptic bouton with 1 μm diameter and 3.6 billion atoms, using modular building blocks to significantly reduce GPU memory requirements through instancing. We employ the Vulkan graphics API to maximize performance on consumer grade hardware and describe how to use the mmCIF format to efficiently store such giant models. An implementation is available as part of the YASARA molecular modeling and simulation program from www.YASARA.org. The free YASARA View program can be used to explore the presented models, which can be downloaded from www.YASARA.org/petworld, a Creative Commons platform for sharing giant biomolecular structures.|Journal of Chemical Information and Modeling|2021|10.1021/acs.jcim.1c00743|Burkhard Rammner, Kornel Ozvoldik, E. Krieger, T. Stockner|7.0|2
1405|Reflecting on the Goal and Baseline for Exascale Computing: A Roadmap Based on Weather and Climate Simulations|We present a roadmap towards exascale computing based on true application performance goals. It is based on two state-of-the art European numerical weather prediction models (IFS from ECMWF and COSMO from MeteoSwiss) and their current performance when run at very high spatial resolution on present-day supercomputers. We conclude that these models execute about 100–250 times too slow for operational throughput rates at a horizontal resolution of 1 km, even when executed on a full petascale system with nearly 5000 state-of-the-art hybrid GPU-CPU nodes. Our analysis of the performance in terms of a metric that assesses the efficiency of memory use shows a path to improve the performance of hardware and software in order to meet operational requirements early next decade.|Computing in science & engineering (Print)|2019|10.1109/MCSE.2018.2888788|P. Bauer, T. Hoefler, C. Schär, N. Wedi, O. Fuhrer, T. Schulthess|7.0|2
223|Grid Shock: Coordinated Load-Changing Attacks on Power Grids: The Non-Smart Power Grid is Vulnerable to Cyber Attacks as Well|Electric power grids are among the largest human-made control structures and are considered as critical infrastructure due to their importance for daily life. When operating a power grid, providers have to continuously maintain a balance between supply (i.e., production in power plants) and demand (i.e., power consumption) to keep the power grid's nominal frequency of 50 Hz or alternatively 60 Hz. Power consumption is forecast by elaborated models including multiple parameters like weather, season, and time of the day; they are based on the premise of many small consumers averaging out their energy consumption spikes. In this paper, we develop attacks violating this assumption, investigate their impact on power grid operation, and assess their feasibility for today's adversaries. In our scenario, an adversary builds (or rents) a botnet of zombie computers and modulates their power consumption, e.g., by utilizing CPU, GPU, hard disks, screen brightness, and laser printers in a coordinated way over the Internet. Outperforming the grid's countervailing mechanisms in time, the grid is pushed into unstable states triggering automated load shedding or tie-line tripping. We show that an adversary does not have to rely on smart grid features to modulate power consumption given that an adequate communication infrastructure for striking the (legacy) power grid is currently nearly omnipresent: the Internet to whom more and more power-consuming devices are connected. Our simulations estimate that between 2.5 and 9.8 million infections are sufficient to attack the European synchronous grid -- depending on the mix of infected devices, the current mix of active power plant types, and the current overall produced power. However, the herein described attack mechanisms are not limited to the European grid.|Asia-Pacific Computer Systems Architecture Conference|2017|10.1145/3134600.3134639|Adrian Dabrowski, E. Weippl, Johanna Ullrich|6.875|2
704|Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections|Tactile sensing is critical for robotic grasping and manipulation of objects under visual occlusion. However, in contrast to simulations of robot arms and cameras, current simulations of tactile sensors have limited accuracy, speed, and utility. In this work, we develop an efficient 3D finite element method (FEM) model of the SynTouch BioTac sensor using an open-access, GPU-based robotics simulator. Our simulations closely reproduce results from an experimentally-validated model in an industry-standard, CPU-based simulator, but at 75x the speed. We then learn latent representations for simulated BioTac deformations and real-world electrical output through self-supervision, as well as projections between the latent spaces using a small supervised dataset. Using these learned latent projections, we accurately synthesize real-world BioTac electrical output and estimate contact patches, both for unseen contact interactions. This work contributes an efficient, freely-accessible FEM model of the BioTac and comprises one of the first efforts to combine self-supervision, cross-modal transfer, and sim-to-real transfer for tactile sensors.|IEEE International Conference on Robotics and Automation|2021|10.1109/ICRA48506.2021.9561969|Yashraj S. Narang, D. Fox, Arsalan Mousavian, M. Macklin, Balakumar Sundaralingam|6.75|2
755|DefGraspSim: Simulation-based grasping of 3D deformable objects|Robotic grasping of 3D deformable objects (e.g., fruits/vegetables, internal organs, bottles/boxes) is critical for real-world applications such as food processing, robotic surgery, and household automation. However, developing grasp strategies for such objects is uniquely challenging. In this work, we efficiently simulate grasps on a wide range of 3D deformable objects using a GPU-based implementation of the corotational finite element method (FEM). To facilitate future research, we open-source our simulated dataset (34 objects, 1e5 Pa elasticity range, 6800 grasp evaluations, 1.1M grasp measurements), as well as a code repository that allows researchers to run our full FEM-based grasp evaluation pipeline on arbitrary 3D object models of their choice. We also provide a detailed analysis on 6 object primitives. For each primitive, we methodically describe the effects of different grasp strategies, compute a set of performance metrics (e.g., deformation, stress) that fully capture the object response, and identify simple grasp features (e.g., gripper displacement, contact area) measurable by robots prior to pickup and predictive of these performance metrics. Finally, we demonstrate good correspondence between grasps on simulated objects and their real-world counterparts.|arXiv.org|2021|10.1109/icra48506.2021.9561969|Tucker Hermans, Isabella Huang, Yashraj S. Narang, Clemens Eppner, D. Fox, M. Macklin, Balakumar Sundaralingam|6.75|2
1334|Weather Forecasting Using GPU-Based Large-Eddy Simulations|Since the advent of computers midway through the twentieth century, computational resources have increased exponentially. It is likely they will continue to do so, especially when accounting for recent trends in multicore processors. History has shown that such an increase tends to directly lead to weather and climate models that readily exploit the extra resources, improving model quality and resolution. We show that Large-Eddy Simulation (LES) models that utilize modern, accelerated (e.g., by GPU or coprocessor), parallel hardware systems can now provide turbulence-resolving numerical weather forecasts over a region the size of the Netherlands at 100-m resolution. This approach has the potential to speed the development of turbulence-resolving numerical weather prediction models.||2015|10.1175/BAMS-D-14-00114.1|E. van Meijgaard, A. P. Siebesma, J. Schalkwijk, Harm J. J. Jonker|6.7|2
447|Calculation of quantum chemical two-electron integrals by applying compiler technology on GPU.|In this article we present an effective approach to calculate quantum chemical two-electron integrals over basis sets consisting of Gaussian type basis functions on GPU. Our framework generates several different variants called routes to the same integral problem with different integral algorithms (McMurchie Davidson, Head-Gordon Pople, Rys) and precision. Each route is benchmarked on more GPU architectures and with this data a model is fitted to select the best available route for an integral task given a GPU architecture. Moreover, this approach supports the computation of high angular momentum orbitals up to g effectively on GPU, tested up to cc-pVQZ sized basis sets. Rigorous analysis is shown regarding the effectiveness of our method. Molecule simulations with several basis sets are measured using NVIDIA GTX 1080 Ti, NVIDIA P100, and NVIDIA V100 cards.|Journal of Chemical Theory and Computation|2019|10.1021/acs.jctc.9b00560|István Ladjánszki, Á. Rák, Gergely Kis, G. Tornai, G. Cserey|6.666666666666667|2
863|GIGA-Lens: Fast Bayesian Inference for Strong Gravitational Lens Modeling|We present GIGA-Lens: a gradient-informed, GPU-accelerated Bayesian framework for modeling strong gravitational lensing systems, implemented in TensorFlow and JAX. The three components, optimization using multistart gradient descent, posterior covariance estimation with variational inference, and sampling via Hamiltonian Monte Carlo, all take advantage of gradient information through automatic differentiation and massive parallelization on graphics processing units (GPUs). We test our pipeline on a large set of simulated systems and demonstrate in detail its high level of performance. The average time to model a single system on four Nvidia A100 GPUs is 105 s. The robustness, speed, and scalability offered by this framework make it possible to model the large number of strong lenses found in current surveys and present a very promising prospect for the modeling of (105) lensing systems expected to be discovered in the era of the Vera C. Rubin Observatory, Euclid, and the Nancy Grace Roman Space Telescope.|Astrophysical Journal|2022|10.3847/1538-4357/ac6de4|S. Perlmutter, A. Filipp, G. Aldering, Kyle Boone, E. Jullo, A. Bolton, Y. Shu, W. Sheu, A. Gu, A. Dey, D. Schlegel, S. Suyu, E. Schlafly, X. Huang, D. Rubin|6.666666666666667|2
1199|ExaGeoStat: A High Performance Unified Software for Geostatistics on Manycore Systems|"We present <italic>ExaGeoStat</italic>, a high performance software for geospatial statistics in climate and environment modeling. In contrast to simulation based on partial differential equations derived from first-principles modeling, <italic>ExaGeoStat</italic> employs a statistical model based on the evaluation of the Gaussian log-likelihood function, which operates on a large dense covariance matrix. Generated by the parametrizable Matérn covariance function, the resulting matrix is symmetric and positive definite. The computational tasks involved during the evaluation of the Gaussian log-likelihood function become daunting as the number <inline-formula> <tex-math notation=""LaTeX"">$n$</tex-math><alternatives><inline-graphic xlink:href=""abdulah-ieq1-2850749.gif""/> </alternatives></inline-formula> of geographical locations grows, as <inline-formula><tex-math notation=""LaTeX""> ${\mathcal O}(n^2)$</tex-math><alternatives><inline-graphic xlink:href=""abdulah-ieq2-2850749.gif""/></alternatives> </inline-formula> storage and <inline-formula><tex-math notation=""LaTeX"">${\mathcal O}(n^3)$</tex-math><alternatives> <inline-graphic xlink:href=""abdulah-ieq3-2850749.gif""/></alternatives></inline-formula> operations are required. While many approximation methods have been devised from the side of statistical modeling to ameliorate these polynomial complexities, we are interested here in the complementary approach of evaluating the exact algebraic result by exploiting advances in solution algorithms and many-core computer architectures. Using state-of-the-art high performance dense linear algebra libraries associated with various leading edge parallel architectures (Intel KNLs, NVIDIA GPUs, and distributed-memory systems), <italic>ExaGeoStat</italic> raises the game for statistical applications from climate and environmental science. <italic>ExaGeoStat</italic> provides a reference evaluation of statistical parameters, with which to assess the validity of the various approaches based on approximation. The software takes a first step in the merger of large-scale data analytics and extreme computing for geospatial statistical applications, to be followed by additional complexity reducing improvements from the solver side that can be implemented under the same interface. Thus, a single uncompromised statistical model can ultimately be executed in a wide variety of emerging exascale environments."|IEEE Transactions on Parallel and Distributed Systems|2017|10.1109/TPDS.2018.2850749|Ying Sun, H. Ltaief, Sameh Abdulah, M. Genton, D. Keyes|6.625|2
647|Exploiting frame coherence in real-time rendering for energy-efficient GPUs|The computation capabilities of mobile GPUs have greatly evolved in the last generations, allowing real-time rendering of realistic scenes. However, the desire for processing even more complex environments clashes with the battery-operated nature of the devices integrating these kind of GPUs, such as smartphones and tablets, for which users expect long operating times per charge and a low-enough temperature to comfortably hold them. Consequently, improving the energy-efficiency of mobile GPUs is paramount to fulfill both performance and low-power goals. Previous works determined that the work of the processors from within the GPU and, notably, their accesses to off-chip memory are the main sources of energy consumption in graphics workloads. Yet most of this energy is spent in redundant computations, as the high frame rate required to produce smooth animations results in a sequence of extremely similar images. The goal of this thesis is to improve the energy-efficiency of mobile GPUs by designing microarchitectural mechanisms that leverage frame coherence in order to reduce the redundant computations and memory accesses inherent in graphics applications. Firstly, we focus on reducing redundant color computations. Mobile GPUs typically employ an architecture called Tile-Based Rendering, in which the screen is divided into multiple tiles that are independently rendered in on-chip buffers, thus reducing memory bandwidth. An analysis of popular Android applications reveals that it is common that more than 80% of the tiles produce exactly the same output between consecutive frames. We propose Rendering Elimination, a mechanism that accurately determines such occurrences by computing and storing signatures of the inputs of all the tiles in a frame. If the signatures of a tile across consecutive frames are the same, the colors computed in the preceding frame are reused, saving all computations and memory accesses associated to the rendering of the tile. Using commercial Android applications and state-of-the-art cycle-accurate simulators and models, we show that Rendering Elimination vastly outperforms related memoization schemes found in the literature, achieving a reduction of energy consumption of 37% and execution time of 33% with minimal overheads. Next, we focus on reducing redundant color computations of fragments that will eventually not be visible. In real-time rendering, objects are processed in the order they are submitted to the GPU by the application, which usually causes that the results of previously-computed objects are overwritten by new objects that turn out to be closer to the observer and, therefore, occlude them. This phenomenon occurs because visibility is resolved on-the-fly along with the rendering process. Consequently, whether or not a particular object will be occluded is not known until the entire scene has been processed. Based on frame coherence and, therefore, the fact that visibility tends to remain constant across consecutive frames, we propose Early Visibility Resolution, a mechanism that predicts visibility based on information obtained in the preceding frame. Early Visibility||2020|10.1038/s42005-020-0345-z|M. Sánchez|6.6|2
680|Development of a computing farm with Cloud Computing on GPU. (Développement d'une ferme informatique utilisant le Cloud Computing sur GPU)|All the work in this thesis has been developed in the context of the Cherenkov Telescope Array (CTA), which is going to be the major next-generation observatory for ground-based very-high-energy gamma-ray astronomy. The plan for this work is to use GPUs and Cloud Computing in order to speed up the computing demanding tasks, developing and optimizing data analysis pipelines.The thesis consists on two main parts: the first one is dedicated to the estimation of the future performances of CTA towards the observation of violent phenomena such as those generating Gamma Ray Bursts and Gravitational Waves, with a initial work done for the creation of the models for the First CTA Data Challenge. The second part of the thesis is related to the development of the pipelines for the reconstruction of the low-level data coming from the Monte Carlo simulations using the software library called ctapipe.In chapter 1 I go into the details of the CTA project, the telescopes and the performances of the array, together with the methods used to derive them from Monte Carlo simulations. The science goals of CTA and the Key Science Projects (KSPs) will be covered in chapter 2, with a focus on Gamma Ray Bursts and the follow-up of Gravitational Waves events.The work done for the First CTA Data Challenge (DC-1) is presented in chapter 3. More than 500 extragalactic sources have been modelled combining informations from different catalogues in order to create a population of AGNs. This Challenge has been important both to involve more people in the analysis of CTA data and to compute the observation time needed by the different KSP. The simulations for the gravitational waves and gamma-ray bursts Consortium papers have been created with the ctools_pipe pipeline (presented in chapter 4), implemented around the libraries ctools and gammalib. The pipeline is composed of two main parts: the task to be executed (background simulation, model creation and detection) and in which computing centre.The second part of the thesis is focused on the development and optimization of the analysis pipelines to be used for the event reconstruction of simulated raw data and for the visualization of the events in a 3D space. This analyses have been performed using ctapipe, a framework for prototyping the low-level data processing algorithms for CTA. The structure of the library is presented in chapter 5 together a focus on the reconstruction methods that are implemented in ctapipe, including the so called ImPACT. This method uses a template of images created from the Monte Carlo simulations and a seed from the standard reconstruction method to fit between the templates to find a better estimation of the shower parameters. The time profiling and the strategies adopted to optimize the ImPACT pipeline are presented in chapter 6. The implementation of the a pipeline for the analysis of the Large Size Telescope observing in monoscopic mode and its GPU implementation with PyTorch is also presented. ctapipe has also been used and developed to estimate the performances of CTA when observing using the “divergent pointing” mode, in which the pointing directions are slightly different with respect to the parallel pointing mode, so that the final hyper field-of-view of all the telescopes is larger with respect to the parallel pointing mode. The angular and energy resolutions and also the sensitivity are worse in this scenario, but having a wider hyper field-of-view can be good for other topics, such are searching for transient sources. The modifications to the reconstruction code introduced in ctapipe and some angular resolution plots for the simulated point source gammas are presented in chapter 7.The results presented in this thesis are a demonstration of the usage of advanced software techniques in very high energy astrophysics.||2020|10.1016/j.carbpol.2020.116810|T. Gasparetto|6.6|2
1452|User-experience-aware system optimisation for mobile systems|This thesis considers the concept of Quality of Experience (QoE) in the context of mobile electronic consumer devices, such as smartphones. The modern smartphone is expected to deliver a high level of user experience across a wide variety of tasks, whilst remaining as power efficient as possible. Commonly, mobile devices undergo runtime optimisation to achieve the required level of performance, with the energy consumption being a secondary concern. In this thesis, we stress that it is vital to not focus on the raw performance of the device, but instead to concentrate on the needs and desires of the end user. This approach ensures that the end-user is satisfied at all times, and that the power consumption for a given level of user experience is minimised. Hence, we advocate user-experience-aware system optimisation. We introduce the concept of Quality of Experience, which has traditionally been used only in the telecommunications industry, to mobile system optimisation. We develop user experience models in the form of utility functions, and use these to translate lowlevel metrics into the delivered user experience. Upon these models we build simple, yet effective, QoE-aware Central Processing Unit (CPU) and Graphics Processing Unit (GPU) governing algorithms which adjust the performance and power consumption at runtime to meet user experience requirements. When creating our algorithms, we first analyse and characterise the operation of both CPU and GPU workloads. Specifically, we investigate how the level of compute-boundedness or memory-boundedness of CPU workloads affects frequency scalability, as well as determining how the available bandwidth and core count for a GPU affects the rendering performance. We combine both gem5-based simulation driven analysis and hardware-based verification in order to validate our QoE-aware governing algorithms. Additionally, we validate the operation of our algorithms using a variety of common mobile workloads. As part of this work, we have also extended the gem5 simulator to allow use to investigate the potential for finegrained Dynamic Voltage and Frequency Scaling (DVFS) adjustment, and use this as a platform to investigate the operation of the Linux CPUFreq governors used on modern mobile platforms.||2016|10.2147/ceg.s111003|A. Bischoff|6.555555555555555|2
1489|High-order absorbing boundary conditions with corner/edge compatibility for GPU-accelerated discontinuous Galerkin wave simulations|Discontinuous Galerkin finite element schemes exhibit attractive features for accurate large-scale wave-propagation simulations on modern parallel architectures. For many applications, these schemes must be coupled with non-reflective boundary treatments to limit the size of the computational domain without losing accuracy or computational efficiency, which remains a challenging task. In this paper, we present a combination of high-order absorbing boundary conditions (HABCs) with a nodal discontinuous Galerkin method for cuboidal computational domains. Compatibility conditions are derived for HABCs intersecting at the edges and the corners of a cuboidal domain. We propose a GPU implementation of the computational procedure, which results in a multidimensional solver with equations to be solved on 0D, 1D, 2D and 3D spatial regions. Numerical results demonstrate both the accuracy and the computational efficiency of our approach. We have considered academic benchmarks, as well as a realistic benchmark based on the SEAM model used in exploration geophysics.||2016|10.1016/j.jcp.2016.04.003|A. Atle, Jesse Chan, T. Warburton, A. Modave|6.555555555555555|2
961|SOAP-GPU: Efficient spectral modeling of stellar activity using graphical processing units|Stellar activity mitigation is one of the major challenges for the detection of earth-like exoplanets in radial velocity (RV) measurements. Several promising techniques are now investigating the use of spectral time-series, to differentiate between stellar and planetary perturbations. In this paper, we present a new version of the Spot Oscillation And Planet (SOAP) 2.0 code that can model stellar activity at the spectral level using graphical processing units (GPUs). We take advantage of the computational power of GPUs to optimise the computationally expensive algorithms behind the original SOAP 2.0 code. We develope GPU kernels that allow to model stellar activity on any given wavelength range. In addition to the treatment of stellar activity at the spectral level, SOAP-GPU also includes the change of spectral line bisectors from center to limb, and can take as input PHOENIX spectra to model the quiet photosphere, spots and faculae, which allow to simulate stellar activity for a wide space in stellar properties. Benchmark calculations show that for the same accuracy, this new code improves the computational speed by a factor of 60 compared with a modified version of SOAP 2.0 that generates spectra, when modeling stellar activity on the full visible spectral range with a resolution of R=115'000. Although the code now includes the variation of spectral line bisector with center to limb angle, the effect on the derived RVs is small. The publicly available SOAP-GPU code allows to efficiently model stellar activity at the spectral level, which is essential to test further stellar activity mitigation techniques working at the level of spectral timeseries not affected by other sources of noise. Besides a huge gain in performance, SOAP-GPU also includes more physics and is able to model different stars than the Sun, from F to K dwarfs, thanks to the use of the PHOENIX spectral library.|Astronomy &amp; Astrophysics|2023|10.1051/0004-6361/202244568|X. Dumusque, Yinan Zhao|6.5|2
1488|Blaze-DEM : a GPU based large scale 3D discrete element particle transport framework|Understanding the dynamic behavior of particulate materials is extremely important to many industrial processes with a wide range of applications ranging from hopper flows in agriculture to tumbling mills in the mining industry. Thus simulating the dynamics of particulate materials is critical in the design and optimization of such processes. The mechanical behavior of particulate materials is complex and cannot be described by a closed form solution for more than a few particles. A popular and successful numerical approach in simulating the underlying dynamics of particulate materials is the discrete element method (DEM). However, the DEM is computationally expensive and computationally viable simulations are typically restricted to a few particles with realistic particle shape or a larger number of particles with an often oversimplified particle shape. It has been demonstrated for numerous applications that an accurate representation of the particle shape is essential to accurately capture the macroscopic transport of particulates. The most common approach to represent particle shape is by using a cluster of spheres to approximate the shape of a particle. This approach is computationally intensive as multiple spherical particles are required to represent a single non-spherical particle. In addition spherical particles are for certain applications a poor approximation when sharp interfaces are essential to capture the bulk transport behavior. An advantage of this approach is that non-convex particles are handled with ease. Polyhedra represent the geometry of most convex particulate materials well and when combined with appropriate contact models exhibit realistic transport behavior to that of the actual system. However detecting collisions between the polyhedra is computationally expensive, often limiting simulations to only a few thousand of particles. Driven by the demand for real-time graphics, the Graphical Processor Unit (GPU) offers cluster type performance at a fraction of the computational cost. The parallel nature of the GPU allows for a large number of simple independent processes to be executed in parallel. This results in a significant speed up over conventional implementations utilizing the Central Processing Unit (CPU) architecture, when algorithms are well aligned and optimized for the threading model of the GPU. This thesis investigates the suitability of the GPU architecture to simulate the transport of particulate materials using the DEM. The focus of this thesis is to develop a computational framework for the GPU architecture that can model (i) tens of millions of spherical particles and (ii) millions of polyhedral particles in a realistic time frame on a desktop computer using a single GPU. The contribution of this thesis is the development of a novel GPU computational framework Blaze-DEM, that encompasses collision detection algorithms and various heuristics that are optimized for the parallel GPU architecture. This research has resulted in a new computational performance level being reached in DEM simulations for both spherical 2 © University of Pretoria and polyhedra shaped particles. In terms of the particle shape there are currently no other freely available codes that can match the geometrical fidelity in terms of accurate particle shape representation on the GPU. To the authors knowledge there is only one study on the GPU that takes particle shape into account with a physics model of similar fidelity to Blaze-DEM. In that study by Longmore at al. the clumped sphere method is used. Blaze-DEM is able to simulate 2 orders of magnitude more particles compared to other published results while being 3 times faster. The only reported implementations for polyhedra are on the CPU platform. Blaze-DEM is hundreds of times faster compared to CPU codes with physics models of a similar fidelity and 24 times faster than CPU codes with physics models of a lower fidelity. For simulations involving spherical particles Blaze-DEM is 5 times faster than other GPU based codes that have physics models of a similar fidelity.||2015|10.1016/j.mineng.2015.05.010|N. Govender|6.5|2
277|Overhauling SC atomics in C11 and OpenCL|Despite the conceptual simplicity of sequential consistency (SC), the semantics of SC atomic operations and fences in the C11 and OpenCL memory models is subtle, leading to convoluted prose descriptions that translate to complex axiomatic formalisations. We conduct an overhaul of SC atomics in C11, reducing the associated axioms in both number and complexity. A consequence of our simplification is that the SC operations in an execution no longer need to be totally ordered. This relaxation enables, for the first time, efficient and exhaustive simulation of litmus tests that use SC atomics. We extend our improved C11 model to obtain the first rigorous memory model formalisation for OpenCL (which extends C11 with support for heterogeneous many-core programming). In the OpenCL setting, we refine the SC axioms still further to give a sensible semantics to SC operations that employ a ‘memory scope’ to restrict their visibility to specific threads. Our overhaul requires slight strengthenings of both the C11 and the OpenCL memory models, causing some behaviours to become disallowed. We argue that these strengthenings are natural, and that all of the formalised C11 and OpenCL compilation schemes of which we are aware (Power and x86 CPUs for C11, AMD GPUs for OpenCL) remain valid in our revised models. Using the HERD memory model simulator, we show that our overhaul leads to an exponential improvement in simulation time for C11 litmus tests compared with the original model, making *exhaustive* simulation competitive, time-wise, with the *non-exhaustive* CDSChecker tool.|ACM-SIGACT Symposium on Principles of Programming Languages|2015|10.1145/2837614.2837637|Mark Batty, John Wickerson|6.4|2
1145|A hybrid parallel cellular automata model for urban growth simulation over GPU/CPU heterogeneous architectures|As an important spatiotemporal simulation approach and an effective tool for developing and examining spatial optimization strategies (e.g., land allocation and planning), geospatial cellular automata (CA) models often require multiple data layers and consist of complicated algorithms in order to deal with the complex dynamic processes of interest and the intricate relationships and interactions between the processes and their driving factors. Also, massive amount of data may be used in CA simulations as high-resolution geospatial and non-spatial data are widely available. Thus, geospatial CA models can be both computationally intensive and data intensive, demanding extensive length of computing time and vast memory space. Based on a hybrid parallelism that combines processes with discrete memory and threads with global memory, we developed a parallel geospatial CA model for urban growth simulation over the heterogeneous computer architecture composed of multiple central processing units (CPUs) and graphics processing units (GPUs). Experiments with the datasets of California showed that the overall computing time for a 50-year simulation dropped from 13,647 seconds on a single CPU to 32 seconds using 64 GPU/CPU nodes. We conclude that the hybrid parallelism of geospatial CA over the emerging heterogeneous computer architectures provides scalable solutions to enabling complex simulations and optimizations with massive amount of data that were previously infeasible, sometimes impossible, using individual computing approaches.|International Journal of Geographical Information Science|2016|10.1080/13658816.2015.1039538|Miaoqing Huang, Xuan Shi, Qingfeng Guan, Chenggang Lai|6.333333333333333|2
324|Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level|Modern GPU frameworks use a two-phase compilation approach. Kernels written in a high-level language are initially compiled to an implementation agnostic intermediate language (IL), then finalized to the machine ISA only when the target GPU hardware is known. Most GPU microarchitecture simulators available to academics execute IL instructions because there is substantially less functional state associated with the instructions, and in some situations, the machine ISA’s intellectual property may not be publicly disclosed. In this paper, we demonstrate the pitfalls of evaluating GPUs using this higher-level abstraction, and make the case that several important microarchitecture interactions are only visible when executing lower-level instructions. Our analysis shows that given identical application source code and GPU microarchitecture models, execution behavior will differ significantly depending on the instruction set abstraction. For example, our analysis shows the dynamic instruction count of the machine ISA is nearly 2× that of the IL on average, but contention for vector registers is reduced by 3× due to the optimized resource utilization. In addition, our analysis highlights the deficiencies of using IL to model instruction fetching, control divergence, and value similarity. Finally, we show that simulating IL instructions adds 33% error as compared to the machine ISA when comparing absolute runtimes to real hardware.|International Symposium on High-Performance Computer Architecture|2018|10.1109/HPCA.2018.00058|Matthew Poremba, Matthew D. Sinclair, Mark Wyse, Onur Kayiran, Xianwei Zhang, Brandon Potter, Joseph Gross, Jieming Yin, Timothy G. Rogers, Anthony Gutierrez, Bradford M. Beckmann, Michael LeBeane, Akshay Jain, A. Duțu, Sooraj Puthoor, J. Kalamatianos|6.285714285714286|2
481|FPGA-Based Simulated Bifurcation Machine|Since many combinatorial optimization problems can be mapped onto ground-state search problems of Ising models, special-purpose machines for Ising problems have attracted intense attention. Simulated bifurcation (SB) is a recently proposed algorithm to solve these Ising problems. One of the remarkable features of SB is the high-degree parallelism underlying in the algorithm, providing an opportunity to solve the Ising problems very fast by massively parallel processing. In this work, we implement the SB algorithm on FPGAs by designing massively parallel custom circuits. We then compare the FPGA-based SB machines with a state-of-the-art machine called a coherent Ising machine (CIM), a highly optimized implementation of simulated annealing (SA), and GPU-based SB machines. SB machines with spin size of 2,048/4,096 (2K/4K) on an Arria10 GX1150 FPGA have 8,192 processing elements for the matrix-vector multiplication (MM) modules (the most computationally intensive part) and achieve computation throughput of 1,873/2,027 GMAC/s for the MM modules, outperforming 2K/4K SB machines on an Nvidia Tesla V100 GPU (113/183 GMAC/s). The 2K FPGA-SB solves all-to-all connected 2000-node MAX-CUT problem 14X (/124X) faster than the CIM (/the highly-optimized SA), with much better energy efficiency (288X better than the CIM).|International Conference on Field-Programmable Logic and Applications|2019|10.1109/FPL.2019.00019|Hayato Goto, Alexander Dixon, K. Tatsumura|6.166666666666667|2
468|Performance evaluation of GPU parallelization, space‐time adaptive algorithms, and their combination for simulating cardiac electrophysiology|The use of computer models as a tool for the study and understanding of the complex phenomena of cardiac electrophysiology has attained increased importance nowadays. At the same time, the increased complexity of the biophysical processes translates into complex computational and mathematical models. To speed up cardiac simulations and to allow more precise and realistic uses, 2 different techniques have been traditionally exploited: parallel computing and sophisticated numerical methods. In this work, we combine a modern parallel computing technique based on multicore and graphics processing units (GPUs) and a sophisticated numerical method based on a new space‐time adaptive algorithm. We evaluate each technique alone and in different combinations: multicore and GPU, multicore and GPU and space adaptivity, multicore and GPU and space adaptivity and time adaptivity. All the techniques and combinations were evaluated under different scenarios: 3D simulations on slabs, 3D simulations on a ventricular mouse mesh, ie, complex geometry, sinus‐rhythm, and arrhythmic conditions. Our results suggest that multicore and GPU accelerate the simulations by an approximate factor of 33×, whereas the speedups attained by the space‐time adaptive algorithms were approximately 48. Nevertheless, by combining all the techniques, we obtained speedups that ranged between 165 and 498. The tested methods were able to reduce the execution time of a simulation by more than 498× for a complex cellular model in a slab geometry and by 165× in a realistic heart geometry simulating spiral waves. The proposed methods will allow faster and more realistic simulations in a feasible time with no significant loss of accuracy.|International Journal for Numerical Methods in Biomedical Engineering|2018|10.1002/cnm.2913|R. Weber dos Santos, C. Constantinides, Bernardo Martins Rocha, Wagner Meira Jr, Rafael Sachetto Oliveira, D. Burgarelli|6.142857142857143|2
488|Graphics processing unit-accelerated mesh-based Monte Carlo photon transport simulations|Abstract. The mesh-based Monte Carlo (MMC) algorithm is increasingly used as the gold-standard for developing new biophotonics modeling techniques in 3-D complex tissues, including both diffusion-based and various Monte Carlo (MC)-based methods. Compared to multilayered and voxel-based MCs, MMC can utilize tetrahedral meshes to gain improved anatomical accuracy but also results in higher computational and memory demands. Previous attempts of accelerating MMC using graphics processing units (GPUs) have yielded limited performance improvement and are not publicly available. We report a highly efficient MMC—MMCL—using the OpenCL heterogeneous computing framework and demonstrate a speedup ratio up to 420× compared to state-of-the-art single-threaded CPU simulations. The MMCL simulator supports almost all advanced features found in our widely disseminated MMC software, such as support for a dozen of complex source forms, wide-field detectors, boundary reflection, photon replay, and storing a rich set of detected photon information. Furthermore, this tool supports a wide range of GPUs/CPUs across vendors and is freely available with full source codes and benchmark suites at http://mcx.space/#mmc.|Journal of Biomedical Optics|2019|10.1117/1.JBO.24.11.115002|Q. Fang, Shijie Yan|6.0|2
932|Multinode Multi-GPU Two-Electron Integrals: Code Generation Using the Regent Language.|The computation of two-electron repulsion integrals (ERIs) is often the most expensive step of integral-direct self-consistent field methods. Formally it scales as O(N4), where N is the number of Gaussian basis functions used to represent the molecular wave function. In practice, this scaling can be reduced to O(N2) or less by neglecting small integrals with screening methods. The contributions of the ERIs to the Fock matrix are of Coulomb (J) and exchange (K) type and require separate algorithms to compute matrix elements efficiently. We previously implemented highly efficient GPU-accelerated J-matrix and K-matrix algorithms in the electronic structure code TeraChem. Although these implementations supported the use of multiple GPUs on a node, they did not support the use of multiple nodes. This presents a key bottleneck to cutting-edge ab initio simulations of large systems, e.g., excited state dynamics of photoactive proteins. We present our implementation of multinode multi-GPU J- and K-matrix algorithms in TeraChem using the Regent programming language. Regent directly supports distributed computation in a task-based model and can generate code for a variety of architectures, including NVIDIA GPUs. We demonstrate multinode scaling up to 45 GPUs (3 nodes) and benchmark against hand-coded TeraChem integral code. We also outline our metaprogrammed Regent implementation, which enables flexible code generation for integrals of different angular momenta.|Journal of Chemical Theory and Computation|2022|10.1021/acs.jctc.2c00414|A. Heirich, K. G. Johnson, A. Aiken, Ellis Hoag, S. Mirchandaney, Todd J. Martinez|6.0|2
1087|magnum.np -- A PyTorch based GPU enhanced Finite Difference Micromagnetic Simulation Framework for High Level Development and Inverse Design|magnum.np is a micromagnetic finite-difference library completely based on the tensor library PyTorch. The use of such a high level library leads to a highly maintainable and extensible code base which is the ideal candidate for the investigation of novel algorithms and modeling approaches. On the other hand magnum.np benefits from the devices abstraction and optimizations of PyTorch enabling the efficient execution of micromagnetic simulations on a number of computational platforms including GPU and potentially TPU systems. We demonstrate a competitive performance to state-of-the art micromagnetic codes such a mumax3 and show how our code enables the rapid implementation of new functionality. Furthermore, handling inverse problems becomes possible by using PyTorch's autograd feature.||2023|10.1038/s41598-023-39192-5|D. Suess, S. Koraltan, C. Abert, F. Bruckner|6.0|2
1616|FullMonteCUDA: a fast, flexible, and accurate GPU-accelerated Monte Carlo simulator for light propagation in turbid media.|Optimizing light delivery for photodynamic therapy, quantifying tissue optical properties or reconstructing 3D distributions of sources in bioluminescence imaging and absorbers in diffuse optical imaging all involve solving an inverse problem. This can require thousands of forward light propagation simulations to determine the parameters to optimize treatment, image tissue or quantify tissue optical properties, which is time-consuming and computationally expensive. Addressing this problem requires a light propagation simulator that produces results quickly given modelling parameters. In previous work, we developed FullMonteSW: currently the fastest, tetrahedral-mesh, Monte Carlo light propagation simulator written in software. Additional software optimizations showed diminishing performance improvements, so we investigated hardware acceleration methods. This work focuses on FullMonteCUDA: a GPU-accelerated version of FullMonteSW which targets NVIDIA GPUs. FullMonteCUDA has been validated across several benchmark models and, through various GPU-specific optimizations, achieves a 288-936x speedup over the single-threaded, non-vectorized version of FullMonteSW and a 4-13x speedup over the highly optimized, hand-vectorized and multi-threaded version. The increase in performance allows inverse problems to be solved more efficiently and effectively.|Biomedical Optics Express|2019|10.1364/BOE.10.004711|T. Young-Schultz, Stephen Brown, L. Lilge, Vaughn Betz|5.833333333333333|2
604|GPU-Accelerated Semi-Empirical Born Oppenheimer Molecular Dynamics using PyTorch.|A new open-source high-performance implementation of Born Oppenheimer Molecular Dynamics based on semi-empirical quantum mechanics models using PyTorch called PYSEQM is presented. PYSEQM was designed to provide researchers in computational chemistry with an open-source, efficient, scalable, and stable quantum-based molecular dynamics engine. In particular, PYSEQM enables computation on modern GPU hardware, and, through the use of automatic differentiation, supplies interfaces for model parameterization with machine learning techniques to perform multi-objective training and prediction. The implemented semi-empirical quantum mechanical methods (MNDO, AM1, PM3) are described. Additional algorithms include a recursive Fermi-operator expansion scheme (SP2) and Extended Lagrangian Born-Oppenheimer molecular dynamics allowing for rapid simulations. Finally, benchmark testing on the nanostar dendrimer and series of polyethylene molecules provide a baseline of code efficiency, time cost, and scaling and stability of energy conservation that verify that PYSEQM provides fast and accurate computations.|Journal of Chemical Theory and Computation|2020|10.1021/acs.jctc.0c00243|A. Niklasson, Guoqing Zhou, B. Nebgen, S. Tretiak, Walter Malone, N. Lubbers|5.8|2
1438|Revealing chemical reactions of coal pyrolysis with GPU-enabled ReaxFF molecular dynamics and cheminformatics analysis|The complex chemistry of coal pyrolysis is difficult to be captured by experimental techniques or simulated with the quantum mechanics computational methods. The emerging of both the large-scale coal models and the promising capability of reactive molecular dynamics (ReaxFF MD) motivated us to develop a new methodology by combining graphics processing unit (GPU)-enabled high performance computing with cheminformatics analysis in order to explore the coal pyrolysis mechanisms using ReaxFF MD. The methodology is rooted in two new software tools, GMD-Reax, the first GPU-enabled ReaxFF MD codes that make it practical to simulate large-scale models (∼10,000 atoms) on desktop workstations, and visualisation and analysis of reactive molecular dynamics (VARMD), the first software dedicated to analysis of detailed chemical reactions from the trajectories of ReaxFF MD simulation. With this methodology, reasonable product profiles and gas generation sequences of pyrolysis for bituminous coal models ranging from ∼1000 to ∼10,000 atoms (including the system with 28,351 atoms, one of the largest systems used in ReaxFF MD) have been obtained. The complex and detailed chemical reactions directly revealed by VARMD can provide further information on radical behaviours and their connection with pyrolysates. The methodology presented here offers a new and promising approach to systematically understand the complex chemical reactions in thermolysis of very complicated molecular systems.||2015|10.1080/08927022.2014.913789|Xiaoxia Li, Jian Liu, Zhengchang Mo, Li Guo|5.8|2
1510|Predicting Granular Segregation: A Continuum Model Using Parameters From GPU-Based DEM Simulations|Modelling size and density segregation of granular materials has important applications in many industrial processes and geophysical situations. We have developed a continuum model for granular segregation that uses parameters based on kinematic details measured from discrete element method (DEM) simulations. Because many simulations involving O(10 6 ) particles were necessary to obtain parameters characteristic of the flow and segregation, we developed a CUDA-based parallelized DEM code so simulations could be performed on low-cost commercial NVIDIA Graphics Processing Units (GPUs). This approach reduces the simulation time by an order of magnitude, making possible parametric studies of flow and segregation phenomena via DEM simulations. The segregation length scale and collisional diffusion coefficient are found as functions of the flow rate, particle diameters, and shear rate based on a large number of DEM simulations. These relations along with appropriate kinematic conditions such as the velocity profile and flowing layer depth are used in an advection-diffusion model that is modified to include a segregation term with parameters obtained from DEM simulations. 1-3 The theory has been tested for size bi-disperse particles for several quasi-2D granular flows including heaps, chutes, and rotating tumblers. In addition, the theory has been extended to model tri-disperse quasi-2D heap flow and log-normally distributed polydisperse quasi-2D chute flow. The theoretical segregation patterns and particle size distributions match results from full-scale DEM simulations and experiments. Ongoing work is focused on further extending the theory to polydisperse size segregation in other geometries, simultaneous density and size segregation, and shape segregation in quasi-2D configurations as well as Couette flow and fully 3D granular flows.||2015|10.1002/aic.14780|J. Ottino, P. Umbanhowar, Richard M. Lueptow, A. Isner|5.8|2
763|The abTEM code: transmission electron microscopy from first principles|Simulation of transmission electron microscopy (TEM) images or diffraction patterns is often required to interpret experimental data. Since nuclear cores dominate electron scattering, the scattering potential is typically described using the independent atom model, which completely neglects valence bonding and its effect on the transmitting electrons. As instrumentation has advanced, new measurements have revealed subtle details of the scattering potential that were previously not accessible to experiment. We have created an open-source simulation code designed to meet these demands by integrating the ability to calculate the potential via density functional theory (DFT) with a flexible modular software design. abTEM can simulate most standard imaging modes and incorporates the latest algorithmic developments. The development of new techniques requires a program that is accessible to domain experts without extensive programming experience. abTEM is written purely in Python and designed for easy modification and extension. The effective use of modern open-source libraries makes the performance of abTEM highly competitive with existing optimized codes on both CPUs and GPUs and allows us to leverage an extensive ecosystem of libraries, such as the Atomic Simulation Environment and the DFT code GPAW. abTEM is designed to work in an interactive Python notebook, creating a seamless and reproducible workflow from defining an atomic structure, calculating molecular dynamics (MD) and electrostatic potentials, to the analysis of results, all in a single, easy-to-read document. This article provides ongoing documentation of abTEM development. In this first version, we show use cases for hexagonal boron nitride, where valence bonding can be detected, a 4D-STEM simulation of molybdenum disulfide including ptychographic phase reconstruction, a comparison of MD and frozen phonon modeling for convergent-beam electron diffraction of a 2.6-million-atom silicon system, and a performance comparison of our fast implementation of the PRISM algorithm for a decahedral 20000-atom gold nanoparticle.|Open Research Europe|2021|10.12688/openreseurope.13015.2|T. Susi, Jacob Madsen|5.75|2
356|Long-time simulation of calcium induced calcium release in a heart cell using the finite element method on a hybrid CPU/GPU node|A mathematical model of Calcium Induced Calcium Release in a heart cell has been developed that consists of three coupled non-linear advection-diffusion-reaction equations. A program in C with MPI based on matrix-free Newton-Krylov method gives very good scalability, but still requires large run times for fine meshes. A programming model with CUDA and MPI that utilizes GPUs on a hybrid node can significantly reduce the wall clock time. This paper reports initial results that demonstrate speedup using a hybrid node with two GPUs over the best results on a CPU node.|Spring Simulation Multiconference|2015|10.1080/13658816.2015.1039538|Xuan Huang, M. Gobbert|5.7|2
465|A massively parallel infrastructure for adaptive multiscale simulations: modeling RAS initiation pathway for cancer|Computational models can define the functional dynamics of complex systems in exceptional detail. However, many modeling studies face seemingly incommensurate requirements: to gain meaningful insights into some phenomena requires models with high resolution (microscopic) detail that must nevertheless evolve over large (macroscopic) length- and time-scales. Multiscale modeling has become increasingly important to bridge this gap. Executing complex multiscale models on current petascale computers with high levels of parallelism and heterogeneous architectures is challenging. Many distinct types of resources need to be simultaneously managed, such as GPUs and CPUs, memory size and latencies, communication bottlenecks, and filesystem bandwidth. In addition, robustness to failure of compute nodes, network, and filesystems is critical. We introduce a first-of-its-kind, massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), which couples a macro scale model spanning micrometer length- and millisecond time-scales with a micro scale model employing high-fidelity molecular dynamics (MD) simulations. MuMMI is a cohesive and transferable infrastructure designed for scalability and efficient execution on heterogeneous resources. A central workflow manager simultaneously allocates GPUs and CPUs while robustly handling failures in compute nodes, communication networks, and filesystems. A hierarchical scheduler controls GPU-accelerated MD simulations and in situ analysis. We present the various MuMMI components, including the macro model, GPU-accelerated MD, in situ analysis of MD data, machine learning selection module, a highly scalable hierarchical scheduler, and detail the central workflow manager that ties these modules together. In addition, we present performance data from our runs on Sierra, in which we validated MuMMI by investigating an experimentally intractable biological system: the dynamic interaction between RAS proteins and a plasma membrane. We used up to 4000 nodes of the Sierra supercomputer, concurrently utilizing over 16,000 GPUs and 176,000 CPU cores, and running up to 36,000 different tasks. This multiscale simulation includes about 120,000 MD simulations aggregating over 200 milliseconds, which is orders of magnitude greater than comparable studies.|International Conference on Software Composition|2019|10.1145/3295500.3356197|S. Gnanakaran, Bruce D. D'Amora, F. Lightstone, Helgi I. Ingólfsson, Timothy S. Carpenter, Chris Neale, Claudia Misale, M. Surh, F. Natale, S. Sundram, L. Stanton, D. Nissley, F. Streitz, S. K. Schumacher, Gautham Dharuman, J. Glosli, H. Bhatia, T. Oppelstrup, Xiaohua Zhang, Changhoan Kim, Carlos H. A. Costa, P. Bremer, L. Schneidenbach, Yue Yang, T. Scogland|5.666666666666667|2
477|Decentralized Distributed PPO: Solving PointGoal Navigation|We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).|arXiv.org|2019|10.1109/iccv.2019.00111|Dhruv Batra, Erik Wijmans, Abhishek Kadian, Irfan Essa, Ari S. Morcos, Devi Parikh, Stefan Lee, M. Savva|5.666666666666667|2
861|Towards Compact Autonomous Driving Perception With Balanced Learning and Multi-Sensor Fusion|We present a novel compact deep multi-task learning model to handle various autonomous driving perception tasks in one forward pass. The model performs multiple views of semantic segmentation, depth estimation, light detection and ranging (LiDAR) segmentation, and bird’s eye view projection simultaneously without being supported by other models. We also provide an adaptive loss weighting algorithm to tackle the imbalanced learning issue that occurred due to plenty of given tasks. Through data pre-processing and intermediate sensor fusion techniques, the model can process and combine multiple input modalities retrieved from RGB cameras, dynamic vision sensors (DVS), and LiDAR placed at several positions on the ego vehicle. Therefore, a better understanding of a dynamically changing environment can be achieved. Based on the ablation study, the model variant trained with our proposed method achieves a better performance. Furthermore, a comparative study is also conducted to clarify its performance and effectiveness against the combination of some recent models. As a result, our model maintains better performance even with much fewer parameters. Hence, the model can inference faster with less GPU memory utilization. Moreover, the result tends to be consistent in 3 different CARLA simulation datasets and 1 real-world nuScenes-lidarseg dataset. To support future research, we share codes and other files publicly at https://github.com/oskarnatan/compact-perception.|IEEE transactions on intelligent transportation systems (Print)|2022|10.1109/TITS.2022.3149370|Oskar Natan, J. Miura|5.666666666666667|2
876|DefGraspSim: Physics-Based Simulation of Grasp Outcomes for 3D Deformable Objects|Robotic grasping of 3D deformable objects (e.g., fruits/vegetables, internal organs, bottles/boxes) is critical for real-world applications such as food processing, robotic surgery, and household automation. However, developing grasp strategies for such objects is uniquely challenging. Unlike rigid objects, deformable objects have infinite degrees of freedom and require field quantities (e.g., deformation, stress) to fully define their state. As these quantities are not easily accessible in the real world, we propose studying interaction with deformable objects through physics-based simulation. As such, we simulate grasps on a wide range of 3D deformable objects using a GPU-based implementation of the corotational finite element method (FEM). To facilitate future research, we open-source our simulated dataset (34 objects, 1e5 Pa elasticity range, 6800 grasp evaluations, 1.1 M grasp measurements), as well as a code repository that allows researchers to run our full FEM-based grasp evaluation pipeline on arbitrary 3D object models of their choice. Finally, we demonstrate good correspondence between grasp outcomes on simulated objects and their real counterparts.|IEEE Robotics and Automation Letters|2022|10.1109/LRA.2022.3158725|Tucker Hermans, Isabella Huang, Yashraj S. Narang, Clemens Eppner, D. Fox, R. Bajcsy, M. Macklin, Balakumar Sundaralingam|5.666666666666667|2
596|Survival Regression with Accelerated Failure Time Model in XGBoost|Abstract Survival regression is used to estimate the relation between time-to-event and feature variables, and is important in application domains such as medicine, marketing, risk management, and sales management. Nonlinear tree based machine learning algorithms as implemented in libraries such as XGBoost, scikit-learn, LightGBM, and CatBoost are often more accurate in practice than linear models. However, existing state-of-the-art implementations of tree-based models have offered limited support for survival regression. In this work, we implement loss functions for learning accelerated failure time (AFT) models in XGBoost, to increase the support for survival modeling for different kinds of label censoring. We demonstrate with real and simulated experiments the effectiveness of AFT in XGBoost with respect to a number of baselines, in two respects: generalization performance and training speed. Furthermore, we take advantage of the support for NVIDIA GPUs in XGBoost to achieve substantial speedup over multi-core CPUs. To our knowledge, our work is the first implementation of AFT that uses the processing power of NVIDIA GPUs. Starting from the 1.2.0 release, the XGBoost package natively supports the AFT model. The addition of AFT in XGBoost has had significant impact in the open source community, and a few statistics packages now use the XGBoost AFT model. Supplementary materials for this article are available online.|Journal of Computational And Graphical Statistics|2020|10.1080/10618600.2022.2067548|T. Hocking, Hyunsu Cho, Avinash Barnwal|5.6|2
649|A Comparative Study of 2.5D and Fan-out Chip on Substrate : Chip First and Chip Last|The demand of integrated circuits (IC) of high band-width and high-performance applications (Networking, GPU) is more and stronger from end user. The heterogeneous integration techniques have been developed and widely used to integrate multi-chips with fine line/space interconnections. Several types of heterogeneous integration packaging techniques are offered in the market today, for example, through silicon via (TSV) interposer technology: 2.5D IC and re-distribution layer (RDL) fan-out process referred to as fan-out chip on substrate package (FOCoS). The fan-out techniques of FOCoS include chip first and chip last processes. In this study, FEA simulations are performed to examine the warpage, ELK layer crack risk, interconnection / RDL trace broken risk, and board level solder joint reliability of the thre package types include 2.5D IC, chip-first FOCoS and chip-last FOCoS. The validity of the simulation model is confirmed by comparing the numerical results for the warpage and thermal mechanical deformation of chip-last FOCoS with the experimental observations by advanced Metrology Analyzer (aMA) system. Further CFD simulations are then performed to investigate the heat dissipation performance of the three package types. It is clear from this study that these packages are very similar in form, format and function. The preliminary results have shown that high CTE polyimide to induce higher die to die (D2D) interconnection trace stress. This paper details the mechanical and thermal characteristics of the pros and cons would be deep understanding depend on different structure layouts and Bill of Material (BOM) selection.|Electronic Components and Technology Conference|2020|10.1109/ectc32862.2020.00064|D. Tarng, C. Hung, Penny Yang, W. Lai, Ian Hu, Tse-Wei Liao, Karenyu Chen|5.6|2
275|Flexible software profiling of GPU architectures|To aid application characterization and architecture design space exploration, researchers and engineers have developed a wide range of tools for CPUs, including simulators, profilers, and binary instrumentation tools. With the advent of GPU computing, GPU manufacturers have developed similar tools leveraging hardware profiling and debugging hooks. To date, these tools are largely limited by the fixed menu of options provided by the tool developer and do not offer the user the flexibility to observe or act on events not in the menu. This paper presents SASSI (NVIDIA assembly code “SASS” Instrumentor), a low-level assembly-language instrumentation tool for GPUs. Like CPU binary instrumentation tools, SASSI allows a user to specify instructions at which to inject user-provided instrumentation code. These facilities allow strategic placement of counters and code into GPU assembly code to collect user-directed, fine-grained statistics at hardware speeds. SASSI instrumentation is inherently parallel, leveraging the concurrency of the underlying hardware. In addition to the details of SASSI, this paper provides four case studies that show how SASSI can be used to characterize applications and explore the architecture design space along the dimensions of instruction control flow, memory systems, value similarity, and resilience.|International Symposium on Computer Architecture|2015|10.1145/2749469.2750375|Mike O'Connor, S. Hari, Daniel R. Johnson, S. Keckler, Yunsup Lee, Eiman Ebrahimi, D. Nellans, M. Stephenson|5.5|2
414|Estimating Roof Solar Energy Potential in the Downtown Area Using a GPU-Accelerated Solar Radiation Model and Airborne LiDAR Data|Solar energy, as a clean and renewable resource is becoming increasingly important in the global context of climate change and energy crisis. Utilization of solar energy in urban areas is of great importance in urban energy planning, environmental conservation, and sustainable development. However, available spaces for solar panel installation in cities are quite limited except for building roofs. Furthermore, complex urban 3D morphology greatly affects sunlit patterns on building roofs, especially in downtown areas, which makes the determination of roof solar energy potential a challenging task. The object of this study is to estimate the solar radiation on building roofs in an urban area in Shanghai, China, and select suitable spaces for installing solar panels that can effectively utilize solar energy. A Graphic Processing Unit (GPU)-based solar radiation model named SHORTWAVE-C simulating direct and non-direct solar radiation intensity was developed by adding the capability of considering cloud influence into the previous SHORTWAVE model. Airborne Light Detection and Ranging (LiDAR) data was used as the input of the SHORTWAVE-C model and to investigate the morphological characteristics of the study area. The results show that the SHORTWAVE-C model can accurately estimate the solar radiation intensity in a complex urban environment under cloudy conditions, and the GPU acceleration method can reduce the computation time by up to 46%. Two sites with different building densities and rooftop structures were selected to illustrate the influence of urban morphology on the solar radiation and solar illumination duration. Based on the findings, an object-based method was implemented to identify suitable places for rooftop solar panel installation that can fully utilize the solar energy potential. Our study provides useful strategic guidelines for the selection and assessment of roof solar energy potential for urban energy planning.|Remote Sensing|2015|10.3390/RS71215877|Junhan Wu, Yan Huang, Liang Chen, Jianping Wu, Feng Zhao, Bailang Yu, Zuoqi Chen, Bin Wu, W. Mao|5.5|2
1293|Simulations of CO2 Migration with a Fully-Integrated VE Model on the GPU|The reduction of carbon dioxide emissions is becoming an increasing global priority and is the subject of many current research projects. One of the possible solutions is geological carbon storage, in which CO2 is captured and injected into geological underground reservoirs for permanent storage. An assessment of the associated CO2 leakage risks is crucial when evaluating potential storage sites. By simulating the movement of CO2 during and in the aftermath of the injection we can determine these risks. In this thesis we present a CO2 migration simulator made for this purpose. There are often great uncertainties in the available geological data required to make realistic simulations. This means that one has to be able to evaluate multiple scenarios within a relatively short time frame, putting performance requirements on the simulator. Because the temporal and spatial scales in question are very big, the full 3D models commonly applied in the related branch of oil and gas simulators are too computationally demanding. Thus, our simulator is based on a 2D VE model. What sets our simulator apart from other VE-based simulators, is that we include non-linearized CO2 properties. This means that the vertical integrals which constitute the VE model are no longer trivial expressions, they must be ”fully-integrated”. Through GPU acceleration we implement a simulator that runs just as fast as other CPU-based VE simulators, in spite of the tedious numerical integrations. The benefits of GPU-acceleration are emphasized when simulating on large domains. Moreover we make a comparison study with a VE-based simulator in which the CO2 properties are linearized. The optimization potential and different optimization strategies directed at the GPU implementation of the numerical integrations are also elaborated.||2015|10.1080/19336918.2015.1112485|Guro Seternes|5.5|2
243|Modeling and Simulation of the Economics of Mining in the Bitcoin Market|In January 3, 2009, Satoshi Nakamoto gave rise to the “Bitcoin Blockchain”, creating the first block of the chain hashing on his computer’s central processing unit (CPU). Since then, the hash calculations to mine Bitcoin have been getting more and more complex, and consequently the mining hardware evolved to adapt to this increasing difficulty. Three generations of mining hardware have followed the CPU’s generation. They are GPU’s, FPGA’s and ASIC’s generations. This work presents an agent-based artificial market model of the Bitcoin mining process and of the Bitcoin transactions. The goal of this work is to model the economy of the mining process, starting from GPU’s generation, the first with economic significance. The model reproduces some “stylized facts” found in real-time price series and some core aspects of the mining business. In particular, the computational experiments performed can reproduce the unit root property, the fat tail phenomenon and the volatility clustering of Bitcoin price series. In addition, under proper assumptions, they can reproduce the generation of Bitcoins, the hashing capability, the power consumption, and the mining hardware and electrical energy expenditures of the Bitcoin network.|PLoS ONE|2016|10.1371/journal.pone.0164603|L. Cocco, M. Marchesi|5.444444444444445|2
544|A GPU-Accelerated Shallow-Water Scheme for Surface Runoff Simulations|The capability of a GPU-parallelized numerical scheme to perform accurate and fast simulations of surface runoff in watersheds, exploiting high-resolution digital elevation models (DEMs), was investigated. The numerical computations were carried out by using an explicit finite volume numerical scheme and adopting a recent type of grid called Block-Uniform Quadtree (BUQ), capable of exploiting the computational power of GPUs with negligible overhead. Moreover, stability and zero mass error were ensured, even in the presence of very shallow water depth, by introducing a proper reconstruction of conserved variables at cell interfaces, a specific formulation of the slope source term and an explicit discretization of the friction source term. The 2D shallow water model was tested against two different literature tests and a real event that recently occurred in Italy for which field data is available. The influence of the spatial resolution adopted in different portions of the domain was also investigated for the last test. The achieved low ratio of simulation to physical times, in some cases less than 1:20, opens new perspectives for flood management strategies. Based on the result of such models, emergency plans can be designed in order to achieve a significant reduction in the economic losses generated by flood events.||2020|10.3390/w12030637|F. Aureli, R. Vacondio, Federico Prost, Susanna Dazzi, A. Ferrari|5.4|2
629|Toward Exascale: Overview of Large Eddy Simulations and Direct Numerical Simulations of Nuclear Reactor Flows with the Spectral Element Method in Nek5000|Abstract At the beginning of the last decade, Petascale supercomputers (i.e., computers capable of more than 1 petaFLOP) emerged. Now, at the dawn of exascale supercomputing, we provide a review of recent landmark simulations of portions of reactor components with turbulence-resolving techniques that this computational power has made possible. In fact, these simulations have provided invaluable insight into flow dynamics, which is difficult or often impossible to obtain with experiments alone. We focus on simulations performed with the spectral element method, as this method has emerged as a powerful tool to deliver massively parallel calculations at high fidelity by using large eddy simulation or direct numerical simulation. We also limit this paper to constant-property incompressible flow of a Newtonian fluid in the absence of other body or external forces, although the method is by no means limited to this class of flows. We briefly review the fundamentals of the method and the reasons it is compelling for the simulation of nuclear engineering flows. We review in detail a series of Petascale simulations, including the simulations of helical coil steam generators, fuel assemblies, and pebble beds. Even with Petascale computing, however, limitations for nuclear modeling and simulation tools remain. In particular, the size and scope of turbulence-resolving simulations are still limited by computing power and resolution requirements, which scale with the Reynolds number. In the final part of this paper, we discuss the future of the field, including recent advancements in emerging architectures such as GPU-based supercomputers, which are expected to power the next generation of high-performance computers.||2020|10.1080/00295450.2020.1748557|G. Busco, P. Fischer, Javier Martínez, Lambert Fick, Haomin Yuan, Alper Yildiz, D. Shaver, A. Obabko, Yiqi Yu, L. Brockmeyer, E. Merzari, S. Kerkemeier, M. Min, Y. Hassan|5.4|2
1290|Extreme-Scale Programming Model for Quantum Acceleration within High Performance Computing|Heterogeneous high-performance computing (HPC) systems offer novel architectures accommodating specialized processors that accelerate specific workloads. Near-term quantum computing technologies are poised to benefit applications as wide-ranging as quantum chemistry, machine learning, and optimization. A novel approach to scale these applications with future heterogeneous HPC is to enhance conventional computing systems with quantum processor accelerators. We present the eXtreme-scale ACCelerator programming model (XACC) to enable near-term quantum acceleration within existing conventional HPC applications and workflows. We design and demonstrate the XACC programming model within the C++ language by following a coprocessor machine model akin to the design of OpenCL or CUDA for GPUs. However, we take into account the subtleties and complexities inherent to the interplay between conventional and quantum processing hardware. The XACC framework provides a high-level API that enables applications to offload computational work represented as quantum kernels for execution on an attached quantum accelerator. Our approach is agnostic to the quantum programming language and the quantum processor hardware, which enables quantum programs to be ported to multiple processors for benchmarking, verification and validation, and performance studies. This includes a set of virtual numerical simulators as well as actual quantum processing units. The XACC programming model and its reference implementation may serve as a foundation for future HPC-ready applications, data structures, and libraries using conventional-quantum hybrid computing.||2017|10.1145/3007651|Mengsu Chen, E. Dumitrescu, A. McCaskey, Wu-chun Feng, D. Liakh, T. Humble|5.375|2
1221|Finite Difference Room Acoustics Simulation with General Impedance Boundaries and Viscothermal Losses in Air: Parallel Implementation on Multiple GPUs|Room acoustics modelling requires numerical methods that can simulate the wave behaviour of sound across a wide band of frequencies while taking into account the frequency-dependent characteristics of absorption in air and at walls, but the accurate and stable numerical modelling of complex room geometries under frequency-dependent boundary conditions has remained an elusive problem. Recently, boundary conditions for finite difference/volume time-domain methods have been proposed to simulate frequency-dependent wall impedances in provably-stable numerical schemes based on the viscothermal wave equation over complex room geometries. The purpose of this paper is to investigate these new frequency-dependent boundary conditions in parallel implementations on graphics processing unit (GPU) devices. An efficient implementation of general impedance boundaries combined with the simplest Cartesian viscothermal scheme is presented and shown to be nearly as fast as simpler frequency-independent boundaries in acoustic simulations of a grid-aligned box domain with six frequency-dependent materials and of the Goldener Saal, Musikverein Vienna concert hall, running on up to four Nvidia K20 GPU devices.||2016|10.1109/taslp.2015.2500018|B. Hamilton, S. Bilbao, Craig J. Webb, N. Fletcher|5.333333333333333|2
1191|Efficient GPU-Based Electromagnetic Transient Simulation for Power Systems With Thread-Oriented Transformation and Automatic Code Generation|Electromagnetic transients (EMT) simulation is the most accurate and intensive computation for power systems. Past research has shown the potential of accelerating such simulations using graphics processing units (GPUs). In this paper, an efficient GPU-based parallel EMT simulator is designed. Thread-oriented model transformations are first proposed for the electrical and control systems. Following the transformations, the electrical system is represented by connected networks of massive primitive electrical elements, the computations of which can be constructed as massive fused multiply-add operations and solutions to a linear equation. The control systems are represented by a layered directed acyclic graph with primitive control elements that can be dealt with using single-instruction-multiple-threads groups. Finally, code automation tools are designed to form the GPU kernels. Compared with past work, the proposed model transformations improve the degree of parallelism. Most importantly, the code automation tools improve computational efficiency by substantially reducing addressing and memory access, and render the implementation of the algorithm more general and convenient. Test systems of different sizes were created by connecting multiple IEEE 33-bus distribution systems and adding distributed generators. Simulations were performed on NVIDIA’s K20 $\times$  and P100 cards. The results indicate that the proposed method significantly accelerates EMT simulations compared with a CPU-based program. Real-time performance was also achieved under certain conditions.|IEEE Access|2018|10.1109/ACCESS.2018.2833506|Shaowei Huang, Zhitong Yu, Yin Xu, Wei Xue, Ying Chen, Yankan Song|5.285714285714286|2
1213|Locality-Aware Software Throttling for Sparse Matrix Operation on GPUs|This paper tackles the cache thrashing problem caused by the non-deterministic scheduling feature of bulk synchronous parallel (BSP) execution in GPUs. In the BSP model, threads can be executed and interleaved in any order before reaching a barrier synchronization point, which requires the entire working set to be in cache for maximum data reuse over time. However, it is not always possible to fit all the data in cache at once. Thus, we propose a locality-aware software throttling framework that throttles the number of active execution tasks, prevents cache thrashing, and enhances data reuse over time. Our locality-aware software throttling framework focuses on an important class of applications that operate on sparse matrices (graphs). These applications come from the domains of linear algebra, graph processing, machine learning and scientific simulation. Evaluated on over 200 real sparse matrices and graphs that suffer from cache thrashing in the Florida sparse matrix collection, our technique achieves an average of 2.01X speedup, a maximum of 6.45X speedup, and a maximum performance loss≤5%.|USENIX Annual Technical Conference|2018|10.2514/1.j056674|Chi Zhang, Yan-Hao Chen, E. Zhang, Ari B. Hayes, T. Salmon|5.285714285714286|2
637|Cross-platform programming model for many-core lattice Boltzmann simulations|We present a novel, hardware-agnostic implementation strategy for lattice Boltzmann (LB) simulations, which yields massive performance on homogeneous and heterogeneous many-core platforms. Based solely on C++17 Parallel Algorithms, our approach does not rely on any language extensions, external libraries, vendor-specific code annotations, or pre-compilation steps. Thanks in particular to a recently proposed GPU back-end to C++17 Parallel Algorithms, it is shown that a single code can compile and reach state-of-the-art performance on both many-core CPU and GPU environments for the solution of a given non trivial fluid dynamics problem. The proposed strategy is tested with six different, commonly used implementation schemes to test the performance impact of memory access patterns on different platforms. Nine different LB collision models are included in the tests and exhibit good performance, demonstrating the versatility of our parallel approach. This work shows that it is less than ever necessary to draw a distinction between research and production software, as a concise and generic LB implementation yields performances comparable to those achievable in a hardware specific programming language. The results also highlight the gains of performance achieved by modern many-core CPUs and their apparent capability to narrow the gap with the traditionally massively faster GPU platforms. All code is made available to the community in form of the open-source project stlbm, which serves both as a stand-alone simulation software and as a collection of reusable patterns for the acceleration of pre-existing LB codes.|PLoS ONE|2020|10.1371/journal.pone.0250306|Joël Bény, C. Coreixas, J. Latt|5.2|2
520|Numerical Study of Liquid Sheet Atomization Using Two-Phase Flow Solver on GPU Architecture|Sprays are encountered in a wide variety of engineering applications such as diesel engines, \ngas turbine engines, coating and painting, inkjet printing etc. To generate sprays in these \napplications, a liquid must be atomized. Atomization usually refers to the disintegration \nof a bulk liquid material into small droplets in the ambient gas. The main objective of \natomization and spray systems is to generate a spray with desired droplet size and velocity \ndistribution. In classical atomization process, the initial liquid jet/sheet emanating from \nthe injector starts disintegrating into ligaments or droplets when the aerodynamic forces \ndue to shearing interaction at the liquid-gas interface is greater than the surface tension \nof the liquid. This is called primary breakup or primary atomization. Majority of the \ndroplets generated during primary atomization are unstable and may further disintegrate \nin to smaller droplets if the parent droplet is larger than a critical size. This is called \nsecondary breakup or secondary atomization. Mechanism of primary atomization is still \nnot well understood and is an active subject of scienti�c research. The ideal approach \nto resolve the primary atomization process is by direct numerical simulations (DNS). The \nobjective of the current study is to predict the primary atomization process by means of high \n�delity numerical simulations (close to DNS). Two basic requirements of DNS of multiphase \n \nows are \n� an Eulerian based method capable of e�cient representation of evolving \now features \nof widely di�erent characteristic spatial scales. \n� e�cient parallelization of the solver to handle computational requirement. \nVolume of \nuid (VOF) method has been implemented to capture the gas liquid interface. \nOne \nuid formulation of Navier-Stokes equations was used to describe the motion \nof the two \nuids present in the domain. Numerical solution of the 2-D Navier-Stokes \nequations in non-conservative form is obtained using simpli�ed marker and cell (SMAC) \nalgorithm. Surface tension was included as a source term and evaluated using continuum \nsurface force (CSF) method. Pressure Poisson equation was solved using two di�erent approaches \nnamely multigrid method and symmetric Gauss Siedel preconditioned conjugate \ngradient (SGSPCG) method. Solving pressure Poisson equation is the most time consuming \npart of the entire solver. To e�ciently handle the computational requirement pressure \nPoisson solvers were parallelized on graphics processing unit (GPU) architecture. GPU is \na many-core multithreaded multiprocessor that can perform both graphics and computing \nand can be used in conjunction with a computer. GPU programming was done using \ncompute uni�ed device architecture (CUDA) platform by NVIDIA. The GPU based two \nphase \now solver was validated against standard benchmark test cases and approximate \nDNS studies were performed to study primary atomization process under Diesel jet, gas \nblast and pre�lming gas blast atomization like conditions. \nvi \nDependence of pre�lming gasblast atomization spray characteristics on di�erent parameters \nlike inner core gas velocity, outer gas velocity and liquid sheet thickness was investigated. \nPre�lming gas blast atomization simulations are done on a grid size of 2048 � 1024 and a \nspeedup of approximately 12 � is obtained with GPU based solver using Tesla K20 GPU \naccelerator. In actual pre�lming gas blast atomization (occurring typically in gas turbines), \nthe gases exiting the injector ori�ce are subjected to velocity modulations. Therefore liquid \nsheet atomization with the e�ect of forcing conditions imposed on discharging gas streams \nwas also studied.||2016|10.1016/j.nucengdes.2016.10.020|S. R. Reddy, R. Banerjee|5.111111111111111|2
427|GUFI: A framework for GPUs reliability assessment|Modern many-core Graphics Processing Units (GPUs) are extensively employed in general purpose computing (GPGPU), offering a remarkable execution speedup to inherently data parallel workloads. Unlike graphics computing, GPGPU computing has more stringent reliability requirements. Thus, accurate reliability assessment of GPU hardware structures is important for making informed decisions for error protection. In this paper we focus on microarchitecture-level reliability assessment for GPU architectures. The paper makes the following contributions. First, it presents a comprehensive fault injection framework that targets key hardware structures of GPU architectures such as the register file, the shared memory, the SIMT stack and the instruction buffer, which altogether occupy large part of a modern GPU silicon area. Second, it reports our reliability assessment findings for the target structures, when the GPU executes a diverse set of twelve GPGPU applications. Third, it discusses remarkable differences in the results of fault injection when the applications are simulated in the virtual NVIDIA GPUs instruction set (ptx) vs. the actual instruction set (sass). Finally, it discusses how the framework can be employed either by architects in the early stages of design phase or by programmers for a GPU application's error resilience enhancement.|IEEE International Symposium on Performance Analysis of Systems and Software|2016|10.1109/ISPASS.2016.7482077|D. Gizopoulos, Sotiris Tselonis|5.0|2
922|DAMO: Deep Agile Mask Optimization for Full-Chip Scale|Continuous scaling of the very-large-scale integration system leaves a significant challenge on manufacturing; thus optical proximity correction (OPC) is widely applied in conventional design flow for manufacturability optimization. Traditional techniques conduct OPC by leveraging a lithography model but may suffer from prohibitive computational overhead. In addition, most of them focus on optimizing a single and local clip instead of addressing how to tackle the full-chip scale. In this article, we present DAMO, a high-performance and scalable deep-learning-enabled OPC system for full-chip scale. It is an end-to-end mask optimization paradigm that contains a deep lithography simulator (DLS) for lithography modeling and a deep mask generator (DMG) for mask pattern generation. Moreover, a novel layout splitting algorithm customized for DAMO is proposed, composed of DBSCAN clustering and KMeans++ clustering, to handle the full-chip OPC problem. Further, graph-based computation and parallelism techniques are proposed to deploy our GPU algorithms to accelerate computations. Extensive experiments show that DAMO outperforms state-of-the-art OPC solutions in both academia and industrial commercial toolkit.|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|2022|10.1109/TCAD.2021.3116511|Guojin Chen, Haoyu Yang, Bei Yu, Wanli Chen, Qi Sun, Yuzhe Ma|5.0|2
1063|cuQuantum SDK: A High-Performance Library for Accelerating Quantum Science|We present the NVIDIA cuQuantum SDK [1], a state-of-the-art library of composable primitives for GPU-accelerated quantum circuit simulations. As the size of quantum devices continues to increase, making their classical simulation progressively more difficult, the availability of fast and scalable quantum circuit simulators becomes vital for quantum algorithm developers, as well as quantum hardware engineers focused on the validation and optimization of quantum devices. The cuQuantum SDK was created to accelerate and scale up quantum circuit simulators developed by the quantum information science community by enabling them to utilize efficient scalable software building blocks optimized for NVIDIA GPU-based platforms. The functional building blocks provided cover the needs of both state vector- and tensor network- based simulators, including approximate tensor network simulation methods based on matrix product state, projected entangled pair state, and other factorized tensor representations. By leveraging the enormous computing power of the latest NVIDIA GPU architectures, quantum circuit simulators that have adopted the cuQuantum SDK demonstrate significant acceleration, compared to CPU-only execution, for both the state vector and tensor network simulation methods. Furthermore, by utilizing the parallel primitives available in the cuQuantum SDK, one can easily transition to distributed GPU-accelerated platforms, including those furnished by cloud service providers and high-performance computing systems deployed by supercomputing centers, extending the scale of possible quantum circuit simulations. The rich capabilities provided by the cuQuantum SDK are conveniently made available via both Python and C application programming interfaces, where the former is directly targeting a broad Python quantum community and the latter allows tight integration with simulators written in any programming language.|International Conference on Quantum Computing and Engineering|2023|10.1109/QCE57702.2023.00119|John A. Gunnels, Yao-Lung L. Fang, A. Charara, Sam W. Stanwyck, David Clark, Markus Hohnerbach, S. Varadhan, P. Springer, A. Hehn, S. Morino, Jonathan Wong, Dmitry I. Lyakh, Takuma Yamaguchi, Saul Cohen, Yang Gao, Matthew T. Jones, Jack Guan, I. Terentyev, H. Bayraktar, Tom Lubowe, Timothy B. Costa, A. Haidar|5.0|2
1124|Tensorial properties via the neuroevolution potential framework: Fast simulation of infrared and Raman spectra|Infrared and Raman spectroscopy are widely used for the characterization of gases, liquids, and solids, as the spectra contain a wealth of information concerning in particular the dynamics of these systems. Atomic scale simulations can be used to predict such spectra but are often severely limited due to high computational cost or the need for strong approximations that limit application range and reliability. Here, we introduce a machine learning (ML) accelerated approach that addresses these shortcomings and provides a significant performance boost in terms of data and computational efficiency compared to earlier ML schemes. To this end, we generalize the neuroevolution potential approach to enable the prediction of rank one and two tensors to obtain the tensorial neuroevolution potential (TNEP) scheme. We apply the resulting framework to construct models for the dipole moment, polarizability, and susceptibility of molecules, liquids, and solids, and show that our approach compares favorably with several ML models from the literature with respect to accuracy and computational efficiency. Finally, we demonstrate the application of the TNEP approach to the prediction of infrared and Raman spectra of liquid water, a molecule (PTAF-), and a prototypical perovskite with strong anharmonicity (BaZrO3). The TNEP approach is implemented in the free and open source software package GPUMD, which makes this methodology readily available to the scientific community.||2023|10.1021/acs.jced.3c00561|Zheyong Fan, Paul Erhart, Yi He, Wei Chen, Petter Rosander, Nicklas Osterbacka, Nan Xu, Eric Lindgren, Christian Schafer, Mandi Fang|5.0|2
1129|Mini-batch Gradient Descent with Buffer|In this paper, we studied a buffered mini-batch gradient descent (BMGD) algorithm for training complex model on massive datasets. The algorithm studied here is designed for fast training on a GPU-CPU system, which contains two steps: the buffering step and the computation step. In the buffering step, a large batch of data (i.e., a buffer) are loaded from the hard drive to the graphical memory of GPU. In the computation step, a standard mini-batch gradient descent(MGD) algorithm is applied to the buffered data. Compared to traditional MGD algorithm, the proposed BMGD algorithm can be more efficient for two reasons.First, the BMGD algorithm uses the buffered data for multiple rounds of gradient update, which reduces the expensive communication cost from the hard drive to GPU memory. Second, the buffering step can be executed in parallel so that the GPU does not have to stay idle when loading new data. We first investigate the theoretical properties of BMGD algorithms under a linear regression setting.The analysis is then extended to the Polyak-Lojasiewicz loss function class. The theoretical claims about the BMGD algorithm are numerically verified by simulation studies. The practical usefulness of the proposed method is demonstrated by three image-related real data analysis.||2023|10.1080/10618600.2023.2204130|Hansheng Wang, Yingqiu Zhu, Du Huang, Danyang Huang, Haobo Qi|5.0|2
1235|Fast Pairwise Approximation of Solvent Accessible Surface Area for Implicit Solvent Simulations of Proteins on CPUs and GPUs.|We propose a pairwise and readily parallelizable SASA-based nonpolar solvation approach for protein simulations, inspired by our previous pairwise GB polar solvation model development. In this work, we developed a novel function to estimate the atomic and molecular SASAs of proteins, which results in comparable accuracy as the LCPO algorithm in reproducing numerical icosahedral-based SASA values. Implemented in Amber software and tested on consumer GPUs, our pwSASA method reasonably reproduces LCPO simulation results, but accelerates MD simulations up to 30 times compared to the LCPO implementation, which is greatly desirable for protein simulations facing sampling challenges. The value of incorporating the nonpolar term in implicit solvent simulations is explored on a peptide fragment containing the hydrophobic core of HP36 and evaluating thermal stability profiles of four small proteins.|Journal of Chemical Theory and Computation|2018|10.1021/acs.jctc.8b00413|He Huang, C. Simmerling|5.0|2
1394|Acceleration of High-Fidelity Wireless Network Simulations|Network simulation with bit-accurate modeling of modulation, coding and channel properties is typically computationally intensive. Simple link-layer models that are frequently used in network simulations sacrifice accuracy to decrease simulation time. We investigate the performance and simulation time of link models that use analytical bounds on link performance and bit-accurate link models executed in Graphical Processing Units (GPUs). We show that properly chosen analytical bounds on link performance can result in simulation results close to those using bit-level simulation while providing a significant reduction in simulation time. We also show that bit-accurate decoding in link models can be expedited using parallel processing in GPUs without compromising accuracy and decreasing the overall simulation time.||2017|10.1039/c7ob01290j|Madhabi Manandhar|4.875|2
1258|Validating quantum-classical programming models with tensor network simulations|The exploration of hybrid quantum-classical algorithms and programming models on noisy near-term quantum hardware has begun. As hybrid programs scale towards classical intractability, validation and benchmarking are critical to understanding the utility of the hybrid computational model. In this paper, we demonstrate a newly developed quantum circuit simulator based on tensor network theory that enables intermediate-scale verification and validation of hybrid quantum-classical computing frameworks and programming models. We present our tensor-network quantum virtual machine (TNQVM) simulator which stores a multi-qubit wavefunction in a compressed (factorized) form as a matrix product state, thus enabling single-node simulations of larger qubit registers, as compared to brute-force state-vector simulators. Our simulator is designed to be extensible in both the tensor network form and the classical hardware used to run the simulation (multicore, GPU, distributed). The extensibility of the TNQVM simulator with respect to the simulation hardware type is achieved via a pluggable interface for different numerical backends (e.g., ITensor and ExaTENSOR numerical libraries). We demonstrate the utility of our TNQVM quantum circuit simulator through the verification of randomized quantum circuits and the variational quantum eigensolver algorithm, both expressed within the eXtreme-scale ACCelerator (XACC) programming model.|PLoS ONE|2018|10.1371/journal.pone.0206704|T. Humble, Dmitry I. Lyakh, Mengsu Chen, E. Dumitrescu, A. McCaskey|4.857142857142857|2
519|Parameterized and GPU-Parallelized Real-Time Model Predictive Control for High Degree of Freedom Robots|This work presents and evaluates a novel input parameterization method which improves the tractability of model predictive control (MPC) for high degree of freedom (DoF) robots. Experimental results demonstrate that by parameterizing the input trajectory more than three quarters of the optimization variables used in traditional MPC can be eliminated with practically no effect on system performance. This parameterization also leads to trajectories which are more conservative, producing less overshoot in underdamped systems with modeling error. In this paper we present two MPC solution methods that make use of this parameterization. The first uses a convex solver, and the second makes use of parallel computing on a graphics processing unit (GPU). We show that both approaches drastically reduce solve times for large DoF, long horizon MPC problems allowing solutions at real-time rates. Through simulation and hardware experiments, we show that the parameterized convex solver MPC has faster solve times than traditional MPC for high DoF cases while still achieving similar performance. For the GPU-based MPC solution method, we use an evolutionary algorithm and that we call Evolutionary MPC (EMPC). EMPC is shown to have even faster solve times for high DoF systems. Solve times for EMPC are shown to decrease even further through the use of a more powerful GPU. This suggests that parallelized MPC methods will become even more advantageous with the improvement and prevalence of GPU technology.|arXiv.org|2020|10.1109/lra.2020.2965393|Connor S. Williams, Phillip Hyatt, Marc D. Killpack|4.8|2
645|Unravelling Effects of the Pore‐Size Correlation Length on the Two‐Phase Flow and Solute Transport Properties: GPU‐based Pore‐Network Modeling|Continuum‐scale models for two‐phase flow and transport in porous media are based on the empirical constitutive relations that highly depend on the porous medium heterogeneity at multiple scales including the microscale pore‐size correlation length. The pore‐size correlation length determines the representative elementary volume and controls the immiscible two‐phase invasion pattern and fluids occupancy. The fluids occupancy controls not only the shape of relative permeability curves but also the transport zonation under two‐phase flow conditions, which results in the non‐Fickian transport. This study aims to quantify the signature of the pore‐size correlation length on two‐phase flow and solute transport properties such as the capillary pressure‐ and relative permeability‐saturation, dispersivity, stagnant saturation, and mass transfer rate. Given the capability of pore‐scale models in capturing the pore morphology and detailed physics of flow and transport, a novel graphics processing unit (GPU)‐based pore‐network model has been developed. This GPU‐based model allows us to simulate flow and transport in networks with multimillions pores, equivalent to the centimeter length scale. The impact of the pore‐size correlation length on all aforementioned properties was studied and quantified. Moreover, by classification of the pore space to flowing and stagnant regions, a simple semianalytical relation for the mass transfer between the flowing and stagnant regions was derived, which showed a very good agreement with pore‐network simulation results. Results indicate that the characterization of the topology of the stagnant regions as a function of pore‐size correlation length is essential for a better estimation of the two‐phase flow and solute transport properties.|Water Resources Research|2020|10.1029/2020WR027403|V. Niasar, Senyou An, S. Hasan, Hamidreza Erfani, M. Babaei|4.8|2
675|The FastEddy® Resident‐GPU Accelerated Large‐Eddy Simulation Framework: Model Formulation, Dynamical‐Core Validation and Performance Benchmarks|This paper introduces a new large‐eddy simulation model, FastEddy®, purpose built for leveraging the accelerated and more power‐efficient computing capacity of graphics processing units (GPUs) toward adopting microscale turbulence‐resolving atmospheric boundary layer simulations into future numerical weather prediction activities. Here a basis for future endeavors with the FastEddy® model is provided by describing the model dry dynamics formulation and investigating several validation scenarios that establish a baseline of model predictive skill for canonical neutral, convective, and stable boundary layer regimes, along with boundary layer flow over heterogeneous terrain. The current FastEddy® GPU performance and efficiency gains versus similarly formulated, state‐of‐the‐art CPU‐based models is determined through scaling tests as 1 GPU to 256 CPU cores. At this ratio of GPUs to CPU cores, FastEddy® achieves 6 times faster prediction rate than commensurate CPU models under equivalent power consumption. Alternatively, FastEddy® uses 8 times less power at this ratio under equivalent CPU/GPU prediction rate. The accelerated performance and efficiency gains of the FastEddy® model permit more broad application of large‐eddy simulation to emerging atmospheric boundary layer research topics through substantial reduction of computational resource requirements and increase in model prediction rate.|Journal of Advances in Modeling Earth Systems|2020|10.1029/2020MS002100|D. Muñoz‐Esparza, J. Sauer|4.8|2
1247|Fast and sensitive rigid-body fitting into cryo-EM density maps with PowerFit|Cryo-EM is a rapidly developing method to investigate the three dimensional structure of large macromolecular complexes. In spite of all the advances in the field, the resolution of most cryo-EM density maps is too low for de novo model building. Therefore, the data are often complemented by fitting high-resolution subunits in the density to allow for an atomic interpretation. Typically, the first step in the modeling process is placing the subunits in the density as a rigid body. An objective method for automatic placement is full-exhaustive six dimensional cross correlation search between the model and the cryo-EM data, where the three translational and three rotational degrees of freedom are systematically sampled. In this article we present PowerFit, a Python package and program for fast and sensitive rigid body fitting. We introduce a novel, more sensitive scoring function, the core-weighted local cross correlation, and show how it can be calculated using FFTs for fast translational cross correlation scans. We further improved the search algorithm by using optimized rotational sets to reduce rotational redundancy and by limiting the cryo-EM data size through resampling and trimming the density. We demonstrate the superior scoring sensitivity of our scoring function on simulated data of the 80S D. melanogaster ribosome and on experimental data for four different cases. Through these advances, a fine-grained rotational search can now be performed within minutes on a CPU and seconds on a GPU. PowerFit is free software and can be downloaded from https://github.com/haddocking/powerfit.||2015|10.3934/BIOPHY.2015.2.73|A. Bonvin, G. V. Zundert|4.8|2
743|Google Colab CAD4U: Hands-On Cloud Laboratories for Digital Design|Google Colab is a cloud Jupyter notebook widespread used to teach machine learning by writing text explanations and Python codes through the browser. This work introduces new Colab extensions to teach logic circuit design, Verilog language, processor, and GPU architectures. Colab allows us to share reproducible experiments on the Web. The students become motivated to do laboratory assignments without download/configure software packages and dependencies on their computers. Furthermore, almost all universities had to shut down due to the COVID-19 pandemic, forcing us to adapt to virtual learning scenarios. Colab provides portability and accessibility since it can even run on smartphones. The lab assignments include intermediate guided exercises, text explanations, figures, online quizzes, problem sets, and basic hands-on tasks. We develop a simple setup for Icarus Verilog, PyEDA, CUDA, Valgrind, and Gem5 frameworks. This work presents Verilog teaching and computer architecture simulation insights by using Valgrind and Gem5, and GPU computer architecture profiling at the thread and instruction assembly level.|International Symposium on Circuits and Systems|2021|10.1109/ISCAS51556.2021.9401151|O. V. Neto, Michael Canesche, L. Bragança, R. Ferreira, J. Nacif|4.75|2
795|GPU-supported simulation for ABEP and QoS analysis of a combined macro diversity system in a gamma-shadowed k-μ fading channel|In this paper we have analyzed macro-diversity (MD) system with one macro SC\n diversity (MD SC) receiver and two micro MRC (mD MRC) receivers over\n correlated Gamma-shadowed k-? fading channel. The average bit error\n probability (ABEP) is calculated using the moment generating function (MGF)\n approach for BDPSK and BPSK modulations. Graphical representation of the\n results illustrates the effects of different parameters of the system on its\n performance as well as the improvements due to the benefits of a combined\n micro and macro diversity. The obtained analytical expressions are used for\n the GPU-enabled mobile network modeling, planning and simulation environment\n to determine the value of Quality of Service (QoS) parameter. Finally,\n linear optimization is proposed as an approach to improve the QoS parameter\n of the fading-affected system observed in this paper.|Facta universitatis - series: Electronics and Energetics|2021|10.2298/fuee2101089p|S. Koničanin, S. Suljovic, Dejan N. Milic, Selena Vasić, N. Petrovic|4.75|2
1309|Apprehending heterogeneity at (very) large scale|The demand for computation power is steadily increasing, driven by the need tosimulate more and more complex phenomena with an increasing amount ofconsumed/produced data.To meet this demand, the High Performance Computing platforms grow in both sizeand heterogeneity.Indeed, heterogeneity allows splitting problems for a more efficient resolutionof sub-problems with ad hoc hardware or algorithms.This heterogeneity arises in the platforms' architecture and in the variety ofprocessed applications.Consequently, the performances become more sensitive to the execution context.We study in this thesis how to qualitatively bring—at a reasonablecost—context-awareness/obliviousness into allocation and scheduling policies.This study is conducted from two standpoints: within single applications, andat the whole platform scale from an inter-applications perspective.We first study the minimization of the makespan of sequential tasks onplatforms with a mixed architecture composed of multiple CPUs and GPUs.We integrate context-awareness into schedulers with an affinity mechanism thatimproves local behavior.This mechanism has been implemented in a parallel run-time, and experimentsshow that it is able to reduce the memory transfers while maintaining a lowmakespan.We then extend the model to implicitly consider parallelism on the CPUs withthe moldable-task model.We propose an efficient algorithm formulated as an integer linear program witha constant performance guarantee of 3/2+e.Second, we devise a new modeling framework where constraints are a first-classtool.Rather than extending existing models to consider all possible interactions, wereduce the set of feasible schedules by further constraining existing models.We propose a set of reasonable constraints to model application spreading andI/O traffic.We then instantiate this framework for unidimensional topologies, and propose acomprehensive case study of the makespan minimization under convex and localconstraints.||2017|10.1007/s11192-017-2375-1|Raphaël Bleuse|4.75|2
1206|Simulating Electron Dynamics of Complex Molecules with Time-Dependent Complete Active Space Configuration Interaction.|Time-dependent electronic structure methods are growing in popularity as tools for modeling ultrafast and/or nonlinear processes, for computing spectra, and as the electronic structure component of mean-field molecular dynamics simulations. Time-dependent configuration interaction (TD-CI) offers several advantages over the widely used real-time time-dependent density functional theory: namely, that it correctly models Rabi oscillations; it offers a spin-pure description of open-shell systems; and a hierarchy of TD-CI methods can be defined that systematically approach the exact solution of the time-dependent Schrodinger equation (TDSE). In this work, we present a novel TD-CI approach that extends TD-CI to large complete active-space configuration expansions. Such extension is enabled by use of a direct configuration interaction approach that eliminates the need to explicitly build, store, or diagonalize the Hamiltonian matrix. Graphics processing unit (GPU) acceleration enables fast solution of the TDSE even for large active spaces-up to 12 electrons in 12 orbitals (853776 determinants) in this work. A symplectic split operator propagator yields long-time norm conservation. We demonstrate the applicability of our approach by computing the response of a large molecule with a strongly correlated ground state, decacene (C42H24), to various pulses (δ-function, transform limited, chirped). Our simulations predict that chirped pulses can be used to induce dipole-forbidden transitions. Simulations of decacene using the 6-31G(d) basis set and a 12 electrons/12 orbitals active space took 20.1 h to propagate for 100 fs with a 1 attosecond time step on a single NVIDIA K40 GPU. Convergence with respect to time step is found to depend on the property being computed and the chosen active space.|Journal of Chemical Theory and Computation|2018|10.1021/acs.jctc.8b00381|Wei-Tao Peng, B. S. Fales, Benjamin G. Levine|4.714285714285714|2
294|Data Partitioning on Multicore and Multi-GPU Platforms Using Functional Performance Models|Heterogeneous multiprocessor systems, which are composed of a mix of processing elements, such as commodity multicore processors, graphics processing units (GPUs), and others, have been widely used in scientific computing community. Software applications incorporate the code designed and optimized for different types of processing elements in order to exploit the computing power of such heterogeneous computing systems. In this paper, we consider the problem of optimal distribution of the workload of data-parallel scientific applications between processing elements of such heterogeneous computing systems. We present a solution that uses functional performance models (FPMs) of processing elements and FPM-based data partitioning algorithms. Efficiency of this approach is demonstrated by experiments with parallel matrix multiplication and numerical simulation of lid-driven cavity flow on hybrid servers and clusters.|IEEE transactions on computers|2015|10.1109/TC.2014.2375202|Alexey L. Lastovetsky, Ziming Zhong, V. Rychkov|4.7|2
866|Parthenon—a performance portable block-structured adaptive mesh refinement framework|On the path to exascale the landscape of computer device architectures and corresponding programming models has become much more diverse. While various low-level performance portable programming models are available, support at the application level lacks behind. To address this issue, we present the performance portable block-structured adaptive mesh refinement (AMR) framework Parthenon, derived from the well-tested and widely used Athena++ astrophysical magnetohydrodynamics code, but generalized to serve as the foundation for a variety of downstream multi-physics codes. Parthenon adopts the Kokkos programming model, and provides various levels of abstractions from multidimensional variables, to packages defining and separating components, to launching of parallel compute kernels. Parthenon allocates all data in device memory to reduce data movement, supports the logical packing of variables and mesh blocks to reduce kernel launch overhead, and employs one-sided, asynchronous MPI calls to reduce communication overhead in multi-node simulations. Using a hydrodynamics miniapp, we demonstrate weak and strong scaling on various architectures including AMD and NVIDIA GPUs, Intel and AMD x86 CPUs, IBM Power9 CPUs, as well as Fujitsu A64FX CPUs. At the largest scale on Frontier (the first TOP500 exascale machine), the miniapp reaches a total of 1.7 × 1013 zone-cycles/s on 9216 nodes (73,728 logical GPUs) at ≈ 92 % weak scaling parallel efficiency (starting from a single node). In combination with being an open, collaborative project, this makes Parthenon an ideal framework to target exascale simulations in which the downstream developers can focus on their specific application rather than on the complexity of handling massively-parallel, device-accelerated AMR.|The international journal of high performance computing applications|2022|10.1177/10943420221143775|J. Lippuner, F. Glines, Christoph Junghans, Daniel Holladay, P. Grete, G. Shipman, J. Stone, B. Ryan, S. Swaminarayan, J. Miller, C. Solomon, Joshua Brown, A. Gaspar, J. Dolence|4.666666666666667|2
925|Decomposition of matrix product states into shallow quantum circuits|Tensor networks (TNs) are a family of computational methods built on graph-structured factorizations of large tensors, which have long represented state-of-the-art methods for the approximate simulation of complex quantum systems on classical computers. The rapid pace of recent advancements in numerical computation, notably the rise of GPU and TPU hardware accelerators, have allowed TN algorithms to scale to even larger quantum simulation problems, and to be employed more broadly for solving machine learning tasks. The ‘quantum-inspired’ nature of TNs permits them to be mapped to parametrized quantum circuits (PQCs), a fact which has inspired recent proposals for enhancing the performance of TN algorithms using near-term quantum devices, as well as enabling joint quantum–classical training frameworks that benefit from the distinct strengths of TN and PQC models. However, the success of any such methods depends on efficient and accurate methods for approximating TN states using realistic quantum circuits, which remains an unresolved question. This work compares a range of novel and previously-developed algorithmic protocols for decomposing matrix product states (MPS) of arbitrary bond dimension into low-depth quantum circuits consisting of stacked linear layers of two-qubit unitaries. These protocols are formed from different combinations of a preexisting analytical decomposition method together with constrained optimization of circuit unitaries, with initialization by the former method helping to avoid poor-quality local minima in the latter optimization process. While all of these protocols have efficient classical runtimes, our experimental results reveal one particular protocol employing sequential growth and optimization of the quantum circuit to outperform all others, with even greater benefits in the setting of limited computational resources. Given these promising results, we expect our proposed decomposition protocol to form a useful ingredient within any joint application of TNs and PQCs, further unlocking the rich and complementary benefits of classical and quantum computation.|Quantum Science and Technology|2022|10.1088/2058-9565/ad04e6|Manuel S. Rudolph, A. Perdomo-Ortiz, J. Miller, Jing Chen, Atithi Acharya|4.666666666666667|2
451|Cosimulation of Shifted-Frequency/Dynamic Phasor and Electromagnetic Transient Models of Hybrid LCC-MMC DC Grids on Integrated CPU–GPUs|To effectively capture interactions of large-scale ac–dc systems integrating line commutated converter (LCC) and modular multilevel converter (MMC) based multiterminal dc grids, a numerically accurate and efficient simulation method is desirable. To achieve this objective, a cosimulation method is proposed in this article, where the target system is decoupled into the shifted-frequency phasor (SFP) subsystem, the dynamic phasor (DP) subsystem, and electromagnetic transient (EMT) subsystem, respectively. The MMCs are included in the SFP subsystem and implemented on massively paralleled graphics processing units (GPUs). Thus, the simulation efficiency is greatly improved by adopting a much larger time step, the model order reduction technique, and GPU acceleration. The LCCs are represented by DPs and are included in the DP subsystem. The majority of ac grids are covered in the EMT subsystem. Further, the interactions between SFP and EMT subsystems are reflected by the proposed multidomain transmission line model, which can produce instantaneous and phasor values simultaneously. The interface model between DP and EMT subsystems is modeled as a special controlled voltage and current circuit. Finally, the overall cosimulation method is realized by the respective SFP/DP and EMT models, among which their interactions are reflected by the proposed interface models and the time sequences of simulations. The performance of the proposed method has been fully validated on a practical large-scale ac–dc system.|IEEE transactions on industrial electronics (1982. Print)|2020|10.1109/TIE.2019.2937044|Zheng Yan, Dewu Shu, Xiaoqian Li, V. Dinavahi, Keyou Wang, Yingdong Wei|4.6|2
625|An Overview of Hardware Implementation of Membrane Computing Models|The model of membrane computing, also known under the name of P systems, is a bio-inspired large-scale parallel computing paradigm having a good potential for the design of massively parallel algorithms. For its implementation it is very natural to choose hardware platforms that have important inherent parallelism, such as field-programmable gate arrays (FPGAs) or compute unified device architecture (CUDA)-enabled graphic processing units (GPUs). This article performs an overview of all existing approaches of hardware implementation in the area of P systems. The quantitative and qualitative attributes of FPGA-based implementations and CUDA-enabled GPU-based simulations are compared to evaluate the two methodologies.|ACM Computing Surveys|2020|10.1145/3402456|Miguel A. Martínez-del-Amor, Sergey Verlan, M. Pérez-Jiménez, Gexiang Zhang, Zeyi Shang, Luis Valencia-Cabrera, Chengxun Yuan|4.6|2
1203|A Survey on Agent-based Simulation Using Hardware Accelerators|Due to decelerating gains in single-core CPU performance, computationally expensive simulations are increasingly executed on highly parallel hardware platforms. Agent-based simulations, where simulated entities act with a certain degree of autonomy, frequently provide ample opportunities for parallelisation. Thus, a vast variety of approaches proposed in the literature demonstrated considerable performance gains using hardware platforms such as many-core CPUs and GPUs, merged CPU-GPU chips as well as Field Programmable Gate Arrays. Typically, a combination of techniques is required to achieve high performance for a given simulation model, putting substantial burden on modellers. To the best of our knowledge, no systematic overview of techniques for agent-based simulations on hardware accelerators has been given in the literature. To close this gap, we provide an overview and categorisation of the literature according to the applied techniques. Since, at the current state of research, challenges such as the partitioning of a model for execution on heterogeneous hardware are still addressed in a largely manual process, we sketch directions for future research towards automating the hardware mapping and execution. This survey targets modellers seeking an overview of suitable hardware platforms and execution techniques for a specific simulation model, as well as methodology researchers interested in potential research gaps requiring further exploration.|ACM Computing Surveys|2018|10.1145/3291048|A. Knoll, D. Eckhoff, Philipp Andelfinger, Jiajian Xiao, Wentong Cai|4.571428571428571|2
818|SV-Sim: Scalable PGAS-Based State Vector Simulation of Quantum Circuits|High-performance quantum circuit simulation in a classic HPC is still imperative in the NISQ era. Observing that the major obstacle of scalable state-vector quantum simulation arises from the massively fine-grained irregular data-exchange with remote nodes, in this paper we present SV-Sim to apply the PGAS-based communication models (i.e., direct peer access for intra-node CPUs/GPUs and SHMEM for inter-node CPU/GPU clusters) for efficient general-purpose quantum circuit simulation. Through an orchestrated design based on device functional pointer, SV-Sim is able to abstract various quantum gates across multiple heterogeneous backends, including IBM/Intel/AMD CPUs, NVIDIA/AMD GPUs, and Intel Xeon Phi, in a unified framework, but still asserting outstanding performance and tractable interface to higher-level quantum programming environments, such as IBM Qiskit, Microsoft Q# and Google Cirq. Circumventing the obstacle from the lack of polymorphism in GPUs and leveraging the device-initiated one-sided communication, SV-Sim can process circuit that are dynamically generated in Python using a single GPU/CPU kernel without the need of expensive JIT or runtime parsing, significantly simplifying the programming complexity and improving performance for QC simulation. This is especially appealing for the variational quantum algorithms given the circuits are synthesized online per iteration. Evaluations on the latest NVIDIA DGX-A100, V100-DGX-2, ALCF Theta, OLCF Spock, and OLCF Summit HPCs show that SV-Sim can deliver scalable performance on various state-of-the-art HPC platforms, offering a useful tool for quantum algorithm validation and verification. SV-Sim has been released at http://github.com/pnnl/sv-sim. A version specially tweaked for Q#/QDK is also provided.|International Conference for High Performance Computing, Networking, Storage and Analysis|2021|10.1145/3458817.3476169|B. Heim, B. Fang, Martin Roetteler, G. Prawiroatmodjo, C. Granade, S. Krishnamoorthy, Ang Li|4.5|2
831|Chrono::GPU: An Open-Source Simulation Package for Granular Dynamics Using the Discrete Element Method|We report on an open-source, publicly available C++ software module called Chrono::GPU, which uses the Discrete Element Method (DEM) to simulate large granular systems on Graphics Processing Unit (GPU) cards. The solver supports the integration of granular material with geometries defined by triangle meshes, as well as co-simulation with the multi-physics simulation engine Chrono. Chrono::GPU adopts a smooth contact formulation and implements various common contact force models, such as the Hertzian model for normal force and the Mindlin friction force model, which takes into account the history of tangential displacement, rolling frictional torques, and cohesion. We report on the code structure and highlight its use of mixed data types for reducing the memory footprint and increasing simulation speed. We discuss several validation tests (wave propagation, rotating drum, direct shear test, crater test) that compare the simulation results against experimental data or results reported in the literature. In another benchmark test, we demonstrate linear scaling with a problem size up to the GPU memory capacity; specifically, for systems with 130 million DEM elements. The simulation infrastructure is demonstrated in conjunction with simulations of the NASA Curiosity rover, which is currently active on Mars.|Processes|2021|10.3390/pr9101813|Ruochun Zhang, Colin Vanden Heuvel, Luning Fang, D. Negrut, R. Serban|4.5|2
964|TeraChem protocol buffers (TCPB): Accelerating QM and QM/MM simulations with a client-server model.|The routine use of electronic structures in many chemical simulation applications calls for efficient and easy ways to access electronic structure programs. We describe how the graphics processing unit (GPU) accelerated electronic structure program TeraChem can be set up as an electronic structure server, to be easily accessed by third-party client programs. We exploit Google's protocol buffer framework for data serialization and communication. The client interface, called TeraChem protocol buffers (TCPB), has been designed for ease of use and compatibility with multiple programming languages, such as C++, Fortran, and Python. To demonstrate the ease of coupling third-party programs with electronic structures using TCPB, we have incorporated the TCPB client into Amber for quantum mechanics/molecular mechanics (QM/MM) simulations. The TCPB interface saves time with GPU initialization and I/O operations, achieving a speedup of more than 2× compared to a prior file-based implementation for a QM region with ∼250 basis functions. We demonstrate the practical application of TCPB by computing the free energy profile of p-hydroxybenzylidene-2,3-dimethylimidazolinone (p-HBDI-)-a model chromophore in green fluorescent proteins-on the first excited singlet state using Hamiltonian replica exchange for enhanced sampling. All calculations in this work have been performed with the non-commercial freely-available version of TeraChem, which is sufficient for many QM region sizes in common use.|Journal of Chemical Physics|2023|10.1063/5.0130886|V. Cruzeiro, Elisa Pieri, Yuanheng Wang, Todd J. Martinez, E. Hohenstein|4.5|2
1173|GPU accelerated computation of Polarized Subsurface BRDF for Flat Particulate Layers|BRDF of most real world materials has two components, the surface BRDF due to the light reflecting at the surface of the material and the subsurface BRDF due to the light entering and going through many scattering events inside the material. Each of these events modifies light's path, power, polarization state. Computing polarized subsurface BRDF of a material requires simulating the light transport inside the material. The transport of polarized light is modeled by the Vector Radiative Transfer Equation (VRTE), an integro-differential equation. Computing solution to that equation is expensive. The Discrete Ordinate Method (DOM) is a common approach to solving the VRTE. Such solvers are very time consuming for complex uses such as BRDF computation, where one must solve VRTE for surface radiance distribution due to light incident from every direction of the hemisphere above the surface. In this paper, we present a GPU based DOM solution of the VRTE to expedite the subsurface BRDF computation. As in other DOM based solutions, our solution is based on Fourier expansions of the phase function and the radiance function. This allows us to independently solve the VRTE for each order of expansion. We take advantage of those repetitions and of the repetitions in each of the sub-steps of the solution process. Our solver is implemented to run mainly on graphics hardware using the OpenCL library and runs up to seven times faster than its CPU equivalent, allowing the computation of subsurface BRDF in a matter of minutes. We compute and present the subsurface BRDF lobes due to powders and paints of a few materials. We also show the rendering of objects with the computed BRDF. The solver is available for public use through the authors' web site.|arXiv.org|2017|10.1039/c7tc02637d|S. Pattanaik, Charly Collin|4.5|2
1437|Exact Nonlinear Micromodeling for Fine-Grained Parallel EMT Simulation of MTDC Grid Interaction With Wind Farm|Detailed high-order models of the insulated-gate bipolar transistor (IGBT) and the diode are rarely included in power converters for large-scale system-level electromagnetic transient (EMT) simulation on the CPU, due to the nonlinear characteristics albeit they are more accurate. The massively parallel architecture of the graphics processing unit (GPU) enables a lower computational burden by avoiding the computation of complex devices repetitively in a sequential manner and thus is utilized in this paper to simulate the wind farm-integrated multiterminal dc (MTdc) grid based on the modular multilevel converter (MMC). Fine-grained circuit partitioning is proposed so that the nonlinear switching elements are physically separated with the smallest circuit unit. By implementing these subsystems with the same attributes as a GPU program and computing it in a massively parallel manner, it is demonstrated that the GPU is able to achieve a significant speedup over multicore CPUs and its computation time incremental is much smaller when the MMC level scales up. The improved insight and accuracy of the proposed modeling methodology and the designed GPU program are validated at the system- and device-level by off-line commercial simulation tools.|IEEE transactions on industrial electronics (1982. Print)|2019|10.1109/TIE.2018.2860566|V. Dinavahi, Ning Lin|4.5|2
576|A Multi-GPU Accelerated Parallel Domain Decomposition One-Step Leapfrog ADI-FDTD|In this letter, a multi-GPU accelerated one-step leapfrog alternative-direction-implicit finite-difference time-domain (ADI-FDTD) based on parallel SPIKE tridiagonal systems solver is presented. Through our algorithm simplification and execution optimization, online computation and data exchange between GPUs of the SPIKE solver is dramatically reduced. In the simulation experiment, the bistatic RCS of the benchmark model calculated by the proposed method is consistent with the solutions given by one-step leapfrog ADI-FDTD based on Thomas tridiagonal systems solver and Yee's FDTD. Compared with the single-GPU-based implementation, the multi-GPU accelerated one-step leapfrog ADI-FDTD solver can achieve 3.94× speedups at most when executed at a workstation with 4 GPUs.|IEEE Antennas and Wireless Propagation Letters|2020|10.1109/LAWP.2020.2981123|Shulei Ren, Shuo Liu, Lamei Zhang, B. Zou|4.4|2
610|A GPU-accelerated fully 3D OSEM image reconstruction for a high-resolution small animal PET scanner using dual-ended readout detectors|In this work, a GPU-accelerated fully 3D ordered-subset expectation maximization (OSEM) image reconstruction with point spread function (PSF) modeling was developed for a small animal PET scanner with a long axial field of view (FOV). Dual-ended readout detectors that provided high depth of interaction (DOI) resolution were used for the small animal PET scanner to simultaneously achieve uniform high spatial resolution and high sensitivity. First, we developed a novel sinogram generation method, in which the dimension of the sinogram was determined first and then an event was assigned to a few neighboring sinogram elements by using weights that are inversely proportional to the distance from the measured line of response (LOR) to the LOR of the sinogram elements. System geometric symmetry, precomputation of LOR-driven ray-tracing and texture memory were applied to accelerate the GPU-based reconstruction. We developed a spatially variant PSF model where the PSF parameters were obtained by using point source images measured at 18 positions in the FOV and a spatial invariant PSF model where the PSF parameters were obtained by using only one image measured at the center FOV. The performance of the image reconstruction method was evaluated by using simulated phantom data as well as phantom and in-vivo mouse data acquired on the scanner. The results showed that the proposed reconstruction method provided better spatial resolution, a higher contrast recovery coefficient and lower noise than the OSEM reconstruction and was more than 1000 times faster than the CPU-based reconstruction. The spatially variant PSF model did not result in any spatial resolution improvement compared to the spatial invariant PSF model, and thus, the latter that is much easier to implement in image reconstruction and can be used in a small animal PET scanner using detectors with very high DOI resolution. A whole body 18F-FDG mouse image with high resolution and a high contrast to noise ratio was obtained by using the proposed reconstruction method.|Physics in Medicine and Biology|2020|10.1088/1361-6560/aba6f9|Dongfang Gao, Qun Chen, Lingzhi Hu, Hairong Zheng, Xu Chu, Zhanli Hu, Yongfeng Yang, Tianyi Zeng, D. Liang, Xin Liu, Z. Kuang, Juan Gao, Ziru Sang, Xiaohui Wang|4.4|2
1344|KRIPKE - A MASSIVELY PARALLEL TRANSPORT MINI-APP|As computer architectures become more complex, developing high performance computing codes becomes more challenging. Processors are getting more cores, which tend to be simpler and support multiple hardware threads, and ever wider SIMD (vector) units. Memory is becoming more hierarchical with more diverse bandwidths and latencies. GPU’s push these trends to an extreme. Existing simulation codes that had good performance on the previous generation of computers will most likely not perform as well on new architectures. Rewriting existing codes from scratch is a monumental task. Refactoring existing codes is often more tractable. Proxy Applications are proving to be valuable research tools that help explore the best approaches to use in existing codes. They provide a much smaller code that can be refactored or rewritten at little cost, but provide insight into how the parent code would behave with a similar (but much more expensive) refactoring effort. In this paper we introduce KRIPKE, a mini-app developed at Lawrence Livermore National Laboratory, designed to be a proxy for a fully functional discrete-ordinates (SN ) transport code. KRIPKE was developed to study the performance characteristics of data layouts, programming models, and sweep algorithms. KRIPKE is designed to support different in-memory data layouts, and allows work to be grouped into sets in order to expose more on-node parallelism. Different data layouts change the way in which software is implemented, how that software is compiled for a given architecture, and how that generated code eventually performs on a given architecture.||2015|10.1016/j.jocs.2015.04.003|Peter N. Brown, A. Kunen, T. S. Bailey|4.4|2
283|Simit: A Language for Physical Simulation|With existing programming tools, writing high-performance simulation code is labor intensive and requires sacrificing readability and portability. The alternative is to prototype simulations in a high-level language like Matlab, thereby sacrificing performance. The Matlab programming model naturally describes the behavior of an entire physical system using the language of linear algebra. However, simulations also manipulate individual geometric elements, which are best represented using linked data structures like meshes. Translating between the linked data structures and linear algebra comes at significant cost, both to the programmer and to the machine. High-performance implementations avoid the cost by rephrasing the computation in terms of linked or index data structures, leaving the code complicated and monolithic, often increasing its size by an order of magnitude. In this article, we present Simit, a new language for physical simulations that lets the programmer view the system both as a linked data structure in the form of a hypergraph and as a set of global vectors, matrices, and tensors depending on what is convenient at any given time. Simit provides a novel assembly construct that makes it conceptually easy and computationally efficient to move between the two abstractions. Using the information provided by the assembly construct, the compiler generates efficient in-place computation on the graph. We demonstrate that Simit is easy to use: a Simit program is typically shorter than a Matlab program; that it is high performance: a Simit program running sequentially on a CPU performs comparably to hand-optimized simulations; and that it is portable: Simit programs can be compiled for GPUs with no change to the program, delivering 4 to 20× speedups over our optimized CPU code.|ACM Transactions on Graphics|2016|10.1145/2866569|Saman P. Amarasinghe, D. Kaufman, Desai Chen, Jonathan Ragan-Kelley, Fredrik Kjolstad, E. Vouga, G. Kanwar, S. Sueda, W. Matusik, S. Kamil, D. Levin|4.333333333333333|2
487|Fast, Scalable, and Interactive Software for Landau-de Gennes Numerical Modeling of Nematic Topological Defects|Numerical modeling of nematic liquid crystals using the tensorial Landau-de Gennes (LdG) theory provides detailed insights into the structure and energetics of the enormous variety of possible topological defect configurations that may arise when the liquid crystal is in contact with colloidal inclusions or structured boundaries. However, these methods can be computationally expensive, making it challenging to predict (meta)stable configurations involving several colloidal particles, and they are often restricted to system sizes well below the experimental scale. Here we present an open-source software package that exploits the embarrassingly parallel structure of the lattice discretization of the LdG approach. Our implementation, combining CUDA/C++ and OpenMPI, allows users to accelerate simulations using both CPU and GPU resources in either single- or multiple-core configurations. We make use of an efficient minimization algorithm, the Fast Inertial Relaxation Engine (FIRE) method, that is well-suited to large-scale parallelization, requiring little additional memory or computational cost while offering performance competitive with other commonly used methods. In multi-core operation we are able to scale simulations up to supra-micron length scales of experimental relevance, and in single-core operation the simulation package includes a user-friendly GUI environment for rapid prototyping of interfacial features and the multifarious defect states they can promote. To demonstrate this software package, we examine in detail the competition between curvilinear disclinations and point-like hedgehog defects as size scale, material properties, and geometric features are varied. We also study the effects of an interface patterned with an array of topological point-defects.|Frontiers of Physics|2019|10.3389/fphy.2019.00204|D. Sussman, Daniel A. Beller|4.333333333333333|2
855|GENGA. II. GPU Planetary N-body Simulations with Non-Newtonian Forces and High Number of Particles|We present recent updates and improvements of the graphical processing unit (GPU) N-body code GENGA. Modern state-of-the-art simulations of planet formation require the use of a very high number of particles to accurately resolve planetary growth and to quantify the effect of dynamical friction. At present the practical upper limit is in the range of 30,000–60,000 fully interactive particles; possibly a little more on the latest GPU devices. While the original hybrid symplectic integration method has difficulties to scale up to these numbers, we have improved the integration method by (i) introducing higher level changeover functions and (ii) code improvements to better use the most recent GPU hardware efficiently for such large simulations. We added treatments of non-Newtonian forces such as general relativity, tidal interaction, rotational deformation, the Yarkovsky effect, and Poynting–Robertson drag, as well as a new model to treat virtual collisions of small bodies in the solar system. We added new tools to GENGA, such as semi-active test particles that feel more massive bodies but not each other, a more accurate collision handling and a real-time openGL visualization. We present example simulations, including a 1.5 billion year terrestrial planet formation simulation that initially started with 65,536 particles, a 3.5 billion year simulation without gas giants starting with 32,768 particles, the evolution of asteroid fragments in the solar system, and the planetesimal accretion of a growing Jupiter simulation. GENGA runs on modern NVIDIA and AMD GPUs.|Astrophysical Journal|2022|10.3847/1538-4357/ac6dd2|M. Meier, J. Stadel, R. Brasser, S. Grimm, C. Mordasini|4.333333333333333|2
904|FFHNet: Generating Multi-Fingered Robotic Grasps for Unknown Objects in Real-time|Grasping unknown objects with multi-fingered hands at high success rates and in real-time is an unsolved problem. Existing methods are limited in the speed of grasp synthesis or the ability to synthesize a variety of grasps from the same observation. We introduce Five-finger Hand Net (FFHNet), an ML model which can generate a wide variety of high-quality multi-fingered grasps for unseen objects from a single view. Generating and evaluating grasps with FFHNet takes only 30ms on a commodity GPU. To the best of our knowledge, FFHNet is the first ML-based real-time system for multi-fingered grasping with the ability to perform grasp inference at 30 frames per second (FPS). For training, we synthetically generate 180k grasp samples for 129 objects. We are able to achieve 91% grasping success for unknown objects in simulation and we demonstrate the model's capabilities of synthesizing high-quality grasps also for real unseen objects.|IEEE International Conference on Robotics and Automation|2022|10.1109/icra46639.2022.9811666|Jun Deng, Qian Feng, Yunlei Shi, Vincent Mayer, Zhaopeng Chen, A. Knoll|4.333333333333333|2
1214|Efficient computation of cartilage contact pressures within dynamic simulations of movement|Abstract The objective of this study was to assess the use of an advanced collision detection algorithm to simulate cartilage contact pressure patterns within dynamic musculoskeletal simulations of movement. We created a knee model that included articular cartilage contact for the tibiofemoral and patellofemoral joints. Knee mechanics were then predicted within the context of a dynamic gait simulation. At each time step of a simulation, ray casting was used in conjunction with hierarchical oriented bounding boxes (OBB) to rapidly identify regions of overlap between articulating cartilage surfaces. Local cartilage contact pressure was then computed using an elastic foundation model. Collision detection implemented in parallel on a GPU provided up to a 10× speed increase when using high-resolution mesh densities that had >10 triangles/mm2. However, pressure magnitudes converged at considerably lower mesh densities (2.6 triangles/mm2) where CPU and GPU implementations of collision detection exhibited equivalent performance. Simulated tibiofemoral contact locations were comparable to prior experimental measurements, while pressure magnitudes were similar to those predicted by finite element models. We conclude the use of ray casting with hierarchical OBB for collision detection is a viable method for simulating joint contact mechanics in human movement.|Comput. methods Biomech. Biomed. Eng. Imaging Vis.|2018|10.1080/21681163.2016.1172346|K. Choi, D. Thelen, Colin R. Smith, D. Negrut|4.285714285714286|2
693|GPU Optimization for High-Quality Kinetic Fluid Simulation|Fluid simulations are often performed using the incompressible Navier-Stokes equations (INSE), leading to sparse linear systems which are difficult to solve efficiently in parallel. Recently, kinetic methods based on the adaptive-central-moment multiple-relaxation-time (ACM-MRT) model [1], [2] have demonstrated impressive capabilities to simulate both laminar and turbulent flows, with quality matching or surpassing that of state-of-the-art INSE solvers. Furthermore, due to its local formulation, this method presents the opportunity for highly scalable implementations on parallel systems such as GPUs. However, an efficient ACM-MRT-based kinetic solver needs to overcome a number of computational challenges, especially when dealing with complex solids inside the fluid domain. In this article, we present multiple novel GPU optimization techniques to efficiently implement high-quality ACM-MRT-based kinetic fluid simulations in domains containing complex solids. Our techniques include a new communication-efficient data layout, a load-balanced immersed-boundary method, a multi-kernel launch method using a simplified formulation of ACM-MRT calculations to enable greater parallelism, and the integration of these techniques into a parametric cost model to enable automated prameter search to achieve optimal execution performance. We also extended our method to multi-GPU systems to enable large-scale simulations. To demonstrate the state-of-the-art performance and high visual quality of our solver, we present extensive experimental results and comparisons to other solvers.|IEEE Transactions on Visualization and Computer Graphics|2021|10.1109/TVCG.2021.3059753|Wei Li, Rui Fan, Xiaopei Liu, Yixin Chen|4.25|2
817|Generalizable Coordination of Large Multiscale Workflows: Challenges and Learnings at Scale|The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale simulation campaign that utilized over 600,000 node hours on Summit and achieved more than 98% GPU occupancy for more than 83% of the time. We present innovations to enable several orders of magnitude scaling, including simultaneously coordinating 24,000 jobs, and managing several TBs of new data per day and over a billion files in total. Finally, we describe the generalizability of our framework and, with an upcoming open-source release, discuss how the presented framework may be used for new applications.|International Conference for High Performance Computing, Networking, Storage and Analysis|2021|10.1145/3458817.3476210|Joseph Y. Moon, S. Gnanakaran, F. Lightstone, Helgi I. Ingólfsson, D. Ahn, Timothy S. Carpenter, Chris Neale, F. Natale, Joseph R. Chavez, S. K. Schumacher, J. Glosli, H. Bhatia, T. Oppelstrup, Christopher Stanley, Fikret Aydin, Stephen Herbein, Xiaohua Zhang, P. Bremer|4.25|2
669|Highly efficient lattice Boltzmann multiphase simulations of immiscible fluids at high-density ratios on CPUs and GPUs through code generation|A high-performance implementation of a multiphase lattice Boltzmann method based on the conservative Allen-Cahn model supporting high-density ratios and high Reynolds numbers is presented. Meta-programming techniques are used to generate optimized code for CPUs and GPUs automatically. The coupled model is specified in a high-level symbolic description and optimized through automatic transformations. The memory footprint of the resulting algorithm is reduced through the fusion of compute kernels. A roofline analysis demonstrates the excellent efficiency of the generated code on a single GPU. The resulting single GPU code has been integrated into the multiphysics framework waLBerla to run massively parallel simulations on large domains. Communication hiding and GPUDirect-enabled MPI yield near-perfect scaling behavior. Scaling experiments are conducted on the Piz Daint supercomputer with up to 2048 GPUs, simulating several hundred fully resolved bubbles. Further, validation of the implementation is shown in a physically relevant scenario—a three-dimensional rising air bubble in water.|The international journal of high performance computing applications|2020|10.1177/10943420211016525|H. Köstler, Martin Bauer, U. Rüde, M. Holzer|4.2|2
1315|A GPU-accelerated continuous and discontinuous Galerkin non-hydrostatic atmospheric model|We present a Graphics Processing Unit (GPU)-accelerated nodal discontinuous Galerkin method for the solution of the three-dimensional Euler equations that govern the motion and thermodynamic state of the atmosphere. Acceleration of the dynamical core of atmospheric models plays an important practical role in not only getting daily forecasts faster, but also in obtaining more accurate (high resolution) results within a given simulation time limit. We use algorithms suitable for the single instruction multiple thread architecture of GPUs to accelerate our model by two orders of magnitude relative to one core of a CPU. Tests on one node of the Titan supercomputer show a speedup of up to 15 times using the K20X GPU as compared to that on the 16-core AMD Opteron CPU. The scalability of the multi-GPU implementation is tested using 16,384 GPUs, which resulted in a weak scaling efficiency of about 90%. Finally, the accuracy and performance of our GPU implementation is verified using several benchmark problems representative of different scales of atmospheric dynamics.|The international journal of high performance computing applications|2019|10.1177/1094342017694427|T. Warburton, F. Giraldo, L. Wilcox, D. Abdi|4.166666666666667|2
342|Parallel Simulation of Complex Evacuation Scenarios with Adaptive Agent Models|Simulation study on evacuation scenarios has gained tremendous attention in recent years. Two major research challenges remain along this direction: (1) how to portray the effect of individuals' adaptive behaviors under various situations in the evacuation procedures and (2) how to simulate complex evacuation scenarios involving huge crowds at the individual level due to the ultrahigh complexity of these scenarios. In this study, a simulation framework for general evacuation scenarios has been developed. Each individual in the scenario is modeled as an adaptable and autonomous agent driven by a weight-based decision-making mechanism. The simulation is intended to characterize the individuals' adaptable behaviors, the interactions among individuals, among small groups of individuals, and between the individuals and the environment. To handle the second challenge, this study adopts GPGPU to sustain massively parallel modeling and simulation of an evacuation scenario. An efficient scheme has been proposed to minimize the overhead to access the global system state of the simulation process maintained by the GPU platform. The simulation results indicate that the “adaptability” in individual behaviors has a significant influence on the evacuation procedure. The experimental results also exhibit the proposed approach's capability to sustain complex scenarios involving a huge crowd consisting of tens of thousands of individuals.|IEEE Transactions on Parallel and Distributed Systems|2015|10.1109/TPDS.2014.2311805|Dan Chen, Ze Deng, Minggang Dou, Lizhe Wang, Jingying Chen, S. Hariri, Albert Y. Zomaya|4.1|2
558|Accelerating Auxiliary-Field Quantum Monte Carlo Simulations of Solids with Graphical Processing Units.|We outline how auxiliary-field quantum Monte Carlo (AFQMC) can leverage graphical processing units (GPUs) to accelerate the simulation of solid state sytems. By exploiting conservation of crystal momentum in the one- and two-electron integrals we show how to efficiently formulate the algorithm to best utilize current GPU architectures. We provide a detailed description of different optimization strategies and profile our implementation relative to standard approaches, demonstrating a factor of 40 speed up over a CPU implementation. With this increase in computational power we demonstrate the ability of AFQMC to systematically converge solid state calculations with respect to basis set and system size by computing the cohesive energy of Carbon in the diamond structure to within 0.02 eV of the experimental result.|Journal of Chemical Theory and Computation|2020|10.1021/acs.jctc.0c00262|M. Morales, F. Malone, Shuai Zhang|4.0|2
587|FUNWAVE‐GPU: Multiple‐GPU Acceleration of a Boussinesq‐Type Wave Model|This paper documents development of a multiple‐Graphics Processing Unit (GPU) version of FUNWAVE‐Total Variation Diminishing (TVD), an open‐source model for solving the fully nonlinear Boussinesq wave equations using a high‐order TVD solver. The numerical schemes of FUNWAVE‐TVD, including Cartesian and spherical coordinates, are rewritten using CUDA Fortran, with inter‐GPU communication facilitated by the Message Passing Interface. Since FUNWAVE‐TVD involves the discretization of high‐order dispersive derivatives, the on‐chip shared memory is utilized to reduce global memory access. To further optimize performance, the batched tridiagonal solver is scheduled simultaneously in multiple‐GPU streams, which can reduce the GPU execution time by 20–30%. The GPU version is validated through a benchmark test for wave runup on a complex shoreline geometry, as well as a basin‐scale tsunami simulation of the 2011 Tohoku‐oki event. Efficiency evaluation shows that, in comparison with the CPU version running at a 36‐core HPC node, speedup ratios of 4–7 and above 10 can be observed for single‐ and double‐GPU runs, respectively. The performance metrics of multiple‐GPU implementation needs to be further evaluated when appropriate.|Journal of Advances in Modeling Earth Systems|2020|10.1029/2019MS001957|Ye Yuan, Fujiang Yu, J. Kirby, F. Shi|4.0|2
612|ddcMD: A fully GPU-accelerated molecular dynamics program for the Martini force field.|We have implemented the Martini force field within Lawrence Livermore National Laboratory's molecular dynamics program, ddcMD. The program is extended to a heterogeneous programming model so that it can exploit graphics processing unit (GPU) accelerators. In addition to the Martini force field being ported to the GPU, the entire integration step, including thermostat, barostat, and constraint solver, is ported as well, which speeds up the simulations to 278-fold using one GPU vs one central processing unit (CPU) core. A benchmark study is performed with several test cases, comparing ddcMD and GROMACS Martini simulations. The average performance of ddcMD for a protein-lipid simulation system of 136k particles achieves 1.04 µs/day on one NVIDIA V100 GPU and aggregates 6.19 µs/day on one Summit node with six GPUs. The GPU implementation in ddcMD offloads all computations to the GPU and only requires one CPU core per simulation to manage the inputs and outputs, freeing up remaining CPU resources on the compute node for alternative tasks often required in complex simulation campaigns. The ddcMD code has been made open source and is available on GitHub at https://github.com/LLNL/ddcMD.|Journal of Chemical Physics|2020|10.1063/5.0014500|Sara I L Kokkila-Schumacher, F. Lightstone, Helgi I. Ingólfsson, Timothy S. Carpenter, S. Sundram, F. Streitz, J. Glosli, T. Oppelstrup, Xiaohua Zhang|4.0|2
630|A thread‐block‐wise computational framework for large‐scale hierarchical continuum‐discrete modeling of granular media|This article presents a novel, scalable parallel computing framework for large‐scale and multiscale simulations of granular media. Key to the new framework is an innovative thread‐block‐wise representative volume element (RVE) parallelism, inspired by the resemblance between a typical multiscale computational hierarchy and the hierarchical thread structure of graphics processing units (GPUs). To solve a hierarchical multiscale problem, all computation in an RVE is assigned a single block of threads so that the RVE runs entirely on a GPU to avoid frequent data exchange with the host CPU. The thread blocks can meanwhile run in an asynchronization mode, which implicitly guarantees the independence of inter‐RVE computation as featured by the hierarchical multiscale structure. The parallel computing algorithms are formulated and implemented in an in‐house code, GoDEM, involving the GPU‐specific techniques such as coalesced access, shared memory utilization, and unified memory implementation. Benchmark and performance tests are conducted against an open‐source CPU‐based DEM code under three typical loading conditions. The performance of GoDEM is examined with varying thread‐block size and register pressure of the GPU, and RVE number. It reveals that increasing GPU occupancy by decreasing register pressure results in a significant degradation rather than improvement in performance. We further demonstrate that the proposed GPU parallelism framework may achieve a saturated speedup of approximately 350 compared with the single‐CPU‐core code. As a demonstration on its application for multiscale modeling of granular media, the material point method is coupled with the new framework powered DEM to simulate a typical engineering‐scale problem involving tens of millions of total particles having to be handled. It demonstrates that a speedup of approximately 91 can be achieved by using the proposed framework, compared with the performance of a similar CPU program running on a cluster node of 44 parallel threads. The study offers a viable future solution to large‐scale and multiscale modeling of granular media.|International Journal for Numerical Methods in Engineering|2020|10.1002/nme.6549|Jidong Zhao, Shiwei Zhao, Weijian Liang|4.0|2
658|Real-Time Electromagnetic Transient Simulation of Multi-Terminal HVDC–AC Grids Based on GPU|High-fidelity electromagnetic transient (EMT) simulation plays a critical role in understanding the dynamic behavior and fast transients involved in operation, control, and protection of multiterminal dc (MTdc) grids. This article proposes a cost-effective high-performance real-time EMT simulation platform for large-scale cross-continental MTdc grids based on graphics processing unit (GPU). Fast dynamic transients from both ac and dc networks are captured in real time with 5$\mu$s time step, using advanced hybrid-discretized modular multilevel converter model, frequency-dependent transmission line model, and EMT-type model of synchronous generators. The proposed simulation platform i) assembles detailed EMT models of all components within an MTdc–ac grid into a single platform. This setup provides a complete simulation solution to capture fast transient signals required for high-bandwidth controller design and protection studies without any compromise; ii) implements the first GPU-based simulation architecture and corresponding algorithms for MTdc–ac grids with real-time performance at scales of 1s; iii) is highly efficient and balances the high utilization of GPU resources and low latency required for the simulation; and iv) outperforms the existing central processing unit- or digital signal processor (DSP)/field-programmable gate array-based simulators in terms of its higher scalability on large-scale MTdc–ac grids and superior price–performance ratio on the hardware. Accuracy and performance of the proposed platform are evaluated with respect to the reference results from power system computer aided design (PSCAD)/EMTdc environment.|IEEE transactions on industrial electronics (1982. Print)|2020|10.1109/TIE.2020.3005059|M. Saeedifard, Jingfan Sun, Phani R. V. Marthi, S. Debnath|4.0|2
679|GPU acceleration of a patient-specific airway image segmentation and its assessment|Image segmentation plays an important role in computer vision, object detection, traffic control, and video surveillance. Typically, it is a critical step in the 3D reconstruction of a specific organ in medical image processing which unveils the detailed tomography of organ, tumor, and nerve, and thus helping to improve the quality of surgical pathology. However, there may be high computational requirements in it. With the advent of GPUs, more complex and realistic models can be simulated, but the deployment of these facilities also requires a huge amount of capital. As a consequence, how to make good use of these computational resource is essential to GPU computing. This study discusses the image segmentation of 3D airway reconstruction, identifies the computing-intensive task, and parallelizes the algorithm of image segmentation in order to obtain theoretical maximum speedup in terms of the benchmark ratio of GPU to CPU. There are five steps involved, which are the image acquisition, pre-processing, segmentation, reconstruction, and object recognition. It is worth to note that it takes 85\% of time on segmentation. This study successfully accelerates the image segmentation of 3D airway reconstruction by optimizing the memory usage, grid and block setting and multiple GPUs communication, thereby gaining a total speedup of 61.8 on two GPUs (Nvidia K40).||2020|10.1029/2019ms001957|T. W. Sheu, Yu-Wei Chang|4.0|2
790|Unprecedented cloud resolution in a GPU-enabled full-physics atmospheric climate simulation on OLCF’s summit supercomputer|Clouds represent a key uncertainty in future climate projection. While explicit cloud resolution remains beyond our computational grasp for global climate, we can incorporate important cloud effects through a computational middle ground called the Multi-scale Modeling Framework (MMF), also known as Super Parameterization. This algorithmic approach embeds high-resolution Cloud Resolving Models (CRMs) to represent moist convective processes within each grid column in a Global Climate Model (GCM). The MMF code requires no parallel data transfers and provides a self-contained target for acceleration. This study investigates the performance of the Energy Exascale Earth System Model-MMF (E3SM-MMF) code on the OLCF Summit supercomputer at an unprecedented scale of simulation. Hundreds of kernels in the roughly 10K lines of code in the E3SM-MMF CRM were ported to GPUs with OpenACC directives. A high-resolution benchmark using 4600 nodes on Summit demonstrates the computational capability of the GPU-enabled E3SM-MMF code in a full physics climate simulation.|The international journal of high performance computing applications|2021|10.1177/10943420211027539|K. Pressel, B. Hillman, W. Hannah, D. A. Bader, M. Norman, C. R. Jones, M. Taylor, C. Eldred, Xingqiu Yuan, S. Sreepathi, Isaac Lyngaas, Jungmin M. Lee, L. Leung|4.0|2
963|A High-Efficiency Spectral Element Method Based on CFS-PML for GPR Numerical Simulation and Reverse Time Migration|Improving the accuracy and efficiency of the numerical simulation of ground penetrating radar (GPR) becomes a pressing need with the rapidly increased amount of inversion data and the growing demand for migration imaging quality. In this article, we present a numerical spectral element time-domain (SETD) simulation procedure for GPR forward modeling and further apply it to the reverse time migration (RTM) with complex geoelectric models. This approach takes into account the flexibility of the finite element methods and the high precision of the spectral methods. Meanwhile, in this procedure, the complex frequency shifted perfectly matched layer (CFS-PML) is loaded to effectively suppress the echo at the truncated boundary, and the per-element GPU parallel framework used can achieve up to 5.7788 times the efficiency compared with the CPU calculation. The experiments on SETD spatial convergence and CFS-PML optimal parameter selection showed that, under the same degree of freedom, the SETD offered substantially better accuracy compared with the traditional FETD. The experiments on RTM of different profiles with different orders of SETD via a complex geoelectric model verify the universality of the algorithm. The results indicate that the RTM imaging effect has been significantly improved with the increase of SETD order. It fully proves the great potential of efficient and high-precision SETD simulation algorithm in the RTM imaging direction and shows certain guiding significance for underground target structure exploration.|IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing|2023|10.1109/JSTARS.2023.3234199|Yuxin Liu, Xun Wang, Deshan Feng, Siyuan Ding, Bingchao Li, Tianxiao Yu, Zheng Feng|4.0|2
1187|Fine-Grained Network Decomposition for Massively Parallel Electromagnetic Transient Simulation of Large Power Systems|Electromagnetic transient (EMT) simulation is one of the most complex power system studies that requires detailed modeling of the study system including all frequency-dependent and nonlinear effects. Large-scale EMT simulation is becoming commonplace due to the increasing growth and interconnection of power grids, and the need to study the impact of system events of the wide area network. To cope with enormous computational burden, the massively parallel architecture of the graphics processing unit (GPU) is exploited in this paper for large-scale EMT simulation. A fine-grained network decomposition, called shattering network decomposition, is proposed to divide the power system network exploiting its topological and physical characteristics into linear and nonlinear networks, which adapt to the unique features of the GPU-based massive thread computing system. Large-scale systems, up to 240 000 nodes, with typical components, including synchronous machines, transformers, transmission lines, and nonlinear elements, and multiple levels modular multilevel converter with up to 6144 submodules, are tested and compared with mainstream simulation software to verify the accuracy and demonstrate the speed-up improvement with respect to sequential computation.|IEEE Power and Energy Technology Systems Journal|2017|10.1109/JPETS.2017.2732360|V. Dinavahi, Zhiyin Zhou|4.0|2
1222|Predictive Modeling for CPU, GPU, and FPGA Performance and Power Consumption: A Survey|CPUs and dedicated accelerators (namely GPUs and FPGAs) continue to grow increasingly large and complex to support todays demanding performance and power requirements. Designers are tasked with evaluating the performance and power of similarly increasingly large design spaces during pre-silicon design for CPUs and GPUs to reduce time-to-market and limit manufacturing costs, or to figure out how to best map applications onto FPGAs using high-level synthesis tools. Typically, cycle-accurate simulators are used to evaluate workloads for pre-silicon CPUs and GPUs and to avoid the overhead of synthesis and place-and-route when targeting FPGAs; however, simulators exhibit prohibitively long run times that limit the number of design points and workloads that can be evaluated in a reasonable timeframe. This survey focuses on predictive modeling as an alternative to cycle-accurate simulation, which enables rapid evaluation of workloads and design points. When applied properly, predictive modeling can improve time to market, and can facilitate more comprehensive design space explorations with far less overhead than simulation. The survey focuses on predictive models applied to CPUs, GPUs, and FPGAs, noting that the general approach has been applied to many other computing platforms as well.|IEEE Computer Society Annual Symposium on VLSI|2018|10.1109/ISVLSI.2018.00143|Kenneth O'Neal, P. Brisk|4.0|2
1426|Nonlinear homogenization using model order reduction: two-scale simulations and novel developments using the pRBMOR on GPUs|Modern composite materials often consist of multiple phases arranged in a complex threedimensional microstructure. The individual constituents are often nonlinear and path-dependent, thereby inducing many computational challenges. The recent pRBMOR provides a reduced order model for the rapid computation of approximate solutions to the microstructural problem. Thereby, two-scale simulations on desktop computers with off-the-shelf consumer graphic cards become feasible. The FE2R method and extensions of the pRBMOR for the consideration of cohesive interfaces are discussed.||2015|10.1007/s00466-015-1163-0|M. Hodapp, F. Fritzen, Matthias Leuschner|4.0|2
244|High-Frequency Nonlinear Earthquake Simulations on Petascale Heterogeneous Supercomputers|The omission of nonlinear effects in large-scale 3D ground motion estimation, which are particularly challenging due to memory and scalability issues, can result in costly misguidance for structural design in earthquake-prone regions. We have implemented nonlinearity using a Drucker-Prager yield condition in AWP-ODC and further optimized the CUDA kernels to more efficiently utilize the GPU's memory bandwidth. The application has resulted in a significant increase in the model region and accuracy for state-of-the-art earthquake simulations in a realistic earth structure, which are now able to resolve the wavefield at frequencies relevant for the most vulnerable buildings (> 1 Hz) while maintaining the scalability and efficiency of the method. We successfully run the code on 4,200 Kepler K20X GPUs on NCSA Blue Waters and OLCF Titan to simulate a M 7.7 earthquake on the southern San Andreas fault with a spatial resolution of 25 m for frequencies up to 4 Hz.|International Conference for High Performance Computing, Networking, Storage and Analysis|2016|10.1109/SC.2016.81|S. Day, W. Savran, Yifeng Cui, Peng Wang, D. Mu, D. Roten, K. Withers, K. Olsen|3.888888888888889|2
322|A New FPGA Architecture of FAST and BRIEF Algorithm for On-Board Corner Detection and Matching|Although some researchers have proposed the Field Programmable Gate Array (FPGA) architectures of Feature From Accelerated Segment Test (FAST) and Binary Robust Independent Elementary Features (BRIEF) algorithm, there is no consideration of image data storage in these traditional architectures that will result in no image data that can be reused by the follow-up algorithms. This paper proposes a new FPGA architecture that considers the reuse of sub-image data. In the proposed architecture, a remainder-based method is firstly designed for reading the sub-image, a FAST detector and a BRIEF descriptor are combined for corner detection and matching. Six pairs of satellite images with different textures, which are located in the Mentougou district, Beijing, China, are used to evaluate the performance of the proposed architecture. The Modelsim simulation results found that: (i) the proposed architecture is effective for sub-image reading from DDR3 at a minimum cost; (ii) the FPGA implementation is corrected and efficient for corner detection and matching, such as the average value of matching rate of natural areas and artificial areas are approximately 67% and 83%, respectively, which are close to PC’s and the processing speed by FPGA is approximately 31 and 2.5 times faster than those by PC processing and by GPU processing, respectively.|Italian National Conference on Sensors|2018|10.3390/s18041014|Guoqing Zhou, Xiaoping Zhou, Jingjin Huang, Rongting Zhang|3.857142857142857|2
1291|AlSub: Fully Parallel and Modular Subdivision|In recent years, mesh subdivision---the process of forging smooth free-form surfaces from coarse polygonal meshes---has become an indispensable production instrument. Although subdivision performance is crucial during simulation, animation and rendering, state-of-the-art approaches still rely on serial implementations for complex parts of the subdivision process. Therefore, they often fail to harness the power of modern parallel devices, like the graphics processing unit (GPU), for large parts of the algorithm and must resort to time-consuming serial preprocessing. In this paper, we show that a complete parallelization of the subdivision process for modern architectures is possible. Building on sparse matrix linear algebra, we show how to structure the complete subdivision process into a sequence of algebra operations. By restructuring and grouping these operations, we adapt the process for different use cases, such as regular subdivision of dynamic meshes, uniform subdivision for immutable topology, and feature-adaptive subdivision for efficient rendering of animated models. As the same machinery is used for all use cases, identical subdivision results are achieved in all parts of the production pipeline. As a second contribution, we show how these linear algebra formulations can effectively be translated into efficient GPU kernels. Applying our strategies to $\sqrt{3}$, Loop and Catmull-Clark subdivision shows significant speedups of our approach compared to state-of-the-art solutions, while we completely avoid serial preprocessing.||2018|10.1109/sc.2018.00063|Rhaleb Zayer, Daniel Mlakar, Martin Winter, H. Seidel, M. Steinberger|3.857142857142857|2
448|4D- quantitative structure–activity relationship modeling: making a comeback|ABSTRACT Introduction: Predictive Quantitative Structure–Activity Relationship (QSAR) modeling has become an essential methodology for rapidly assessing various properties of chemicals. The vast majority of these QSAR models utilize numerical descriptors derived from the two- and/or three-dimensional structures of molecules. However, the conformation-dependent characteristics of flexible molecules and their dynamic interactions with biological target(s) is/are not encoded by these descriptors, leading to limited prediction performances and reduced interpretability. 2D/3D QSAR models are successful for virtual screening, but typically suffer at lead optimization stages. That is why conformation-dependent 4D-QSAR modeling methods were developed two decades ago. However, these methods have always suffered from the associated computational cost. Recently, 4D-QSAR has been experiencing a significant come-back due to rapid advances in GPU-accelerated molecular dynamic simulations and modern machine learning techniques. Areas covered: Herein, the authors briefly review the literature regarding 4D-QSAR modeling and describe its modern workflow called MD-QSAR. Challenges and current limitations are also highlighted. Expert opinion: The development of hyper-predictive MD-QSAR models could represent a disruptive technology for analyzing, understanding, and optimizing dynamic protein-ligand interactions with countless applications for drug discovery and chemical toxicity assessment. Therefore, there has never been a better time and relevance for molecular modeling teams to engage in hyper-predictive MD-QSAR modeling.|Expert Opinion on Drug Discovery|2019|10.1080/17460441.2019.1664467|Jeremy R. Ash, D. Fourches|3.8333333333333335|2
1591|Animated rendering of cardiac model simulations|Heart disease has been the leading cause of death both in the world and the United States in the past decade. Computational cardiac modeling and simulation, especially patient-specific cardiac modeling has been recognized as one of the best ways to improve diagnosis of heart disease by providing insights in individual disease characteristics that cannot be obtained by other means. However presenting the results of cardiac simulations to cardiologists in an interactive manner can considerably improve the utility of cardiac models in understanding the heart function. In this work, we have developed virtual reality and animated volume rendering techniques to render the results of cardiac simulations. We have developed a GPU accelerated algorithm that produces time varying voxelized representation of the quantities of interest in a cardiac model, which can then be interactively rendered in real time. We voxelize the different time frames of the analysis model and transfer the time-varying data to the GPU memory using a flat data structure. This technique allows us to visualize and interact with animation in real time. As a proof-of-concept, we test our method on interactively rendering the simulation results of cardiac biomechanics simulations. We also present the timing results on post-processing and rendering two different cardiac IGA at different resolutions. We achieve an interactive frame rate of over 50 fps for all test cases.||2019|10.1016/j.commatsci.2018.09.019|Xin Huang|3.8333333333333335|2
337|Fast calculation method of a CGH for a patch model using a point-based method.|Holography is three-dimensional display technology. Computer-generated holograms (CGHs) are created by simulating light propagation on a computer, and they are able to display a virtual object. There are mainly two types of calculation methods of CGHs, a point-based method and the fast Fourier-transform (FFT)-based method. The FFT-based method is based on a patch model, and it is suited to accelerating the calculations as it calculates the light propagation across a patch as a whole. The calculations with the point-based method are characterized by a high degree of parallelism, and it is suited to accelerating graphics processing units (GPUs). The point-based method is not suitable for calculation with the patch model. This paper proposes a fast calculation algorithm for a patch model with the point-based method. The proposed method calculates the line on a patch as a whole regardless of the number of points on the line. When the proposed method is implemented on a GPU, the calculation time of the proposed method is shorter than with the point-based method.|Applied Optics|2015|10.1364/AO.54.000A76|Yuji Sakamoto, Yuki Ogihara|3.8|2
548|Comprehensive Modeling of Large Photovoltaic Systems for Heterogeneous Parallel Transient Simulation of Integrated AC/DC Grid|Detailed nonlinear transient modeling of the photovoltaic (PV) system enables an accurate study of the host integrated AC/DC grid. In this article, the parallel architecture of the graphics processing unit (GPU) catering to a massive number of PV modules is utilized in conjunction with CPU for efficient transient simulation. To reflect the exact operation status of the solar power system subjected to various temperatures and nonuniform solar irradiance in the electromagnetic transient (EMT) simulation, all necessary panels are modeled individually, and therefore, a scalable PV array model with a flexible level of aggregation is proposed in addition to its fully detailed discrete counterpart so as to improve the computational efficiency. The single-instruction multiple-thread implementation mode of the GPU enables up to 10 million PV panels, regardless of the size or type, to be computed concurrently, and noticing that the hybrid AC/DC grid has a significant irregularity, the CPU is also adopted to tackle systems with inadequate parallelism. Meanwhile, since the AC grid dynamic interaction has a distinct tolerance on the time-step to that of the remaining part, a multi-rate scheme is employed to expedite the heterogeneous CPU-GPU computation for dynamic-EMT co-simulation, whose results are validated by the commercial off-line tools MATLAB/Simulink and DSATools/TSAT.|IEEE transactions on energy conversion|2020|10.1109/TEC.2020.2966729|V. Dinavahi, Shiqi Cao, Ning Lin|3.8|2
296|Comparing Neuromorphic Solutions in Action: Implementing a Bio-Inspired Solution to a Benchmark Classification Task on Three Parallel-Computing Platforms|Neuromorphic computing employs models of neuronal circuits to solve computing problems. Neuromorphic hardware systems are now becoming more widely available and “neuromorphic algorithms” are being developed. As they are maturing toward deployment in general research environments, it becomes important to assess and compare them in the context of the applications they are meant to solve. This should encompass not just task performance, but also ease of implementation, speed of processing, scalability, and power efficiency. Here, we report our practical experience of implementing a bio-inspired, spiking network for multivariate classification on three different platforms: the hybrid digital/analog Spikey system, the digital spike-based SpiNNaker system, and GeNN, a meta-compiler for parallel GPU hardware. We assess performance using a standard hand-written digit classification task. We found that whilst a different implementation approach was required for each platform, classification performances remained in line. This suggests that all three implementations were able to exercise the model's ability to solve the task rather than exposing inherent platform limits, although differences emerged when capacity was approached. With respect to execution speed and power consumption, we found that for each platform a large fraction of the computing time was spent outside of the neuromorphic device, on the host machine. Time was spent in a range of combinations of preparing the model, encoding suitable input spiking data, shifting data, and decoding spike-encoded results. This is also where a large proportion of the total power was consumed, most markedly for the SpiNNaker and Spikey systems. We conclude that the simulation efficiency advantage of the assessed specialized hardware systems is easily lost in excessive host-device communication, or non-neuronal parts of the computation. These results emphasize the need to optimize the host-device communication architecture for scalability, maximum throughput, and minimum latency. Moreover, our results indicate that special attention should be paid to minimize host-device communication when designing and implementing networks for efficient neuromorphic computing.|Frontiers in Neuroscience|2016|10.3389/fnins.2015.00491|M. Schmuker, T. Nowotny, A. Diamond|3.7777777777777777|2
473|MD Simulations of Viruslike Particles with Supra CG Solvation Affordable to Desktop Computers.|Viruses are tremendously efficient molecular devices that optimize the packing of genetic material using a minimalistic number of proteins to form a capsid or envelope that protects them from external threats, being also part of cell recognition, fusion, and budding machineries. Progress in experimental techniques has provided a large number of high-resolution structures of viruses and viruslike particles (VLP), while molecular dynamics simulations may furnish lively and complementary insights on the fundamental forces ruling viral assembly, stability, and dynamics. However, the large size and complexity of these macromolecular assemblies pose significant computational challenges. Alternatively, Coarse-Grained (CG) methods, which resign atomistic resolution privileging computational efficiency, can be used to characterize the dynamics of VLPs. Still, the massive amount of solvent present in empty capsids or envelopes suggests that hybrid schemes keeping a higher resolution on regions of interest (i.e., the viral proteins and their surroundings) and a progressively coarser description on the bulk may further improve efficiency. Here we introduce a mesoscale explicit water model to be used in double- or triple-scale simulations in combination with popular atomistic parameters and the CG water used by the SIRAH force field. Simulations performed on VLPs of different sizes, along with a comprehensive analysis of the PDB, indicate that most of the VLPs so far reported are amenable to be handled on a GPU-accelerated desktop computer using this simulation scheme.|Journal of Chemical Theory and Computation|2017|10.1021/acs.jctc.7b00659|M. R. Machado, Humberto González, S. Pantano|3.75|2
1398|Coupled indoor/outdoor airflow simulation comparing ANSYS Fluent with a GPU-based lattice Boltzmann model for urban environments|eprints@whiterose.ac.uk https://eprints.whiterose.ac.uk/ Reuse Items deposited in White Rose Research Online are protected by copyright, with all rights reserved unless indicated otherwise. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. This is indicated by the licence information on the White Rose Research Online record for the item.||2018|10.3389/fmech.2018.00015|A. Khan, M. King, C. Noakes|3.7142857142857144|2
323|Recent developments and comprehensive evaluations of a GPU-based Monte Carlo package for proton therapy|Monte Carlo (MC) simulation is commonly considered as the most accurate dose calculation method for proton therapy. Aiming at achieving fast MC dose calculations for clinical applications, we have previously developed a graphics-processing unit (GPU)-based MC tool, gPMC. In this paper, we report our recent updates on gPMC in terms of its accuracy, portability, and functionality, as well as comprehensive tests on this tool. The new version, gPMC v2.0, was developed under the OpenCL environment to enable portability across different computational platforms. Physics models of nuclear interactions were refined to improve calculation accuracy. Scoring functions of gPMC were expanded to enable tallying particle fluence, dose deposited by different particle types, and dose-averaged linear energy transfer (LETd). A multiple counter approach was employed to improve efficiency by reducing the frequency of memory writing conflict at scoring. For dose calculation, accuracy improvements over gPMC v1.0 were observed in both water phantom cases and a patient case. For a prostate cancer case planned using high-energy proton beams, dose discrepancies in beam entrance and target region seen in gPMC v1.0 with respect to the gold standard tool for proton Monte Carlo simulations (TOPAS) results were substantially reduced and gamma test passing rate (1%/1 mm) was improved from 82.7%–93.1%. The average relative difference in LETd between gPMC and TOPAS was 1.7%. The average relative differences in the dose deposited by primary, secondary, and other heavier particles were within 2.3%, 0.4%, and 0.2%. Depending on source proton energy and phantom complexity, it took 8–17 s on an AMD Radeon R9 290x GPU to simulate 107 source protons, achieving less than 1% average statistical uncertainty. As the beam size was reduced from 10  ×  10 cm2 to 1  ×  1 cm2, the time on scoring was only increased by 4.8% with eight counters, in contrast to a 40% increase using only one counter. With the OpenCL environment, the portability of gPMC v2.0 was enhanced. It was successfully executed on different CPUs and GPUs and its performance on different devices varied depending on processing power and hardware structure.|Physics in Medicine and Biology|2016|10.1088/0031-9155/61/20/7347|H. Paganetti, N. Qin, X. Jia, J. Schuemann, Steve B. Jiang, P. Botas, D. Giantsoudi, Z. Tian|3.6666666666666665|2
449|Kronos: A 5G Scheduler for AoI Minimization Under Dynamic Channel Conditions|Age of information (AoI) is a powerful new metric to quantify the freshness of information and has gained increasing popularity in IoT applications. Existing models on AoI remain primitive and do not consider state-of-the-art transmission technologies such as 5G. They also fail to consider the impact of dynamic channel conditions. In this paper, we present Kronos, a 5G-compliant AoI scheduling algorithm that can cope with highly dynamic channel conditions. Kronos is capable of performing RB allocation and selecting MCS for each source node based on channel conditions, with the objective of minimizing long-term AoI. To meet the stringent real-time requirement for 5G, we propose a GPU-based implementation of Kronos on low-cost offthe-shelf GPUs. Through simulations and experiments, we show that Kronos can find near-optimal AoI scheduling solutions in sub-millisecond time scale. To the best of our knowledge, this is the first 5G-compliant real-time AoI scheduler that can cope with dynamic channel conditions.|IEEE International Conference on Distributed Computing Systems|2019|10.1109/ICDCS.2019.00146|W. Lou, B. Jalaeian, Chengzhang Li, Y. Huang, Yongce Chen, Y. T. Hou|3.6666666666666665|2
850|Portable Acceleration of Materials Modeling Software: CASTEP, GPUs, and OpenACC|In this article, we present work to port the CASTEP first-principles materials modeling program to accelerators using open accelerator (OpenACC). We discuss the challenges and opportunities presented by graphical processing units (GPU) architectures in particular, and the approach taken in the CASTEP OpenACC port. Whilst the port is still under active development, early performance results show that significant speed-ups may be gained, particularly for materials simulations using so-called “nonlocal functionals,” where speed-ups can exceed a factor of ten.|Computing in science & engineering (Print)|2022|10.1109/mcse.2022.3141714|Matthew Smith, A. Tamerus, P. Hasnip|3.6666666666666665|2
896|GPU-accelerated approximate kernel method for quantum machine learning.|We introduce Quantum Machine Learning (QML)-Lightning, a PyTorch package containing graphics processing unit (GPU)-accelerated approximate kernel models, which can yield trained models within seconds. QML-Lightning includes a cost-efficient GPU implementation of FCHL19, which together can provide energy and force predictions with competitive accuracy on a microsecond per atom timescale. Using modern GPU hardware, we report learning curves of energies and forces as well as timings as numerical evidence for select legacy benchmarks from atomistic simulation including QM9, MD-17, and 3BPA.|Journal of Chemical Physics|2022|10.1063/5.0108967|Felix A Faber, O. Anatole von Lilienfeld, N. Browning|3.6666666666666665|2
1103|VIPER: Volume Invariant Position-based Elastic Rods|We extend the formulation of position-based rods to include elastic volumetric deformations. We achieve this by introducing an additional degree of freedom per vertex -- isotropic scale (and its velocity). Including scale enriches the space of possible deformations, allowing the simulation of volumetric effects, such as a reduction in cross-sectional area when a rod is stretched. We rigorously derive the continuous formulation of its elastic energy potentials, and hence its associated position-based dynamics (PBD) updates to realize this model, enabling the simulation of up to 26000 DOFs at 140 Hz in our GPU implementation. We further show how rods can provide a compact alternative to tetrahedral meshes for the representation of complex muscle deformations, as well as providing a convenient representation for collision detection. This is achieved by modeling a muscle as a bundle of rods, for which we also introduce a technique to automatically convert a muscle surface mesh into a rods-bundle. Finally, we show how rods and/or bundles can be skinned to a surface mesh to drive its deformation, resulting in an alternative to cages for real-time volumetric deformation.||2019|10.1145/3340260|Shahram Izadi, Julien Valentin, Daniel Rebain, Loic Barthe, JP Lewis, Miles Macklin, Baptiste Angles, B. Wyvill, Javier von der Pahlen, A. Tagliasacchi, Sofien Bouaziz|3.6666666666666665|2
1559|VIPER: Volume Invariant Position-based Elastic Rods|We extend the formulation of position-based rods to include elastic volumetric deformations. We achieve this by introducing an additional degree of freedom per vertex -- isotropic scale (and its velocity). Including scale enriches the space of possible deformations, allowing the simulation of volumetric effects, such as a reduction in cross-sectional area when a rod is stretched. We rigorously derive the continuous formulation of its elastic energy potentials, and hence its associated position-based dynamics (PBD) updates to realize this model, enabling the simulation of up to 26000 DOFs at 140 Hz in our GPU implementation. We further show how rods can provide a compact alternative to tetrahedral meshes for the representation of complex muscle deformations, as well as providing a convenient representation for collision detection. This is achieved by modeling a muscle as a bundle of rods, for which we also introduce a technique to automatically convert a muscle surface mesh into a rods-bundle. Finally, we show how rods and/or bundles can be skinned to a surface mesh to drive its deformation, resulting in an alternative to cages for real-time volumetric deformation. The source code of our physics engine will be openly available1.|PACMCGIT|2019|10.1145/3340260|J. P. Lewis, Daniel Rebain, L. Barthe, Julien P. C. Valentin, Baptiste Angles, B. Wyvill, Javier von der Pahlen, A. Tagliasacchi, Sofien Bouaziz, M. Macklin, S. Izadi|3.6666666666666665|2
474|Dynamic Thread Block Launch: A lightweight execution mechanism to support irregular applications on GPUs|GPUs have been proven effective for structured applications that map well to the rigid 1D-3D grid of threads in modern bulk synchronous parallel (BSP) programming languages. However, less success has been encountered in mapping data intensive irregular applications such as graph analytics, relational databases, and machine learning. Recently introduced nested device-side kernel launching functionality in the GPU is a step in the right direction, but still falls short of being able to effectively harness the GPUs performance potential. We propose a new mechanism called Dynamic Thread Block Launch (DTBL) to extend the current bulk synchronous parallel model underlying the current GPU execution model by supporting dynamic spawning of lightweight thread blocks. This mechanism supports the nested launching of thread blocks rather than kernels to execute dynamically occurring parallel work elements. This paper describes the execution model of DTBL, device-runtime support, and microarchitecture extensions to track and execute dynamically spawned thread blocks. Experiments with a set of irregular data intensive CUDA applications executing on a cycle-level simulator show that DTBL achieves average 1.21x speedup over the original flat implementation and average 1.40x over the implementation with device-side kernel launches using CUDA Dynamic Parallelism.|International Symposium on Computer Architecture|2015|10.1145/2749469.2750393|A. Sidelnik, Jin Wang, S. Yalamanchili, Norman Rubin|3.6|2
315|New Modeling Method and Design Optimization for a Soft-Switched DC–DC Converter|High-performance cloud computing enables many key future technologies such as artificial intelligence (AI), self-driving vehicle, big data analysis, and the Internet of things (IoT), using clustered CPU and GPU servers in the datacenter. To improve the power efficiency and the infrastructure flexibility, the computing industry is adopting 54 VDC to power the servers in the open compute racks. In this paper, a new modeling technique for a soft-switched dc–dc converter is presented and suitable to guide optimal design in different applications, for example, 54 V to point of load (PoL) for the new open compute rack. To improve the model accuracy and reduce the complexity, this paper proposes a reduced-order linear differential equation (LDE) based modeling technique to discover the following: 1) the tank resonance involving the output inductor; 2) the output current ripple and its impact on power efficiency; 3) the proper on-time control for soft switching; 4) the unique bleeding mode under the heavy load; 5) the output power capability of the converter; and 6) component tolerance analysis and impact on the performance of the converter. With the power loss estimation, design guidelines are provided for a reference design and design improvement based on this new modeling technique. Using the proposed method, great accuracy can be expected in the efficiency estimation. Simulation and experimental results are provided to verify the modeling technique in a 54–1.2 V 25 A dc–dc converter prototype.|IEEE transactions on power electronics|2018|10.1109/TPEL.2017.2751064|Xin Li, Liang Jia, Yanfei Liu, Srikanth Lakshmikanthan|3.5714285714285716|2
186|A Technology of 3D Elastic Wave Propagation Simulation Using Hybrid Supercomputers|We present a technology of 3D seismic field simulation for high-performance computing systems with GPUs or Intel Xeon Phi coprocessors. This technology covers adaptation of a mathematical modeling method and development of a parallel algorithm. We describe the parallel realization designed for simulation based on using staggeredgrids and 3D domain decomposition method. We study the parallel algorithm behavior on computing devices: CPUs, GPUs, Xeon Phi coprocessors. We consider the results of experiments that were carried out on cluster NKS-30T of SSCC and cluster MVS-10P of JSCC for different tests for parallel algorithm.||2015|10.1093/gji/ggv029|B. Glinsky, V. Kovalevsky, D. Karavaev|3.5|2
718|Resolving Wave Propagation in Anisotropic Poroelastic Media Using Graphical Processing Units (GPUs)|Biot's equations describe the physics of hydromechanically coupled systems establishing the widely recognized theory of poroelasticity. This theory has a broad range of applications in Earth and biological sciences as well as in engineering. The numerical solution of Biot's equations is challenging because wave propagation and fluid pressure diffusion processes occur simultaneously but feature very different characteristic time scales. Analogous to geophysical data acquisition, high resolution and three dimensional numerical experiments lately redefined state of the art. Tackling high spatial and temporal resolution requires a high‐performance computing approach. We developed a multi‐ graphical processing units (GPU) numerical application to resolve the anisotropic elastodynamic Biot's equations that relies on a conservative numerical scheme to simulate, in a few seconds, wave fields for spatial domains involving more than 1.5 billion grid cells. We present a comprehensive dimensional analysis reducing the number of material parameters needed for the numerical experiments from ten to four. Furthermore, the dimensional analysis emphasizes the key material parameters governing the physics of wave propagation in poroelastic media. We perform a dispersion analysis as function of dimensionless parameters leading to simple and transparent dispersion relations. We then benchmark our numerical solution against an analytical plane wave solution. Finally, we present several numerical modeling experiments, including a three‐dimensional simulation of fluid injection into a poroelastic medium. We provide the Matlab, symbolic Maple, and GPU CUDA C routines to reproduce the main presented results. The high efficiency of our numerical implementation makes it readily usable to investigate three‐dimensional and high‐resolution scenarios of practical applications.|Journal of Geophysical Research: Solid Earth|2021|10.1029/2020JB021175|L. Räss, Y. Podladchikov, Y. Alkhimenkov, L. Khakimova, B. Quintal|3.5|2
806|READYS: A Reinforcement Learning Based Strategy for Heterogeneous Dynamic Scheduling|In this paper, we propose READYS, a reinforcement learning algorithm for the dynamic scheduling of computations modeled as a Directed Acyclic Graph (DAGs). Our goal is to develop a scheduling algorithm in which allocation and scheduling decisions are made at runtime, based on the state of the system, as performed in runtime systems such as StarPU or ParSEC. Reinforcement Learning is a natural candidate to achieve this task, since its general principle is to build step by step a strategy that, given the state of the system (the state of the resources and a view of the ready tasks and their successors in our case), makes a decision to optimize a global criterion. Moreover, the use of Reinforcement Learning is natural in a context where the duration of tasks (and communications) is stochastic. We propose READYS that combines Graph Convolutional Networks (GCN) with an Actor-Critic Algorithm (A2C): it builds an adaptive representation of the scheduling problem on the fly and learns a scheduling strategy, aiming at minimizing the makespan. A crucial point is that READYS builds a general scheduling strategy which is neither limited to only one specific application or task graph nor one particular problem size, and that can be used to schedule any DAG. We focus on different types of task graphs originating from linear algebra factorization kernels (CHOLESKY, LU, QR) and we consider heterogeneous platforms made of a few CPUs and GPUs. We first propose to analyze the performance of READYS when learning is performed on a given (platform, kernel, problem size) combination. Using simulations, we show that the scheduling agent obtains performances very similar or even superior to algorithms from the literature, and that it is especially powerful when the scheduling environment contains a lot of uncertainty. We additionally demonstrate that our agent exhibits very promising generalization capabilities. To the best of our knowledge, this is the first paper which shows that reinforcement learning can really be used for dynamic DAG scheduling on heterogeneous resources.|IEEE International Conference on Cluster Computing|2021|10.1109/Cluster48925.2021.00031|Nathan Grinsztajn, Olivier Beaumont, P. Preux, E. Jeannot|3.5|2
960|DiffXPBD|We present DiffXPBD, a novel and efficient analytical formulation for the differentiable position-based simulation of compliant constrained dynamics (XPBD). Our proposed method allows computation of gradients of numerous parameters with respect to a goal function simultaneously leveraging a performant simulation model. The method is efficient, thus enabling differentiable simulations of high resolution geometries and degrees of freedom (DoFs). Collisions are naturally included in the framework. Our differentiable model allows a user to easily add additional optimization variables. Every control variable gradient requires the computation of only a few partial derivatives which can be computed using automatic differentiation code. We demonstrate the efficacy of the method with examples such as elastic cloth and volumetric material parameter estimation, initial value optimization, optimizing for underlying body shape and pose by only observing the clothing, and optimizing a time-varying external force sequence to match sparse keyframe shapes at specific times. Our approach demonstrates excellent efficiency and we demonstrate this on high resolution meshes with optimizations involving over 26 million degrees of freedom. Making an existing solver differentiable requires only a few modifications and the model is compatible with both modern CPU and GPU multi-core hardware.|Proceedings of the ACM on Computer Graphics and Interactive Techniques|2023|10.1145/3606923|Tuur Stuyck, Hsiao-yu Chen|3.5|2
1010|Simulating Stellar Merger using HPX/Kokkos on A64FX on Supercomputer Fugaku|The increasing availability of machines relying on non-GPU architectures, such as ARM A64FX in high-performance computing, provides a set of interesting challenges to application developers. In addition to requiring code portability across different parallelization schemes, programs targeting these architectures have to be highly adaptable in terms of compute kernel sizes to accommodate different execution characteristics for various heterogeneous workloads. In this paper, we demonstrate an approach to code and performance portability that is based entirely on established standards in the industry. In addition to applying Kokkos as an abstraction over the execution of compute kernels on different heterogeneous execution environments, we show that the use of standard C++ constructs as exposed by the HPX runtime system enables superb portability in terms of code and performance based on the real-world Octo-Tiger astrophysics application. We report our experience with porting Octo-Tiger to the ARM A64FX architecture provided by Stony Brook’s Ookami and Riken’s Supercomputer Fugaku and compare the resulting performance with that achieved on well established GPU-oriented HPC machines such as ORNL’s Summit, NERSC’s Perlmutter and CSCS’s Piz Daint systems. Octo-Tiger scaled well on Supercomputer Fugaku without any major code changes due to the abstraction levels provided by HPX and Kokkos. Adding vectorization support for ARM’s SVE to Octo-Tiger was trivial thanks to using standard C++. interfaces.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2023|10.1109/IPDPSW59300.2023.00116|D. Pflüger, H. Kaiser, Patrick Diehl, Sagiv Shiber, Dominic C. Marcello, Gregor Daiß, K. Huck|3.5|2
1013|OpenABC enables flexible, simplified, and efficient GPU accelerated simulations of biomolecular condensates|Biomolecular condensates are important structures in various cellular processes but are challenging to study using traditional experimental techniques. In silico simulations with residue-level coarse-grained models strike a balance between computational efficiency and chemical accuracy. They could offer valuable insights by connecting the emergent properties of these complex systems with molecular sequences. However, existing coarse-grained models often lack easy-to-follow tutorials and are implemented in software that is not optimal for condensate simulations. To address these issues, we introduce OpenABC, a software package that greatly simplifies the setup and execution of coarse-grained condensate simulations with multiple force fields using Python scripting. OpenABC seamlessly integrates with the OpenMM molecular dynamics engine, enabling efficient simulations with performances on a single GPU that rival the speed achieved by hundreds of CPUs. We also provide tools that convert coarse-grained configurations to all-atom structures for atomistic simulations. We anticipate that Open-ABC will significantly facilitate the adoption of in silico simulations by a broader community to investigate the structural and dynamical properties of condensates. Open-ABC is available at https://github.com/ZhangGroup-MITChemistry/OpenABC|bioRxiv|2023|10.1371/journal.pcbi.1011442|Cong Wang, Andrew P. Latham, Shuming Liu, Xinqiang Ding, Bin Zhang|3.5|2
1090|Multilayer-HySEA model validation for landslide-generated tsunamis – Part 1: Rigid slides|Abstract. This paper is devoted to benchmarking the Multilayer-HySEA model using laboratory\nexperimental data for landslide-generated tsunamis.\nThis article deals with rigid slides, and the second part, in a companion paper,\naddresses granular slides.\nThe US National Tsunami Hazard and Mitigation Program (NTHMP) has proposed the experimental data used\nand established for the NTHMP Landslide Benchmark Workshop, held in January 2017 at Galveston (Texas).\nThe first three benchmark problems proposed in this workshop deal with rigid slides. Rigid slides must be simulated as a moving\nbottom topography, and, therefore, they must be modeled as a prescribed boundary condition.\nThese three benchmarks are used here to validate the Multilayer-HySEA model.\nThis new HySEA model consists of an efficient hybrid finite-volume–finite-difference implementation on GPU architectures of a non-hydrostatic multilayer model.\nA brief description of model equations, dispersive properties, and the numerical scheme is included.\nThe benchmarks are described and the numerical results compared against the lab-measured data for each of them.\nThe specific aim is to validate this new code for tsunamis generated by rigid slides.\nNevertheless, the overall objective of the current benchmarking effort is to produce a ready-to-use numerical\ntool for real-world landslide-generated tsunami hazard assessment.\nThis tool has already been used to reproduce the Port Valdez, Alaska, 1964 and Stromboli, Italy, 2002 events.\n|Natural Hazards and Earth System Sciences|2021|10.5194/NHESS-21-775-2021|C. Escalante, J. Macías, Manuel J. Castro|3.5|2
1556|Deep-learning-based reduced-order modeling for subsurface flow simulation|A new deep-learning-based reduced-order modeling (ROM) framework is proposed for application in subsurface flow simulation. The reduced-order model is based on an existing embed-to-control (E2C) framework and includes an auto-encoder, which projects the system to a low-dimensional subspace, and a linear transition model, which approximates the evolution of the system states in low dimension. In addition to the loss function for data mismatch considered in the original E2C framework, we introduce a physics-based loss function that penalizes predictions that are inconsistent with the governing flow equations. The loss function is also modified to emphasize accuracy in key well quantities of interest (e.g., fluid production rates). The E2C ROM is shown to be very analogous to an existing ROM, POD-TPWL, which has been extensively developed for subsurface flow simulation. The new ROM is applied to oil-water flow in a heterogeneous reservoir, with flow driven by nine wells operating under time-varying control specifications. A total of 300 high-fidelity training simulations are performed in the offline stage, and the network training requires 10-12~minutes on a Tesla V100 GPU node. Online (runtime) computations achieve speedups of $\mathcal{O}$(1000) relative to full-order simulations. Extensive test case results, with well controls varied over large ranges, are presented. Accurate ROM predictions are achieved for global saturation and pressure fields at particular times, and for injection and production well responses as a function of time. Error is shown to increase when 100 or 200 (rather than 300) training runs are used to construct the E2C ROM.||2019|10.1016/j.jcp.2018.11.038|Yimin Liu, L. Durlofsky|3.5|2
348|A Virtual Reality System for PTCD Simulation Using Direct Visuo-Haptic Rendering of Partially Segmented Image Data|This study presents a new visuo-haptic virtual reality (VR) training and planning system for percutaneous transhepatic cholangio-drainage (PTCD) based on partially segmented virtual patient models. We only use partially segmented image data instead of a full segmentation and circumvent the necessity of surface or volume mesh models. Haptic interaction with the virtual patient during virtual palpation, ultrasound probing and needle insertion is provided. Furthermore, the VR simulator includes X-ray and ultrasound simulation for image-guided training. The visualization techniques are GPU-accelerated by implementation in Cuda and include real-time volume deformations computed on the grid of the image data. Computation on the image grid enables straightforward integration of the deformed image data into the visualization components. To provide shorter rendering times, the performance of the volume deformation algorithm is improved by a multigrid approach. To evaluate the VR training system, a user evaluation has been performed and deformation algorithms are analyzed in terms of convergence speed with respect to a fully converged solution. The user evaluation shows positive results with increased user confidence after a training session. It is shown that using partially segmented patient data and direct volume rendering is suitable for the simulation of needle insertion procedures such as PTCD.|IEEE journal of biomedical and health informatics|2016|10.1109/JBHI.2014.2381772|D. Fortmeier, Julian Schröder, H. Handels, André Mastmeyer|3.4444444444444446|2
1402|Comparison of the effectiveness of shared memory optimizations for stencil computations on NVIDIA GPU architectures|Stencil computations are commonly used in for example scientific simulations of heat distribution, fluid dynamics and seismic activity. The effectiveness of shared memory optimization in stencil computations on different NVIDIA GPU architectures is investigated in this thesis. Experiments on the previous GPU architectures Fermi and Kepler show that shared memory optimizations result in increasing efficiency as expected based on previous work, where the Kepler architecture GPU gets a greater performance increase than the Fermi architecture GPU. For two newer GPU architectures, Maxwell and Pascal, extensive GPU analysis is done to discover why the Maxwell GPUs almost doubles in speed with shared memory optimization while the Pascal architecture GPU only shows a speedup of about 10%. A conclusive explanation for this difference is not found which calls for future research to be done on the use of shared memory in stencil kernels on Pascal architecture GPUs.||2017|10.1080/17449626.2017.1320577|G. Verweij|3.375|2
311|OSCAR: Orchestrating STT-RAM cache traffic for heterogeneous CPU-GPU architectures|As we integrate data-parallel GPUs with general-purpose CPUs on a single chip, the enormous cache traffic generated by GPUs will not only exhaust the limited cache capacity, but also severely interfere with CPU requests. Such heterogeneous multicores pose significant challenges to the design of shared last-level cache (LLC). This problem can be mitigated by replacing SRAM LLC with emerging non-volatile memories like Spin-Transfer Torque RAM (STT-RAM), which provides larger cache capacity and near-zero leakage power. However, without careful design, the slow write operations of STT-RAM may offset the capacity benefit, and the system may still suffer from contention in the shared LLC and on-chip interconnects. While there are cache optimization techniques to alleviate such problems, we reveal that the true potential of STT-RAM LLC may still be limited because now that the cache hit rate has been improved by the increased capacity, the on-chip network can become a performance bottleneck. CPU and GPU packets contend with each other for the shared network bandwidth. Moreover, the mixed-criticality read/write packets to STT-RAM add another layer of complexity to the network resource allocation. Therefore, being aware of the disparate latency tolerance of CPU/GPU applications and the asymmetric read/write latency of STT-RAM, we propose OSCAR to Orchestrate STT-RAM Caches traffic for heterogeneous ARchitectures. Specifically, an integration of asynchronous batch scheduling and priority based allocation for on-chip interconnect is proposed to maximize the potential of STT-RAM based LLC. Simulation results on a 28-GPU and 14-CPU system demonstrate an average of 17.4% performance improvement for CPUs, 10.8% performance improvement for GPUs, and 28.9% LLC energy saving compared to SRAM based LLC design.|Micro|2016|10.1109/MICRO.2016.7783731|C. Das, G. Loh, J. Zhan, Onur Kayiran, Yuan Xie|3.3333333333333335|2
456|Machine learning design of a trapped-ion quantum spin simulator|Trapped ions have emerged as one of the highest quality platforms for the quantum simulation of interacting spin models of interest to various fields of physics. In such simulators, two effective spins can be made to interact with arbitrary strengths by coupling to the collective vibrational or phonon states of ions, controlled by precisely tuned laser beams. However, the task of determining laser control parameters required for a given spin–spin interaction graph is a type of inverse problem, which can be highly mathematically complex. In this paper, we adapt a modern machine learning technique developed for similar inverse problems to the task of finding the laser control parameters for a number of interaction graphs. We demonstrate that typical graphs, forming regular lattices of interest to physicists, can easily be produced for up to 50 ions using a single GPU workstation. The scaling of the machine learning method suggests that this can be expanded to hundreds of ions with moderate additional computational effort.|Quantum Science and Technology|2019|10.1088/2058-9565/ab657a|R. Melko, R. Islam, Y. Teoh, M. Drygala|3.3333333333333335|2
875|Effusion Rates on Mt. Etna and Their Influence on Lava Flow Hazard Assessment|The rate at which lava is discharged plays a key role in controlling the distance covered by lava flows from eruptive vents. We investigate the available time-averaged discharge rates (TADRs) estimated for recent flank eruptions at Mt. Etna volcano (Italy), in order to define a possible generalized effusion rate trend which is consistent with observed real data. Our analysis indicates a rapid waxing phase in which effusion rate peaks occur for between 0.5 and 29% of the total eruption time, followed by a progressive decrease in the waning phase. Three generalized curves are built by calculating the 25th, 50th and 75th percentiles values associated with the occurrence of effusion peaks, and with the slope variations of descending curves in the waning phase. The obtained curves are used as an input for the GPUFLOW model in order to perform numerical simulations of the lava flows paths on inclined planes, and are compared with those generated by using effusion rate curves with a bell-shaped time-distribution. Our tests show how these characteristic curves could impact single-vent scenarios, as well as short- and long-term hazard maps, with maximum variations of up to 40% for a specific category of eruptive events.|Remote Sensing|2022|10.3390/rs14061366|A. Cappello, F. Zuccarello, G. Bilotta, G. Ganci|3.3333333333333335|2
1105|The ESCAPE project: Energy-efficient Scalable Algorithms for Weather Prediction at Exascale|Abstract. In the simulation of complex multi-scale flows arising in weather and climate modelling, one of the biggest challenges is to satisfy strict service requirements in terms of time to solution and to satisfy budgetary constraints in terms of energy to solution, without compromising the accuracy and stability of the application. These simulations require algorithms that minimise the energy footprint along with the time required to produce a solution, maintain the physically required level of accuracy, are numerically stable, and are resilient in case of hardware failure. The European Centre for Medium-Range Weather Forecasts (ECMWF) led the ESCAPE (Energy-efficient Scalable Algorithms for Weather Prediction at Exascale) project, funded by Horizon 2020 (H2020) under the FET-HPC (Future and Emerging Technologies in High Performance Computing) initiative. The goal of ESCAPE was to develop a sustainable strategy to evolve weather and climate prediction models to next-generation computing technologies. The project partners incorporate the expertise of leading European regional forecasting consortia, university research, experienced high-performance computing centres, and hardware vendors. This paper presents an overview of the ESCAPE strategy: (i) identify domain-specific key algorithmic motifs in weather prediction and climate models (which we term Weather & Climate Dwarfs), (ii) categorise them in terms of computational and communication patterns while (iii) adapting them to different hardware architectures with alternative programming models, (iv) analyse the challenges in optimising, and (v) find alternative algorithms for the same scheme. The participating weather prediction models are the following: IFS (Integrated Forecasting System); ALARO, a combination of AROME (Application de la Recherche à l'Opérationnel à Meso-Echelle) and ALADIN (Aire Limitée Adaptation Dynamique Développement International); and COSMO–EULAG, a combination of COSMO (Consortium for Small-scale Modeling) and EULAG (Eulerian and semi-Lagrangian fluid solver). For many of the weather and climate dwarfs ESCAPE provides prototype implementations on different hardware architectures (mainly Intel Skylake CPUs, NVIDIA GPUs, Intel Xeon Phi, Optalysys optical processor) with different programming models. The spectral transform dwarf represents a detailed example of the co-design cycle of an ESCAPE dwarf. The dwarf concept has proven to be extremely useful for the rapid prototyping of alternative algorithms and their interaction with hardware; e.g. the use of a domain-specific language (DSL). Manual adaptations have led to substantial accelerations of key algorithms in numerical weather prediction (NWP) but are not a general recipe for the performance portability of complex NWP models. Existing DSLs are found to require further evolution but are promising tools for achieving the latter. Measurements of energy and time to solution suggest that a future focus needs to be on exploiting the simultaneous use of all available resources in hybrid CPU–GPU arrangements.\n|Geoscientific Model Development|2019|10.5194/GMD-12-4425-2019|K. P. Nielsen, Pawel Spychala, Oliver Fuhrer, Per Berg, M. Hamrud, M. Ciznicki, M. Glinton, P. Marguinaud, P. Smolarkiewicz, M. Diamantakis, Alan Gray, A. Wyszogrodzki, G. Mozdzynski, Marcin Procyk, Enda O'Brien, E. Raffin, P. Termonia, J. Van Bever, Oisín Robinson, Michael Lysaght, Louis Douriez, J. Szmelter, Daniel Thiemert, Charles Colavolpe, Zbigniew P. Piotrowski, C. Kühnlein, Alex Macfaden, Nils Wedi, Michael Lange, M. Gillard, Xavier Vigouroux, Pierre Bénard, Peter Bauer, W. Deconinck, Jacob Poulsen, Wojciech Piątek, Yongjun Zheng, Michael Baldauf, Nick New, Valentin Clement, C. Mazauric, D. Guibert, Carlos Osuna, Krzysztof Kurowski, Fabrice Voitus, B. Bosak, G. Smet, Parijat Shukla, Alastair McKinstry, Marek Blazewicz, D. Degrauwe, Sarah-Jane Lock, Peter Messmer, Andreas Müller, G. Mengaldo, Sebastian Ciesielski, M. Kulczewski, B. Sass, Sami Saarinen|3.3333333333333335|2
742|GPU-accelerated Monte Carlo simulations of anisotropic Heisenberg ferromagnets|The Monte Carlo method is a powerful technique for computing thermodynamic magnetic states of otherwise unsolvable spin Hamiltonians, but the method becomes computationally prohibitive with increasing number of spins and the simulation of real materials and nanostructures is cumbersome. This paper presents the acceleration of Monte Carlo simulations of the three-dimensional anisotropic Heisenberg model on Graphics-Processing Units (GPU). The GPU implementation of the method presented here provides an acceleration of two orders of magnitude over conventional implementations and enables the simulation of large systems, with any crystal lattice, containing up to $10^8$ spins on a single GPU. This offers the possibility to simulate complex structures and devices that are hundreds of nanometers in size in order to compute their magnetic state at finite temperature with atomistic resolution.||2021|10.1016/j.matpr.2020.11.392|M. Charilaou|3.25|2
759|Virtual Reality Design and Realization of Interactive Garden Landscape|It is very important to study and explore the application of virtual reality technology in landscape garden design, especially in the current environment of triple network integration and Internet of Things construction, to promote and facilitate the rapid development of digital landscape garden design in China. In this paper, we study the implementation method of virtual landscape gardening system and establish a virtual environment based on the ancient city of Yangcheng. On the computer platform, we study and realize a virtual roaming system of medium complexity with more complete roaming functions. Using the Quest3D software platform, a desktop-type virtual garden simulation system was developed, focusing on the virtual reality modeling technology method and virtual system implementation. The experimental results show that the GPU-accelerated drawing method based on GLSL can significantly improve the drawing frame rate of 3D garden landscape vegetation scenes with a small amount of scene data and has a certain feasibility. Based on the OpenSceneGraph (OSG) graphics rendering engine, the visualization of various types of 3D landscape models is realized, and the spatial layout of various types of landscape with parametric control is realized through digital vector layers, which flexibly manage and organize various garden elements and reasonably organize the spatial topological relationship between various types of landscape in 3D space. By integrating cross-platform ArcGISEngine components, the basic data of garden scenes including terrain data and vector data are managed. Through scene view cropping and hierarchical detail modeling technologies, the drawing efficiency and rendering real time of the garden landscape are improved. It realizes interactive 3D scene browsing and provides a six-degree-of-freedom all-round display of the overall landscape.|Complex|2021|10.1155/2021/6083655|Xuanfeng Zhang, QuanQi, Song Yan|3.25|2
1458|Advanced Disaster Simulation Researches on Earthquakes and Tsunamis using High Performance Computing System ‘Kei’ Part2|‘Kei’ computer is one of the highest computing system in the world. Using ‘Kei’ computer, we are performing the advanced simulation for disaster mitigation by earthquakes and tsunamis in a project ‘Study for Advancement of Prediction Accuracy on Earthquake and Tsunami’. In this research project, we have three research ﬁelds as Earthquake simulation research ﬁeld, Tsunami research ﬁled and Damage estimation research ﬁeld. In Earthquake simulation research ﬁeld, we are developing the scenario simulations of earthquake recurrences on the subduct-ing plate around Japan. As other earthquake simulation researches, we are simulating seismic waves based on the scenarios, and the underground structures using seismographs. The second research simulation research ﬁeld on Tsunami hazard, we are developing applications for the simulating tsunami damages at East Japan earthquake 2011. In this research ﬁeld, not only damage simulations, but also we are developing the early tsunami detection system using simulation and real time data. Finally, we will apply it to the Nankai trough seismogenic zone and etc. The third research ﬁeld is the civil engineering research as the adavanced civil engineering structural analyses, seismic response analyses on large scale cities, and agent simulation for more precise and practical evacuations. Finally, we will integrate these research ﬁelds in this project for the seismic simulator on disaster mitigation. Large-scale high ﬁdelity model can be constructed with recent accumulation of spatial data. Although computational cost of earthquake simulations using such model is huge, supercomputer (e.g. K computer) is now resolving difﬁculties and creating new frontier in this ﬁeld. In this presentation, recent illustrative examples (crust deformation, earthquake ground motion, soil ampliﬁcation, city response etc.) will be shown. Large-scale parallel computing is important for numerically reproducing actual measurement results and dynamics of phenom-ena in various science and engineering areas, such as civil engineering, bioengineering, and earth sciences. The computational performance of parallelized software tools plays a critical role in such simulation studies, as these improve the computational accuracy relative to the simulation resolution within a limited computation time. Recent massively parallel computer systems based on shared- and distributed-memory architectures employ various types of arithmetic processors. Current processor designs are known to exhibit totally different computational performance depending on the numerical algorithms and implementation methods employed. Currently, parallel computing generally uses either a multi-core CPU, graphics processing unit (GPU), or many integrated core (MIC) processor. Multi-core CPUs have traditionally been used in high-performance computing, whereas GPUs were originally designed for computer graphics with many arithmetic cores. The common progress of current processor designs is the increase in the number of cores using vector operations such as single-instruction?multiple-data (SIMD). In such a situation, the shared-memory parallelization plays a basic but critical role in dealing with the increasing number of arithmetic cores in an efﬁcient manner. Numerical simulation methods used in science and engineering include the ﬁnite difference method (FDM), ﬁnite element method (FEM), ﬁnite volume method (FVM), boundary element method (BEM), and particle simulation method (PSM). Among these, PSM has a beneﬁt of being mesh-free, allowing the computation of large-scale deformations and fractures of a continuum body without expensive remeshing tasks. As a PSM, smoothed particle hydrodynamics (SPH) is often used for tsunami disaster simulations because of its robustness in free-surface ﬂuid dynamics. The discrete element method (DEM) is one popular PSM for granular dynamics in which geometrical size and shape attributes are provided for each particle. In the most conventional formulation of the DEM, the Voigt model in both the normal and tangential directions is considered at each contact point. In the tangential direction, Coulomb friction is introduced to determine the maximum tangential force and the slip condition. In addition, rolling friction can be considered at the contact points. Therefore, the DEM is attractive to simulate granular materials such as sand, pebbles, and other grains. However, PSM programs must be implemented carefully to avoid write-access conﬂicts under shared-memory parallelization, especially when calculating a resultant force. In addition, it is important for distributed-memory parallelization to dynamically balance the computational load between computational nodes. To address these issues, we have proposed parallel algorithms that use the action-reaction law and parallelize the interaction summation with a reference table to avoid memory access conﬂicts. We have also implemented the algorithm of dynamic load balancing by resizing the domain decomposition region. Our methods were implemented on various parallel processors such as GPU, MIC processor, multi-core CPU on K computer, and vector processor on Earth simulator. In this presentation, we will talk about these parallel algorithms and applications for contributing to human society; Tsunami disaster simulations in consideration of structures?soil?ﬂuid interactions and impact dynamics of ballast particles in rail track are important topics that require a high performance computing resources. The boundary integral equation method (BIEM) is a powerful tool to analyze the earthquake rupture dynamics on non-planar faults. The non-planar fault analysis requires of the boundary integral equations (BIEs) that they are formulated in the real space and time domain, while those formulate in the spectral domain are limited to the application of the planar fault geometry. However BIEM in the space-time domain has extremely large numerical costs. Due to such large costs particular for the memory requirement, efﬁcient use of the memory storage of the integration kernel have not be possible. In this study, we develop a new method to reduce the calculation time and memory requirement greatly without degrading the accuracy in 3-D. We extend the method proposed by Ando et al. (2007) in 2-D. This method divide the causality cone appealing in the integration kernel to the domains related to the wave fronts, the near-ﬁeld term and the static term. We implement the algorithm on K-computer, and demonstrate the memory storage of the integration kernel becomes possible on the currently available computational environment owing to the reduced memory requirement. This contributes the efﬁciency of the numerical analysis considerably. For example, by using the same 6400 nodes, the analysis of the model consisting of 160 thousands fault elements and 1600 time steps took about a half year with the original method, however it is reduced to about two hours with the current efﬁcient method. The current method is also shown to be scalable on distributed memory environment to the scale of these nodes. This method is expected to break through the emerging limitations of the dynamic earthquake rupture simulations with realistic 3-D geometrical models, and will contribute to widen the spectrum of the applicational works using the dynamic simulations. The Great East Japan Earthquake of 2011 has shown that a tsunami disaster is not limited to inundation damage in a speciﬁed region, but destruction over a wide area can cause a severe disaster. Because various structures stand on the land, in order to evaluate damage to these structures, it is necessary to perform highly precise evaluations of three-dimensional ﬂuid motion. But the calculation cost of high precision three-dimensional ﬂuid analysis is very high. So the goals of this research were to develop a method of coupling STOC (Tomita et al., 2005) and CADMAS-SURF/3D (Arikawa et al., 2005) to establish a method of efﬁciently calculating every stage from wave source to runup, and to verify its applicability. Summing up shows that in order to improve overall calculation speed, as long as this method is adopted, the calculation domain of STOC-ML is as small as possible and CS3D is as large as possible, and it is important the number of calculation nodes be increased to the level eliminating synchronicity standby state. Under this condition, calculation time is about 2s/step, and if the mesh is 1m wide, the time interval is an average of about 0.004s/step, so in order to calculate integration time of 1s, about 500s are necessary. The coupling simulator with structure analysis is also shown. The breakwater under tsunami overﬂow was reproduced. Finally, the issues of the future tsunami simulation will be discussed. A numerical implementation based on a Graphics Processing Unit (GPU) is proposed for the acceleration of the two-phase simulation using Lattice Boltzmann Method (LBM). The LBM yields regular, data-parallel computations; therefore, it is especially well ﬁtted to GPU calculations. This study focuses on the application of the LBM for ﬂuid displacement computations in real rock sample. For this purpose, the digital rock model is reconstructed from the micro-CT scanned images of reservoir sample with a resolution of 2.0 um. In order to obtain reliable and accurate results from the developed numerical model, the computational domain must be large enough to cover the representative element size (REV) of sample rock. As a result, pore scale LBM simulation of multiphase porous medium systems with sufﬁcient resolution and large grid-number are very compu-tationally challenging. To achieve this extremely large-scale simulation, multi-GPU parallel scheme by using CUDA and MPI is developed. Careful optimizations include sparse storage scheme, efﬁcient domain decomposition and non-blocking commu-nication are desired for algorithm implementation. Finally, we succee||2016|10.1016/j.tecto.2016.05.012|M. Hori, T. Furumura, Y. Kaneda, F. Imamura|3.2222222222222223|2
572|A GPU-Based Quantum Annealing Simulator for Fully-Connected Ising Models Utilizing Spatial and Temporal Parallelism|Simulated quantum annealing (SQA) is a probabilistic approximation method to find a solution for a combinatorial optimization problem using digital computers. The processing time of SQA increases exponentially with the number of variables. Therefore, acceleration of SQA is regarded as a very important topic. However, parallel implementation is difficult due to the serial nature of the quantum Monte Carlo algorithm used in SQA. In this paper, we propose a method to implement SQA in parallel on a GPU while preserving the data dependency. According to the experimental results, we have achieved over 97 times speed-up while maintaining the same accuracy-level compared to a single-core CPU implementation.|IEEE Access|2020|10.1109/ACCESS.2020.2985699|H. M. Waidyasooriya, M. Hariyama|3.2|2
1518|A New Deformation Model of Brain Tissues for Neurosurgical Simulation|An accurate and realistic brain tissue deformation model with real-time performance is very important for virtual neurosurgical simulation. In this paper, a new finite element method (FEM) brain tissue deformation model, which is based on the optimization implicit Euler method, is introduced. Biomechanical properties of brain tissue such as anisotropy and viscoelasticity are incorporated into the model, which provides more accurate and realistic imitation of the deformation of brain tissue. A descent method with GPU-based implementation is used to solve the optimization problem, which makes it possible to achieve a high degree of computational efficiency. Simulation results show that both the anisotropic and viscoelastic behaviors are presented in the deformation model. The GPU-based implementation of the proposed model significantly improves the computational efficiency over CPU-based FEM models with the implicit integration scheme. Moreover, the result of the proposed model converges to the exact solution of implicit Euler integration after 96 iterations. The proposed model was implemented on the development of a neurosurgical simulator. A relative high degree of realistic brain tissue deformation was rendered at a refreshment rate of 32.5 frames/s on a regular PC.|IEEE Transactions on Instrumentation and Measurement|2020|10.1109/TIM.2019.2909247|Shichao Liu, Wenguo Hou, Minhua Zheng, P. X. Liu|3.2|2
1304|Efficient Tsunami Modeling on Adaptive Grids with Graphics Processing Units (GPUs)|Solving the shallow water equations efficiently is critical to the study of natural hazards induced by tsunami and storm surge, since it provides more response time in an early warning system and allows more runs to be done for probabilistic assessment where thousands of runs may be required. Using Adaptive Mesh Refinement (AMR) speeds up the process by greatly reducing computational demands, while accelerating the code using the Graphics Processing Unit (GPU) does so through using faster hardware. Combining both, we present an efficient CUDA implementation of GeoClaw, an open source Godunov-type high-resolution finite volume numerical scheme on adaptive grids for shallow water system with varying topography. The use of AMR and spherical coordinates allows modeling transoceanic tsunami simulation. Numerical experiments on several realistic tsunami modeling problems illustrate the correctness and efficiency of the code, which implements a simplified dimensionally-split version of the algorithms. This implementation is shown to be accurate and faster than the original when using CPUs alone. The GPU implementation, when running on a single GPU, is observed to be 3.6 to 6.4 times faster than the original model running in parallel on a 16-core CPU. Three metrics are proposed to evaluate relative performance of the model, which shows efficient usage of hardware resources.|arXiv.org|2019|10.1029/2019ms001635|M. Motley, R. LeVeque, Xinsheng Qin|3.1666666666666665|2
1608|Accelerating an Adaptive Mesh Refinement Code for Depth‐Averaged Flows Using GPUs|Solving the shallow water equations efficiently is critical to the study of natural hazards induced by tsunami and storm surge, since it provides more response time in an early warning system and allows more runs to be done for probabilistic assessment where thousands of runs may be required. Using adaptive mesh refinement speeds up the process by greatly reducing computational demands while accelerating the code using the graphics processing unit (GPU) does so through using faster hardware. Combining both, we present an efficient CUDA implementation of GeoClaw, an open source Godunov‐type high‐resolution finite volume numerical scheme on adaptive grids for shallow water system with varying topography. The use of adaptive mesh refinement and spherical coordinates allows modeling transoceanic tsunami simulation. Numerical experiments on the 2011 Japan tsunami and a local tsunami triggered by a hypothetical Mw 7.3 earthquake on the Seattle Fault illustrate the correctness and efficiency of the code, which implements a simplified dimensionally split version of the algorithms. Both numerical simulations are conducted on subregions on a sphere with adaptive grids that adequately resolve the propagating waves. The implementation is shown to be accurate and faster than the original when using Central Processing Units (CPUs) alone. The GPU implementation, when running on a single GPU, is observed to be 3.6 to 6.4 times faster than the original model running in parallel on a 16‐core CPU. Three metrics are proposed to evaluate relative performance of the model, which shows efficient usage of hardware resources.|Journal of Advances in Modeling Earth Systems|2019|10.1029/2019MS001635|M. Motley, R. LeVeque, Xinsheng Qin|3.1666666666666665|2
319|Multiscale modeling of layer formation in epidermis|The mammalian skin epidermis is a stratified epithelium composed of multiple layers of epithelial cells that exist in appropriate sizes and proportions, and with distinct boundaries separating each other. How the epidermis develops from a single layer of committed precursor cells to form a complex multilayered structure of multiple cell types remains elusive. Here, we construct stochastic, three-dimensional, and multiscale models consisting of a lineage of multiple cell types to study the control of epidermal development. Symmetric and asymmetric cell divisions, stochastic cell fate transitions within the lineage, extracellular morphogens, cell-to-cell adhesion forces, and cell signaling are included in model. A GPU algorithm was developed and implemented to accelerate the simulations. These simulations show that a balance between cell proliferation and differentiation during lineage progression is crucial for the development and maintenance of the epidermal tissue. We also find that selective intercellular adhesion is critical to sharpening the boundary between layers and to the formation of a highly ordered structure. The long-range action of a morphogen provides additional feedback regulations, enhancing the robustness of overall layer formation. Our model is built upon previous experimental findings revealing the role of Ovol transcription factors in regulating epidermal development. Direct comparisons of experimental and simulation perturbations show remarkable consistency. Taken together, our results highlight the major determinants of a well-stratified epidermis: balanced proliferation and differentiation, and a combination of both short- (symmetric/asymmetric division and selective cell adhesion) and long-range (morphogen) regulations. These underlying principles have broad implications for other developmental or regenerative processes leading to the formation of multilayered tissue structures, as well as for pathological processes such as epidermal wound healing.|PLoS Comput. Biol.|2018|10.1371/journal.pcbi.1006006|Q. Nie, X. Dai, D. Haensel, Briana Lee, H. Du, Yangyang Wang|3.142857142857143|2
1211|Through-the-Lens Drone Filming|Aerial filming in action scenes using a drone is difficult for inexperienced flyers because manipulating a remote controller and meeting the desired image composition are two independent, while concurrent, tasks. Existing systems attempt to utilize wearable GPS-based or infrared-based sensors to track the human movement and to assist in capturing footage. However, these sensors work only in either indoor (infrared-based) or outdoor environments (GPS-based), but not both. In this paper, we introduce a novel drone filming system which integrates monocular 3D human pose estimation and localization into a drone platform to remove the constraints imposed by wearable-sensor-based solutions. Meanwhile, given the estimated position, we propose a novel drone control system, called “through-the-lens drone filming”, to allow a cameraman to conveniently control the drone by manipulating a 3D model in the preview, which closes the gap between the flight control and the viewpoint design. Our system includes two key enabling techniques: 1) subject localization based on visual-inertial fusion, and 2) through-the-lens camera planning. This is the first drone camera system which allows users to capture human actions by manipulating the camera in a virtual environment. From the drone hardware, we integrate a gimbal camera and two GPUs into the limited space of a drone and demonstrate the feasibility of running the entire system onboard with insignificant delays, which are sufficient for filming in our real-time application. Experimental results, in both simulation and real-world scenarios, demonstrate that our techniques can greatly ease camera control and capture better videos.|IEEE/RJS International Conference on Intelligent RObots and Systems|2018|10.1109/IROS.2018.8594333|Xin Yang, Zhenyu Yang, Chong Huang, Peng Chen, Yan Kong, K. Cheng|3.142857142857143|2
1223|Assimilating Radial Distribution Functions To Build Water Models with Improved Structural Properties|The structural properties of three- and four-site water models are improved by extending the ForceBalance parametrization code to include a new methodology allowing for the targeting of any radial distribution function (RDF) during the parametrization of a force field. The mean squared difference (MSD) between the experimental and simulated RDFs contributes to an objective function, allowing for the systematic optimization of force field parameters to reach closer overall agreement with experiment. RDF fitting is applied to develop modified versions of the TIP3P and TIP4P/2005 water models in which the Lennard-Jones potential is replaced by a Buckingham potential. The optimized TIP3P-Buckingham and TIP4P-Buckingham potentials feature 93 and 98% lower MSDs in the OO RDF compared to the TIP3P and TIP4P/2005 models respectively, with marked decreases in the height of the first peak. Additionally, these Buckingham models predict the entropy of water more accurately, reducing the error in the entropy of TIP3P from 11 to 3% and the error in the entropy of TIP4P/2005 from 11 to 2%. These new Buckingham models have improved predictive power for many nonfitted properties particularly in the case of TIP3P. Our work directly demonstrates how the Buckingham potential can improve the description of water's structural properties beyond the Lennard-Jones potential. Moreover, adding a Buckingham potential is a favorable alternative to adding interaction sites in terms of computational speed on modern GPU hardware.|Journal of Chemical Information and Modeling|2018|10.1021/acs.jcim.8b00166|Lee‐Ping Wang, D. Huggins, A. Wade|3.142857142857143|2
1411|Analysis Of High Performance Scientific Programming Workflows|Substantial time is spent on building, optimizing and maintaining large-scale software that is run on supercomputers. However, little has been done to utilize overall resources efficiently when it comes to including expensive human resources. The community is beginning to acknowledge that optimizing the hardware performance such as speed and memory bottlenecks contributes less to the overall productivity than does the development lifecycle of high-performance scientific applications. Researchers are beginning to look at overall scientific workflows for high performance computing. Scientific programming productivity is measured by time and effort required to develop, configure, and maintain a simulation experiment and its constituent parts, together with the time to get to the solution when the programs are executed. There is no systematic framework by means of which scientific programming productivity of the available tools can be evaluated. We propose an evaluation approach that compares programming workflows to identify productivity bottlenecks and suboptimal paths as well as productivity gains. Based on a set of predefined criteria we can evaluate both short-term and long-term productivity criteria. We use these results to suggest improvements to the programming environment or tools. This thesis includes three studies of scientific programming workflows: 1) We apply our evaluation approach to two case studies involving the use of numerical libraries. 2) We evaluate GPU programming models using software engineering complexity metrics. 3) We evaluate use of a high level directive based programming model and a source-to-source compiler with respect to productivity of programming FPGAs using a computational chemistry code. We compare the programmability and performance of the FPGA port with the GPU port of the same code.||2018|10.1109/mcg.2018.011461525|W. Klaassen|3.142857142857143|2
403|Optimization of lattice Boltzmann simulations on heterogeneous computers|High-performance computing systems are more and more often based on accelerators. Computing applications targeting those systems often follow a host-driven approach, in which hosts offload almost all compute-intensive sections of the code onto accelerators; this approach only marginally exploits the computational resources available on the host CPUs, limiting overall performances. The obvious step forward is to run compute-intensive kernels in a concurrent and balanced way on both hosts and accelerators. In this paper, we consider exactly this problem for a class of applications based on lattice Boltzmann methods, widely used in computational fluid dynamics. Our goal is to develop just one program, portable and able to run efficiently on several different combinations of hosts and accelerators. To reach this goal, we define common data layouts enabling the code to exploit the different parallel and vector options of the various accelerators efficiently, and matching the possibly different requirements of the compute-bound and memory-bound kernels of the application. We also define models and metrics that predict the best partitioning of workloads among host and accelerator, and the optimally achievable overall performance level. We test the performance of our codes and their scaling properties using, as testbeds, HPC clusters incorporating different accelerators: Intel Xeon Phi many-core processors, NVIDIA GPUs, and AMD GPUs.|The international journal of high performance computing applications|2017|10.1177/1094342017703771|A. Gabbana, R. Tripiccione, S. Schifano, E. Calore|3.125|2
357|Scheduling Independent Moldable Tasks on Multi-Cores with GPUs|"We present a new approach for scheduling independent tasks on multiple CPUs and multiple GPUs. The tasks are assumed to be parallelizable on CPUs using the moldable model: the final number of cores allotted to a task can be decided and set by the scheduler. More precisely, we design an algorithm aiming at minimizing the makespan—the maximum completion time of all tasks—for this scheduling problem. The proposed algorithm combines a dual approximation scheme with a fast integer linear program (ILP). It determines both the partitioning of the tasks, i.e., whether a task should be mapped to CPUs or a GPU, and the number of CPUs allotted to a moldable task if mapped to the CPUs. A worst-case analysis shows that the algorithm has an approximation ratio of <inline-formula><tex-math notation=""LaTeX""> $\frac{3}{2} + \epsilon$</tex-math><alternatives><inline-graphic xlink:href=""trystram-ieq1-2675891.gif""/></alternatives> </inline-formula>. Since the time complexity of the ILP-based algorithm could be non-polynomial, we also present a polynomial-time algorithm with an approximation ratio of <inline-formula><tex-math notation=""LaTeX"">$2+\epsilon$ </tex-math><alternatives><inline-graphic xlink:href=""trystram-ieq2-2675891.gif""/></alternatives></inline-formula>. We complement the theoretical analysis of our two novel algorithms with a simulation study. In these simulations, we compare our algorithms to a modified version of the classical HEFT algorithm, which we adapted to handle moldable tasks. The simulation results show that our algorithm with the <inline-formula><tex-math notation=""LaTeX""> $\left(\frac{3}{2} + \epsilon \right)$</tex-math><alternatives><inline-graphic xlink:href=""trystram-ieq3-2675891.gif""/> </alternatives></inline-formula>-approximation ratio produces significantly shorter schedules than the modified HEFT for most of the instances. In addition, our results provide evidence that our ILP-based algorithm can solve larger problem instances in a reasonable amount of time."|IEEE Transactions on Parallel and Distributed Systems|2017|10.1109/TPDS.2017.2675891|G. Mounié, Florence Monna, S. Hunold, S. Kedad-Sidhoum, Raphaël Bleuse, D. Trystram|3.0|2
626|LOC program for line radiative transfer|Context. Radiative transfer (RT) modelling is part of many astrophysical simulations. It is used to make synthetic observations and to assist the analysis of observations. We concentrate on modelling the radio lines emitted by the interstellar medium. In connection with high-resolution models, this can be a significant computationally challenge.\nAims. Our aim is to provide a line RT program that makes good use of multi-core central processing units (CPUs) and graphics processing units (GPUs). Parallelisation is essential to speed up computations and to enable large modelling tasks with personal computers.\nMethods. The program LOC is based on ray-tracing (i.e. not Monte Carlo) and uses standard accelerated lambda iteration methods for faster convergence. The program works on 1D and 3D grids. The 1D version makes use of symmetries to speed up the RT calculations. The 3D version works with octree grids, and to enable calculations with large models, is optimised for low memory usage.\nResults. Tests show that LOC results agree with other RT codes to within ∼2%. This is typical of code-to-code differences, which are often related to different interpretations of the model set-up. LOC run times compare favourably especially with those of Monte Carlo codes. In 1D tests, LOC runs were faster by up to a factor ∼20 on a GPU than on a single CPU core. In spite of the complex path calculations, a speed-up of up to ∼10 was also observed for 3D models using octree discretisation. GPUs enable calculations of models with hundreds of millions of cells, as are encountered in the context of large-scale simulations of interstellar clouds.\nConclusions. LOC shows good performance and accuracy and is able to handle many RT modelling tasks on personal computers. It is written in Python, with only the computing-intensive parts implemented as compiled OpenCL kernels. It can therefore also a serve as a platform for further experimentation with alternative RT implementation details.|Astronomy & Astrophysics|2020|10.1051/0004-6361/202039456|M. Juvela|3.0|2
681|Autoferry Gemini: a real-time simulation platform for electromagnetic radiation sensors on autonomous ships|Testing that ships are compliant to specified safety requirements have traditionally relied on real world data, which is not scalable and limited to testable scenarios due to financial and ethical reasons. Low fidelity simulations have been used to counteract some of these problems, which is sufficient for emulating simpler systems such as radar detectors, but not for testing complex systems as found in computer vision. In the automotive industry the use of game engines have shown to be a great testing platform due to their customizability, and combination of real-time physics with computer graphics to create large volumes of high fidelity images. In the work presented here, development of an open-source maritime platform named Autoferry Gemini based on the Unity game engine is used to simulate sensors in real-time. Utilizing simulated optics and general purpose GPU programs, the render pipeline is capable of modeling lidar, radar, visible-light and infrared camera sensors simultaneously. Results from visible-light cameras and lidar have already proven to satisfy other research activities on sensor fusion for autonomous ship technology. Infrared cameras motivates further research in gathering empirical data, while GPU algorithms have made it possible to simulate 3D radar models and multiple lidar types in real-time.|IOP Conference Series: Materials Science and Engineering|2020|10.1088/1757-899X/929/1/012032|E. Eide, E. Brekke, R. Mester, K. Vasstein|3.0|2
708|Improved Hopfield Network Optimization Using Manufacturable Three-Terminal Electronic Synapses|We illustrate novel optimization techniques via simulations for Hopfield networks constructed from manufacturable three-terminal Silicon-Oxide-Nitride-Oxide-Silicon (SONOS) synaptic circuit elements. We first present a computationally-light, memristor-based, highly accurate static compact model for the SONOS synapses used in our simulations. We then show how to exploit analog errors in programming resistances and current leakage, and the continuous tunability of the SONOS synapses to enable transient chaotic group dynamics, to accelerate the convergence of a Hopfield network. We project improvements in energy consumption and time to solution relative to existing CPUs and GPUs by at least 4 orders of magnitude, and also exceed the projected performance of two-terminal memristor-based crossbars in addition to a 100-fold increase in error-resilient array size (i.e. problem size).|IEEE Transactions on Circuits and Systems Part 1: Regular Papers|2021|10.1109/tcsi.2021.3119648|Suhas Kumar, R. S. Williams, Su-in Yi|3.0|2
723|Fast linking numbers for topology verification of loopy structures|It is increasingly common to model, simulate, and process complex materials based on loopy structures, such as in yarn-level cloth garments, which possess topological constraints between inter-looping curves. While the input model may satisfy specific topological linkages between pairs of closed loops, subsequent processing may violate those topological conditions. In this paper, we explore a family of methods for efficiently computing and verifying linking numbers between closed curves, and apply these to applications in geometry processing, animation, and simulation, so as to verify that topological invariants are preserved during and after processing of the input models. Our method has three stages: (1) we identify potentially interacting loop-loop pairs, then (2) carefully discretize each loop's spline curves into line segments so as to enable (3) efficient linking number evaluation using accelerated kernels based on either counting projected segment-segment crossings, or by evaluating the Gauss linking integral using direct or fast summation methods (Barnes-Hut or fast multipole methods). We evaluate CPU and GPU implementations of these methods on a suite of test problems, including yarn-level cloth and chainmail, that involve significant processing: physics-based relaxation and animation, user-modeled deformations, curve compression and reparameterization. We show that topology errors can be efficiently identified to enable more robust processing of loopy structures.|ACM Transactions on Graphics|2021|10.1145/3450626.3459778|Doug L. James, Ante Qu|3.0|2
725|Modeling and Analysis of Cardiac Hybrid Cellular Automata via GPU-Accelerated Monte Carlo Simulation|The heart consists of a complex network of billions of cells. Under physiological conditions, cardiac cells propagate electrical signals in space, generating the heartbeat in a synchronous and coordinated manner. When such a synchronization fails, life-threatening events can arise. The inherent complexity of the underlying nonlinear dynamics and the large number of biological components involved make the modeling and the analysis of electrophysiological properties in cardiac tissue still an open challenge. We consider here a Hybrid Cellular Automata (HCA) approach modeling the cardiac cell-cell membrane resistance with a free variable. We show that the modeling approach can reproduce important and complex spatiotemporal properties paving the ground for promising future applications. We show how GPU-based technology can considerably accelerate the simulation and the analysis. Furthermore, we study the cardiac behavior within a unidimensional domain considering inhomogeneous resistance and we perform a Monte Carlo analysis to evaluate our approach.||2021|10.3390/MATH9020164|Lilly Maria Treml, A. Gizzi, E. Bartocci|3.0|2
736|Strong-lensing source reconstruction with variationally optimised Gaussian processes|Strong-lensing images provide a wealth of information both about the magnified source and about the dark matter distribution in the lens. Precision analyses of these images can be used to constrain the nature of dark matter. However, this requires high-fidelity image reconstructions and careful treatment of the uncertainties of both lens mass distribution and source light, which are typically difficult to quantify. In anticipation of future high-resolution datasets, in this work we leverage a range of recent developments in machine learning to develop a new Bayesian strong-lensing image analysis pipeline. Its highlights are: (A) a fast, GPU-enabled, end-to-end differentiable strong-lensing image simulator; (B) a new, statistically principled source model based on a computationally highly efficient approximation to Gaussian processes that also takes into account pixellation; and (C) a scalable variational inference framework that enables simultaneously deriving posteriors for tens of thousands of lens and source parameters and optimising hyperparameters via stochastic gradient descent. Besides efficient and accurate parameter estimation and lens model uncertainty quantification, the main aim of the pipeline is the generation of training data for targeted simulation-based inference of dark matter substructure, which we will exploit in a companion paper.||2021|10.1093/mnras/stac311|K. Karchev, A. Coogan, C. Weniger|3.0|2
886|Efficient Pipeline Planning for Expedited Distributed DNN Training|To train modern large DNN models, pipeline parallelism has recently emerged, which distributes the model across GPUs and enables different devices to process different microbatches in pipeline. Earlier pipeline designs allow multiple versions of model parameters to co-exist (similar to asynchronous training), and cannot ensure the same model convergence and accuracy performance as without pipelining. Synchronous pipelining has recently been proposed which ensures model performance by enforcing a synchronization barrier between training iterations. Nonetheless, the synchronization barrier requires waiting for gradient aggregation from all microbatches and thus delays the training progress. Optimized pipeline planning is needed to minimize such wait and hence the training time, which has not been well studied in the literature. This paper designs efficient, near-optimal algorithms for expediting synchronous pipeline-parallel training of modern large DNNs over arbitrary inter-GPU connectivity. Our algorithm framework comprises two components: a pipeline partition and device mapping algorithm, and a pipeline scheduler that decides processing order of microbatches over the partitions, which together minimize the per-iteration training time. We conduct thorough theoretical analysis, extensive testbed experiments and trace-driven simulation, and demonstrate our scheme can accelerate training up to 157% compared with state-of-the-art designs.|IEEE Conference on Computer Communications|2022|10.1109/INFOCOM48880.2022.9796787|Shiqing Fan, Guoping Long, Wei Lin, Xiaodong Yi, Ziyue Luo, Jun Yang, Chuan Wu|3.0|2
950|Tissue Forge: Interactive biological and biophysics simulation environment|Tissue Forge is an open-source interactive environment for particle-based physics, chemistry and biology modeling and simulation. Tissue Forge allows users to create, simulate and explore models and virtual experiments based on soft condensed matter physics at multiple scales, from the molecular to the multicellular, using a simple, consistent interface. While Tissue Forge is designed to simplify solving problems in complex subcellular, cellular and tissue biophysics, it supports applications ranging from classic molecular dynamics to agent-based multicellular systems with dynamic populations. Tissue Forge users can build and interact with models and simulations in real-time and change simulation details during execution, or execute simulations off-screen and/or remotely in high-performance computing environments. Tissue Forge provides a growing library of built-in model components along with support for user-specified models during the development and application of custom, agent-based models. Tissue Forge includes an extensive Python API for model and simulation specification via Python scripts, an IPython console and a Jupyter Notebook, as well as C and C++ APIs for integrated applications with other software tools. Tissue Forge supports installations on 64-bit Windows, Linux and MacOS systems and is available for local installation via conda. 1 Author Summary Tissue Forge is a physics-based modeling and simulation software environment for research problems in physics, chemistry and biology. Tissue Forge supports modeling at a wide range of scales, from as small as the sub-nanometer, to as large as hundreds of micrometers, using particle-based models. It provides rich features for simulation development and application at all stages of model-based research, like real-time simulation visualization and interactivity, and off-screen batch execution, rendering, and GPU acceleration. Users can employ built-in models to represent a wide variety of physical processes, like chemical reactions, fluid convection and intercellular adhesion, or define their own models for agentand rule-based modeling. Tissue Forge is open-source, free and easy to install, supports simulation development in C, C++ and Python programming languages, and can be used as integrated software or in an interactive IPython console and Jupyter Notebook. Tissue Forge also provides a dedicated space for application-specific and user-contributed modeling and simulation features, and developers are welcome to contribute their custom features for distribution in future releases.|bioRxiv|2022|10.1371/journal.pcbi.1010768|H. Sauro, ID T.J.Sego, ID JamesP.Sluka, ID JamesA.Glazier|3.0|2
990|SPARTA: Spatial Acceleration for Efficient and Scalable Horizontal Diffusion Weather Stencil Computation|Fast and accurate climate simulations and weather predictions are critical for understanding and preparing for the impact of climate change. Real-world climate and weather simulations involve the use of complex compound stencil kernels, which are composed of a combination of different stencils. Horizontal diffusion is one such important compound stencil found in many climate and weather prediction models. Its computation involves a large amount of data access and manipulation that leads to two main issues on current computing systems. First, such compound stencils have high memory bandwidth demands as they require large amounts of data access. Second, compound stencils have complex data access patterns and poor data locality, as the memory access pattern is typically irregular with low arithmetic intensity. As a result, state-of-the-art CPU and GPU implementations suffer from limited performance and high energy consumption. Recent works propose using FPGAs as an alternative to traditional CPU and GPU-based systems to accelerate weather stencil kernels. However, we observe that stencil computation cannot leverage the bit-level flexibility available on an FPGA because of its complex memory access patterns, leading to high hardware resource utilization and low peak performance. We introduce SPARTA, a novel spatial accelerator for horizontal diffusion weather stencil computation. We exploit the two-dimensional spatial architecture to efficiently accelerate the horizontal diffusion stencil by designing the first scaled-out spatial accelerator using the MLIR (Multi-Level Intermediate Representation) compiler framework. We evaluate SPARTA on a real cutting-edge AMD-Xilinx Versal AI Engine (AIE) spatial architecture. Our real-system evaluation results demonstrate that SPARTA outperforms state-of-the-art CPU, GPU, and FPGA implementations by 17.1×, 1.2×, and 2.1×, respectively. Compared to the most energy-efficient design on an HBM-based FPGA, SPARTA provides 2.43× higher energy efficiency. Our results reveal that balancing workload across the available processing resources is crucial in achieving high performance on spatial architectures. We also implement and evaluate five elementary stencils that are commonly used as benchmarks for stencil computation research. We freely open-source all our implementations to aid future research in stencil computation and spatial computing systems at https://github.com/CMU-SAFARI/SPARTA.|International Conference on Supercomputing|2023|10.1145/3577193.3593719|Gagandeep Singh, Juan G'omez-Luna, K. Denolf, Joseph Melber, Alireza Khodamoradi, Andra Bisca, O. Mutlu, Jack Lo, H. Corporaal|3.0|2
1089|A Bayesian method to cluster single-cell RNA sequencing data using Copy Number Alterations.|MOTIVATION\nCancers are composed by several heterogeneous subpopulations, each one harbouring different genetic and epigenetic somatic alterations that contribute to disease onset and therapy response. In recent years, copy number alterations leading to tumour aneuploidy have been identified as potential key drivers of such populations, but the definition of the precise makeup of cancer subclones from sequencing assays remains challenging. In the end, little is known about the mapping between complex copy number alterations and their effect on cancer phenotypes.\n\n\nRESULTS\nWe introduce CONGAS, a Bayesian probabilistic method to phase bulk DNA and single-cell RNA measurements from independent assays. CONGAS jointly identifies clusters of single cells with subclonal copy number alterations, and differences in RNA expression. The model builds statistical priors leveraging bulk DNA sequencing data, does not require a normal reference and scales fast thanks to a GPU backend and variational inference. We test CONGAS on both simulated and real data, and find that it can determine the tumour subclonal composition at the single-cell level together with clone-specific RNA phenotypes in tumour data generated from both 10x and Smart-Seq assays.\n\n\nAVAILABILITY\nCONGAS is available as 2 packages: CONGAS (https://github.com/caravagnalab/congas), which implements the model in Python, and RCONGAS (https://caravagnalab.github.io/rcongas/), which provides R functions to process inputs, outputs, and run CONGAS fits. The analysis of real data and scripts to generate figures of this paper are available via RCONGAS; code associated to simulations is available at https://github.com/caravagnalab/rcongas_test.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.|Bioinformatics|2022|10.1093/bioinformatics/btac143|Lucrezia Patruno, R. Bergamin, N. Calonaci, G. Caravagna, Salvatore Milite|3.0|2
1104|GPU-Accelerated Monte Carlo Simulation for a Single-Photon Underwater Lidar|The Monte Carlo (MC) simulation, due to its ability to accurately simulate the backscattered signal of lidar, plays a crucial role in the design, optimization, and interpretation of the backscattered signal in lidar systems. Despite the development of several MC models for lidars, a suitable MC simulation model for underwater single-photon lidar, which is a vital ocean remote sensing technique utilized in underwater scientific investigations, obstacle avoidance for underwater platforms, and deep-sea environmental exploration, is still lacking. There are two main challenges in underwater lidar simulation. Firstly, the simulation results are significantly affected by near-field abnormal signals. Secondly, the simulation process is time-consuming due to the requirement of a high number of random processes to obtain reliable results. To address these issues, an algorithm is proposed to minimize the impacts of abnormal simulation signals. Additionally, a graphics processing unit (GPU)-accelerated semi-analytic MC simulation with a compute unified device architecture is proposed. The performance of the GPU-based program was validated using 109 photons and compared to a central processing unit (CPU)-based program. The GPU-based program achieved up to 68 times higher efficiency and a maximum relative deviation of less than 1.5%. Subsequently, the MC model was employed to simulate the backscattered signal in inhomogeneous water using the Henyey–Greenstein phase functions. By utilizing the look-up table method, simulations of backscattered signals were achieved using different scattering phase functions. Finally, a comparison between the simulation results and measurements derived from an underwater single-photon lidar demonstrated the reliability and robustness of our GPU-based MC simulation model.|Remote Sensing|2023|10.3390/rs15215245|Yupeng Liao, Zhifeng Yang, Mingjia Shangguan, Zaifa Lin, Yuanlun Wang, Sihui Li|3.0|2
1121|A Framework for Exploring Nuclear Physics Sensitivity in Numerical Simulations|We describe the AMReX-Astrophysics framework for exploring the sensitivity of astrophysical simulations to the details of a nuclear reaction network, including the number of nuclei, choice of reaction rates, and approximations used. This is explored by modeling a simple detonation with the Castro simulation code. The entire simulation methodology is open-source and GPU-enabled.||2023|10.3847/1538-4357/acec72|Brendan Boyd, Zhi Chen, Michael Zingale, Eric T. Johnson, Max Katz, Alexander Smith Clark|3.0|2
1131|A parallelized cellular Potts model that enables simulations at tissue scale|The Cellular Potts Model (CPM) is a widely used simulation paradigm for systems of interacting cells that has been used to study scenarios ranging from plant development to morphogenesis, tumour growth and cell migration. Despite their wide use, CPM simulations are considered too computationally intensive for three-dimensional (3D) models at organ scale. CPMs have been difficult to parallelise because of their inherently sequential update scheme. Here, we present a Graphical Processing Unit (GPU)-based parallelisation scheme that preserves local update statistics and is up to 3-4 orders of magnitude faster than serial implementations. We show several examples where our scheme preserves simulation behaviors that are drastically altered by existing parallelisation methods. We use our framework to construct tissue-scale models of liver and lymph node environments containing millions of cells that are directly based on microscopy-imaged tissue structures. Thus, our GPU-based CPM framework enables in silico studies of multicellular systems of unprecedented scale.||2023|10.1136/jitc-2023-sitc2023.1457|Shabaz Sultan, Sapna Devi, Scott N. Mueller, Johannes Textor|3.0|2
1462|Fully GPU-based electromagnetic transient simulation considering large-scale control systems for system-level studies|As more generators and loads are integrated by power electronic converters with complicated controls, electromagnetic transients (EMTs) simulation becomes an important tool for studying dynamic characteristics of large-scale power systems. To accelerate system-level EMT simulations, a fine-grained parallel algorithm on graphics processing units (GPU) is proposed. By decomposing the computational models of the EMT simulation into heterogeneous, homogeneous and network solution computations, the simulations are mapped into three unified GPU kernels. To incorporate control signals and non-linear features of electrical components, heterogeneous computations are formulated as layered direct acyclic graphs (LDAG) of primitive operations. An LDAG kernel is designed to carry out theses primitive operations efficiently by grouped threads. Then, homogeneous computations for state updates of electrical components are modelled as sets of fused multiply-add (FMA) operations, which are concurrently processed by an FMA kernel. Moreover, a hybrid network solution kernel is designed to solve the network equations, which can adaptively select dense or sparse solvers. Large-scale test systems are created and simulated on an NVIDIA K20x GPU. The results show that the proposed GPU-based EMT simulations are accurate and achieve 10x speedups over the CPU-based ones.||2017|10.1049/IET-GTD.2016.2078|Shaowei Huang, J. Martí, Ying Chen, Zhitong Yu, Yin Xu, Yankan Song|3.0|2
1571|Accelerated FDPS: Algorithms to use accelerators with FDPS|\n We describe algorithms implemented in FDPS (Framework for Developing Particle Simulators) to make efficient use of accelerator hardware such as GPGPUs (general-purpose computing on graphics processing units). We have developed FDPS to make it possible for researchers to develop their own high-performance parallel particle-based simulation programs without spending large amounts of time on parallelization and performance tuning. FDPS provides a high-performance implementation of parallel algorithms for particle-based simulations in a “generic” form, so that researchers can define their own particle data structure and interparticle interaction functions. FDPS compiled with user-supplied data types and interaction functions provides all the necessary functions for parallelization, and researchers can thus write their programs as though they are writing simple non-parallel code. It has previously been possible to use accelerators with FDPS by writing an interaction function that uses the accelerator. However, the efficiency was limited by the latency and bandwidth of communication between the CPU and the accelerator, and also by the mismatch between the available degree of parallelism of the interaction function and that of the hardware parallelism. We have modified the interface of the user-provided interaction functions so that accelerators are more efficiently used. We also implemented new techniques which reduce the amount of work on the CPU side and the amount of communication between CPU and accelerators. We have measured the performance of N-body simulations on a system with an NVIDIA Volta GPGPU using FDPS and the achieved performance is around 27% of the theoretical peak limit. We have constructed a detailed performance model, and found that the current implementation can achieve good performance on systems with much smaller memory and communication bandwidth. Thus, our implementation will be applicable to future generations of accelerator system.|Nippon Tenmon Gakkai obun kenkyu hokoku|2019|10.1093/pasj/psz133|M. Tsubouchi, D. Namekata, Long Wang, Keigo Nitadori, K. Nomura, J. Makino, M. Iwasawa|3.0|2
476|Phase quality map based on local multi-unwrapped results for two-dimensional phase unwrapping.|The efficiency of a phase unwrapping algorithm and the reliability of the corresponding unwrapped result are two key problems in reconstructing the digital elevation model of a scene from its interferometric synthetic aperture radar (InSAR) or interferometric synthetic aperture sonar (InSAS) data. In this paper, a new phase quality map is designed and implemented in a graphic processing unit (GPU) environment, which greatly accelerates the unwrapping process of the quality-guided algorithm and enhances the correctness of the unwrapped result. In a local wrapped phase window, the center point is selected as the reference point, and then two unwrapped results are computed by integrating in two different simple ways. After the two local unwrapped results are computed, the total difference of the two unwrapped results is regarded as the phase quality value of the center point. In order to accelerate the computing process of the new proposed quality map, we have implemented it in a GPU environment. The wrapped phase data are first uploaded to the memory of a device, and then the kernel function is called in the device to compute the phase quality in parallel by blocks of threads. Unwrapping tests performed on the simulated and real InSAS data confirm the accuracy and efficiency of the proposed method.|Applied Optics|2015|10.1364/AO.54.000739|Sen Zhang, Jinsong Tang, Heping Zhong|2.9|2
1529|Multiscale modeling of dynamic recrystallization|During thermomechanical processing of metals, changes occur in the microstructure of the material which affect its macroscopic properties. By understanding these transformations in the microstructure, it becomes possible to design the processes in a way which yields the desired properties in the finished product. For this purpose, computer simulation plays an increasingly important role.The present work is focused on developing an efficient numerical model that captures the macroscopic material behavior as well as the microstructure evolution. The main part of the thesis is made up of four papers, A-D. In paper A, different numerical solution methods for crystal plasticity are compared and implemented to run on the Graphical Processing Unit (GPU). The use of GPUs for scientific computation allows for considerable parallelism to be achieved in an ordinary desktop, or even laptop, computer, and has also been proven to be a cheap and energy efficient alternative for use in clusters. Since polycrystal plasticity is well suited for parallelization, it is shown that considerable speedup, up to a factor of 100 in some cases, can be achieved.In paper B, the crystal plasticity model is coupled with a vertex model of grain structure evolution. This provides a versatile framework which can be used to model dynamic recrystallization at large deformations. The crystal plasticity model captures hardening and texture evolution during deformation, while the vertex model describes the recrystallization process in terms of nucleation and grain growth. This model is then applied to simulations of a hot rolling process in paper C, making itpossible to study how temperature, and thereby recrystallization, affects the texture evolution during rolling, and also to study the development of inhomogeneities in the microstructure throughout the workpiece. In the final paper, D, the model from paper B is further developed such that it can also account for the effects of grain size hardening and particle pinning of migrating grain boundaries.Taken together, the four papers A-D provide a numerical simulation framework with multiscale capabilities. By taking advantage of recent developments in computer hardware and using a combination of modeling approaches, a versatile tool is established. The model is capable of describing development of crystallographic texture and dynamic recrystallization, including effects of temperature and impurities in the material. Employed in a finite element setting, the effects of the microstructure evolution on the macroscopic properties of the metal are captured, providing a powerfulconstitutive model for thermomechanical processing. (Less)||2016|10.1088/0965-0393/24/7/075004|Y. Mellbin|2.888888888888889|2
328|Real-time particle filtering and smoothing algorithms for detecting abrupt changes in neural ensemble spike activity.|Sequential change-point detection from time series data is a common problem in many neuroscience applications, such as seizure detection, anomaly detection, and pain detection. In our previous work (Chen Z, Zhang Q, Tong AP, Manders TR, Wang J. J Neural Eng 14: 036023, 2017), we developed a latent state-space model, known as the Poisson linear dynamical system, for detecting abrupt changes in neuronal ensemble spike activity. In online brain-machine interface (BMI) applications, a recursive filtering algorithm is used to track the changes in the latent variable. However, previous methods have been restricted to Gaussian dynamical noise and have used Gaussian approximation for the Poisson likelihood. To improve the detection speed, we introduce non-Gaussian dynamical noise for modeling a stochastic jump process in the latent state space. To efficiently estimate the state posterior that accommodates non-Gaussian noise and non-Gaussian likelihood, we propose particle filtering and smoothing algorithms for the change-point detection problem. To speed up the computation, we implement the proposed particle filtering algorithms using advanced graphics processing unit computing technology. We validate our algorithms, using both computer simulations and experimental data for acute pain detection. Finally, we discuss several important practical issues in the context of real-time closed-loop BMI applications. NEW & NOTEWORTHY Sequential change-point detection is an important problem in closed-loop neuroscience experiments. This study proposes novel sequential Monte Carlo methods to quickly detect the onset and offset of a stochastic jump process that drives the population spike activity. This new approach is robust with respect to spike sorting noise and varying levels of signal-to-noise ratio. The GPU implementation of the computational algorithm allows for parallel processing in real time.|Journal of Neurophysiology|2018|10.1152/jn.00684.2017|Qiaosheng Zhang, Sile Hu, Jing Wang, Z. Chen|2.857142857142857|2
1227|A Monte‐Carlo‐based and GPU‐accelerated 4D‐dose calculator for a pencil beam scanning proton therapy system|PURPOSE\nThe presence of respiratory motion during radiation treatment leads to degradation of the expected dose distribution, both for target coverage and healthy tissue sparing, particularly for techniques like pencil beam scanning proton therapy which have dynamic delivery systems. While tools exist to estimate this degraded four-dimensional (4D) dose, they typically have one or more deficiencies such as not including the particular effects from a dynamic delivery, using analytical dose calculations, and/or using nonphysical dose-accumulation methods. This work presents a clinically useful 4D-dose calculator that addresses each of these shortcomings.\n\n\nMETHODS\nTo quickly compute the 4D dose, the three main tasks of the calculator were run on graphics processing units (GPUs). These tasks were (a) simulating the delivery of the plan using measured delivery parameters to distribute the plan amongst 4DCT phases characterizing the patient breathing, (b) using an in-house Monte Carlo simulation (MC) dose calculator to determine the dose delivered to each breathing phase, and (c) accumulating the doses from the various breathing phases onto a single phase for evaluation. The accumulation was performed by individually transferring the energy and mass of dose-grid subvoxels, a technique that models the transfer of dose in a more physically realistic manner. The calculator was run on three test cases, with lung, esophagus, and liver targets, respectively, to assess the various uncertainties in the beam delivery simulation as well as to characterize the dose-accumulation technique.\n\n\nRESULTS\nFour-dimensional doses were successfully computed for the three test cases with computation times ranging from 4-6 min on a server with eight NVIDIA Titan X graphics cards; the most time-consuming component was the MC dose engine. The subvoxel-based dose-accumulation technique produced stable 4D-dose distributions at subvoxel scales of 0.5-1.0 mm without impairing the total computation time. The uncertainties in the beam delivery simulation led to moderate variations of the dose-volume histograms for these cases; the variations were reduced by implementing repainting or phase-gating motion mitigation techniques in the calculator.\n\n\nCONCLUSIONS\nA MC-based and GPU-accelerated 4D-dose calculator was developed to estimate the effects of respiratory motion on pencil beam scanning proton therapy treatments. After future validation, the calculator could be used to assess treatment plans and its quick runtime would make it easily usable in a future 4D-robust optimization system.|Medical Physics (Lancaster)|2018|10.1002/mp.13182|M. Herman, H. S. Wan Chan Tseung, Jedediah E. Johnson, M. Pepin, E. Tryggestad, C. Beltran|2.857142857142857|2
1552|Highly Parallelized Data-driven MPC for Minimal Intervention Shared Control|We present a shared control paradigm that improves a user's ability to operate complex, dynamic systems in potentially dangerous environments without a priori knowledge of the user's objective. In this paradigm, the role of the autonomous partner is to improve the general safety of the system without constraining the user's ability to achieve unspecified behaviors. Our approach relies on a data-driven, model-based representation of the joint human-machine system to evaluate, in parallel, a significant number of potential inputs that the user may wish to provide. These samples are used to (1) predict the safety of the system over a receding horizon, and (2) minimize the influence of the autonomous partner. The resulting shared control algorithm maximizes the authority allocated to the human partner to improve their sense of agency, while improving safety. We evaluate the efficacy of our shared control algorithm with a human subjects study (n=20) conducted in two simulated environments: a balance bot and a race car. During the experiment, users are free to operate each system however they would like (i.e., there is no specified task) and are only asked to try to avoid unsafe regions of the state space. Using modern computational resources (i.e., GPUs) our approach is able to consider more than 10,000 potential trajectories at each time step in a control loop running at 100Hz for the balance bot and 60Hz for the race car. The results of the study show that our shared control paradigm improves system safety without knowledge of the user's goal, while maintaining high-levels of user satisfaction and low-levels of frustration. Our code is available online at this https URL.|Robotics: Science and Systems|2019|10.15607/RSS.2019.XV.008|B. Argall, Alexander Broad, T. Murphey|2.8333333333333335|2
525|A new open-source GPU-based microscopic Monte Carlo simulation tool for the calculations of DNA damages caused by ionizing radiation --- Part II: sensitivity and uncertainty analysis.|PURPOSE\nCalculations of DNA damages involves many parameters in the computation process. As these parameters are often subject to uncertainties, it is of central importance to comprehensively quantify their impacts on DNA single strand break (SSB) and double strand break (DSB) yields. This has been a challenging task due to the required large number of simulations and the relatively low computational efficiency using CPU-based MC packages. In this study, we present comprehensive evaluations on sensitivities and uncertainties of DNA SSB and DSB yields on 12 parameters using our GPU-based MC tool, gMicroMC.\n\n\nMETHODS\nWe sampled one electron at a time in a water sphere containing a human lymphocyte nucleus and transport the electrons and generated radicals until 2 Gy dose was accumulated in the nucleus. We computed DNA damages caused by electron energy deposition events in the physical stage and the hydroxyl radicals at the end of the chemical stage. We repeated the computations by varying 12 parameters: 1) physics cross section, 2) cutoff energy for electron transport, 3)-5) three branching ratios of hydroxyl radicals in the de-excitation of excited water molecules, 6) temporal length of the chemical stage, 7)-8) reaction radii for direct and indirect damages, 9) threshold energy defining the threshold damage model to generate a physics damage, 10)-11) minimum and maximum energy values defining the linear-probability damage model to generate a physics damage, and 12) probability to generate a damage by a radical. We quantified sensitivity of SSB and DSB yields with respect to these parameters for cases with 1.0 and 4.5 keV electrons. We further estimated uncertainty of SSB and DSB yields caused by uncertainties of these parameters.\n\n\nRESULTS\nUsing a threshold of 10% uncertainty as a criterion, threshold energy in the threshold damage model, maximum energy in the linear-probability damage model, and probability for a radical to generate a damage were found to cause large uncertainties in both SSB and DSB yields. The scaling factor of the cross section, cutoff energy, physics reaction radius, and minimum energy in the linear-probability damage model were found to generate large uncertainties in DSB yields.\n\n\nCONCLUSIONS\nWe identified parameters that can generate large uncertainties in the calculations of SSB and DSB yields. Our study could serve as a guidance to reduce uncertainties of parameters and hence uncertainties of the simulation results.|Medical Physics (Lancaster)|2020|10.1002/mp.14036|N. Qin, Congchong Yan, X. Jia, Y. Lai, Y. Chi, Shih-Hao Hung, Min-yu Tsai, Z. Tian|2.8|2
700|Compression of Volume-Surface Integral Equation Matrices via Tucker Decomposition for Magnetic Resonance Applications|In this work, we propose a method for the compression of the coupling matrix in volume-surface integral equation (VSIE) formulations. VSIE methods are used for electromagnetic (EM) analysis in magnetic resonance imaging (MRI) applications, for which the coupling matrix models the interactions between the coil and the body. We showed that these effects can be represented as independent interactions between remote elements in 3-D tensor formats, and subsequently decomposed with the Tucker model. Our method can work in tandem with the adaptive cross approximation (ACA) technique to provide fast solutions of VSIE problems. We demonstrated that our compression approaches can enable the use of VSIE matrices of prohibitive memory requirements, by allowing the effective use of modern graphical processing units (GPUs) to accelerate the arising matrix-vector products. This is critical to enable numerical MRI simulations at clinical voxel resolutions in a feasible computation time. In this article, we demonstrate that the VSIE matrix-vector products needed to calculate the EM field produced by an MRI coil inside a numerical body model with 1 mm3 voxel resolution, could be performed in ~33 s in a GPU, after compressing the associated coupling matrix from ~80 TB to ~43 MB.|IEEE Transactions on Antennas and Propagation|2021|10.1109/TAP.2021.3090835|J. E. C. Serrallés, L. Daniel, Jacob K. White, Ilias I. Giannakopoulos, R. Lattanzi, Georgy D. Guryev, Ioannis P. Georgakis|2.75|2
843|Evaluating Performance and Portability of a core bioinformatics kernel on multiple vendor GPUs|Traditional scientific simulations have for quite some time, dominated the workloads of high-performance computing infrastructures across the world. With recent advancement in data generation capabilities of systems biology equipment, a rise in bioinformatics workloads has been observed. Bioinformatics applications deploy algorithmic motifs that use unique memory access patterns and rely heavily on integer-only computations. These applications place unique requirements on modern programming environments as well as GPU accelerators which are becoming an integral part of next generation of supercomputers. In this paper, we evaluate the performance and code portability of a core bioinformatics kernel that uses dynamic programming method for performing DNA and protein sequence alignments in several bioinformatics software pipelines. Our study evaluates the performance of a GPU accelerated sequence alignment algorithm across multiple vendor GPUs and programming models. We use a highly optimized adaptation of sequence alignment kernel and find the most productive way of porting it across multiple vendor GPUs and then assess its performance portability using Pennycook's method. Methods used in this paper and the insights drawn from those can be extended to a large number of integer-heavy scientific kernels and may aid in future accelerator design and design of programming model requirements.|International Workshop on Performance, Portability and Productivity in HPC|2021|10.1109/P3HPC54578.2021.00010|M. Awan, Nan Ding, Muhammad Haseeb, J. Deslippe|2.75|2
325|PSCC: Parallel Self-Collision Culling with Spatial Hashing on GPUs|We present a GPU-based self-collision culling method (PSCC) based on a combination of normal cone culling and spatial hashing techniques. We first describe a normal cone test front (NCTF) based parallel algorithm that maps well to GPU architectures. We use sprouting and shrinking operators to maintain compact NCTFs. Moreover, we use the NCTF nodes to efficient build an enhanced spatial hashing for triangles meshes and use that for inter-object and intra-object collisions. Compared with conventional spatial hashing, our approach provides higher culling efficiency and reduces the cost of narrow phrase culling. As compared to prior GPU-based parallel collision detection algorithm, our approach demonstrates 6-8X speedup. We also present an efficient approach for GPU-based cloth simulation based on PSCC. In practice, our GPU-based cloth simulation takes about one second per frame on complex scenes with tens or hundreds of thousands of triangles, and is about 4-6X faster than prior GPU-based simulation algorithms.|PACMCGIT|2018|10.1145/3203188|Zhongyuan Liu, Min Tang, Ruofeng Tong, Dinesh Manocha|2.7142857142857144|2
705|Towards the application of SPH technique to the simulation of fluid machines|Smoothed Particle Hydrodynamics (SPH) is a purely Lagrangian method developed at the end of seventies to cope with the limitations of finite difference methods in modeling some physical phenomena. Initially designed for astrophysical purposes, in the recent years SPH has been successfully applied to coastal engineering, fracture mechanics and elasticity, with also interesting results in gaming graphical simulation and film special effects. Compared to traditional finite difference methods, SPH is able to deal with free surface, deformable boundary and solid mechanics at nearly zero expense. Furthermore, several particle software such as DualSPHysics and GPUSPH exploit parallel computing and graphical processing unit calculation reducing considerably the simulation running time. Despite the actual researches are mainly focused on the analysis of coastal engineering problems, these features makes SPH suitable for the study of fluid machines. Indeed, the Lagrangian nature of the method allows a simplified pre-processing and an easy handling of the rigid motion. The study of these characteristics has been the subject of this work. In particular, this thesis focuses on the evaluation of the performance of SPH in dealing with confined flows and internal fluid dynamics. Initially, it is performed a detailed analysis of the Hagen-Poiseuille flow. Then, it is investigated the simulation of more complicated cases as the description of gear or piston pump operations. Despite some software limitations, this work has found that weakly-compressible SPH formulation and GPU computing are able to return quick and accurate results in almost all the tested phenomena. Furthermore, huge stability issues have been observed only with the simulation of laminar viscous flow at high Reynolds number where the retention of the initial homogeneous particle distribution is still critical.||2018|10.1016/j.jfluidstructs.2018.03.012|A. Oliva|2.7142857142857144|2
1174|A GPU OpenCL based cross-platform Monte Carlo dose calculation engine (goMC)|Monte Carlo (MC) simulation has been recognized as the most accurate dose calculation method for radiotherapy. However, the extremely long computation time impedes its clinical application. Recently, a lot of effort has been made to realize fast MC dose calculation on graphic processing units (GPUs). However, most of the GPU-based MC dose engines have been developed under NVidia’s CUDA environment. This limits the code portability to other platforms, hindering the introduction of GPU-based MC simulations to clinical practice. The objective of this paper is to develop a GPU OpenCL based cross-platform MC dose engine named goMC with coupled photon–electron simulation for external photon and electron radiotherapy in the MeV energy range. Compared to our previously developed GPU-based MC code named gDPM (Jia et al Phys. Med. Biol. 57 7783–97), goMC has two major differences. First, it was developed under the OpenCL environment for high code portability and hence could be run not only on different GPU cards but also on CPU platforms. Second, we adopted the electron transport model used in EGSnrc MC package and PENELOPE’s random hinge method in our new dose engine, instead of the dose planning method employed in gDPM. Dose distributions were calculated for a 15 MeV electron beam and a 6 MV photon beam in a homogenous water phantom, a water-bone-lung-water slab phantom and a half-slab phantom. Satisfactory agreement between the two MC dose engines goMC and gDPM was observed in all cases. The average dose differences in the regions that received a dose higher than 10% of the maximum dose were 0.48–0.53% for the electron beam cases and 0.15–0.17% for the photon beam cases. In terms of efficiency, goMC was ~4–16% slower than gDPM when running on the same NVidia TITAN card for all the cases we tested, due to both the different electron transport models and the different development environments. The code portability of our new dose engine goMC was validated by successfully running it on a variety of different computing devices including an NVidia GPU card, two AMD GPU cards and an Intel CPU processor. Computational efficiency among these platforms was compared.|Physics in Medicine and Biology|2015|10.1088/0031-9155/60/19/7419|M. Folkerts, Z. Tian, N. Qin, X. Jia, Steve B. Jiang, F. Shi|2.7|2
1354|High Performance Computing : 30th International Conference, ISC High Performance 2015, Frankfurt, Germany, July 12-16, 2015 : proceedings|Asynchronous Iterative Algorithm for Computing Incomplete Factorizations on GPUs.- Matrix Multiplication on High-Density Multi-GPU Architectures: Theoretical and Experimental Investigations.- A Framework for Batched and GPU-Resident Factorization Algorithms Applied to Block Householder Transformations.- Parallel Efficient Sparse Matrix-Matrix Multiplication on Multicore Platforms.- On the Design, Development, and Analysis of Optimized Matrix-Vector Multiplication Routines for Coprocessors.- Large-Scale Neo-Heterogeneous Programming and Optimization of SNP Detection on Tianhe-2.- ACCOLADES: A Scalable Workflow Framework for Large-Scale Simulation and Analyses of Automotive Engines.- Accelerating LBM and LQCD Application Kernels by In-Memory Processing.- On Quantum Chemistry Code Adaptation for RSC PetaStream Architecture.- Dtree: Dynamic Task Scheduling at Petascale.- Feasibility Study of Porting a Particle Transport Code to FPGA.- A Scalable, Linear-Time Dynamic Cutoff Algorithm for Molecular Dynamics.- BWTCP: A Parallel Method for Constructing BWT in Large Collection of Genomic Reads.- Lattice-CSC: Optimizing and Building an Efficient Supercomputer for Lattice-QCD and to Achieve First Place in Green500.- An Efficient Clique-Based Algorithm of Compute Nodes Allocation for In-memory Checkpoint System.- A Scalable Algorithm for Radiative Heat Transfer Using Reverse Monte Carlo Ray Tracing.- Optimizing Processes Mapping for Tasks with Non-uniform Data Exchange Run on Cluster with Different Interconnects.- Dynamically Adaptable I/O Semantics for High Performance Computing.- Predicting Performance of Non-contiguous I/O with Machine Learning.- A Best Practice Analysis of HDF5 and NetCDF-4 Using Lustre.- Striping Layout Aware Data Aggregation for High Performance I/O on a Lustre File System.- Hop: Elastic Consistency for Exascale Data Stores.- Energy-Efficient Data Processing Through Data Sparsing with Artifacts.- Updating the Energy Model for Future Exascale Systems.- High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol.- Modeling the Productivity of HPC Systems on a Computing Center Scale.- Taking Advantage of Node Power Variation in Homogenous HPC Systems to Save Energy.- A Run-Time System for Power-Constrained HPC Applications.- A Machine Learning Approach for a Scalable, Energy-Efficient Utility-Based Cache Partitioning.- A Case Study - Cost of Preemption for Urgent Computing on SuperMUC.- Designing Non-blocking Personalized Collectives with Near Perfect Overlap for RDMA-Enabled Clusters.- Design Methodology for Optimizing Optical Interconnection Networks in High Performance Systems.- Quantifying Communication in Graph Analytics.- Formal Metrics for Large-Scale Parallel Performance.- Hunting Down Load Imbalance: A Moving Target.- Orchestrating Docker Containers in the HPC Environment.- Performance and Scaling of WRF on Three Different Parallel Supercomputers.||2015|10.1007/978-3-319-20943-2_1|P. Balaji, J. Dongarra, J. Kunkel|2.7|2
942|AdaSplats: Adaptive Splatting of Point Clouds for Accurate 3D Modeling and Real-Time High-Fidelity LiDAR Simulation|LiDAR sensors provide rich 3D information about their surroundings and are becoming increasingly important for autonomous vehicles tasks such as localization, semantic segmentation, object detection, and tracking. Simulation accelerates the testing, validation, and deployment of autonomous vehicles while also reducing cost and eliminating the risks of testing in real-world scenarios. We address the problem of high-fidelity LiDAR simulation and present a pipeline that leverages real-world point clouds acquired by mobile mapping systems. Point-based geometry representations, more specifically splats (2D oriented disks with normals), have proven their ability to accurately model the underlying surface in large point clouds, mainly with uniform density. We introduce an adaptive splat generation method that accurately models the underlying 3D geometry to handle real-world point clouds with variable densities, especially for thin structures. Moreover, we introduce a fast LiDAR sensor simulator, working in the splatted model, that leverages the GPU parallel architecture with an acceleration structure while focusing on efficiently handling large point clouds. We test our LiDAR simulation in real-world conditions, showing qualitative and quantitative results compared to basic splatting and meshing techniques, demonstrating the interest of our modeling technique.|Remote Sensing|2022|10.3390/rs14246262|Jean-Emmanuel Deschaud, Jean Pierre Richa, Nicolas Dalmasso, Franccois Goulette|2.6666666666666665|2
1619|On the efficient numerical simulation of heterogeneous anisotropic diffusion models for tumor invasion using GPUs.|The aim of this article is to show how continuous mathematical models for tumor dynamics can be solved efficiently using commodity Graphics Processing Units (GPUs) found in personal and portable computers. The test set of equations models haptotaxis and heterogeneous anisotropic diffusion of the cancer cells population. The numerical solution is obtained by using a second order finite difference Euler scheme. It is proven that, as the space resolution improves, the GPU implementation of the numerical scheme shows an increasingly better performance than that of the Central Processing Units (CPUs).||2019|10.24033/asens.2391|D. Pera, C. Málaga, R. Plaza, C. Simeoni|2.6666666666666665|2
182|Model-Driven Auto-Tuning of Stencil Computations on GPUs|Stencil computations are a class of algorithms which perform nearest-neighbor computation, often on a multi-dimensional grid. This type of calculation forms the basis for computer simulations across almost every field of science. The increasing computational speed of graphics processing units (GPUs) make their use for stencil computations an interesting goal. However, achieving highly ecient implementations is often nontrivial, as numerous publications attest. In this work, we propose an analytic performance model for stencil codes on GPUs, which both delivers close-to optimal performance, but at the same time does not require extensive tuning at compile or run time. We evaluate the effectiveness of our performance model using dierent stencil benchmarks and with various stencil radii.||2015|10.1145/2751205.2751226|F. Löer, David M. Koppelman, Steven R. Brandt, Yue Hu|2.6|2
455|Online Scheduling of Task Graphs on Heterogeneous Platforms|"Modern computing platforms commonly include accelerators. We target the problem of scheduling applications modeled as task graphs on hybrid platforms made of two types of resources, such as CPUs and GPUs. We consider that task graphs are uncovered dynamically, and that the scheduler has information only on the available tasks, i.e., tasks whose predecessors have all been completed. Each task can be processed by either a CPU or a GPU, and the corresponding processing times are known. Our study extends a previous <inline-formula><tex-math notation=""LaTeX"">$4\sqrt{m/k}$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>4</mml:mn><mml:msqrt><mml:mrow><mml:mi>m</mml:mi><mml:mo>/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math><inline-graphic xlink:href=""simon-ieq1-2942909.gif""/></alternatives></inline-formula>-competitive online algorithm by Amaris et al. <xref ref-type=""bibr"" rid=""ref1"">[1]</xref> , where <inline-formula><tex-math notation=""LaTeX"">$m$</tex-math><alternatives><mml:math><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href=""simon-ieq2-2942909.gif""/></alternatives></inline-formula> is the number of CPUs and <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=""simon-ieq3-2942909.gif""/></alternatives></inline-formula> the number of GPUs (<inline-formula><tex-math notation=""LaTeX"">$m\geq k$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>m</mml:mi><mml:mo>≥</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""simon-ieq4-2942909.gif""/></alternatives></inline-formula>). We prove that no online algorithm can have a competitive ratio smaller than <inline-formula><tex-math notation=""LaTeX"">$\sqrt{m/k}$</tex-math><alternatives><mml:math><mml:msqrt><mml:mrow><mml:mi>m</mml:mi><mml:mo>/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msqrt></mml:math><inline-graphic xlink:href=""simon-ieq5-2942909.gif""/></alternatives></inline-formula>. We also study how adding flexibility on task processing, such as task migration or spoliation, or increasing the knowledge of the scheduler by providing it with information on the task graph, influences the lower bound. We provide a <inline-formula><tex-math notation=""LaTeX"">$(2\sqrt{m/k}+1)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:mi>m</mml:mi><mml:mo>/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msqrt><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""simon-ieq6-2942909.gif""/></alternatives></inline-formula>-competitive algorithm as well as a tunable combination of a system-oriented heuristic and a competitive algorithm; this combination performs well in practice and has a competitive ratio in <inline-formula><tex-math notation=""LaTeX"">$\Theta (\sqrt{m/k})$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>Θ</mml:mi><mml:mo>(</mml:mo><mml:msqrt><mml:mrow><mml:mi>m</mml:mi><mml:mo>/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msqrt><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""simon-ieq7-2942909.gif""/></alternatives></inline-formula>. We also adapt all our results to the case of multiple types of processors. Finally, simulations on different sets of task graphs illustrate how the instance properties impact the performance of the studied algorithms and show that our proposed tunable algorithm performs the best among the online algorithms in almost all cases and has even performance close to an offline algorithm."|IEEE Transactions on Parallel and Distributed Systems|2020|10.1109/TPDS.2019.2942909|Louis-Claude Canon, F. Vivien, B. Simon, L. Marchal|2.6|2
575|GPU-Accelerated Automatic Microseismic Monitoring Algorithm (GAMMA) and Its Application to the 2019 Ridgecrest Earthquake Sequence|\n Foreshocks and/or aftershocks play critical roles in improving our understanding of the processes of faulting, such as nucleation of earthquakes, earthquake triggering, and postseismic deformation. A rapid and accurate earthquake detection and location algorithm can provide timely information of seismic activities, thereby benefitting our understanding of physical mechanisms of faulting and seismic hazard assessment. We have developed a graphic processing unit (GPU)-accelerated automatic microseismic monitoring algorithm (GAMMA) for accurate and near real-time detection and location of earthquakes. GAMMA utilizes methods based on backprojection to automatically detect potential earthquakes, and then the waveforms of qualified earthquakes are selected as templates when searching for small earthquakes in continuous recordings using the template-matching algorithm. The use of GPUs has substantially accelerated the calculations and has made GAMMA capable of (near-)real-time earthquake monitoring. We have successfully applied GAMMA to the 2019 Ridgecrest earthquake sequence in southern California. The number of earthquakes detected by GAMMA is more than 21 times that documented in the regional catalog. The more complete catalog determined by GAMMA may provide crucial information for improving our understanding of the physical mechanisms of faulting and also supply useful constraints for a variety of types of studies, including dynamic rupture simulations and crustal deformation modeling.||2020|10.1785/0220190323|Po Chen, Wei Wang, Wu-Yu Liao, En-Jui Lee, D. Mu|2.6|2
639|On the Deployability of Augmented Reality Using Embedded Edge Devices|Edge Computing exploits computational capabilities deployed at the very edge of the network to support applications with low latency requirements. Such capabilities can reside in small embedded devices that integrate dedicated hardware - e.g., a GPU - in a low cost package. But these devices have limited computing capabilities compared to standard server grade equipment. When deploying an Edge Computing based application, understanding whether the available hardware can meet target requirements is key in meeting the expected performance. In this paper, we study the feasibility of deploying Augmented Reality applications using Embedded Edge Devices (EEDs). We compare such deployment approach to one exploiting a standard dedicated server grade machine. Starting from an empirical evaluation of the capabilities of these devices, we propose a simple theoretical model to compare the performance of the two approaches. We then validate such model with NS-3 simulations and study their feasibility. Our results show that there is no one-fits-all solution. If we need to deploy high responsiveness applications, we need a centralized server grade architecture and we can in any case only support very few users. The centralized architecture fails to serve a larger number of users, even when low to mid responsiveness is required. In this case, we need to resort instead to a distributed deployment based on EEDs.|Consumer Communications and Networking Conference|2020|10.1109/CCNC49032.2021.9369590|Ayoub Ben-Ameur, Andrea Araldo, F. Bronzino|2.6|2
1474|Kinematic modelling of disc galaxies using graphics processing units|With large-scale Integral Field Spectroscopy (IFS) surveys of thousands of galaxies currently under-way or planned, the astronomical community is in need of methods, techniques and tools that will allow the analysis of huge amounts of data. We focus on the kinematic modelling of disc galaxies and investigate the potential use of massively parallel architectures, such as the Graphics Processing Unit (GPU), as an accelerator for the computationally expensive model-fitting procedure. We review the algorithms involved in model-fitting and evaluate their suitability for GPU implementation. We employ different optimization techniques, including the Levenberg-Marquardt and Nested Sampling algorithms, but also a naive brute-force approach based on Nested Grids. We find that the GPU can accelerate the model-fitting procedure up to a factor of ~100 when compared to a single-threaded CPU, and up to a factor of ~10 when compared to a multi-threaded dual CPU configuration. Our method's accuracy, precision and robustness are assessed by successfully recovering the kinematic properties of simulated data, and also by verifying the kinematic modelling results of galaxies from the GHASP and DYNAMO surveys as found in the literature. The resulting GBKFIT code is available for download from: this http URL||2015|10.1093/mnras/stv2292|G. Bekiaris, C. Fluke, K. Glazebrook, R. Abraham|2.6|2
1220|GPU Data Access on Complex Geometries for D3Q19 Lattice Boltzmann Method|GPU performance of the lattice Boltzmann method (LBM) depends heavily on memory access patterns. When LBM is advanced with GPUs on complex computational domains, geometric data is typically accessed indirectly, and lattice data is typically accessed lexicographically in the Structure of Array (SoA) layout. Although there are a variety of existing access patterns beyond the typical choices, no study has yet examined the relative efficacy between them. Here, we compare a suite of memory access schemes via empirical testing and performance modeling. We find strong evidence that semi-direct addressing is the superior addressing scheme for the majority of cases examined: Semi-direct addressing increases computational speed and often reduces memory consumption. For lattice layout, we find that the Collected Structure of Arrays (CSoA) layout outperforms the SoA layout. When compared to state-of-the-art practices, our recommended addressing modifications lead to performance gains between 10-40% across different complex geometries, fluid volume fractions, and resolutions. The modifications also lead to a decrease in memory consumption by as much as 17%. Having discovered these improvements, we examine a highly resolved arterial geometry on a leadership class system. On this system we present the first near-optimal strong results for LBM with arterial geometries run on GPUs. We also demonstrate that the above recommendations remain valid for large scale, many device simulations, which leads to an increased computational speed and average memory usage reductions. To understand these observations, we employ performance modeling which reveals that semi-direct methods outperform indirect methods due to a reduced number of total loads/stores in memory, and that CSoA outperforms SoA and bundling due to improved caching behavior.|IEEE International Parallel and Distributed Processing Symposium|2018|10.1109/IPDPS.2018.00092|J. Vetter, G. Herschlag, Seyong Lee, A. Randles|2.5714285714285716|2
341|Efficient implementation of constant pH molecular dynamics on modern graphics processors|The treatment of pH sensitive ionization states for titratable residues in proteins is often omitted from molecular dynamics (MD) simulations. While static charge models can answer many questions regarding protein conformational equilibrium and protein–ligand interactions, pH‐sensitive phenomena such as acid‐activated chaperones and amyloidogenic protein aggregation are inaccessible to such models. Constant pH molecular dynamics (CPHMD) coupled with the Generalized Born with a Simple sWitching function (GBSW) implicit solvent model provide an accurate framework for simulating pH sensitive processes in biological systems. Although this combination has demonstrated success in predicting pKa values of protein structures, and in exploring dynamics of ionizable side‐chains, its speed has been an impediment to routine application. The recent availability of low‐cost graphics processing unit (GPU) chipsets with thousands of processing cores, together with the implementation of the accurate GBSW implicit solvent model on those chipsets (Arthur and Brooks, J. Comput. Chem. 2016, 37, 927), provide an opportunity to improve the speed of CPHMD and ionization modeling greatly. Here, we present a first implementation of GPU‐enabled CPHMD within the CHARMM‐OpenMM simulation package interface. Depending on the system size and nonbonded force cutoff parameters, we find speed increases of between one and three orders of magnitude. Additionally, the algorithm scales better with system size than the CPU‐based algorithm, thus allowing for larger systems to be modeled in a cost effective manner. We anticipate that the improved performance of this methodology will open the door for broad‐spread application of CPHMD in its modeling pH‐mediated biological processes. © 2016 Wiley Periodicals, Inc.|Journal of Computational Chemistry|2016|10.1002/jcc.24435|C. Brooks, E. J. Arthur|2.5555555555555554|2
1439|Solvated and generalised Born calculations differences using GPU CUDA and multi-CPU simulations of an antifreeze protein with AMBER|Abstract While there has been an increase in the number of biomolecular computational studies employing graphics processing units (GPU), results describing their use with the molecular dynamics package AMBER with the CUDA implementation are scarce. No information is available comparing MD methodologies pmemd.cuda, pmemd.mpi or sander.mpi, available in AMBER, for generalised Born (GB) simulations or with solvated systems. As part of our current studies with antifreeze proteins (AFP), and for the previous reasons, we present details of our experience comparing performance of MD simulations at varied temperatures between multi-CPU runs using sander.mpi, pmemd.mpi and pmemd.cuda with the AFP from the fish ocean pout (1KDF). We found extremely small differences in total energies between multi-CPU and GPU CUDA implementations of AMBER12 in 1ns production simulations of the solvated system using the TIP3P water model. Additionally, GPU computations achieved typical one order of magnitude speedups when using mixed precision but were similar to CPU speeds when computing with double precision. However, we found that GB calculations were highly sensitive to the choice of initial GB parametrisation regardless of the type of methodology, with substantial differences in total energies.||2016|10.1080/08927022.2016.1183000|A. Peramo|2.5555555555555554|2
397|Alternating Dual Updates Algorithm for X-ray CT Reconstruction on the GPU|Model-based image reconstruction (MBIR) for X-ray computed tomography (CT) offers improved image quality and potential low-dose operation, but has yet to reach ubiquity in the clinic. MBIR methods form an image by solving a large statistically motivated optimization problem, and the long time it takes to numerically solve this problem has hampered MBIR's adoption. We present a new optimization algorithm for X-ray CT MBIR based on duality and group coordinate ascent that may converge even with approximate updates and can handle a wide range of regularizers, including total variation (TV). The algorithm iteratively updates groups of dual variables corresponding to terms in the cost function; these updates are highly parallel and map well onto the GPU. Although the algorithm stores a large number of variables, the “working size” for each of the algorithm's steps is small and can be efficiently streamed to the GPU while other calculations are being performed. The proposed algorithm converges rapidly on both real and simulated data and shows promising parallelization over multiple devices.|IEEE Transactions on Computational Imaging|2015|10.1109/TCI.2015.2479555|J. Fessler, Madison G. McGaffin|2.5|2
444|GPU-accelerated dislocation dynamics using subcycling time-integration|Discrete dislocation dynamics (DDD) simulations are being increasingly employed to investigate metal plasticity at the mesoscale. However, in spite of its ability to access beyond the length and time limits of atomistic methods, the DDD model is still limited by its high computational cost, with ranges of achievable strains too low and strain rates too high by several orders of magnitude compared with typical experimental conditions. By combining the efficiency of the recently developed subcycling time-integrator with the highly parallel architecture of graphics processing unit (GPU) devices, we developed a DDD model that provides significant acceleration compared to existing implementations. Our GPU-accelerated implementation enables large-scale DDD simulations that can reach relevant levels of strain using a moderate amount of computational resources.|Modelling and Simulation in Materials Science and Engineering|2019|10.1088/1361-651X/ab3a03|W. Cai, A. Arsenlis, N. Bertin, S. Aubry|2.5|2
483|gPET: a GPU-based, accurate and efficient Monte Carlo simulation tool for PET|Monte Carlo (MC) simulation method plays an essential role in the refinement and development of positron emission tomography (PET) systems. However, most existing MC simulation packages suffer from long execution time for practical PET simulations. To fully address this issue, we developed and validated gPET, a graphics processing unit (GPU)-based MC simulation tool for PET. gPET was built on the NVidia CUDA platform. The simulation process was modularized into three functional parts and carried out by the GPU parallel threads: (1) source management, including positron decay, transport and annihilation; (2) gamma transport inside the phantom; and (3) signal detection and processing inside the detector. A hybrid of voxelized (for patient phantoms) and parametrized (for detectors) geometries were employed to sufficiently support particle navigations. Multiple inputs and outputs were available. Hence, a user can flexibly examine different aspects of a PET simulation. We evaluated the performance of gPET in three test cases with benchmark work from GATE8.0, in terms of the testing of the functional modules, the physics models used for gamma transport inside the detector, and the geometric configuration of an irregularly shaped PET detector. Both accuracy and efficiency were quantified. In all test cases, the differences between gPET and GATE for the coincidences with respect to the energy and crystal index distributions are below 3.18% and 2.54%, respectively. The speedup factor is 500 for gPET on a single Titan Xp GPU (1.58 GHz) over GATE8.0 on a single core of Intel i7-6850K CPU (3.6 GHz) for all test cases. In summary, gPET is an accurate and efficient MC simulation tool for PET.|Physics in Medicine and Biology|2019|10.1088/1361-6560/ab5610|A. Chalise, X. Jia, Y. Zhong, Y. Lai, Y. Chi, Y. Shao, M. Jin|2.5|2
767|Onboard Real-Time Dense Reconstruction in Large Terrain Scene Using Embedded UAV Platform|Using unmanned aerial vehicles (UAVs) for remote sensing has the advantages of high flexibility, convenient operation, low cost, and wide application range. It fills the need for rapid acquisition of high-resolution aerial images in modern photogrammetry applications. Due to the insufficient parallaxes and the computation-intensive process, dense real-time reconstruction for large terrain scenes is a considerable challenge. To address these problems, we proposed a novel SLAM-based MVS (Multi-View-Stereo) approach, which can incrementally generate a dense 3D (three-dimensional) model of the terrain by using the continuous image stream during the flight. The pipeline of the proposed methodology starts with pose estimation based on SLAM algorithm. The tracked frames were then selected by a novel scene-adaptive keyframe selection method to construct a sliding window frame-set. This was followed by depth estimation using a flexible search domain approach, which can improve accuracy without increasing the iterate time or memory consumption. The whole system proposed in this study was implemented on the embedded GPU based on an UAV platform. We proposed a highly parallel and memory-efficient CUDA-based depth computing architecture, enabling the system to achieve good real-time performance. The evaluation experiments were carried out in both simulation and real-world environments. A virtual large terrain scene was built using the Gazebo simulator. The simulated UAV equipped with an RGB-D camera was used to obtain synthetic evaluation datasets, which were divided by flight altitudes (800-, 1000-, 1200 m) and terrain height difference (100-, 200-, 300 m). In addition, the system has been extensively tested on various types of real scenes. Comparison with commercial 3D reconstruction software is carried out to evaluate the precision in real-world data. According to the results on the synthetic datasets, over 93.462% of the estimation with absolute error distance of less then 0.9%. In the real-world dataset captured at 800 m flight height, more than 81.27% of our estimated point cloud are less then 5 m difference with the results of Photoscan. All evaluation experiments show that the proposed approach outperforms the state-of-the-art ones in terms of accuracy and efficiency.|Remote Sensing|2021|10.3390/rs13142778|Xiantong Meng, Wenhao Li, Shaokun Han, Fei Liu, Zhengchao Lai, Shangwei Guo|2.5|2
1019|FLAME GPU 2: A framework for flexible and performant agent based simulation on GPUs|Agent based modelling (ABM) offers a powerful abstraction for scientific study in a broad range of domains. The use of agent based simulators encourages good software engineering design such as separation of concerns, that is, the uncoupling of the model description from its implementation detail. A major limitation in current approaches to ABM simulation is that of the trade off between simulator flexibility and performance. It is common that highly optimised simulations, such as those which target graphics processing units (GPU) hardware, are implemented as standalone software. This work presents a software framework (FLAME GPU 2) which balances flexibility with performance for general purpose ABM. Methods for ensuring high computational efficacy are demonstrated by, minimising data movement, and ensuring high device utilisation by exploiting opportunities for concurrent code execution within a model and through the use of ensembles of simulations. A novel hierarchical sub‐modelling approach is also presented which can be used to model certain types of recursive behaviours. This feature is shown to be essential in providing a mechanism to resolve competition for resources between agents within a parallel environment which would otherwise introduce race conditions. To understand the performance characteristics of the software, a benchmark model with millions of agents is used to explore the use of simulation ensembles and to parametrically investigate concurrent code execution within a model. Performance speedups are demonstrated of 3.5 ×$$ \times $$ and 10 ×$$ \times $$ respectively over a baseline GPU implementation. Our hierarchical sub‐modelling approach is used to demonstrate the implementation of a recursive algorithm to resolve competition of agent movement which occurs as a result of agent desire to simultaneously occupy discrete areas high in a ‘resource’. The algorithm is used to implement a classical socio‐economics model, Sugarscape, with populations of up to 16M agents.|Software, Practice & Experience|2023|10.1002/spe.3207|Matthew Leach, Robert Chisholm, Peter Heywood, M. Chimeh, P. Richmond|2.5|2
1050|Energy Optimization of Distributed Video Processing System using Genetic Algorithm with Bayesian Attractor Model|For the future cyber-physical system (CPS) society, it is necessary to construct digital twins (DTs) of a real world in real time using a lot of cameras and sensors. Hence, the energy efficiency of both networks and computers for largescale distributed video analysis is a major challenge for the full-scale spread of CPSs and DTs. Toward this goal, we first propose a model to arbitrarily split and distribute the video analysis task to terminals, edge servers, and cloud servers and dynamically assign appropriate CNN models to them. System-wide optimization of such distributed processing can reduce overall system power consumption by reducing network bandwidth and efficiently utilizing distributed CPU/GPU resources. To realize this optimization in a real system, we also propose a model to estimate the GPU load, processing time, and power consumption of these devices based on massive experimental measurements. Since such a large-scale optimization is difficult because of the dynamic and multi-objective nature of the problem, we propose a new optimization algorithm composed of Genetic Algorithm and Bayesian Attractor Model. Finally, simulation evaluations are performed to demonstrate that the proposed method can minimize system power consumption and satisfy latency and recognition accuracy requirements of each video analysis, even under changing environmental conditions.|IEEE Conference on Network Softwarization|2023|10.1109/NetSoft57336.2023.10175483|H. Shimonishi, G. Hasegawa, M. Murata, Nattaon Techasarntikul|2.5|2
1135|X-ray Crystallography Module in MD Simulation Program Amber 2023. Refining the Models of Protein Crystals|The MD simulation package Amber offers an attractive platform to refine crystallographic structures of proteins: (i) state-of-the-art force fields help to regularize protein coordinates and reconstruct the poorly diffracting elements of the structure, such as flexible loops; (ii) MD simulations restrained by the experimental diffraction data provide an effective strategy to optimize structural models of protein crystals, including explicitly modeled interstitial solvent as well as crystal contacts. Here, we present the new crystallography module xray, released as a part of the Amber 2023 package. This module contains functions to calculate and scale structure factors (including the contributions from bulk solvent), evaluate the maximum-likelihood-type crystallographic potential, and compute its derivative forces. The X-ray functionality of Amber no longer relies on external dependencies so that the full advantage of GPU acceleration can be taken. This makes it possible to refine in a short time hundreds of crystal models, including supercell models comprised of multiple unit cells. The new automated Amber-based refinement procedure leads to an appreciable improvement in Rfree (in some cases, by as much as 0.067) as well as MolProbity scores.|Journal of Chemical Information and Modeling|2023|10.1021/acs.jcim.3c01531|Oleg Mikhailovskii, S. A. Izmailov, N. Skrynnikov, David A Case, Yi Xue|2.5|2
1433|PIM-VR: Erasing Motion Anomalies In Highly-Interactive Virtual Reality World with Customized Memory Cube|With the revolutionary innovations emerging in the computer graphics domain, virtual reality (VR) has become increasingly popular and shown great potential for entertainment, medical simulation and education. In the highly interactive VR world, the motion-to-photon delay (MPD) which represents the delay from users’ head motion to the responded image displayed on their head devices, is the most critical factor for a successful VR experience. Long MPD may cause users to experience significant motion anomalies: judder, lagging and sickness. In order to achieve the short MPD and alleviate the motion anomalies, asynchronous time warp (ATW) which is known as an image re-projection technique, has been proposed by VR vendors to map the previously rendered frame to the correct position using the latest headmotion information. However, after a careful investigation on the efficiency of the current GPU-accelerated ATW through executing real VR applications on modern VR hardware, we observe that the state-of-the-art ATW technique cannot deliver the ideal MPD and often misses the refresh deadline, resulting in reduced frame rate and motion anomalies. This is caused by two major challenges: inefficient VR execution model and intensive off-chip memory accesses. To tackle these, we propose a preemption-free Processing-In-Memory based ATW design which asynchronously executes ATW within a 3D-stacked memory, without interrupting the rendering tasks on the host GPU. We also identify a redundancy reduction mechanism to further simplify and accelerate the ATW operation. A comprehensive evaluation of our proposed design demonstrates that our PIM-based ATW can achieve the ideal MPD and provide superior user experience. Finally, we provide a design space exploration to showcase different design choices for the PIM-based ATW design, and the results show that our design scales well in future VR scenarios with higher frame resolution and even lower ideal MPD.|International Symposium on High-Performance Computer Architecture|2019|10.1109/HPCA.2019.00013|Xingyao Zhang, Xin Fu, S. Song, Chenhao Xie, Ang Li|2.5|2
1563|An extended model to support detailed GPGPU reliability analysis|General Purpose Graphics Processing Units (GPGPUs) have been used in the last decades as accelerators in high demanding data processing applications, such as multimedia processing and high-performance computing. Nowadays, these devices are becoming popular even in safety-critical applications, such as autonomous and semi-autonomous vehicles. However, these devices can suffer from the effects of transient faults, such as those produced by radiation effects. These effects can be represented in the system as Single Event Upsets (SEUs) and are able to generate intolerable application misbehaviors in safety critical environments. In this work, we extended the capabilities of an open-source VHDL GPGPU model (FlexGrip) in order to study and analyze in a much more detailed manner the effects of SEUs in some critical modules within a GPGPU. Simulation results showed that scheduler controller has different levels of SEU sensibility depending on the affected location. Moreover, a reduced number of execution units, in the GPGPU can decrease the system reliability.|International Conference on Design & Technology of Integrated Systems in Nanoscale Era|2019|10.1109/DTIS.2019.8735047|J. E. R. Condia, B. Du, M. Reorda|2.5|2
472|Parallelization and improvements of the generalized born model with a simple sWitching function for modern graphics processors|Two fundamental challenges of simulating biologically relevant systems are the rapid calculation of the energy of solvation and the trajectory length of a given simulation. The Generalized Born model with a Simple sWitching function (GBSW) addresses these issues by using an efficient approximation of Poisson–Boltzmann (PB) theory to calculate each solute atom's free energy of solvation, the gradient of this potential, and the subsequent forces of solvation without the need for explicit solvent molecules. This study presents a parallel refactoring of the original GBSW algorithm and its implementation on newly available, low cost graphics chips with thousands of processing cores. Depending on the system size and nonbonded force cutoffs, the new GBSW algorithm offers speed increases of between one and two orders of magnitude over previous implementations while maintaining similar levels of accuracy. We find that much of the algorithm scales linearly with an increase of system size, which makes this water model cost effective for solvating large systems. Additionally, we utilize our GPU‐accelerated GBSW model to fold the model system chignolin, and in doing so we demonstrate that these speed enhancements now make accessible folding studies of peptides and potentially small proteins. © 2016 Wiley Periodicals, Inc.|Journal of Computational Chemistry|2016|10.1002/jcc.24280|C. Brooks, E. J. Arthur|2.4444444444444446|2
1395|Fast Simulation of Large-Scale Floods Based on GPU Parallel Computing|Computing speed is a significant issue of large-scale flood simulations for real-time response to disaster prevention and mitigation. Even today, most of the large-scale flood simulations are generally run on supercomputers due to the massive amounts of data and computations necessary. In this work, a two-dimensional shallow water model based on an unstructured Godunov-type finite volume scheme was proposed for flood simulation. To realize a fast simulation of large-scale floods on a personal computer, a Graphics Processing Unit (GPU)-based, high-performance computing method using the OpenACC application was adopted to parallelize the shallow water model. An unstructured data management method was presented to control the data transportation between the GPU and CPU (Central Processing Unit) with minimum overhead, and then both computation and data were offloaded from the CPU to the GPU, which exploited the computational capability of the GPU as much as possible. The parallel model was validated using various benchmarks and real-world case studies. The results demonstrate that speed-ups of up to one order of magnitude can be achieved in comparison with the serial model. The proposed parallel model provides a fast and reliable tool with which to quickly assess flood hazards in large-scale areas and, thus, has a bright application prospect for dynamic inundation risk identification and disaster assessment.||2018|10.3390/W10050589|Guodong Li, Qiang Liu, Yi Qin|2.4285714285714284|2
202|GPU Computing Pipeline Inefficiencies and Optimization Opportunities in Heterogeneous CPU-GPU Processors|Emerging heterogeneous CPU-GPU processors have introduced unified memory spaces and cache coherence. CPU and GPU cores will be able to concurrently access the same memories, eliminating memory copy overheads and potentially changing the application-level optimization targets. To date, little is known about how developers may organize new applications to leverage the available, finer-grained communication in these processors. However, understanding potential application optimizations and adaptations is critical for directing heterogeneous processor programming model and architectural development. This paper quantifies opportunities for applications and architectures to evolve to leverage the new capabilities of heterogeneous processors. To identify these opportunities, we ported and simulated a broad set of benchmarks originally developed for discrete GPUs to remove memory copies, and applied analytical models to quantify their application-level pipeline inefficiencies. For existing benchmarks, GPU bulk-synchronous software pipelines result in considerable core and cache utilization inefficiency. For heterogeneous processors, the results indicate increased opportunity for techniques that provide flexible compute and data granularities, and support for efficient producer-consumer data handling and synchronization within caches.|IEEE International Symposium on Workload Characterization|2015|10.1109/IISWC.2015.15|Joel Hestness, S. Keckler, D. Wood|2.4|2
540|New capabilities of the Monte Carlo dose engine ARCHER-RT: clinical validation of the Varian TrueBeam machine for VMAT external beam radiotherapy.|PURPOSE\nThe Monte Carlo radiation transport method is considered the most accurate approach for absorbed dose calculations in external beam radiation therapy. In this study, an efficient and accurate source model of the Varian TrueBeam 6X STx Linac is developed and integrated with a fast Monte Carlo photon-electron transport absorbed dose engine, ARCHER-RT, which is capable of being executed on CPUs, NVIDIA GPUs and AMD GPUs. This capability of fast yet accurate radiation dose calculation is essential for clinical utility of this new technology. This paper describes the software and algorithmic developments made to the ARCHER-RT absorbed dose engine.\n\n\nMETHODS\nAMD's Heterogeneous-Compute Interface for Portability (HIP) was implemented in ARCHER-RT to allow for device independent execution on NVIDIA and AMD GPUs. Architecture specific atomic-add algorithms have been identified and both more accurate single precision and double precision computational absorbed dose calculation methods have been added to ARCHER-RT and validated through a test case to evaluate the accuracy and performance of the algorithms. The validity of the source model and the radiation transport physics were benchmarked against Monte Carlo simulations performed with EGSnrc. Secondary dose-check physics plans, and a clinical prostate treatment plan were calculated to demonstrate the applicability of the platform for clinical use. Absorbed dose difference maps and gamma analyses were conducted to establish the accuracy and consistency between the two Monte Carlo models. Timing studies were conducted on a CPU, an NVIDIA GPU and an AMD GPU to evaluate the computational speed of ARCHER-RT.\n\n\nRESULTS\nPDDs were computed for different field sizes ranging from 1.5x1.5 to 22x40cm2 and the two codes agreed for all points outside high gradient regions within 3%. Axial profiles computed for a 10x10cm2 field for multiple depths agreed for all points outside high gradient regions within 2%. The test case investigating the impact of native single-precision compared to double-precision showed differences in voxels as large as 71.47% and the implementation of KAS single-precision reduced the difference to less than 0.01%. The 3%/3mm gamma pass rates for an MPPG5a MLC test case and a clinical VMAT prostate plan were 99.1% and 99.7% respectively. Timing studies conducted demonstrated that calculation of VMAT plans can were completed in 50.3 seconds, 187.9 seconds, and 216.8 seconds on an NVIDIA GPU, AMD GPU, and Intel CPU, respectively.\n\n\nCONCLUSION\nARCHER-RT is capable of patient specific VMAT external beam photon absorbed dose calculations and its potential has been demonstrated by benchmarking against a well validated EGSnrc model of a Varian TrueBeam. Additionally, the implementation of AMD's HIP has shown the flexibility of the ARCHER-RT platform for device independent calculations. This work demonstrates the significant addition of functionality added to ARCHER-RT framework which has marked utility for both research and clinical applications and demonstrates further that Monte Carlo based absorbed dose engines like ARCHER-RT have the potential for widespread clinical implementation.|Medical Physics (Lancaster)|2020|10.1002/mp.14143|B. Bednarz, X. George Xu, P. Caracappa, Tianyu Liu, D. Adam|2.4|2
695|Scaling the Hartree-Fock Matrix Build on Summit|Usage of Graphics Processing Units (GPU) has become strategic for simulating the chemistry of large molecular systems, with the majority of top supercomputers utilizing GPUs as their main source of computational horsepower. In this paper, a new fragmentation-based Hartree-Fock matrix build algorithm designed for scaling on many-GPU architectures is presented. The new algorithm uses a novel dynamic load balancing scheme based on a binned shell-pair container to distribute batches of significant shell quartets with the same code path to different GPUs. This maximizes computational throughput and load balancing, and eliminates GPU thread divergence due to integral screening. Additionally, the code uses a novel Fock digestion algorithm to contract electron repulsion integrals into the Fock matrix, which exploits all forms of permutational symmetry and eliminates thread synchronization requirements. The implementation demonstrates excellent scalability on the Summit computer, achieving good strong scaling performance up to 4096 nodes, and linear weak scaling up to 612 nodes.|International Conference for High Performance Computing, Networking, Storage and Analysis|2020|10.1109/SC41405.2020.00085|David L. Poole, Giuseppe M. J. Barca, Alistair P. Rendell, Melisa Alkan, M. Gordon, J. Vallejo, C. Bertoni|2.4|2
849|ImageMech: From Image to Particle Spring Network for Mechanical Characterization|The emerging demand for advanced structural and biological materials calls for novel modeling tools that can rapidly yield high-fidelity estimation on materials properties in design cycles. Lattice spring model , a coarse-grained particle spring network, has gained attention in recent years for predicting the mechanical properties and giving insights into the fracture mechanism with high reproducibility and generalizability. However, to simulate the materials in sufficient detail for guaranteed numerical stability and convergence, most of the time a large number of particles are needed, greatly diminishing the potential for high-throughput computation and therewith data generation for machine learning frameworks. Here, we implement CuLSM, a GPU-accelerated compute unified device architecture C++ code realizing parallelism over the spring list instead of the commonly used spatial decomposition, which requires intermittent updates on the particle neighbor list. Along with the image-to-particle conversion tool Img2Particle, our toolkit offers a fast and flexible platform to characterize the elastic and fracture behaviors of materials, expediting the design process between additive manufacturing and computer-aided design. With the growing demand for new lightweight, adaptable, and multi-functional materials and structures, such tailored and optimized modeling platform has profound impacts, enabling faster exploration in design spaces, better quality control for 3D printing by digital twin techniques, and larger data generation pipelines for image-based generative machine learning models.|Frontiers in Materials|2022|10.3389/fmats.2021.803875|T. Chiu, Chiang Yuan, Shu-Wei Chang|2.3333333333333335|2
853|A Spintronic In-Memory Computing Network for Efficient Hamming Codec Implementation|Hamming code is a linear error correcting code that is widely used in memory and communication systems. In general, a codec hardware is required to encode or decode the information. In this brief, we propose for the first time a spintronic in-memory computing (IMC) network consisting of magnetic tunnel junctions (MTJs) for Hamming codec hardware implementation. Such an IMC network stores the generation matrix or parity-check matrix in a spin transfer torque (STT) MTJ array and performs vector matrix multiplication (VMM) with modulo-2 to generate desired codewords or syndrome-vectors during the encoding or decoding process, respectively. The output results are represented by the states of the spin orbit torque (SOT) MTJs, which can unipolarly flip based on the modulo-2 VMM results. Based on our developed physics-based STT and SOT MTJ SPICE models, we verified the functionality and evaluated the performance of the design in the 40nm technology node. The simulation results show that our work can generate codewords and syndrome-vectors with much lower (103–105 times) energy consumption compared to state-of-the-art designs based on memristor network, CPU and GPU.|IEEE Transactions on Circuits and Systems - II - Express Briefs|2022|10.1109/tcsii.2022.3144678|W. Kang, Zhaohao Wang, E. Deng, Weisheng Zhao, Linjun Jiang, He Zhang|2.3333333333333335|2
973|Exploring the Versal AI Engines for Accelerating Stencil-based Atmospheric Advection Simulation|AMD Xilinx's new Versal Adaptive Compute Acceleration Platform (ACAP) is an FPGA architecture combining reconfigurable fabric with other on-chip hardened compute resources. AI engines are one of these and, by operating in a highly vectorized manner, they provide significant raw compute that is potentially beneficial for a range of workloads including HPC simulation. However, this technology is still early-on, and as yet unproven for accelerating HPC codes, with a lack of benchmarking and best practice. This paper presents an experience report, exploring porting of the Piacsek and Williams (PW) advection scheme onto the Versal ACAP, using the chip's AI engines to accelerate the compute. A stencil-based algorithm, advection is commonplace in atmospheric modelling, including several Met Office codes who initially developed this scheme. Using this algorithm as a vehicle, we explore optimal approaches for structuring AI engine compute kernels and how best to interface the AI engines with programmable logic. Evaluating performance using a VCK5000 against non-AI engine FPGA configurations on the VCK5000 and Alveo U280, as well as a 24-core Xeon Platinum Cascade Lake CPU and Nvidia V100 GPU, we found that whilst the number of channels between the fabric and AI engines are a limitation, by leveraging the ACAP we can double performance compared to an Alveo U280.|Symposium on Field Programmable Gate Arrays|2022|10.1145/3543622.3573047|Nick Brown|2.3333333333333335|2
1432|A Hybrid Framework for Fast and Accurate GPU Performance Estimation through Source-Level Analysis and Trace-Based Simulation|This paper proposes a hybrid framework for fast and accurate performance estimation of OpenCL kernels running on GPUs. The kernel execution flow is statically analyzed and thereupon the execution trace is generated via a loop-based bidirectional branch search. Then the trace is dynamically simulated to perform a dummy execution of the kernel to obtain the estimated time. The framework does not rely on profiling or measurement results which are used in conventional performance estimation techniques. Moreover, the lightweight trace-based simulation consumes much less time than a fine-grained GPU simulator. Our framework can accurately grasp the variation trend of the execution time in the design space and robustly predict the performance of the kernels across two generations of recent Nvidia GPU architectures. Experiments on four Commercial Off-The-Shelf (COTS) GPUs show that our framework can predict the runtime performance with average Mean Absolute Percentage Error (MAPE) of 17.04% and time consumption of a few seconds. We also demonstrate the practicability of our framework with a realworld application.|International Symposium on High-Performance Computer Architecture|2019|10.1109/HPCA.2019.00062|Xuehai Qian, A. Knoll, Xiebing Wang, Kai Huang|2.3333333333333335|2
288|Bufferless NOC Simulation of Large Multicore System on GPU Hardware|Last level cache management and core interconnection network play important roles in performance and power consumption in multicore system. Large scale chip multicore uses mesh interconnect widely due to scalability and simplicity of the mesh interconnection design. As interconnection network occupied significant area and consumes significant percent of system power, bufferless network is an appealing alternative design to reduce power consumption and hardware cost. We have designed and implemented a simulator for simulation of distributed cache management of large chip multicore where cores are connected using bufferless interconnection network. Also, we have redesigned and implemented the our simulator which is a GPU compatible parallel version of the same simulator using CUDA programming model. We have simulated target large chip multicore with up to 43,000 cores and achieved up to 25 times speedup on NVIDIA GeForce GTX 690 GPU over serial simulation.|arXiv.org|2015|10.1016/s0894-9166(15)30026-4|A. Sahu, Navin Kumar|2.3|2
350|Adaptation of fluid model EULAG to graphics processing unit architecture|The goal of this study is to adapt the multiscale fluid solver EULerian or LAGrangian framewrok (EULAG) to future graphics processing units (GPU) platforms. The EULAG model has the proven record of successful applications, and excellent efficiency and scalability on conventional supercomputer architectures. Currently, the model is being implemented as the new dynamical core of the COSMO weather prediction framework. Within this study, two main modules of EULAG, namely the multidimensional positive definite advection transport algorithm (MPDATA) and the variational generalized conjugate residual, elliptic pressure solver Generalized Conjugate Residual (GCR) are analyzed and optimized. In this paper, a method is proposed, which ensures a comprehensive analysis of the resource consumption including registers, shared, and global memories. This method allows us to identify bottlenecks of the algorithm, including data transfers between host and global memory, global and shared memories, as well as GPU occupancy. We put the emphasis on providing a fixed memory access pattern, padding as well as organizing computation in the MPDATA algorithm. The testing and validation of the new GPU implementation have been carried out based on modeling decaying turbulence of a homogeneous incompressible fluid in a triply‐periodic cube. Simulations performed using the standard version of EULAG and its new GPU implementation give similar solutions. Preliminary results show a promising increase in terms of computational efficiency. Copyright © 2014 John Wiley & Sons, Ltd.|Concurrency and Computation|2015|10.1002/cpe.3417|D. Wójcik, R. Wyrzykowski, Z. Piotrowski, K. Kurowski, Lukasz Szustak, M. Kulczewski, P. Kopta, B. Rosa, M. Ciznicki, K. Rojek|2.3|2
1197|OpenCL-Accelerated Probabilistic Power Flow for Active Distribution Networks|Open computing language (OpenCL) is an open standard developed for general purpose parallel programming of heterogeneous computing devices. OpenCL can be used to exploit the massively parallel architecture of graphics processing units (GPU) in performing non graphic tasks. This paper presents an OpenCL parallel implementation of Monte-Carlo simulation based probabilistic power flow analysis for large active distribution systems. The proposed implementation adopts single precision arithmetic and is independent of third-party linear algebra libraries, which makes it suitable for standalone implementation and use on any mainstream GPU. Moreover, given that it adopts the OpenCL programing model, the proposed approach may also be used on multicore CPUs. The accuracy and speedup of the proposed implementation is validated using several test systems, of sizes ranging from 32 to 10476 buses. The results show that the proposed parallelized implementation can achieve substantial speedups without requiring high performance computing purposed hardware.|IEEE Transactions on Sustainable Energy|2018|10.1109/TSTE.2017.2781148|M. Abdelaziz|2.2857142857142856|2
699|DEBISim: A simulation pipeline for dual energy CT-based baggage inspection systems1.|BACKGROUND\nMaterials characterization made possible by dual energy CT (DECT) scanners is expected to considerably improve automatic detection of hazardous objects in checked and carry-on luggage at our airports. Training a computer to identify the hazardous items from DECT scans however implies training on a baggage dataset that can represent all the possible ways a threat item can packed inside a bag. Practically, however, generating such data is made challenging by the logistics (and the permissions) related to the handling of the hazardous materials.\n\n\nOBJECTIVE\nThe objective of this study is to present a software simulation pipeline that eliminates the need for a human to handle dangerous materials and that allows for virtually unlimited variability in the placement of such materials in a bag alongside benign materials.\n\n\nMETHODS\nIn this paper, we present our DEBISim software pipeline that carries out an end-to-end simulation of a DECT scanner for virtual bags. The key highlights of DEBISim are: (i) A 3D user-interactive graphics editor for constructing a virtual 3D bag with manual placement of different types of objects in it; (ii) An automated virtual bag generation algorithm for creating randomized baggage datasets; (iii) An ability to spawn deformable sheets and liquid-filled containers in a virtual bag to represent plasticized and liquid explosives; and (iv) A GPU-based X-ray forward modelling block for spiral cone-beam scanners used in checked baggage screening.\n\n\nRESULTS\nWe have tested our simulator using two standard CT phantoms: the American College of Radiology (ACR) phantom and the NIST security screening phantom as well as on a set of reference materials representing commonly encountered items in checked baggage. For these phantoms, we have assessed the quality of the simulator by comparing the simulated data reconstructions with real CT scans of the same phantoms. The comparison shows that the material-specific properties as well as the CT artifacts in the scans generated by DEBISim are close to those produced by an actual scanner.\n\n\nCONCLUSION\nDEBISim is an end-to-end simulation framework for rapidly generating X-ray baggage data for dual energy cone-beam scanners.|Journal of X-Ray Science and Technology|2021|10.3233/XST-200808|Fangda Li, A. Kak, Ankit V. Manerikar|2.25|2
717|Monte Carlo simulation fused with target distribution modeling via deep reinforcement learning for automatic high-efficiency photon distribution estimation|Particle distribution estimation is an important issue in medical diagnosis. In particular, photon scattering in some medical devices extremely degrades image quality and causes measurement inaccuracy. The Monte Carlo (MC) algorithm is regarded as the most accurate particle estimation approach but is still time-consuming, even with graphic processing unit (GPU) acceleration. The goal of this work is to develop an automatic scatter estimation framework for high-efficiency photon distribution estimation. Specifically, a GPU-based MC simulation initially yields a raw scatter signal with a low photon number to hasten scatter generation. In the proposed method, assume that the scatter signal follows Poisson distribution, where an optimization objective function fused with sparse feature penalty is modeled. Then, an over-relaxation algorithm is deduced mathematically to solve this objective function. For optimizing the parameters in the over-relaxation algorithm, the deep Q-network in the deep reinforcement learning scheme is built to intelligently interact with the over-relaxation algorithm to accurately and rapidly estimate a scatter signal with the large range of photon numbers. Experimental results demonstrated that our proposed framework can achieve superior performance with structural similarity >0.94, peak signal-to-noise ratio >26.55  dB, and relative absolute error <5.62%, and the lowest computation time for one scatter image generation can be within 2 s.||2021|10.1364/prj.413486|Xiaoman Duan, Yuan Xu, Jianhui Ma, Zun Piao, G. Qin, Linghong Zhou, Shuang Huang|2.25|2
837|DSPS: Differentiable Stellar Population Synthesis|\n Models of stellar population synthesis (SPS) are the fundamental tool that relates the physical properties of a galaxy to its spectral energy distribution (SED). In this paper, we present DSPS: a python package for stellar population synthesis. All of the functionality in DSPS is implemented natively in the JAX library for automatic differentiation, and so our predictions for galaxy photometry are fully differentiable, and directly inherit the performance benefits of JAX, including portability onto GPUs. DSPS also implements several novel features, such as i) a flexible empirical model for stellar metallicity that incorporates correlations with stellar age, ii) support for the Diffstar model that provides a physically-motivated connection between the star formation history of a galaxy (SFH) and the mass assembly of its underlying dark matter halo. We detail a set of theoretical techniques for using autodiff to calculate gradients of predictions for galaxy SEDs with respect to SPS parameters that control a range of physical effects, including SFH, stellar metallicity, nebular emission, and dust attenuation. When forward modeling the colors of a synthetic galaxy population, we find that DSPS can provide a factor of 5 speedup over standard SPS codes on a CPU, and a factor of 300-400 on a modern GPU. When coupled with gradient-based techniques for optimization and inference, DSPS makes it practical to conduct expansive likelihood analyses of simulation-based models of the galaxy–halo connection that fully forward model galaxy spectra and photometry.|Monthly notices of the Royal Astronomical Society|2021|10.1093/mnras/stad456|A. Alarcon, M. Becker, J. Chaves-Montero, A. Benson, Andrew P. Hearin|2.25|2
290|LaPerm: Locality Aware Scheduler for Dynamic Parallelism on GPUs|Recent developments in GPU execution models and architectures have introduced dynamic parallelism to facilitate the execution of irregular applications where control flow and memory behavior can be unstructured, time-varying, and hierarchical. The changes brought about by this extension to the traditional bulk synchronous parallel (BSP) model also creates new challenges in exploiting the current GPU memory hierarchy. One of the major challenges is that the reference locality that exists between the parent and child thread blocks (TBs) created during dynamic nested kernel and thread block launches cannot be fully leveraged using the current TB scheduling strategies. These strategies were designed for the current implementations of the BSP model but fall short when dynamic parallelism is introduced since they are oblivious to the hierarchical reference locality. We propose LaPerm, a new locality-aware TB scheduler that exploits such parent-child locality, both spatial and temporal. LaPerm adopts three different scheduling decisions to i) prioritize the execution of the child TBs, ii) bind them to the stream multiprocessors (SMXs) occupied by their parents TBs, and iii) maintain workload balance across compute units. Experiments with a set of irregular CUDA applications executed on a cycle-level simulator employing dynamic parallelism demonstrate that LaPerm is able to achieve an average of 27% performance improvement over the baseline round-robin TB scheduler commonly used in modern GPUs.|International Symposium on Computer Architecture|2016|10.1145/3007787.3001199|A. Sidelnik, Jin Wang, S. Yalamanchili, Norman Rubin|2.2222222222222223|2
368|Evaluation of Emerging Energy-Efficient Heterogeneous Computing Platforms for Biomolecular and Cellular Simulation Workloads|Many of the continuing scientific advances achieved through computational biology are predicated on the availability of ongoing increases in computational power required for detailed simulation and analysis of cellular processes on biologically-relevant timescales. A critical challenge facing the development of future exascale supercomputer systems is the development of new computing hardware and associated scientific applications that dramatically improve upon the energy efficiency of existing solutions, while providing increased simulation, analysis, and visualization performance. Mobile computing platforms have recently become powerful enough to support interactive molecular visualization tasks that were previously only possible on laptops and workstations, creating future opportunities for their convenient use for meetings, remote collaboration, and as head mounted displays for immersive stereoscopic viewing. We describe early experiences adapting several biomolecular simulation and analysis applications for emerging heterogeneous computing platforms that combine power-efficient system-on-chip multi-core CPUs with high-performance massively parallel GPUs. We present low-cost power monitoring instrumentation that provides sufficient temporal resolution to evaluate the power consumption of individual CPU algorithms and GPU kernels. We compare the performance and energy efficiency of scientific applications running on emerging platforms with results obtained on traditional platforms, identify hardware and algorithmic performance bottlenecks that affect the usability of these platforms, and describe avenues for improving both the hardware and applications in pursuit of the needs of molecular modeling tasks on mobile devices and future exascale computers.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2016|10.1109/IPDPSW.2016.130|K. Schulten, J. Stone, Z. Luthey-Schulten, Joseph R. Peterson, James C. Phillips, Michael J. Hallock|2.2222222222222223|2
432|Hierarchical particle filtering for 3D hand tracking|We present a fast and accurate 3D hand tracking method which relies on RGB-D data. The method follows a model based approach using a hierarchical particle filter variant to track the model's state. The filter estimates the probability density function of the state's posterior. As such, it has increased robustness to observation noise and compares favourably to existing methods that can be trapped in local minima resulting in track loses. The data likelihood term is calculated by measuring the discrepancy between the rendered 3D model and the observations. Extensive experiments with real and simulated data show that hand tracking is achieved at a frame rate of 90fps with less that 10mm average error using a GPU implementation, thus comparing favourably to the state of the art in terms of both speed and tracking accuracy.|2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)|2015|10.1109/CVPRW.2015.7301343|Antonis A. Argyros, Nikolaos Kyriazis, Alexandros Makris|2.2|2
597|Gum-Net: Unsupervised Geometric Matching for Fast and Accurate 3D Subtomogram Image Alignment and Averaging|We propose a Geometric unsupervised matching Net-work (Gum-Net) for finding the geometric correspondence between two images with application to 3D subtomogram alignment and averaging. Subtomogram alignment is the most important task in cryo-electron tomography (cryo-ET), a revolutionary 3D imaging technique for visualizing the molecular organization of unperturbed cellular landscapes in single cells. However, subtomogram alignment and averaging are very challenging due to severe imaging limits such as noise and missing wedge effects. We introduce an end-to-end trainable architecture with three novel modules specifically designed for preserving feature spatial information and propagating feature matching information. The training is performed in a fully unsupervised fashion to optimize a matching metric. No ground truth transformation information nor category-level or instance-level matching supervision information is needed. After systematic assessments on six real and nine simulated datasets, we demonstrate that Gum-Net reduced the alignment error by 40 to 50% and improved the averaging resolution by 10%. Gum-Net also achieved 70 to 110 times speedup in practice with GPU acceleration compared to state-of-the-art subtomogram alignment methods. Our work is the first 3D unsupervised geometric matching method for images of strong transformation variation and high noise level. The training code, trained model, and datasets are available in our open-source software AITom.|Computer Vision and Pattern Recognition|2020|10.1109/cvpr42600.2020.00413|Min Xu, Xiangrui Zeng|2.2|2
615|A GPU implementation of classical density functional theory for rapid prediction of gas adsorption in nanoporous materials.|Nanoporous materials are promising as the next generation of absorbents for gas storage and separation with ultrahigh capacity and selectivity. The recent advent of data-driven approaches in materials modeling provides alternative routes to tailor nanoporous materials for customized applications. Typically, a data-driven model requires a large amount of training data that cannot be generated solely by experimental methods or molecular simulations. In this work, we propose an efficient implementation of classical density functional theory with a graphic processing unit (GPU) for the fast yet accurate prediction of gas adsorption isotherms in nanoporous materials. In comparison to serial computing with the central processing unit, the massively parallelized GPU implementation reduces the computational cost by more than two orders of magnitude. The proposed algorithm renders new opportunities not only for the efficient screening of a large materials database for gas adsorption but it may also serve as an important stepping stone toward the inverse design of nanoporous materials tailored to desired applications.|Journal of Chemical Physics|2020|10.1063/5.0020797|Jianzhong Wu, Musen Zhou|2.2|2
623|MDM: The GPU Memory Divergence Model|Analytical models enable architects to carry out early-stage design space exploration several orders of magnitude faster than cycle-accurate simulation by capturing first-order performance phenomena with a set of mathematical equations. However, this speed advantage is void if the conclusions obtained through the model are misleading due to model inaccuracies. Therefore, a practical analytical model needs to be sufficiently accurate to capture key performance trends across a broad range of applications and architectural configurations.In this work, we focus on analytically modeling the performance of emerging memory-divergent GPU-compute applications which are common in domains such as machine learning and data analytics. The poor spatial locality of these applications leads to frequent L1 cache blocking due to the application issuing significantly more concurrent cache misses than the cache can support, which cripples the GPU’s ability to use Thread-Level Parallelism (TLP) to hide memory latencies. We propose the GPU Memory Divergence Model (MDM) which faithfully captures the key performance characteristics of memory-divergent applications, including memory request batching and excessive NoC/DRAM queueing delays. We validate MDM against detailed simulation and real hardware, and report substantial improvements in (1) scope: the ability to model prevalent memory-divergent applications in addition to non-memory divergent applications; (2) practicality: 6.1× faster by computing model inputs using binary instrumentation as opposed to functional simulation; and (3) accuracy: 13.9% average prediction error versus 162% for the state-of-the-art GPUMech model.|Micro|2020|10.1109/MICRO50266.2020.00085|Magnus Jahre, Lu Wang, L. Eeckhout, Almutaz Adileh|2.2|2
1307|Accelerated Nodal Discontinuous Galerkin Simulations for Reverse Time Migration with Large Clusters|Improving both accuracy and computational performance of numerical tools is a major challenge for seismic imaging and generally requires specialized implementations to make full use of modern parallel architectures. We present a computational strategy for reverse-time migration (RTM) with acceleratoraided clusters. A new imaging condition computed from the pressure and velocity fields is introduced. The model solver is based on a high-order discontinuous Galerkin time-domain (DGTD) method for the pressure-velocity system with unstructured meshes and multi-rate local time-stepping. We adopted the MPI+X approach for distributed programming where X is a threaded programming model. In this work we chose OCCA, a unified framework that makes use of major multi-threading languages (e.g. CUDA and OpenCL) and offers the flexibility to run on several hardware architectures. DGTD schemes are suitable for efficient computations with accelerators thanks to localized element-to-element coupling and the dense algebraic operations required for each element. Moreover, compared to high-order finite-difference schemes, the thin halo inherent to DGTD method reduces the amount of data to be exchanged between MPI processes and storage requirements for RTM procedures. The amount of data to be recorded during simulation is reduced by storing only boundary values in memory rather than on disk and recreating the forward wavefields Computational results are presented that indicate that these methods are strong scalable up to at least 32 GPUs for a large-scale three-dimensional case.||2015|10.1093/gji/ggv380|W. Mulder, A. St.-Cyr, T. Warburton, A. Modave|2.2|2
1472|A nodal discontinuous Galerkin method for reverse-time migration on GPU clusters|Improving both accuracy and computational performance of numerical tools is a major challenge for seismic imaging and generally requires specialized implementations to make full use of modern parallel architectures. We present a computational strategy for reverse-time migration (RTM) with accelerator-aided clusters. A new imaging condition computed from the pressure and velocity fields is introduced. The model solver is based on a high-order discontinuous Galerkin time-domain (DGTD) method for the pressure–velocity system with unstructured meshes and multirate local time stepping. We adopted the MPI+X approach for distributed programming where X is a threaded programming model. In this work we chose OCCA, a unified framework that makes use of major multithreading languages (e.g. CUDA and OpenCL) and offers the flexibility to run on several hardware architectures. DGTD schemes are suitable for efficient computations with accelerators thanks to localized element-to-element coupling and the dense algebraic operations required for each element. Moreover, compared to high-order finite-difference schemes, the thin halo inherent to DGTD method reduces the amount of data to be exchanged between MPI processes and storage requirements for RTM procedures. The amount of data to be recorded during simulation is reduced by storing only boundary values in memory rather than on disk and recreating the forward wavefields. Computational results are presented that indicate that these methods are strong scalable up to at least 32 GPUs for a three-dimensional RTM case.||2015|10.1093/gji/ggv380|A. St.-Cyr, T. Warburton, A. Modave, W. Mulder|2.2|2
1416|Path Planning in Dynamic Environments Using Time-Warped Grids and a Parallel Implementation|This paper proposes a solution to the problem of smooth path planning for mobile robots in dynamic and unknown environments. A novel concept of Time-Warped Grid is introduced to predict the pose of obstacles in the environment and avoid collisions. The algorithm is implemented using C/C++ and the CUDA programming environment, and combines stochastic estimation (Kalman filter), harmonic potential fields and a rubber band model, and it translates naturally into the parallel paradigm of GPU programming. In simple terms, time-warped grids are progressively wider orbits around the mobile robot. Those orbits represent the variable time intervals estimated by the robot to reach detected obstacles. The proposed method was tested using several simulation scenarios for the Pioneer P3-DX robot, which demonstrated the robustness of the algorithm by finding the optimum path in terms of smoothness, distance, and collision-free, in both static or dynamic environments, and with large number of obstacles.|arXiv.org|2019|10.23919/acc.2019.8814894|G. DeSouza, Siavash Farzan|2.1666666666666665|2
1543|Characterizing and Detecting CUDA Program Bugs|While CUDA has become a major parallel computing platform and programming model for general-purpose GPU computing, CUDA-induced bug patterns have not yet been well explored. In this paper, we conduct the first empirical study to reveal important categories of CUDA program bug patterns based on 319 bugs identified within 5 popular CUDA projects in GitHub. Our findings demonstrate that CUDA-specific characteristics may cause program bugs such as synchronization bugs that are rather difficult to detect. To efficiently detect such synchronization bugs, we establish the first lightweight general CUDA bug detection framework, namely Simulee, to simulate CUDA program execution by interpreting the corresponding llvm bytecode and collecting the memory-access information to automatically detect CUDA synchronization bugs. To evaluate the effectiveness and efficiency of Simulee, we conduct a set of experiments and the experimental results suggest that Simulee can detect 20 out of the 27 studied synchronization bugs and successfully detects 26 previously unknown synchronization bugs, 10 of which have been confirmed by the developers.|arXiv.org|2019|10.1109/ase.2019.00075|Yuqun Zhang, Cong Liu, Mingyuan Wu, Husheng Zhou, Lingming Zhang|2.1666666666666665|2
282|Rendering Rough Opaque Materials with Interfaced Lambertian Microfacets|Specular microfacet distributions have been successfully employed by many authors for representing glossiness of materials. They are generally combined with a Lambertian term to account for the colored aspect. These representations make use of the Fresnel reflectance factor at the interface, but the transmission factor at the interface should also be managed. One solution is to employ a multi-layered model with a single layer for the rough interface, which requires a numerical simulation for handling the multiple reflections of light between the substrate and the interface. In this paper, we propose rather to use a representation corresponding to a Fresnel interface lying on a Lambertian substrate, for which the multiple reflections of light between the interface and the substrate can be expressed analytically. With this interfaced Lambertian model, we show how Fresnel transmission affects the material appearance for flat and rough surfaces with isotropic and anisotropic distributions, that produce light backscattering effects. We also propose a methodology for using such materials in any physically based Monte Carlo rendering system, as well as an approximate representation, suitable for GPU applications or measured data fitting. Our approach generalizes several previous models, including flat Lambertian materials as well as specular and Lambertian microfacets. Our results illustrate the wide range of materials that can be rendered with this representation.|IEEE Transactions on Visualization and Computer Graphics|2018|10.1109/TVCG.2017.2660490|B. Bringier, Emmanuelle Tauzia, L. Simonot, Mickaël Ribardière, Daniel Méneveaux|2.142857142857143|2
208|SIFI: AMD southern islands GPU microarchitectural level fault injector|General Purpose computing on Graphics Processing Unit offers a remarkable speedup for data parallel workloads, leveraging GPUs computational power. However, differently from graphic computing, it requires highly reliable operation in several application domains. In this paper we present SIFI a reliability evaluation framework for soft-errors on AMD GPUs built on top of Multi2Sim, a micro-architectural level simulator. SIFI is capable of computing different reliability metrics by means of two different techniques: fault injection and ACE analysis. Experiments performed on a set of 14 GPGPU applications targeting the AMD Southern Islands GPU architecture show the capability of the tool and the potential of its use to support decisions about the best architectural parameters for a given application.|IEEE International Symposium on On-Line Testing and Robust System Design|2017|10.1109/IOLTS.2017.8046209|D. Gizopoulos, S. Carlo, Alessandro Vallero|2.125|2
1353|Immersive VR Trainer for Landing Signal Officers|Training regimen of Landing Signal Officers (LSOs), members of a team that helps pilots land their planes on aircraft carriers deployed deep in the oceans, consists of two major components: (1) training on a large two stories-tall simulator LSO Trainer (LSOT) also known as 2H111 device that is located in Oceana VA (part of LSOs Initial Formal Ground Training (IFGT)), and (2) training on the job. This approach to LSOs training has remained unchanged for several decades. The time that LSOs spend in 2H111 simulator amounts to six one-hour long sessions during which they practice operating in all five different positions and roles that members of LSO crew have during an aircraft recovery. Access to this simulator is generally a prohibitive factor in training of LSOs; in order for an LSO to use it outside of IFGT classes, the simulator needs to be available i.e. it should not be already in use as a part of IFGT rotation for another unit, and not undergoing any repair or upgrade. Additionally, unless LSO is not located near Oceana, that individual needs to travel there. This is typically very hard to organize given extremely busy schedule of each LSO in their respective squadrons. The time spent in simulator during IFTG training rotation, is widely considered to be insufficient. There are three areas that have been identified as gaps in training: (1) training while preparing for subsequent deployment prior to attending IFTG, (2) low number of hours spent in simulator when LSOs attend IFTG, and (3) refresher training after LSOs complete their IFTG. This gap in training served as a major driver and motivation for engaging in design and development of a lightweight training system that would be both affordable and mobile, providing LSOs with training opportunity any time such support is needed. Additional motivation factor was a number of reported mishaps, and their most probable causes. According to the Naval Safety Center, during the period 2005 and July of 2015, there were 108 landing-related mishaps on aircraft carriers, where 99 of them involved the LSO in some manner [1]. Both the gaps in training of LSO officers with 2H111 device and the number of costly mishaps that involved LSOs, were clear indicators that it would be highly beneficial to offer training force with additional, novel training system and that goes beyond current training capabilities. It was clear that that a novel solution will need to be lightweight and mobile, and as such be capable of providing unlimited number of training opportunities unrestricted by location and time. The main goal of this research demo is to introduce and demonstrate a prototype system that was built to address described training need in LSO domain. More extensive discussions of domain and descriptions of developed system can be found in [2] and [3]. Prior to design and development of the prototype of new training system, we adopted five major design goals. The system should: (1) support all major capabilities and training objectives supported by the 2H111 simulator, (2) leverage capabilities of immersive VR technology to enable training and interactive capabilities not currently supported by 2H111 device, (3) use only commercial off-the-shelf (COTS) solutions and make sure the system is truly 'lightweight', (4) minimize the potential for symptoms of cybersickness it should be capable of maintaining a high frame rate throughout the interactive experience, (5) integrate a variety of typical COTS input devices and support both the trainees and instructors with different input modalities. Additional goals included ability to maintain and operate it with minimal costs, and the ease of adding new hardware and software components and upgrades while minimizing or completely avoiding the cost special software licenses. Domain analysis: We conducted detailed task analysis for five different positions in LSO team, and collected data that reflected community understanding of user domain. The latter included information about current training practices; technical characteristics of 2H111 device; LSOs understandings of benefits and deficiencies of training in 2H111 device; LSOs system of values directly related to elements of training systems and processes; and LSOs attitudes and acceptance level towards different forms of training systems and training approaches (including potential obstacles in adoption of those solutions in everyday training practice). System architecture: The prototype training system that has been developed in this effort, uses Alienware 17 R2 laptop with Intel Core i7-4980HQ CPU @ 2.80 GHz, 16 GB RAM and GeForce GTX 980M GPU. A set of COTS input and output devices include two Xbox Controllers for trainee LSO and instructor (one is used by a trainee for navigation and object manipulation, and one by instructor for scene manipulation), in addition to devices used only by the trainee LSO: headphones with microphone, head-mounted display (HMD) Oculus DK2 headset and Leap Motion Controller mounted onto front of Oculus Rift headset. Oculus DK2 has a resolution of 960 x 1080 per eye, with max refresh rate of 75 Hz, field of view (FOV) 100 degrees, and weight of device: .97 lbs (Figure 1). Figure 1: Hardware and software architecture. Input devices: Having several input devices like game controller and Leap Motion controller, provides the system with desired operational flexibility. It supports individual operator's LEAVE 0.5 INCH SPACE AT BOTTOM OF LEFT COLUMN ON FIRST PAGE FOR COPYRIGHT BLOCK * lcgreunke@gmail.com, ** asadagic@nps.edu preference when it comes to the choice of interaction modality and input device to accomplish a variety of tasks in the system, and it provides useful backup functionality in case one input device or one mode of interaction fails. Leap Motion controller is used to support intuitive manipulation and interaction with virtual Landing Signal Officer Display System LSODS (Figure 2). This instrument provides a wide range of information to LSO, from live video feed of platform camera (space in the center of landing area), to gear status and divert information. A sizable effort had been invested to simulate all capabilities of this important instrument. Audio communication: The communication between LSO and pilot in an operational situation is done by UHF headset. In prototype training system this is accomplished by using a microphone and a headset. The actual utterances (communications) done by LSO are processed by a custom-made voice recognition application. The result of this communication is reflected in changed behavior of the approaching aircraft. Head-mounted display: The choice of display solution in this case head-mounted display was governed by a need to have fully simulated (virtual) training environment and to provide stereoscopic depth cue to the trainees. Recent developments in HMD technology and a rise of inexpensive, good quality HMD solutions, was an additional factor that benefited the work on this system. Our choice was to use Oculus DK2 headset. The line of Oculus Rift products has been evaluated as very promising they were well supported by industry allowing for easy integration with Unity package and a range of input devices, and they were perceived as having a high probability of continued line of perfected products in the future. This held a promise of easy upgrades and extension to networked version of the training system. Figure 2: Aircraft landing and user interaction with Landing Signal Officer Display System (LSODS). 3D tools and assets: The three-dimensional (3D) content and object behaviors used in this application were developed using Unity game engine; the same system supports real time scene simulation and user interaction. Blender and 3DS Max were used for additional model creation and editing, Photoshop for editing of textures and Audacity for audio editing. We purchased several models of aircraft from the Turbo Squid 3D modeling website and 3DWarehouse; our goal was to ensure that system had 3D models of higher fidelity both in terms of geometry and textures. This action was in direct response to LSOs remarks about low fidelity of models being used in current 2H111 device (that had a most direct influence over perceived lack of realism). Having highly complex 3D models of aircraft and carrier also resulted in reduced frame rate. In order to address that issue we invested considerable efforts to optimize solution by reducing the size of the models while making sure that the overall level of realism and visual appearance remained high. As an example, the final 3D objects used in basic scenarios had following polygon counts: 3D model of T-45C 3D aircraft had 110K vertices, 3D model of A-18G 3D aircraft had 50K vertices, and 3D model of Nimitz Class carrier had 9K vertices. Behaviors and environment simulation: Basic Unity assets were used to simulate additional elements important for LSO’s decision-making during recovery of the aircraft. Those included ocean wave motion that affects pitching of the flight deck (LSO needs to constantly compare this with the trajectory of the approaching aircraft). Unity package was leveraged to support visual water effects, while simple behavior for ship movement had to be added; this enabled believable movement in the pitch, roll, and heave of the ship. Skybox assets from Unity package were used to simulate a variety of environmental conditions in which LSOs need to operate (examples: clear day, clear night, and overcast day). System performance: The framerate that the final system is capable of generating (this includes rendering of all 3D models and behaviors) ranges between 60 frames per second (FPS) for situation when only a smaller portion of scene is in a field of view, and 37 FPS when user is looking at six aircrafts lined up on the deck of the carrier. Further system optimization is planned to address this and increase framerate even in the worst-case scenario when the||2016|10.1109/tvcg.2016.2518098|Larry Greunke, Amela Sadagic|2.111111111111111|2
278|Generating Efficient Tensor Contractions for GPUs|Many scientific and numerical applications, including quantum chemistry modeling and fluid dynamics simulation, require tensor product and tensor contraction evaluation. Tensor computations are characterized by arrays with numerous dimensions, inherent parallelism, moderate data reuse and many degrees of freedom in the order in which to perform the computation. The best-performing implementation is heavily dependent on the tensor dimensionality and the target architecture. In this paper, we map tensor computations to GPUs, starting with a high-level tensor input language and producing efficient CUDA code as output. Our approach is to combine tensor-specific mathematical transformations with a GPU decision algorithm, machine learning and auto tuning of a large parameter space. Generated code shows significant performance gains over sequential and Open MP parallel code, and a comparison with Open ACC shows the importance of auto tuning and other optimizations in our framework for achieving efficient results.|International Conference on Parallel Processing|2015|10.1109/ICPP.2015.106|Axel Rivera, P. Hovland, Mary W. Hall, Prasanna Balaprakash, T. Nelson, E. Jessup, B. Norris|2.1|2
364|Spectrotemporal CT data acquisition and reconstruction at low dose.|PURPOSE\nX-ray computed tomography (CT) is widely used, both clinically and preclinically, for fast, high-resolution anatomic imaging; however, compelling opportunities exist to expand its use in functional imaging applications. For instance, spectral information combined with nanoparticle contrast agents enables quantification of tissue perfusion levels, while temporal information details cardiac and respiratory dynamics. The authors propose and demonstrate a projection acquisition and reconstruction strategy for 5D CT (3D+dual energy+time) which recovers spectral and temporal information without substantially increasing radiation dose or sampling time relative to anatomic imaging protocols.\n\n\nMETHODS\nThe authors approach the 5D reconstruction problem within the framework of low-rank and sparse matrix decomposition. Unlike previous work on rank-sparsity constrained CT reconstruction, the authors establish an explicit rank-sparse signal model to describe the spectral and temporal dimensions. The spectral dimension is represented as a well-sampled time and energy averaged image plus regularly undersampled principal components describing the spectral contrast. The temporal dimension is represented as the same time and energy averaged reconstruction plus contiguous, spatially sparse, and irregularly sampled temporal contrast images. Using a nonlinear, image domain filtration approach, the authors refer to as rank-sparse kernel regression, the authors transfer image structure from the well-sampled time and energy averaged reconstruction to the spectral and temporal contrast images. This regularization strategy strictly constrains the reconstruction problem while approximately separating the temporal and spectral dimensions. Separability results in a highly compressed representation for the 5D data in which projections are shared between the temporal and spectral reconstruction subproblems, enabling substantial undersampling. The authors solved the 5D reconstruction problem using the split Bregman method and GPU-based implementations of backprojection, reprojection, and kernel regression. Using a preclinical mouse model, the authors apply the proposed algorithm to study myocardial injury following radiation treatment of breast cancer.\n\n\nRESULTS\nQuantitative 5D simulations are performed using the MOBY mouse phantom. Twenty data sets (ten cardiac phases, two energies) are reconstructed with 88 μm, isotropic voxels from 450 total projections acquired over a single 360° rotation. In vivo 5D myocardial injury data sets acquired in two mice injected with gold and iodine nanoparticles are also reconstructed with 20 data sets per mouse using the same acquisition parameters (dose: ∼60 mGy). For both the simulations and the in vivo data, the reconstruction quality is sufficient to perform material decomposition into gold and iodine maps to localize the extent of myocardial injury (gold accumulation) and to measure cardiac functional metrics (vascular iodine). Their 5D CT imaging protocol represents a 95% reduction in radiation dose per cardiac phase and energy and a 40-fold decrease in projection sampling time relative to their standard imaging protocol.\n\n\nCONCLUSIONS\nTheir 5D CT data acquisition and reconstruction protocol efficiently exploits the rank-sparse nature of spectral and temporal CT data to provide high-fidelity reconstruction results without increased radiation dose or sampling time.|Medical Physics (Lancaster)|2015|10.1118/1.4931407|D. Kirsch, Chang-Lung Lee, C. Badea, D. Clark|2.1|2
304|Parallelized nonlinear model predictive control on GPU|This paper describes a nonlinear model predictive control (NMPC) algorithm on graphics processing unit (GPU). Existing NMPC uses iteration for finding stationary condition and hence serial algorithm. Due to declining increase in processor clock speed, more parallelism is necessary for faster computing, particularly for GPU where thousands of threads can be executed in parallel. In this paper, we introduce Monte Carlo method for finding optimal time sequence of control input within the time horizon. Forward simulations are executed in parallel with different input sequence. The best solution is inherited as the initial value for the next step. The method requires only executable forward simulation code, cost function and constraints but no derivative. Our method returns the results within a few milliseconds. Comparison with existing NMPC shows that it produces similar result but more robust with state constraint. The proposed method is evaluated in simulation of semi-active damper and swing-up inverted pendulum. For latter case, experiments are also conducted.|Asian Control Conference|2017|10.1109/ASCC.2017.8287416|Shimpei Ohyama, H. Date|2.0|2
450|Classical molecular dynamics on graphics processing unit architectures|Molecular dynamics (MD) has experienced a significant growth in the recent decades. Simulating systems consisting of hundreds of thousands of atoms is a routine task of computational chemistry researchers nowadays. Thanks to the straightforwardly parallelizable structure of the algorithms, the most promising method to speed‐up MD calculations is exploiting the large‐scale processing power offered by the parallel hardware architecture of graphics processing units or GPUs. Programming GPUs is becoming easier with general‐purpose GPU computing frameworks and higher levels of abstraction. In the recent years, implementing MD simulations on graphics processors has gained a large interest, with multiple popular software packages including some form of GPU‐acceleration support. Different approaches have been developed regarding various aspects of the algorithms, with important differences in the specific solutions. Focusing on published works in the field of classical MD, we describe the chosen implementation methods and algorithmic techniques used for porting to GPU, as well as how recent advances of GPU architectures will provide even more optimization possibilities in the future.|WIREs Computational Molecular Science|2020|10.1002/wcms.1444|István Ladjánszki, Á. Rák, Ádám Jász, G. Cserey|2.0|2
480|Code generation for massively parallel phase-field simulations|This article describes the development of automatic program generation technology to create scalable phase-field methods for material science applications. To simulate the formation of microstructures in metal alloys, we employ an advanced, thermodynamically consistent phase-field method. A state-of-the-art large-scale implementation of this model requires extensive, time-consuming, manual code optimization to achieve unprecedented fine mesh resolution. Our new approach starts with an abstract description based on free-energy functionals which is formally transformed into a continuous PDE and discretized automatically to obtain a stencil-based time-stepping scheme. Subsequently, an automatized performance engineering process generates highly optimized, performance-portable code for CPUs and GPUs. We demonstrate the efficiency for real-world simulations on large-scale GPU-based (PizDaint) and CPU-based (SuperMUC-NG) supercomputers. Our technique simplifies program development and optimization for a wide class of models. We further outperform existing, manually optimized implementations as our code can be generated specifically for each phase-field model and hardware configuration.|International Conference on Software Composition|2019|10.1145/3295500.3356186|G. Wellein, J. Hötzer, Dominik Ernst, M. Seiz, H. Hierl, H. Köstler, Martin Bauer, Julian Hammer, U. Rüde, Jan Hönig, B. Nestler|2.0|2
510|Accelerating the Generalized Born with Molecular Volume and Solvent Accessible Surface Area Implicit Solvent Model Using Graphics Processing Units|The generalized Born with molecular volume and solvent accessible surface area (GBMV2/SA) implicit solvent model provides an accurate description of molecular volume and has the potential to accurately describe the conformational equilibria of structured and disordered proteins. However, its broader application has been limited by the computational cost and poor scaling in parallel computing. Here, we report an efficient implementation of both the electrostatic and nonpolar components of GBMV2/SA on graphics processing unit (GPU) within the CHARMM/OpenMM module. The GPU‐GBMV2/SA is numerically equivalent to the original CPU‐GBMV2/SA. The GPU acceleration offers ~60‐ to 70‐fold speedup on a single NVIDIA TITAN X (Pascal) graphics card for molecular dynamic simulations of both folded and unstructured proteins of various sizes. The current implementation can be further optimized to achieve even greater acceleration with minimal reduction on the numerical accuracy. The successful development of GPU‐GBMV2/SA greatly facilitates its application to biomolecular simulations and paves the way for further development of the implicit solvent methodology. © 2019 Wiley Periodicals, Inc.|Journal of Computational Chemistry|2019|10.1002/jcc.26133|Xiaorong Liu, Jianhan Chen, Xiping Gong, Erik B. Nordquist, C. Brooks, M. Feig, M. Chiricotto|2.0|2
524|Heterogeneous Computing (CPU-GPU) for Pollution Dispersion in an Urban Environment|The use of Computational Fluid Dynamics (CFD) to assist in air quality studies in urban environments can provide accurate results for the dispersion of pollutants. However, due to the computational resources needed, simulation domain sizes tend to be limited. This study aims to improve the computational efficiency of an emission and dispersion model implemented in a CPU-based solver by migrating it to a CPU–GPU-based one. The migration of the functions that handle boundary conditions and source terms for the pollutants is explained, as well as the main differences present in the solvers used. Once implemented, the model was used to run simulations with both engines on different platforms, enabling the comparison between them and reaching promising time improvements in favor of the use of GPUs.|De Computis|2020|10.3390/computation8010003|M. Mendina, G. Fernández, G. Usera|2.0|2
551|A Novel GPU-Based Acceleration Algorithm for a Longwave Radiative Transfer Model|Graphics processing unit (GPU)-based computing for climate system models is a longstanding research area of interest. The rapid radiative transfer model for general circulation models (RRTMG), a popular atmospheric radiative transfer model, can calculate atmospheric radiative fluxes and heating rates. However, the RRTMG has a high calculation time, so it is urgent to study its GPU-based efficient acceleration algorithm to enable large-scale and long-term climatic simulations. To improve the calculative efficiency of radiation transfer, this paper proposes a GPU-based acceleration algorithm for the RRTMG longwave radiation scheme (RRTMG_LW). The algorithm concept is accelerating the RRTMG_LW in the g- p o i n t dimension. After implementing the algorithm in CUDA Fortran, the G-RRTMG_LW was developed. The experimental results indicated that the algorithm was effective. In the case without I/O transfer, the G-RRTMG_LW on one K40 GPU obtained a speedup of 30.98× over the baseline performance on one single Intel Xeon E5-2680 CPU core. When compared to its counterpart running on 10 CPU cores of an Intel Xeon E5-2680 v2, the G-RRTMG_LW on one K20 GPU in the case without I/O transfer achieved a speedup of 2.35×.|Applied Sciences|2020|10.3390/app10020649|He Zhang, Jinrong Jiang, Yuzhu Wang, Yuan Zhao|2.0|2
578|Veros v0.1 – a fast and versatile ocean simulator in pure Python|Abstract. A\ngeneral circulation ocean model is translated from Fortran to Python. Its\ncode structure is optimized to exploit available Python utilities, remove\nsimulation bottlenecks, and comply with modern best practices. Furthermore, support for\nBohrium is added, a framework that provides a just-in-time compiler for array\noperations and that supports parallel execution on both CPU and GPU targets. For applications containing more than a million grid elements, such as a\ntypical 1∘×1∘ horizontal resolution global ocean model,\nVeros is approximately half as fast as the MPI-parallelized Fortran base code\non 24 CPUs and as fast as the Fortran reference when running on a high-end\nGPU. By replacing the original conjugate gradient stream function solver with\na solver from the pyAMG Python package, this particular subroutine\noutperforms the corresponding Fortran version by up to 1 order of magnitude. The study is concluded with a simple application in which the North Atlantic\nwave response to a Southern Ocean wind perturbation is investigated. It is\nfound that even in a realistic setting the phase speeds of boundary waves\nmatched the expectations based on theory and idealized models.\n|Geoscientific Model Development|2018|10.5194/GMD-11-3299-2018|C. Eden, M. R. B. Kristensen, Dion Häfner, R. Jacobsen, B. Vinter, M. Jochum, R. Nuterman|2.0|2
706|Real-Time Simulation of a Cerebellar Scaffold Model on Graphics Processing Units|Large-scale simulation of detailed computational models of neuronal microcircuits plays a prominent role in reproducing and predicting the dynamics of the microcircuits. To reconstruct a microcircuit, one must choose neuron and synapse models, placements, connectivity, and numerical simulation methods according to anatomical and physiological constraints. For reconstruction and refinement, it is useful to be able to replace one module easily while leaving the others as they are. One way to achieve this is via a scaffolding approach, in which a simulation code is built on independent modules for placements, connections, and network simulations. Owing to the modularity of functions, this approach enables researchers to improve the performance of the entire simulation by simply replacing a problematic module with an improved one. Casali et al. (2019) developed a spiking network model of the cerebellar microcircuit using this approach, and while it reproduces electrophysiological properties of cerebellar neurons, it takes too much computational time. Here, we followed this scaffolding approach and replaced the simulation module with an accelerated version on graphics processing units (GPUs). Our cerebellar scaffold model ran roughly 100 times faster than the original version. In fact, our model is able to run faster than real time, with good weak and strong scaling properties. To demonstrate an application of real-time simulation, we implemented synaptic plasticity mechanisms at parallel fiber–Purkinje cell synapses, and carried out simulation of behavioral experiments known as gain adaptation of optokinetic response. We confirmed that the computer simulation reproduced experimental findings while being completed in real time. Actually, a computer simulation for 2 s of the biological time completed within 750 ms. These results suggest that the scaffolding approach is a promising concept for gradual development and refactoring of simulation codes for large-scale elaborate microcircuits. Moreover, a real-time version of the cerebellar scaffold model, which is enabled by parallel computing technology owing to GPUs, may be useful for large-scale simulations and engineering applications that require real-time signal processing and motor control.|Frontiers in Cellular Neuroscience|2021|10.3389/fncel.2021.623552|E. D’Angelo, Tadashi Yamazaki, C. Casellato, Rin Kuriyama|2.0|2
713|Neuromorphic Computing is Turing-Complete|Neuromorphic computing is a non-von Neumann computing paradigm that performs computation by emulating the human brain. Neuromorphic systems are extremely energy-efficient and known to consume thousands of times less power than CPUs and GPUs. They have the potential to drive critical use cases such as autonomous vehicles, edge computing and internet of things in the future. For this reason, they are sought to be an indispensable part of the future computing landscape. Neuromorphic systems are mainly used for spike-based machine learning applications, although there are some non-machine learning applications in graph theory, differential equations, and spike-based simulations. These applications suggest that neuromorphic computing might be capable of general-purpose computing. However, general-purpose computability of neuromorphic computing has not been established yet. In this work, we prove that neuromorphic computing is Turing-complete and therefore capable of general-purpose computing. Specifically, we present a model of neuromorphic computing, with just two neuron parameters (threshold and leak), and two synaptic parameters (weight and delay). We devise neuromorphic circuits for computing all the μ-recursive functions (i.e., constant, successor and projection functions) and all the μ-recursive operators (i.e., composition, primitive recursion and minimization operators). Given that the μ-recursive functions and operators are precisely the ones that can be computed using a Turing machine, this work establishes the Turing-completeness of neuromorphic computing.|International Conference on Systems|2021|10.1145/3546790.3546806|Bill Kay, T. Potok, Prasanna Date, Catherine D. Schuman|2.0|2
734|Real-Time Animation Complexity of Interactive Clothing Design Based on Computer Simulation|With the innovation of computer, virtual clothing has also emerged. This research mainly discusses the real-time animation complex of interactive clothing design based on computer simulation. In the process of realizing virtual clothing, the sample interpolation synthesis method is used, and the human body sample library is constructed using the above two methods (primitive construction method and model reconstruction method) first, and then, the human body model is obtained by interpolation calculation according to the personalized parameters. Building a clothing model is particularly important for the effect of trying on. The clothing that needs to be displayed can be scanned and then input into the computer to build the model. The model can be directly built in 3DMAX and other software and then its surface texture can be mapped, or the clothing model can be directly built. The 3D model in the 3ds file is loaded by the loop body nested switch branch selection structure. Correspondingly, the write-back operation of 3ds files is similar. Just follow the general structure of the 3ds file and write the root block, version information block, edit information block, key frame information block, etc. to a brand new file in sequence. The main reason for this article to perform the 3ds file write-back operation is that, after the clothing model is dynamically simulated through the dynamic principle, the deformed key animation frame needs to be saved as a 3ds file so that it can be further imported into the 3DSMAX software and generated by the renderer, form high-quality picture information, and finally get high-definition animation video. In the CPU-GPU hybrid method, modules such as force calculation, collision processing, and position update use the GPU method, while overstretching is processed by the CPU method, making the overall performance 10 times higher than the pure CPU method. This research helps to promote the development of 3D virtual clothing design.|Complex|2021|10.1155/2021/9988623|Yufeng Xin, Guopeng Qiu, Dongliang Zhang|2.0|2
777|High-throughput, accurate Monte Carlo simulation on CPU hardware for PET applications|Monte Carlo simulations (MCS) represent a fundamental approach to modelling the photon interactions in positron emission tomography (PET). A variety of PET-dedicated MCS tools are available to assist and improve PET imaging applications. Of these, GATE has evolved into one of the most popular software for PET MCS because of its accuracy and flexibility. However, simulations are extremely time-consuming. The use of graphics processing units (GPU) has been proposed as a solution to this, with reported acceleration factors about 400–800. These factors refer to GATE benchmarks performed on a single CPU core. Consequently, CPU-based MCS can also be easily accelerated by one order of magnitude or beyond when exploiting multi-threading on powerful CPUs. Thus, CPU-based implementations become competitive when further optimisations can be achieved. In this context, we have developed a novel, CPU-based software called the PET physics simulator (PPS), which combines several efficient methods to significantly boost the performance. PPS flexibly applies GEANT4 cross-sections as a pre-calculated database, thus obtaining results equivalent to GATE. This is demonstrated for an elaborated PET scanner with 3-layer block detectors. All code optimisations yield an acceleration factor of ≈20 (single core). Multi-threading on a high-end CPU workstation (96 cores) further accelerates the PPS by a factor of 80. This results in a total speed-up factor of ≈1600, which outperforms comparable GPU-based MCS by a factor of ≳2. Optionally, the proposed method of coincidence multiplexing can further enhance the throughput by an additional factor of ≈15. The combination of all optimisations corresponds to an acceleration factor of ≈24 000. In this way, the PPS can simulate complex PET detector systems with an effective throughput of 106 photon pairs in less than 10 milliseconds.|Physics in Medicine and Biology|2021|10.1088/1361-6560/ac1ca0|M. Lenz, U. Pietrzyk, N. Shah, J. Scheins, C. Lerche|2.0|2
814|Training and Serving ML workloads with Kubeflow at CERN|Machine Learning (ML) has been growing in popularity in multiple areas and groups at CERN, covering fast simulation, tracking, anomaly detection, among many others. We describe a new service available at CERN, based on Kubeflow and managing the full ML lifecycle: data preparation and interactive analysis, large scale distributed model training and model serving. We cover specific features available for hyper-parameter tuning and model metadata management, as well as infrastructure details to integrate accelerators and external resources. We also present results and a cost evaluation from scaling out a popular ML use case using public cloud resources, achieving close to linear scaling when using a large number of GPUs.|EPJ Web of Conferences|2021|10.1051/epjconf/202125102067|R. Rocha, D. Golubović|2.0|2
836|Tensor‐CA: A high‐performance cellular automata model for land use simulation based on vectorization and GPU|With the ability to understand linkages and feedbacks between land use dynamics and human–land relationships, cellular automata (CA) are extensively applied in land use/cover change (LUCC) simulation. However, with complex transition rules and a growing volume of spatial data, conventional serial CA models cannot meet the demands of efficient computation. In this article, a Tensor‐CA model using vectorization and Graphics Processing Unit (GPU) technology based on a tensor computation framework for optimizing multiple LUCC simulations is presented. Complex transition rules of LUCC‐CA models are vectorized and formalized to tensor operations which are effectively solved by GPU. The proposed Tensor‐CA model was applied to LUCC simulations in the Pearl River Delta of China. The experimental results indicate that the proposed model effectively improved the performance compared to Serial‐CA, Parallel‐CA, and GPU‐CA.|Trans. GIS|2021|10.1111/tgis.12881|Jinqiang He, Honghui Zhang, Haoming Zhuang, Xinchang Zhang, Changjiang Wu, Xun Liang, Xiaoping Liu, Yiling Cai, Yuchao Yan|2.0|2
881|A Data-Driven Fragmentation Model for Carbon Therapy GPU-Accelerated Monte-Carlo Dose Recalculation|The advent of Graphics Processing Units (GPU) has prompted the development of Monte Carlo (MC) algorithms that can significantly reduce the simulation time with respect to standard MC algorithms based on Central Processing Unit (CPU) hardware. The possibility to evaluate a complete treatment plan within minutes, instead of hours, paves the way for many clinical applications where the time-factor is important. FRED (Fast paRticle thErapy Dose evaluator) is a software that exploits the GPU power to recalculate and optimise ion beam treatment plans. The main goal when developing the FRED physics model was to balance accuracy, calculation time and GPU execution guidelines. Nowadays, FRED is already used as a quality assurance tool in Maastricht and Krakow proton clinical centers and as a research tool in several clinical and research centers across Europe. Lately the core software has been updated including a model of carbon ions interactions with matter. The implementation is phenomenological and based on carbon fragmentation data currently available. The model has been tested against the MC FLUKA software, commonly used in particle therapy, and a good agreement was found. In this paper, the new FRED data-driven model for carbon ion fragmentation will be presented together with the validation tests against the FLUKA MC software. The results will be discussed in the context of FRED clinical applications to 12C ions treatment planning.|Frontiers in Oncology|2022|10.3389/fonc.2022.780784|G. Battistoni, G. Franciosini, A. De Gregorio, A. Trigilio, M. Toppi, A. Sarti, V. Patera, G. Traini, A. Schiavi, M. Fischetti, P. De Maria, M. Marafini, M. De Simoni|2.0|2
889|EasyVRModeling|The latest innovations of VR make it possible to construct 3D models in a holographic immersive simulation environment. In this paper, we develop a user-friendly mid-air interactive modeling system named EasyVRModeling. We first prepare a dataset consisting of diverse components and precompute the discrete signed distance function (SDF) for each component. During the modeling phase, users can freely design complicated shapes with a pair of VR controllers. Based on the discrete SDF representation, any CSG-like operation (union, intersect, subtract) can be performed voxel-wise. Throughout the modeling process, we maintain one single dynamic SDF for the whole scene so that the zero-level set surface of the SDF exactly encodes the up-to-date constructed shape. Both SDF fusion and surface extraction are implemented via GPU to allow for smooth user experience. We asked 34 volunteers to create their favorite models using EasyVRModeling. With a simple training process for several minutes, most of them can create a fascinating shape or even a descriptive scene very quickly.|Proceedings of the ACM on Computer Graphics and Interactive Techniques|2022|10.1145/3522613|Rui Xu, Shuangmin Chen, Zhiying Fu, Changhe Tu, Shiqing Xin, Lin Lu, Chenglei Yang|2.0|2
894|GCoM: a detailed GPU core model for accurate analytical modeling of modern GPUs|Analytical models can greatly help computer architects perform orders of magnitude faster early-stage design space exploration than using cycle-level simulators. To facilitate rapid design space exploration for graphics processing units (GPUs), prior studies have proposed GPU analytical models which capture first-order stall events causing performance degradation; however, the existing analytical models cannot accurately model modern GPUs due to their outdated and highly abstract GPU core microarchitecture assumptions. Therefore, to accurately evaluate the performance of modern GPUs, we need a new GPU analytical model which accurately captures the stall events incurred by the significant changes in the core microarchitectures of modern GPUs. We propose GCoM, an accurate GPU analytical model which faithfully captures the key core-side stall events of modern GPUs. Through detailed microarchitecture-driven GPU core modeling, GCoM accurately models modern GPUs by revealing the following key core-side stalls overlooked by the existing GPU analytical models. First, GCoM identifies the compute structural stall events caused by the limited per-sub-core functional units. Second, GCoM exposes the memory structural stalls due to the limited banks and shared nature of per-core L1 data caches. Third, GCoM correctly predicts the memory data stalls induced by the sectored L1 data caches which split a cache line into a set of sectors sharing the same tag. Fourth, GCoM captures the idle stalls incurred by the inter- and intra-core load imbalances. Our experiments using an NVIDIA RTX 2060 configuration show that GCoM greatly improves the modeling accuracy by achieving a mean absolute error of 10.0% against Accel-Sim cycle-level simulator, whereas the state-of-the-art GPU analytical model achieves a mean absolute error of 44.9%.|International Symposium on Computer Architecture|2022|10.1145/3470496.3527384|Suhyun Lee, Jinho Lee, Youngsok Kim, Jounghoo Lee, Jinyoung Woo, Hanhwi Jang, Yeonan Ha|2.0|2
937|Developments in Performance and Portability for MadGraph5_aMC@NLO|Event generators simulate particle interactions using Monte Carlo techniques, providing the pri-mary connection between experiment and theory in experimental high energy physics. These software packages, which are the ﬁrst step in the simulation workﬂow of collider experiments, represent approximately 5 to 20% of the annual WLCG usage for the ATLAS and CMS experiments. With computing architectures becoming more heterogeneous, it is important to ensure that these key software frameworks can be run on future systems, large and small. In this contribution, recent progress on porting and speeding up the Madgraph5_aMC@NLO event generator on hybrid architectures, i.e. CPU with GPU accelerators, is discussed. The main focus of this work has been in the calculation of scattering amplitudes and “matrix elements”, which is the computational bottleneck of an event generation application. For physics processes limited to QCD leading order, the code generation toolkit has been expanded to produce matrix element calculations using C++ vector instructions on CPUs and using CUDA for NVidia GPUs, as well as using Alpaka, Kokkos and SYCL for multiple CPU and GPU architectures. Performance is reported in terms of matrix element calculations per time on NVidia, Intel, and AMD devices. The status and outlook for the integration of this work into a production release usable by the LHC experiments, with the same functionalities and very similar user interfaces as the current Fortran version, is also described.|Proceedings of 41st International Conference on High Energy physics — PoS(ICHEP2022)|2022|10.22323/1.414.0212|S. Roiser, Stefan Hageböck, O. Mattelaer, A. Valassi, Nathan Nichols, L. Field, T. Childers, W. Hopkins, David Smith|2.0|2
956|Point Cloud Ray-Launching Simulations of Indoor Multipath Channels at 60 GHz|In this work we present a novel ray-launching (RL) method for field prediction utilizing a laser-scanned point cloud model of the environment. The method takes advantage of the high level of detail found in the point cloud to simulate propagation of rays as they undergo reflection and transmission through local surfaces represented by points of the point cloud. The method is implemented using MATLAB's Parallel Computing Toolbox and its GPU Computing library for straightforward parallelization and acceleration of computations. Indoor multipath channels are simulated at the 60 GHz band and compared to their measured counterparts to study accuracy of the presented method. Up to 6 reflections and 3 transmissions can be simulated in approximately 6 minutes. Utilizing a post-processing step to eliminate propagation paths with near-identical trajectories, an issue seemingly unique to point clouds, the channel is well reproduced in terms of path gains and a relative error of less than 10% for delay spread.|IEEE International Symposium on Personal, Indoor and Mobile Radio Communications|2022|10.1109/PIMRC54779.2022.9977493|Pasi Koivumäki, K. Haneda|2.0|2
976|MATILDA.FT: A mesoscale simulation package for inhomogeneous soft matter.|In this paper, we announce the public release of a massively parallel, graphics processing unit (GPU)-accelerated software, which is the first to combine both coarse-grained particle simulations and field-theoretic simulations in one simulation package. MATILDA.FT (Mesoscale, Accelerated, Theoretically Informed, Langevin, Dissipative particle dynamics, and Field Theory) was designed from the ground-up to run on CUDA-enabled GPUs with Thrust library acceleration, enabling it to harness the possibility of massive parallelism to efficiently simulate systems on a mesoscopic scale. It has been used to model a variety of systems, from polymer solutions and nanoparticle-polymer interfaces to coarse-grained peptide models and liquid crystals. MATILDA.FT is written in CUDA/C++ and is object oriented, making its source-code easy to understand and extend. Here, we present an overview of the currently available features, and the logic of parallel algorithms and methods. We provide the necessary theoretical background and present examples of systems simulated using MATILDA.FT as the simulation engine. The source code, along with the documentation, additional tools, and examples, can be found on the GitHub MATILDA.FT repository.|Journal of Chemical Physics|2023|10.1063/5.0145006|Zuzanna M Jedlinska, C. Gillespie, Nathaniel Hess, Robert A. Riggleman, Anita S Yang, Christian Tabedzki|2.0|2
997|Skybox: Open-Source Graphic Rendering on Programmable RISC-V GPUs|Graphics rendering remains one of the most compute intensive and memory bound applications of GPUs and has been driving their push for performance and energy efficiency since its inception. Early GPU architectures focused only on accelerating graphics rendering and implemented dedicated fixed- function rasterizer hardware to speed-up their rendering pipeline. As GPUs have become more programmable and ubiquitous in other application domains such as scientific computing, machine learning, graph analytics, and crypto-currency, generalizing GPU microarchitectures for area and power efficiency becomes necessary, especially for mobile and IoT devices. In this work, we present Skybox, a full-stack open-source GPU architecture with integrated software, compiler, hardware, and simulation environment, that enables end-to-end GPU research. Using Skybox, we explore the design space of software versus hardware graphics rendering and propose and hybrid micro-architecture that accelerates the state-of-the art Vulkan graphics API. Skybox also introduces novel compiler and system optimizations to support its unique RISC-V ISA baseline. We evaluated Skybox on high- end Altera and also Xilinx FPGAs. We were able to generate and execute a 32 cores (512 threads) Skybox graphics processor on Altera Stratix 10 FPGA, delivering a peak fill rate of 3.7 GPixels at 230 MHz. Skybox is the first open-source full-stack GPU software and hardware implementation that supports the Vulkan API|International Conference on Architectural Support for Programming Languages and Operating Systems|2023|10.1145/3582016.3582024|Santosh Srivatsan, Varun Saxena, Fadi Alzammar, Joshua R. Simpson, Blaise Tine, Hyesoon Kim, Liam Cooper|2.0|2
1009|Particle-in-cell Simulations of Relativistic Magnetic Reconnection with Advanced Maxwell Solver Algorithms|Relativistic magnetic reconnection is a nonideal plasma process that is a source of nonthermal particle acceleration in many high-energy astrophysical systems. Particle-in-cell (PIC) methods are commonly used for simulating reconnection from first principles. While much progress has been made in understanding the physics of reconnection, especially in 2D, the adoption of advanced algorithms and numerical techniques for efficiently modeling such systems has been limited. With the GPU-accelerated PIC code WarpX, we explore the accuracy and potential performance benefits of two advanced Maxwell solver algorithms: a nonstandard finite-difference scheme (CKC) and an ultrahigh-order pseudo-spectral method (PSATD). We find that, for the relativistic reconnection problem, CKC and PSATD qualitatively and quantitatively match the standard Yee-grid finite-difference method. CKC and PSATD both admit a time step that is 40% longer than that of Yee, resulting in a ∼40% faster time to solution for CKC, but no performance benefit for PSATD when using a current deposition scheme that satisfies Gauss’s law. Relaxing this constraint maintains accuracy and yields a 30% speedup. Unlike Yee and CKC, PSATD is numerically stable at any time step, allowing for a larger time step than with the finite-difference methods. We found that increasing the time step 2.4–3 times over the standard Yee step still yields accurate results, but it only translates to modest performance improvements over CKC, due to the current deposition scheme used with PSATD. Further optimization of this scheme will likely improve the effective performance of PSATD.|Astrophysical Journal|2023|10.3847/1538-4357/acd75b|R. Lehe, A. Huebl, M. Rowan, R. Jambunathan, Hannah Klion, A. Myers, Eloise Yang, D. Willcox, Weiqun Zhang, J. Vay|2.0|2
1029|Building a Virtual Weakly-Compressible Wind Tunnel Testing Facility|Virtual wind tunnel testing is a key ingredient in the engineering design process for the automotive and aeronautical industries as well as for urban planning: through visualization and analysis of the simulation data, it helps optimize lift and drag coefficients, increase peak speed, detect high pressure zones, and reduce wind noise at low cost prior to manufacturing. In this paper, we develop an efficient and accurate virtual wind tunnel system based on recent contributions from both computer graphics and computational fluid dynamics in high-performance kinetic solvers. Running on one or multiple GPUs, our massively-parallel lattice Boltzmann model meets industry standards for accuracy and consistency while exceeding current mainstream industrial solutions in terms of efficiency --- especially for unsteady turbulent flow simulation at very high Reynolds number (on the order of 107) --- due to key contributions in improved collision modeling and boundary treatment, automatic construction of multiresolution grids for complex models, as well as performance optimization. We demonstrate the efficacy and reliability of our virtual wind tunnel testing facility through comparisons of our results to multiple benchmark tests, showing an increase in both accuracy and efficiency compared to state-of-the-art industrial solutions. We also illustrate the fine turbulence structures that our system can capture, indicating the relevance of our solver for both VFX and industrial product design.|ACM Transactions on Graphics|2023|10.1145/3592394|Yiheng Wu, Chaoyang Lyu, Kai-Yi Bai, Changxi Zheng, M. Desbrun, Xiaopei Liu|2.0|2
1039|Thicket: Seeing the Performance Experiment Forest for the Individual Run Trees|Thicket is an open-source Python toolkit for Exploratory Data Analysis (EDA) of multi-run performance experiments. It enables an understanding of optimal performance configuration for large-scale application codes. Most performance tools focus on a single execution (e.g., single platform, single measurement tool, single scale). Thicket bridges the gap to convenient analysis in multi-dimensional, multi-scale, multi-architecture, and multi-tool performance datasets by providing an interface for interacting with the performance data. Thicket has a modular structure composed of three components. The first component is a data structure for multi-dimensional performance data, which is composed automatically on the portable basis of call trees, and accommodates any subset of dimensions present in the dataset. The second is the metadata, enabling distinction and sub-selection of dimensions in performance data. The third is a dimensionality reduction mechanism, enabling analysis such as computing aggregated statistics on a given data dimension. Extensible mechanisms are available for applying analyses (e.g., top-down on Intel CPUs), data science techniques (e.g., K-means clustering from scikit-learn), modeling performance (e.g., Extra-P), and interactive visualization. We demonstrate the power and flexibility of Thicket through two case studies, first with the open-source RAJA Performance Suite on CPU and GPU clusters and another with a large physics simulation run on both a traditional HPC cluster and an AWS Parallel Cluster instance.|IEEE International Symposium on High-Performance Parallel Distributed Computing|2023|10.1145/3588195.3592989|Vanessa Lama, Ian Lumsden, S. Brink, David Boehme, Katherine E. Isaacs, M. Taufer, M. Mckinsey, Olga Pearce, Daryl Hawkins, Treece Burgess, Connor Scully-Allison, Jakob Lüttgau|2.0|2
1047|GP-Guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments|Robotic navigation in unknown, cluttered environ-ments with limited sensing capabilities poses significant chal-lenges in robotics. Local trajectory optimization methods, such as Model Predictive Path Intergal (MPPI), are a promising solution to this challenge. However, global guidance is required to ensure effective navigation, especially when encountering challenging environmental conditions or navigating beyond the planning horizon. This study presents the GP-MPPI, an online learning-based control strategy that integrates MPPI with a local perception model based on Sparse Gaussian Process (SGP). The key idea is to leverage the learning capability of SGP to construct a variance (uncertainty) surface, which enables the robot to learn about the navigable space surrounding it, identify a set of suggested subgoals, and ultimately recommend the optimal subgoal that minimizes a predefined cost function to the local MPPI planner. Afterward, MPPI computes the optimal control sequence that satisfies the robot and collision avoidance constraints. Such an approach eliminates the necessity of a global map of the environment or an offline training process. We validate the efficiency and robustness of our proposed control strategy through both simulated and real-world experiments of 2D autonomous navigation tasks in complex unknown en-vironments, demonstrating its superiority in guiding the robot safely towards its desired goal while avoiding obstacles and escaping entrapment in local minima. The GPU implementation of GP-MPPI, including the supplementary video, is available at https://github.com/IhabMohamed/GP-MPPI.|IEEE/RJS International Conference on Intelligent RObots and Systems|2023|10.1109/IROS55552.2023.10341382|Ihab S. Mohamed, Lantao Liu, Mahmoud Ali|2.0|2
1091|Combining the D3 dispersion correction with the neuroevolution machine-learned potential|Machine-learned potentials (MLPs) have become a popular approach of modeling interatomic interactions in atomistic simulations, but to keep the computational cost under control, a relatively short cutoff must be imposed, which put serious restrictions on the capability of the MLPs for modeling relatively long-ranged dispersion interactions. In this paper, we propose to combine the neuroevolution potential (NEP) with the popular D3 correction to achieve a unified NEP-D3 model that can simultaneously model relatively short-ranged bonded interactions and relatively long-ranged dispersion interactions. We show that improved descriptions of the binding and sliding energies in bilayer graphene can be obtained by the NEP-D3 approach compared to the pure NEP approach. We implement the D3 part into the gpumd package such that it can be used out of the box for many exchange-correlation functionals. As a realistic application, we show that dispersion interactions result in approximately a 10% reduction in thermal conductivity for three typical metal-organic frameworks.|Journal of Physics: Condensed Matter|2023|10.1088/1361-648X/ad1278|Zheyong Fan, Penghua Ying|2.0|2
1099|Neural style transfer of weak lensing mass maps|We propose a new generative model of projected cosmic mass density maps inferred from weak gravitational lensing observations of distant galaxies (weak lensing mass maps). We construct the model based on a neural style transfer so that it can transform Gaussian weak lensing mass maps into deeply non-Gaussian counterparts as predicted in ray-tracing lensing simulations. We develop an unpaired image-to-image translation method with Cycle-Consistent Generative Adversarial Networks (Cycle GAN), which learn efficient mapping from an input domain to a target domain. Our model is designed to enjoy important advantages; it is trainable with no need for paired simulation data, flexible to make the input domain visually meaningful, and expandable to rapidly-produce a map with a larger sky coverage than training data without additional learning. Using 10,000 lensing simulations, we find that appropriate labeling of training data based on field variance requires the model to exhibit a desired diversity of various summary statistics for weak lensing mass maps. Compared with a popular log-normal model, our model improves in predicting the statistical natures of three-point correlations and local properties of rare high-density regions. We also demonstrate that our model enables us to produce a continuous map with a sky coverage of $\sim166\, \mathrm{deg}^2$ but similar non-Gaussian features to training data covering $\sim12\, \mathrm{deg}^2$ in a GPU minute. Hence, our model can be beneficial to massive productions of synthetic weak lensing mass maps, which is of great importance in future precise real-world analyses.||2023|10.1103/physrevd.108.123526|Masato Shirasaki, Shiro Ikeda|2.0|2
1181|Interactive Generation of Time‐evolving, Snow‐Covered Landscapes with Avalanches|We introduce a novel method for interactive generation of visually consistent, snow‐covered landscapes and provide control of their dynamic evolution over time. Our main contribution is the real‐time phenomenological simulation of avalanches and other user‐guided events, such as tracks left by Nordic skiing, which can be applied to interactively sculpt the landscape. The terrain is modeled as a height field with additional layers for stable, compacted, unstable, and powdery snow, which behave in combination as a semi‐viscous fluid. We incorporate the impact of several phenomena, including sunlight, temperature, prevailing wind direction, and skiing activities. The snow evolution includes snow‐melt and snow‐drift, which affect stability of the snow mass and the probability of avalanches. A user can shape landscapes and their evolution either with a variety of interactive brushes, or by prescribing events along a winter season time‐line. Our optimized GPU‐implementation allows interactive updates of snow type and depth across a large (10 × 10 km) terrain, including real‐time avalanches, making this suitable for visual assets in computer games. We evaluate our method through perceptual comparison against exiting methods and real snow‐depth data.|Computer graphics forum (Print)|2018|10.1111/cgf.13379|Marie-Paule Cani, Guillaume Cordonnier, Bedrich Benes, P. Ecormier, J. Gain, Eric Galin|2.0|2
1184|Multi-faceted microarchitecture level reliability characterization for NVIDIA and AMD GPUs|State-of-the-art GPU chips are designed to deliver extreme throughput for graphics as well as for data-parallel general purpose computing workloads (GPGPU computing). Unlike computing for graphics, GPGPU computing requires highly reliable operations. Since provisioning for high reliability may affect performance, the design of GPGPU systems requires the vulnerability of GPU workloads to soft-errors to be jointly evaluated with the performance of GPU chips. We present an extended study based on a consolidated workflow for the evaluation of the reliability in correlation with the performance of four GPU architectures and corresponding chips: AMD Southern Islands and NVIDIA G80/GT200/Fermi. We obtained reliability measurements (AVF and FIT) employing both fault injection and ACE-analysis based on microarchitecture-level simulators. Apart from the reliability-only and performance-only measurements, we propose combined metrics for performance and reliability that assist comparisons for the same application among GPU chips of different ISAs and vendors, as well as among benchmarks on the same GPU chip.|IEEE VLSI Test Symposium|2018|10.1109/VTS.2018.8368665|D. Gizopoulos, Sotiris Tselonis, S. Carlo, Alessandro Vallero|2.0|2
1370|Numerical Simulation in Physics and Engineering: Lecture Notes of the XVI 'Jacques-Louis Lions' Spanish-French School|This book presents lecture notes from the XVI Jacques-Louis Lions Spanish-French School on Numerical Simulation in Physics and Engineering, held in Pamplona (Navarra, Spain) in September 2014.The subjects covered include: numerical analysis of isogeometric methods, convolution quadrature for wave simulations, mathematical methods in image processing and computer vision, modeling and optimization techniques in food processes, bio-processes and bio-systems, and GPU computing for numerical simulation.The book is highly recommended to graduate students in Engineering or Science who want to focus on numerical simulation, either as a research topic or in the field of industrial applications. It can also benefit senior researchers and technicians working in industry who are interested in the use of state-of-the-art numerical techniques in the fields addressed here. Moreover, the book can be used as a textbook for master courses in Mathematics, Physics, or Engineering.||2016|10.1007/s12144-016-9459-6|Teo Roldn, Juan Jos Torrens, I. Higueras|2.0|2
1444|A Parallel CE-LOD-FDTD Model for Instrument Landing System Signal Disturbance Analyzing|Before a new airport runway is built, the electromagnetic compatibility of the instrument landing system (ILS) must be fully taken into consideration. The electromagnetic compatibility requirements for airports are becoming more and more complex, so rather than using traditional ray-tracing (RT) simulation models, a more accurate approach such as the finite-difference time domain (FDTD) is needed. The application of FDTD to ILS signal modeling suffers from an intensively high computation burden. The computation burden is owing to: 1) the FDTD has to loop a large number of time steps and 2) for every step, a huge number of cells has to be updated. This paper solves the above-mentioned issues by twofold. First, this model allows a large Courant–Friedrich–Levy number for the FDTD to reduce the total time step. This is achieved by combining the locally one-dimension (LOD) FDTD and the complex-envelope (CE) technique. The combination of CE, LOD, and FDTD (CE-LOD-FDTD) makes the model update with larger time step and retain the same level of accuracy. Second, we propose a set of novel GPU-based acceleration algorithms to update the field in parallel for reducing the computation time. By comparing the measurement results, it is shown that the FDTD model is more accurate than the traditional RT-based method. Our GPU-based CE-LOD-FDTD model is 22 times faster than the conventional parallel FDTD model in a multicore machine.|IEEE Transactions on Antennas and Propagation|2019|10.1109/TAP.2019.2891294|Qi Guo, Jundong Tan, Zihao Li, Zhuo Su, Y. Long|2.0|2
1511|Towards large-scale parallel GPU-accelerated SPH for FSI problems|A method to simulate the water removal from the windscreen of cars by windscreen wipers using Smoothed Particle Hydrodynamics on parallel GPU platforms is being developed based on the nanoFluidX code. A fluid modeling approach based on Hu & Adams [1] is applied with a density summation approach with free surface treatment. Currently large-scale fluid simulations are performed on more than two hundred GPUs on the TSUBAME supercomputer operated by the Global Scientific Information and Computing Center (GSIC) at Tokyo Institute of Technology. Dynamic load-balancing is performed using the Zoltan library providing a Recursive Coordinate Bisection decomposition method (see Boman et al. [2]). An overlapping communication procedure is applied using CUDA streams. The current research focus is to extend to Fluid-Structure Interaction problems as a first step to be able to simulate the deformation of the rubber lip of the wiper when interacting with the water film. For elastic solid modeling, the generalized transport-velocity formulation by Zhang et al. [3] is adapted. A coupling based on the wall boundary condition as proposed by Adami et al. [4] is applied. Preliminary results of Fluid-Structure Interaction benchmarks such as a dambreak with an elastic opening gate (see Antoci et al. [5]) are shown to demonstrate the applicability. Work to adapt to 3D Fluid-Structure Interaction problems in parallel is currently in progress.||2017|10.1109/cluster.2017.97|T. Indinger, S. Adami, Michael Gestrich, T. Aoki|2.0|2
1612|QuADD: QUantifying Accelerator Disaggregated Datacenter Efficiency|In the current era of data explosion accelerators such as GPUs facilitate data-driven applications with requisite compute boost. Availability of GPUs in Public Cloud offerings has expedited their mass adoption. Consequently, varied customer demands and exclusive allocation of GPUs to VMs can leave stranded GPUs across the datacenter. Hardware disaggregation can alleviate this issue to enable a powerefficient datacenter. However, it is important to first quantify the gains associated with this new deployment paradigm. In the absence of real deployments, simulations can be helpful to evaluate the benefits of disaggregation at scale. In this paper, we evaluate the gains associated primarily with disaggregated GPU deployments. For this, we use QUADD-SIM, a simulator we built to model, quantify, and contrast different facets of these emerging GPU deployments. Using QUADD-SIM we model different VM and resource provisioning aspects of disaggregated GPU deployments. We simulate realistic AI workload requests for a period of 3 months with characteristics derived from recent public datacenter traces. Our results attest that disaggregated GPU deployment strategies outperform traditional GPU deployments in terms of failed VM requests and GPU Watt-hours consumption. Overall, 5.14% and 7.90% additional failed VM requests were serviced by disaggregated GPU deployments consuming 10.92% and 3.30% lesser GPU Watt-hours compared to traditional deployment.|IEEE International Conference on Cloud Computing|2019|10.1109/CLOUD.2019.00064|J. Lakshmi, Anubhav Guleria, C. Padala|2.0|2
209|Fully iterative scatter corrected digital breast tomosynthesis using GPU-based fast Monte Carlo simulation and composition ratio update.|PURPOSE\nIn digital breast tomosynthesis (DBT), scatter correction is highly desirable, as it improves image quality at low doses. Because the DBT detector panel is typically stationary during the source rotation, antiscatter grids are not generally compatible with DBT; thus, a software-based scatter correction is required. This work proposes a fully iterative scatter correction method that uses a novel fast Monte Carlo simulation (MCS) with a tissue-composition ratio estimation technique for DBT imaging.\n\n\nMETHODS\nTo apply MCS to scatter estimation, the material composition in each voxel should be known. To overcome the lack of prior accurate knowledge of tissue composition for DBT, a tissue-composition ratio is estimated based on the observation that the breast tissues are principally composed of adipose and glandular tissues. Using this approximation, the composition ratio can be estimated from the reconstructed attenuation coefficients, and the scatter distribution can then be estimated by MCS using the composition ratio. The scatter estimation and image reconstruction procedures can be performed iteratively until an acceptable accuracy is achieved. For practical use, (i) the authors have implemented a fast MCS using a graphics processing unit (GPU), (ii) the MCS is simplified to transport only x-rays in the energy range of 10-50 keV, modeling Rayleigh and Compton scattering and the photoelectric effect using the tissue-composition ratio of adipose and glandular tissues, and (iii) downsampling is used because the scatter distribution varies rather smoothly.\n\n\nRESULTS\nThe authors have demonstrated that the proposed method can accurately estimate the scatter distribution, and that the contrast-to-noise ratio of the final reconstructed image is significantly improved. The authors validated the performance of the MCS by changing the tissue thickness, composition ratio, and x-ray energy. The authors confirmed that the tissue-composition ratio estimation was quite accurate under a variety of conditions. Our GPU-based fast MCS implementation took approximately 3 s to generate each angular projection for a 6 cm thick breast, which is believed to make this process acceptable for clinical applications. In addition, the clinical preferences of three radiologists were evaluated; the preference for the proposed method compared to the preference for the convolution-based method was statistically meaningful (p < 0.05, McNemar test).\n\n\nCONCLUSIONS\nThe proposed fully iterative scatter correction method and the GPU-based fast MCS using tissue-composition ratio estimation successfully improved the image quality within a reasonable computational time, which may potentially increase the clinical utility of DBT.|Medical Physics (Lancaster)|2015|10.1118/1.4928139|Kyungsang Kim, J. C. Ye, Seungryong Cho, J. Cha, H. Kim, Jongha Lee, Taewon Lee, Y. Seong, K. Jang, Young-Wook Choi, H. J. Shin, Jaegu Choi|1.9|2
434|Parallel Optimization of 3D Cardiac Electrophysiological Model Using GPU|Large-scale 3D virtual heart model simulations are highly demanding in computational resources. This imposes a big challenge to the traditional computation resources based on CPU environment, which already cannot meet the requirement of the whole computation demands or are not easily available due to expensive costs. GPU as a parallel computing environment therefore provides an alternative to solve the large-scale computational problems of whole heart modeling. In this study, using a 3D sheep atrial model as a test bed, we developed a GPU-based simulation algorithm to simulate the conduction of electrical excitation waves in the 3D atria. In the GPU algorithm, a multicellular tissue model was split into two components: one is the single cell model (ordinary differential equation) and the other is the diffusion term of the monodomain model (partial differential equation). Such a decoupling enabled realization of the GPU parallel algorithm. Furthermore, several optimization strategies were proposed based on the features of the virtual heart model, which enabled a 200-fold speedup as compared to a CPU implementation. In conclusion, an optimized GPU algorithm has been developed that provides an economic and powerful platform for 3D whole heart simulations.|Comput. Math. Methods Medicine|2015|10.1155/2015/862735|Yong Xia, Henggui Zhang, Kuanquan Wang|1.9|2
1338|Multiobjective Pathfinding Using Hardware Accelerated Genetic Algorithms|Multiobjective pathfinding is an extremely computationally expensive problem, yet it is important for many applications. Classical algorithms fail as the complexity of the environment increases, and we must look to other methods for finding high-quality paths. This is especially true when we move beyond assigning a cost to each point on a map, and consider environmental properties that have a magnitude and a direction, such as wind. In this thesis, I present a scalable custom hardware designed by Geir Åge Noven for evaluating the fitness of paths through real-world environments. The hardware is based on a grid of processing elements, and each path traverses this grid as its fitness is being calculated. The concept of topologically folding the environment is discussed. This operation allows each processing tile to represent multiple areas of the map, which allows for even load distribution among the processing tiles. The proposed hardware is compared to other evaluation hardwares, including the GPU, and some simulations are performed on genetic algorithm performance on multiobjective pathfinding. I find that the proposed hardware has several properties that makes it suitable for evaluating path fitness, such as even load distribution among the processing tiles. I also find that genetic algorithms can continually improve paths in a real-world terrain for a large number of generations. Finally, the computational qualities of the proposed hardware and GPU architectures are compared.||2016|10.1016/j.marstruc.2016.01.004|Vegard Knutsen Lillevoll|1.8888888888888888|2
231|Robust Multi-Resource Allocation with Demand Uncertainties in Cloud Scheduler|Cloud scheduler manages multi-resources (e.g., CPU, GPU, memory, storage etc.) in cloud platform to improve resource utilization and achieve cost-efficiency for cloud providers. The optimal allocation for multi-resources has become a key technique in cloud computing and attracted more and more researchers' attentions. The existing multi-resource allocation methods are developed based on a condition that the job has constant demands for multi-resources. However, these methods may not apply in a real cloud scheduler due to the dynamic resource demands in jobs' execution. In this paper, we study a robust multi-resource allocation problem with uncertainties brought by varying resource demands. To this end, the cost function is chosen as either of two multi-resource efficiency-fairness metrics called Fairness on Dominant Shares and Generalized Fairness on Jobs, and we model the resource demand uncertainties through three typical models, i.e., scenario demand uncertainty, box demand uncertainty and ellipsoidal demand uncertainty. By solving an optimization problem we get the solution for robust multi-resource allocation with uncertainties for cloud scheduler. The extensive simulations show that the proposed approach can handle the resource demand uncertainties and the cloud scheduler runs in an optimized and robust manner.|IEEE International Symposium on Reliable Distributed Systems|2017|10.1109/SRDS.2017.12|Q. Lu, Haibing Guan, Jianguo Yao, H. Jacobsen|1.875|2
367|FUX-Sim: Implementation of a fast universal simulation/reconstruction framework for X-ray systems|The availability of digital X-ray detectors, together with advances in reconstruction algorithms, creates an opportunity for bringing 3D capabilities to conventional radiology systems. The downside is that reconstruction algorithms for non-standard acquisition protocols are generally based on iterative approaches that involve a high computational burden. The development of new flexible X-ray systems could benefit from computer simulations, which may enable performance to be checked before expensive real systems are implemented. The development of simulation/reconstruction algorithms in this context poses three main difficulties. First, the algorithms deal with large data volumes and are computationally expensive, thus leading to the need for hardware and software optimizations. Second, these optimizations are limited by the high flexibility required to explore new scanning geometries, including fully configurable positioning of source and detector elements. And third, the evolution of the various hardware setups increases the effort required for maintaining and adapting the implementations to current and future programming models. Previous works lack support for completely flexible geometries and/or compatibility with multiple programming models and platforms. In this paper, we present FUX-Sim, a novel X-ray simulation/reconstruction framework that was designed to be flexible and fast. Optimized implementation for different families of GPUs (CUDA and OpenCL) and multi-core CPUs was achieved thanks to a modularized approach based on a layered architecture and parallel implementation of the algorithms for both architectures. A detailed performance evaluation demonstrates that for different system configurations and hardware platforms, FUX-Sim maximizes performance with the CUDA programming model (5 times faster than other state-of-the-art implementations). Furthermore, the CPU and OpenCL programming models allow FUX-Sim to be executed over a wide range of hardware platforms.|PLoS ONE|2017|10.1371/journal.pone.0180363|Estefania Serrano, I. García, M. Abella, C. de Molina, M. Desco, Javier García-Blas, J. Carretero|1.875|2
1144|Parallel implementation of aircraft disembarking and emergency evacuation based on cellular automata|In this paper we present a model based on the parallel computational tool of cellular automata (CA) capable of simulating the process of disembarking in a small airplane seat layout, corresponding to Airbus A320/ Boeing 737 layout, in search of ways to make it faster and safer under normal evacuation conditions, as well as emergency scenarios. The proposed model is highly customizable, with the number of exits, the walking speed of passengers, depending on their sex, age and height, and the effects of retrieving and carrying luggage. Additionally, the presence of obstacles in the aisles as well as the emergence of panic being parameters whose values can be varied in order to enlighten the disembarking and emergency evacuation processes are considered in detail. The simulation results were compared to existing aircraft disembarking and evacuation times and indicate the efficacy of the proposed model in investigating and revealing passenger attributes during these processes in all the examined cases. Moreover, we parallelized our code in order to run on a graphics processing unit (GPU) using the CUDA programming language, speeding up the simulation process. Finally, in order to present a fully dynamical anticipative real-time system helpful for decision-making we implemented the proposed CA model in a field programmable gate array (FPGA) device, and recreated the results given by the software simulations in a fraction of the time. We then compared and exported the performance results among a sequential software implementation, the implementation running on a GPU, and a hardware implementation, proving the consequent acceleration that results from the parallel CA implementation in specific hardware.|The international journal of high performance computing applications|2017|10.1177/1094342015584533|Themistoklis Giitsidis, Nikolaos I. Dourvas, G. Sirakoulis|1.875|2
1501|Multiscale computer simulation studies of entangled branched polymers|In this thesis, we investigate two problems of entangled branched polymers, i.e., the numerical solutions of the arm-retraction problem for well-entangled star arms and the relaxation behaviours of branched polymers with different architectures. For the first problem, the arm retraction dynamics is studied using both the onedimensional Rouse chain model and the slip-spring model by an advanced numerical method for the first-passage time problems, namely the forward flux sampling (FFS) method. In the one-dimensional Rouse chain model, we measured the first-passage time that the arm free end extends to a distance away from the origin, showing \nthat the mean first-passage time is getting shorter if the Rouse chain is represented by more beads. The simulation results validate the prediction of an asymptotic solution for the multi-dimensional first-passage problem, which suggests the arm retraction time is much shorter than the prediction of the Milner-McLeish theory without constraint release. Then, we implement the FFS method to the slip-spring model and get the relaxation spectra for different arm lengths, ranging from mildly entangled to well-entangled star arms. We also proposed an algorithm to extract the dynamic observables, i.e., the end-to-end vector and stress relaxation functions, from the FFS simulation results. For the second problem, we conduct a series of molecular dynamics (MD) simulations using high-performance GPU methods on the mildly entangled branched polymers of different architectures, including 3-arm symmetric \nand asymmetric stars, and H-shaped polymers. The slip-spring model, whose parameters are carefully calibrated according to the MD results of linear chains, is also implemented to predict the relaxation behaviours of the branched polymers. We present a detailed analysis of the arm end-to-end vector relaxation functions and the monomer mean-squared displacements. By comparing the MD and slipspring model simulation results, we propose a slip-link “hopping” mechanism, which accounts for the behaviour that the entanglements can pass through the branch point when the third arm is disentangled||2017|10.1063/1.4995422|Jian Zhu|1.875|2
1406|A Real-Time Modelling and Simulation Platform for Virtual Engineering Design and Analysis|The ability to perform credible CFD simulations at accelerated speeds has opened up the potential for a new use-mode for CFD as a tool in engineering: the application of CFD for rst-order parameter-space exploration, analysis, and design communication. When coupled with a suitable real-time rendering and interaction capability for in-situ visualisation and manipulation of 3D results, CFD may be used as part of an interactive design tool in virtual engineering. These steerable applications represent a paradigm shift in the application of CFD for engineering and o er the potential to transform the way CFD is used within the industry. This article presents developments towards a production-ready virtual wind tunnel including presentation of an integrated, interactive modelling and simulation tool for aerodynamic design and analysis built using the Unreal Engine 4 game engine. The virtual wind tunnel application provides a mechanism for integrating virtual reality observation, navigation, visualisation and in-game interaction with a ow eld simulated using our own GPU-accelerated CFD library based on the lattice-Boltzmann method. Objects may be imported from CAD or reconstructed using Microsoft Kinect-based 3D scanning. Simulation parameters may be modi ed at run-time by the user. The ow solver has been validated against experimental data for a representative turbulent ow and demonstrates excellent agreement with available data.||2018|10.1016/j.advengsoft.2017.10.005|P. Wenisch, A. Harwood, A. Revell|1.8571428571428572|2
1541|Comparative study of parallel implementation for searching algorithms with openMP|Investigating the multi-core architecture is an essential issue to get superior in parallel reenactments. However, the simulation highlights must fit on parallel programming model to build the execution. The main goal of this research is to choose and evaluate parallelism using OpenMP over sequential program. For this purpose, there is a portrayal of two searching algorithms. The calculation is to discover the next edge of Prim's algorithm and single source shortest way of Dijkstra's algorithm. These two algorithm actualized in sequential formulation. Parallel searching algorithms are then implemented in view of multicore processor. The speed-up ratio and efficiency of parallel searching algorithms are tested and investigated in SGEMM GPU Kernel performance dataset with 241600 records and 18 attributes. Results show the dataset with different data sizes achieved super linear speed-up ratio and efficiency on OpenMP by running on 4 cores processor and reduction of the running time over sequential program. More importantly, the new implementation drastically decreases the time of execution for thread 8 for Prims algorithm from 5.16ms to 1.48 ms for Dijkstra algorithm. Parallel calculation is impressively powerful for huge graph size. General outcome shows that multi-threaded parallelism is exceptionally successful to accomplish better performance for dataset based on searching algorithms by separating the primary dataset into sub-datasets to increase diversity on arrangement investigation.||2018|10.1016/j.imu.2017.10.008|R. C. Muniyandi, D. Nandi, Renea Chowdhury|1.8571428571428572|2
513|Exploring Complex Brain-Simulation Workloads on Multi-GPU Deployments|In-silico brain simulations are the de-facto tools computational neuroscientists use to understand large-scale and complex brain-function dynamics. Current brain simulators do not scale efficiently enough to large-scale problem sizes (e.g., >100,000 neurons) when simulating biophysically complex neuron models. The goal of this work is to explore the use of true multi-GPU acceleration through NVIDIA’s GPUDirect technology on computationally challenging brain models and to assess their scalability. The brain model used is a state-of-the-art, extended Hodgkin-Huxley, biophysically meaningful, three-compartmental model of the inferior-olivary nucleus. The Hodgkin-Huxley model is the most widely adopted conductance-based neuron representation, and thus the results from simulating this representative workload are relevant for many other brain experiments. Not only the actual network-simulation times but also the network-setup times were taken into account when designing and benchmarking the multi-GPU version, an aspect often ignored in similar previous work. Network sizes varying from 65K to 2M cells, with 10 and 1,000 synapses per neuron were executed on 8, 16, 24, and 32 GPUs. Without loss of generality, simulations were run for 100 ms of biological time. Findings indicate that communication overheads do not dominate overall execution while scaling the network size up is computationally tractable. This scalable design proves that large-network simulations of complex neural models are possible using a multi-GPU design with GPUDirect.|ACM Transactions on Architecture and Code Optimization (TACO)|2019|10.1145/3371235|C. Strydis, Michiel A. van der Vlag, Georgios Smaragdos, Z. Al-Ars|1.8333333333333333|2
564|Exploiting Hardware-Accelerated Ray Tracing for Monte Carlo Particle Transport with OpenMC|OpenMC is a CPU-based Monte Carlo particle transport simulation code recently developed in the Computa- tional Reactor Physics Group at MIT, and which is currently being evaluated by the UK Atomic Energy Authority for use on the ITER fusion reactor project. In this paper we present a novel port of OpenMC to run on the new ray tracing (RT) cores in NVIDIA’s latest Turing GPUs. We show here that the OpenMC GPU port yields up to 9.8x speedup on a single node over a 16-core CPU using the native constructive solid geometry, and up to 13x speedup using approximate triangle mesh geometry. Furthermore, since the expensive 3D geometric operations re- quired during particle transport simulation can be formulated as a ray tracing problem, there is an opportunity to gain even higher performance on triangle meshes by exploiting the RT cores in Turing GPUs to enable hardware-accelerated ray tracing. Extending the GPU port to support RT core acceleration yields between 2x and 20x additional speedup. We note that geometric model complexity has a significant impact on performance, with RT core acceleration yielding comparatively greater speedups as complexity increases. To the best of our knowledge, this is the first work showing that exploitation of RT cores for scientific workloads is possible. We finish by drawing conclusions about RT cores in terms of wider applicability, limitations and performance portability.|International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems|2019|10.1109/PMBS49563.2019.00008|Simon McIntosh-Smith, Justin Salmon|1.8333333333333333|2
1447|Kinetic Plasma Simulation: Meeting the Demands of Increased Complexity|"Author(s): Tableman, Adam Ryan | Advisor(s): Mori, Warren B | Abstract: This dissertation concerns the development and use of numerical simulation techniques for studying nonlinear plasma systems in which accurate representations of the electron distribution function are required. The kinetic description of the electrons is accomplished via two different simulation modalities: the code OSHUN, which directly solves the Vlasov-Fokker-Planck (VFP) partial differential equation, and the code OSIRIS, which uses the particle-in-cell (PIC) method including an option for a separate Monte Carlo collision model.The dissertation consists of ten chapters that are based on reprints of refereed publications that describe the development and use of OSHUN and OSIRIS. The increasing complexity of today’s computers necessitates an increase in the complexity of software to take full advantage of the available computing resources. This requires that software be engineered properly to ensure correct functioning and to enable more developers to contribute. The dissertation includes examples of the creation --- that is, combining new and novel algorithms with software engineering techniques --- and novel usage of simulation software packages capable of exploiting the power of today's computers to enable new capability and discovery. OSHUN includes relativistic corrections to the Vlasov equation but uses a non-relativistic description for the collision operator. The fields can be advanced in time using the full set of Maxwell’s equations explicitly, just the electrostatic fields, or an implicit set of equations that includes Ampere’s law without the displacement current. An arbitrary number of spherical harmonics can be included permitting efficient studies of physics when the distribution function is nearly in or far from equilibrium. This can drastically reduce the computational cost when only a few spherical harmonics are required. OSHUN was tested against a variety of problems spanning collisional and collisionless systems including Landau Damping, the two stream instability, Spitzer-Harm, and Epperlein-Haines heat flow coefficients in warm magnetized and unmagnetized plasmas. It was also used to explore how the heat flow in the laser entrance hole could modify Stimulated Raman Backscatter in Inertial Confinement Fusion relevant plasmas.New numerical/algorithmic techniques where implemented in the PIC code OSIRIS. In particular, new software engineering techniques facilitated the addition of an algorithm which uses PIC in the r-z coordinates system with a gridless description in the azimuthal angle \phi. The fields, equations, and current are decomposed into an azimuthal mode, m, expansion. This Quasi-3D description permits 3D simulations at a drastically lower computational cost (approaching the cost of 2D simulations) in systems that exhibit nearly azimuthal (cylindrical) symmetry. This capability was used to examine laser wakefield acceleration (LWFA). It was used to verify scaling laws for LWFA in a nonlinear, self-guide regime. The Quasi-3D algorithm was coupled to an independently developed module in OSIRIS that allows simulation of LWFA in a Lorentz-boosted frame. Doing the calculations in this frame yields a computational savings that scales as gamma^2 (where gamma is the Lorentz boost factor) which typically ranges from 100 to 100,000 in the systems under consideration. These modules required the development of novel field solvers and current deposition algorithms to eliminate a numerical instability called the Numerical Cerenkov Instability (NCI). These were added to OSIRIS using the new software engineering techniques now possible with Fortran 2003.OSIRIS was updated to utilize the Graphics Processing Units (GPUs) present in exascale systems like the Summit supercomputer recently built at the Oak Ridge National Laboratory. A GPU version of OSIRIS was used to examine the interactions of Laser Speckles from Stimulated Raman Scattering (SRS). It was found that speckles can mutually interact via scattering light, plasma waves, or non-thermal electrons transporting from speckles above threshold from SRS. This can trigger SRS in speckles that were below threshold.Efforts towards the ultimate (and ongoing) goal of fully integrating the Quasi-3D, Lorentz-boosted frame, and GPU modules is described. When combined, these modules have the potential speed up 3D laser-plasma simulations by immense factors of a million or more."||2019|10.1063/1.5110513|A. Tableman|1.8333333333333333|2
1548|coreMRI: A high-performance, publicly available MR simulation platform on the cloud|Introduction A Cloud-ORiented Engine for advanced MRI simulations (coreMRI) is presented in this study. The aim was to develop the first advanced MR simulation platform delivered as a web service through an on-demand, scalable cloud-based and GPU-based infrastructure. We hypothesized that such an online MR simulation platform could be utilized as a virtual MRI scanner but also as a cloud-based, high-performance engine for advanced MR simulations in simulation-based quantitative MR (qMR) methods. Methods and results The simulation framework of coreMRI was based on the solution of the Bloch equations and utilized a ground-up-approach design based on the principles already published in the literature. The development of a front-end environment allowed the connection of the end-users to the GPU-equipped instances on the cloud. The coreMRI simulation platform was based on a modular design where individual modules (such as the Gadgetron reconstruction framework and a newly developed Pulse Sequence Designer) could be inserted in the main simulation framework. Different types and sources of pulse sequences and anatomical models were utilized in this study revealing the flexibility that the coreMRI simulation platform offers to the users. The performance and scalability of coreMRI were also examined on multi-GPU configurations on the cloud, showing that a multi-GPU computer on the cloud equipped with a newer generation of GPU cards could significantly mitigate the prolonged execution times that accompany more realistic MRI and qMR simulations. Conclusions coreMRI is available to the entire MR community, whereas its high performance and scalability allow its users to configure advanced MRI experiments without the constraints imposed by experimentation in a true MRI scanner (such as time constraint and limited availability of MR scanners), without upfront investment for purchasing advanced computer systems and without any user expertise on computer programming or MR physics. coreMRI is available to the users through the webpage https://www.coreMRI.org.|PLoS ONE|2019|10.1371/journal.pone.0216594|A. Aletras, C. Xanthis|1.8333333333333333|2
1558|Parallel Beamlet Dose Calculation via Beamlet Contexts in a Distributed Multi-GPU Framework.|PURPOSE\nDose calculation is one of the most computationally intensive, yet essential tasks in the treatment planning process. With the recent interest in automatic beam orientation and arc trajectory optimization techniques, there is a great need for more efficient model-based dose calculation algorithms that can accommodate hundreds to thousands of beam candidates at once. Foundational work has shown the translation of dose calculation algorithms to graphical processing units (GPUs), lending to remarkable gains in processing efficiency. But these methods provide parallelization of dose for only a single beamlet, serializing the calculation of multiple beamlets and under-utilizing the potential of modern GPUs. In this paper, the authors propose a framework enabling parallel computation of many beamlet doses using a novel beamlet context transformation and further embed this approach in a scalable network of multi-GPU computational nodes.\n\n\nMETHODS\nThe proposed context-based transformation separates beamlet-local density and TERMA into distinct beamlet contexts that independently provide sufficient data for beamlet dose calculation. Beamlet contexts are arranged in a composite context array with dosimetric isolation, and the context array is subjected to a GPU collapsed-cone convolution superposition procedure, producing the set of beamlet-specific dose distributions in a single pass. Dose from each context is converted to a sparse representation for efficient storage and retrieval during treatment plan optimization. The context radius is a new parameter permitting flexibility between the speed and fidelity of the dose calculation process. A distributed manager-worker architecture is constructed around the context-based GPU dose calculation approach supporting an arbitrary number of worker nodes and resident GPUs. Phantom experiments were executed to verify the accuracy of the context-based approach compared to Monte Carlo and a reference CPU-CCCS implementation for single beamlets and broad beams composed by addition of beamlets. Dose for representative 4π beam sets was calculated in lung and prostate cases to compare its efficiency with that of an existing beamlet-sequential GPU-CCCS implementation. Code profiling was also performed to evaluate the scalability of the framework across many networked GPUs.\n\n\nRESULTS\nThe dosimetric accuracy of the context-based method displays less than 1.35% and 2.35% average error from the existing serialized CPU-CCCS algorithm and Monte Carlo simulation for beamlet-specific PDDs in water and slab phantoms, respectively. The context-based method demonstrates substantial speed up of up to two orders of magnitude over the beamlet-sequential GPU-CCCS method in the tested configurations. The context-based framework demonstrates near linear scaling in the number of distributed compute nodes and GPUs employed, indicating that it is flexible enough to meet the performance requirements of most users by simply increasing the hardware utilization.\n\n\nCONCLUSIONS\nThe context-based approach demonstrates a new expectation of performance for beamlet-based dose calculation methods. This approach has been successful in accelerating the dose calculation process for very-large-scale treatment planning problems - such as automatic 4π IMRT beam orientation and VMAT arc trajectory selection, with hundreds of thousands of beamlets - in clinically feasible timeframes. The flexibility of this framework makes it's a strong candidate for use in a variety of other very-large-scale treatment planning tasks and clinical workflows. This article is protected by copyright. All rights reserved.|Medical Physics (Lancaster)|2019|10.1002/mp.13651|K. Sheng, J. Neylon, C. Ouyang, Youming Yang, R. Neph|1.8333333333333333|2
1599|2D GPU-Accelerated High Resolution Numerical Scheme for Solving Diffusive Wave Equations|We developed a GPU-accelerated 2D physically based distributed rainfall runoff model for a PC environment. The governing equations were derived from the diffusive wave model for surface flow and the Horton infiltration model for rainfall loss. A numerical method for the diffusive wave equations was implemented based on a Godunov-type finite volume scheme. The flux at the computational cell interface was reconstructed using the piecewise linear monotonic upwind scheme for conservation laws with a van Leer slope total variation diminishing limiter. Parallelization was implemented using CUDA-Fortran with an NVIDIA GeForce GTX 1060 GPU. The proposed model was tested and verified against several 1D and 2D rainfall runoff processes with various topographies containing depressions. Simulated hydrographs, water depth, and velocity were compared to analytical solutions, dynamic wave modeling results, and measurement data. The diffusive wave model reproduced the runoff processes of impermeable basins with results similar to those of analytical solutions and the numerical results of a dynamic wave model. For ideal permeable basins containing depressions such as furrows and ponds, physically reasonable rainfall runoff processes were observed. From tests on a real basin with complex terrain, reasonable agreement with the measured data was observed. The performance of parallel computing was very efficient as the number of grids increased, achieving a maximum speedup of approximately 150 times compared to a CPU version using an Intel i7 4.7-GHz CPU in a PC environment.|Water|2019|10.3390/W11071447|Dae‐Hong Kim, Seonryang Park, Boram Kim|1.8333333333333333|2
561|CryoGAN: A New Reconstruction Paradigm for Single-Particle Cryo-EM via Deep Adversarial Learning|We present CryoGAN, a new paradigm for single-particle cryo-EM reconstruction based on unsupervised deep adversarial learning. The major challenge in single-particle cryo-EM is that the imaged particles have unknown poses. Current reconstruction techniques are based on a marginalized maximum-likelihood formulation that requires calculations over the set of all possible poses for each projection image, a computationally demanding procedure. CryoGAN sidesteps this problem by using a generative adversarial network (GAN) to learn the 3D structure that has simulated projections that most closely match the real data in a distributional sense. The architecture of CryoGAN resembles that of standard GAN, with the twist that the generator network is replaced by a model of the cryo-EM image acquisition process. CryoGAN is an unsupervised algorithm that only demands projection images and an estimate of the contrast transfer function parameters. No initial volume estimate or prior training is needed. Moreover, CryoGAN requires minimal user interaction and can provide reconstructions in a matter of hours on a high-end GPU. In addition, we provide sound mathematical guarantees on the recovery of the correct structure. CryoGAN currently achieves a 8.6 Å resolution on a realistic synthetic dataset. Preliminary results on real β-galactosidase data demonstrate CryoGAN’s ability to exploit data statistics under standard experimental imaging conditions. We believe that this paradigm opens the door to a family of novel likelihood-free algorithms for cryo-EM reconstruction.|bioRxiv|2020|10.1101/2020.03.20.001016|Laurène Donati, Harshit Gupta, M. Unser, Michael T. McCann|1.8|2
606|Microsimulation analysis for network traffic assignment (MANTA) at metropolitan-scale for agile transportation planning|Abrupt changes in the environment have triggered massive and precipitous changes in human mobility. This requires modeling entire metropolitan areas to recognize the broader effects on the network. However, there is a trade-off between increasing the level of detail of a model and decreasing computational performance. Current implementations compromise by simulating small spatial scales, and those that operate at larger scales often require access to expensive high performance computing systems or have computation times on the order of days or weeks that discourage productive research and planning. This paper introduces a new platform, MANTA (Microsimulation Analysis for Network Traffic Assignment), for traffic microsimulation at the metropolitan-scale, employing a highly efficient and parallelized GPU implementation. The runtime to simulate all morning trips, using half-second timesteps, for the nine-county San Francisco Bay Area is just over four minutes, significantly improving the state of the art in large-scale traffic microsimulation.|Transportmetrica A: Transport Science|2020|10.1080/23249935.2021.1936281|Krishna Kumar, P. Waddell, Pavan Yedavalli|1.8|2
616|EasyAmber: A comprehensive toolbox to automate the molecular dynamics simulation of proteins|"Conformational plasticity of the functionally important regions and binding sites in protein/enzyme structures is one of the key factors affecting their function and interaction with substrates/ligands. Molecular dynamics (MD) can address the challenge of accounting for protein flexibility by predicting the time-dependent behavior of a molecular system. It has a potential of becoming a particularly important tool in protein engineering and drug discovery, but requires specialized training and skills, what impedes practical use by many investigators. We have developed the easyAmber - a comprehensive set of programs to automate the molecular dynamics routines implemented in the Amber package. The toolbox can address a wide set of tasks in computational biology struggling to account for protein flexibility. The automated workflow includes a complete set of steps from the initial ""static"" molecular model to the MD ""production run"": the full-atom model building, optimization/equilibration of the molecular system, classical/conventional and accelerated molecular dynamics simulations. The easyAmber implements advanced MD protocols, but is highly automated and easy-to-operate to attract a broad audience. The toolbox can be used on a personal desktop station equipped with a compatible gaming GPU-accelerator, as well as help to manage huge workloads on a powerful supercomputer. The software provides an opportunity to operate multiple simulations of different proteins at the same time, thus significantly increasing work efficiency. The easyAmber takes the molecular dynamics to the next level in terms of usability for complex processing of large volumes of data, thus supporting the recent trend away from inefficient ""static"" approaches in biology toward a deeper understanding of the dynamics in protein structures. The software is freely available for download at https://biokinet.belozersky.msu.ru/easyAmber, no login required."|J. Bioinform. Comput. Biol.|2020|10.1142/s0219720020400119|D. Suplatov, Yana A. Sharapova, V. Svedas|1.8|2
640|Approximate diffraction modeling for real-time sound propagation simulation.|Convincing simulation of diffraction around obstacles is critical in modeling sound propagation in virtual environments. Due to the computational complexity of large-scale wavefield simulations, ray-based models of diffraction are used in real-time interactive multimedia applications. Among popular diffraction models, the Biot-Tolstoy-Medwin (BTM) edge diffraction model is the most accurate, but it suffers from high computational complexity and hence is difficult to apply in real time. This paper introduces an alternative ray-based approach to approximating diffraction, called Volumetric Diffraction and Transmission (VDaT). VDaT is a volumetric diffraction model, meaning it performs spatial sampling of paths along which sound can traverse the scene around obstacles. VDaT uses the spatial sampling results to estimate the BTM edge-diffraction amplitude response and path length, with a much lower computational cost than computing BTM directly. On average, VDaT matches BTM results within 1-3 dB over a wide range of size scales and frequencies in basic cases, and VDaT can handle small objects and gaps better than comparable state-of-the-art real-time diffraction implementations. A GPU-parallelized implementation of VDaT is shown to be capable of simulating diffraction on thousands of direct and specular reflection path segments in small-to-medium-size scenes, within strict real-time constraints and without any precomputed scene information.|Journal of the Acoustical Society of America|2020|10.1121/10.0002115|J. Burnett, Louis Pisha, Shahrokh Yadegari, Siddharth Atre|1.8|2
729|Adaptive Heterogeneous Transient Analysis of Wind Farm Integrated Comprehensive AC/DC Grids|The increasingly complex AC/DC network as a result of the massive integration of wind farms manifests the significance of a comprehensive transient study. In this work, the wind turbine (WT) and the DC grid are modeled in detail for the electromagnetic transient (EMT) simulation to maximize its fidelity, whilst the AC grid transient stability is analyzed by dynamic simulation (DS). An interactive EMT-DS interface is thus introduced to enable their concurrency and subsequently form a co-simulation. The CPU which is dominant in system study faces a tremendous challenge in handling a great number of components albeit they exhibit homogeneity. The many-core graphics processing unit (GPU) featuring massive parallelism is therefore exploited and following the definition of an adaptive computing boundary, a flexible heterogeneous sequential-parallel processing architecture is proposed for efficient analysis of the wind-farm-integrated AC/DC grid. Topological reconfiguration of WTs is specifically carried out to reduce the numerical order whilst enhancing system homogeneity that enables the GPU to thoroughly utilize its peculiar property of single-instruction multiple-thread (SIMT) compute paradigm. Consequently, significant speedups can be attained by the proposed computing framework over pure CPU computation, while its accuracy is validated by the commercial EMT and dynamic security analysis tools PSCAD/EMTDC and DSATools, respectively.|IEEE transactions on energy conversion|2020|10.1109/TEC.2020.3043307|V. Dinavahi, Shiqi Cao, Ning Lin|1.8|2
1351|Portability and Performance of Nuclear Reactor Simulations on Many-Core Architectures|High-fidelity simulation of a full scale nuclear reactor core is a computational challenge that has yet to be met but is predicted to be achievable on exascale-class supercomputers through established hardware-specific programming models (such as OpenMP and CUDA). Recently-developed, hardware-agnostic programming models offer opportunities to express multi-threaded parallelism in a portable fashion and allow a single, more-unified code base to run on many divergent high-performance computing architectures. Though the benefits of portability are clear, questions remain as to what practical performance tradeoffs apply to real world applications. In the present study, we port two existing proxy applications that represent key algorithms in nuclear reactor simulations to the hardware-agnostic language of OCCA. Performance and efficiency of the OCCA ports are compared to the native OpenMP versions on CPU and CUDA versions on GPU architectures. This study attempts to quantify tradeoffs between performance and portability of real world applications, specifically on exascale-class simulations for nuclear industry, using newer programming models.||2015|10.1016/j.anucene.2014.08.038|R. Rahaman, John R. Tramm, T. Warburton, A. L. Lund, David S. Medina, A. Siegel|1.8|2
1557|Direct List Mode Parametric Reconstruction for Dynamic Cardiac SPECT|Recently introduced stationary dedicated cardiac SPECT scanners provide new opportunities to quantify myocardial blood flow (MBF) using dynamic SPECT. However, comparing to PET, the low sensitivity of SPECT scanners affects MBF quantification due to the high noise level, especially for 201 Thallium (201Tl) due to its typically low injected dose. The conventional indirect method for generating parametric images typically starts by reconstructing a time series of frame images followed by fitting the time-activity curve (TAC) for each voxel or segment with an appropriate kinetic model. The indirect method is simple and easy to implement; however, it usually suffers from substantial image noise that could also lead to bias. In this paper, we developed a list mode direct parametric image reconstruction algorithm to substantially reduce noise in MBF quantification using dynamic SPECT and allow for patient radiation dose reduction. GPU-based parallel computing was used to achieve more than 2000-fold acceleration. The proposed method was evaluated in both simulation and in vivo canine studies. Compared with the indirect method, the proposed direct method achieved substantially lower image noise and variability, particularly at large number of iterations and at low-count levels.|IEEE Transactions on Medical Imaging|2020|10.1109/TMI.2019.2921969|A. Sinusas, Luyao Shi, R. Carson, N. Boutagy, Yihuan Lu, Chi Liu, S. Thorn, J. Gallezot, Jing Wu|1.8|2
438|Online Body Schema Adaptation Based on Internal Mental Simulation and Multisensory Feedback|In this paper, we describe a novel approach to obtain automatic adaptation of the robot body schema and to improve the robot perceptual and motor skills based on this body knowledge. Predictions obtained through a mental simulation of the body are combined with the real sensory feedback to achieve two objectives simultaneously: body schema adaptation and markerless 6D hand pose estimation. The body schema consists of a computer graphics simulation of the robot, which includes the arm and head kinematics (adapted online during the movements) and an appearance model of the hand shape and texture. The mental simulation process generates predictions on how the hand will appear in the robot camera images, based on the body schema and the proprioceptive information (i.e. motor encoders). These predictions are compared to the actual images using Sequential Monte Carlo techniques to feed a particle-based Bayesian estimation method to estimate the parameters of the body schema. The updated body schema will improve the estimates of the 6D hand pose, which is then used in a closed-loop control scheme (i.e. visual servoing), enabling precise reaching. We report experiments with the iCub humanoid robot that support the validity of our approach. A number of simulations with precise ground-truth were performed to evaluate the estimation capabilities of the proposed framework. Then, we show how the use of high-performance GPU programming and an edge-based algorithm for visual perception allow for real-time implementation in real world scenarios.|Frontiers in Robotics and AI|2016|10.3389/frobt.2016.00007|L. Jamone, Alexandre Bernardino, Pedro Vicente|1.7777777777777777|2
1264|Computationally Efficient Tsunami Modeling on Graphics Processing Units (GPUs)|Tsunamis generated by earthquakes commonly propagate as long waves in the deep ocean and develop into sharp-fronted surges moving rapidly towards the coast in shallow water, which may be effectively simulated by hydrodynamic models solving the nonlinear shallow water equations (SWEs). However, most of the existing tsunami models suffer from long simulation time for large-scale real-world applications. In this work, a graphics processing unit (GPU) accelerated finite volume shock-capturing hydrodynamic model is presented for computationally efficient tsunami simulations. The improved performance of the GPU-accelerated tsunami model is demonstrated through a laboratory benchmark test and a field-scale simulation.||2016|10.17736/IJOPE.2016.AK10|H. Mase, T. Yasuda, Q. Liang, Reza Amouzgar, P. Clarke|1.7777777777777777|2
235|Modeling and simulation of biological systems using SPICE language|The article deals with BB-SPICE (SPICE for Biochemical and Biological Systems), an extension of the famous Simulation Program with Integrated Circuit Emphasis (SPICE). BB-SPICE environment is composed of three modules: a new textual and compact description formalism for biological systems, a converter that handles this description and generates the SPICE netlist of the equivalent electronic circuit and NGSPICE which is an open-source SPICE simulator. In addition, the environment provides back and forth interfaces with SBML (System Biology Markup Language), a very common description language used in systems biology. BB-SPICE has been developed in order to bridge the gap between the simulation of biological systems on the one hand and electronics circuits on the other hand. Thus, it is suitable for applications at the interface between both domains, such as development of design tools for synthetic biology and for the virtual prototyping of biosensors and lab-on-chip. Simulation results obtained with BB-SPICE and COPASI (an open-source software used for the simulation of biochemical systems) have been compared on a benchmark of models commonly used in systems biology. Results are in accordance from a quantitative viewpoint but BB-SPICE outclasses COPASI by 1 to 3 orders of magnitude regarding the computation time. Moreover, as our software is based on NGSPICE, it could take profit of incoming updates such as the GPU implementation, of the coupling with powerful analysis and verification tools or of the integration in design automation tools (synthetic biology).|PLoS ONE|2017|10.1371/journal.pone.0182385|C. Lallement, M. Madec, J. Haiech|1.75|2
334|Challenges of Integrating Stochastic Dynamics and Cryo-Electron Tomograms in Whole-Cell Simulations.|Cryo-electron tomography (cryo-ET) has rapidly emerged as a powerful tool to investigate the internal, three-dimensional spatial organization of the cell. In parallel, the GPU-based technology to perform spatially resolved stochastic simulations of whole cells has arisen, allowing the simulation of complex biochemical networks over cell cycle time scales using data taken from -omics, single molecule experiments, and in vitro kinetics. By using real cell geometry derived from cryo-ET data, we have the opportunity to imbue these highly detailed structural data-frozen in time-with realistic biochemical dynamics and investigate how cell structure affects the behavior of the embedded chemical reaction network. Here we present two examples to illustrate the challenges and techniques involved in integrating structural data into stochastic simulations. First, a tomographic reconstruction of Saccharomyces cerevisiae is used to construct the geometry of an entire cell through which a simple stochastic model of an inducible genetic switch is studied. Second, a tomogram of the nuclear periphery in a HeLa cell is converted directly to the simulation geometry through which we study the effects of cellular substructure on the stochastic dynamics of gene repression. These simple chemical models allow us to illustrate how to build whole-cell simulations using cryo-ET derived geometry and the challenges involved in such a process.|Journal of Physical Chemistry B|2017|10.1021/acs.jpcb.7b00672|J. Stone, Z. Luthey-Schulten, R. Watanabe, E. Villa, T. Earnest, W. Baumeister, J. Mahamid|1.75|2
703|PySDM v1: particle-based cloud modelling package for warm-rain microphysics and aqueous chemistry|PySDM is an open-source Python package for simulating the dynamics of particles undergoing condensational and collisional growth, interacting with a fluid flow and subject to chemical composition changes. It is intended to serve as a building block for process-level as well as computational-fluid-dynamics simulation systems involving representation of a continuous phase (air) and a dispersed phase (aerosol), with PySDM being responsible for representation of the dispersed phase. The PySDM package core is a Pythonic high-performance implementation of the Super-Droplet Method (SDM) Monte-Carlo algorithm for representing collisional growth, hence the name. PySDM has two alternative parallel number-crunching backends available: multi-threaded CPU backend based on Numba and GPU-resident backend built on top of ThrustRTC. The usage examples are built on top of four simple atmospheric cloud modelling frameworks: box, adiabatic parcel, single-column and 2D prescribed flow kinematic models. In addition, the package ships with tutorial code depicting how PySDM can be used from Julia and Matlab.|Journal of Open Source Software|2021|10.21105/joss.03219|S. Arabas, Aleksandra Talar, Piotr Bartman, K. G'orski, Michael A. Olesik, Bartosz Piasecki, Grzegorz Lazarski, A. Jaruga|1.75|2
751|Probabilistic Inference of Simulation Parameters via Parallel Differentiable Simulation|Reproducing real world dynamics in simulation is critical for the development of new control and perception methods. This task typically involves the estimation of simu-lation parameter distributions from observed rollouts through an inverse inference problem characterized by multi-modality and skewed distributions. We address this challenging problem through a novel Bayesian inference approach that approximates a posterior distribution over simulation parameters given real sensor measurements. By extending the commonly used Gaus-sian likelihood model for trajectories via the multiple-shooting formulation, our gradient-based particle inference algorithm, Stein Variational Gradient Descent, is able to identify highly nonlinear, underactuated systems. We leverage GPU code gen-eration and differentiable simulation to evaluate the likelihood and its gradient for many particles in parallel. Our algorithm infers nonparametric distributions over simulation parame-ters more accurately than comparable baselines and handles constraints over parameters efficiently through gradient-based optimization. We evaluate estimation performance on several physical experiments. On an underactuated mechanism where a 7-DOF robot arm excites an object with an unknown mass configuration, we demonstrate how the inference technique can identify symmetries between the parameters and provide highly accurate predictions. Website: https://uscresl.github.io/prob-diff-sim|IEEE International Conference on Robotics and Automation|2021|10.1109/icra46639.2022.9812293|Fabio Ramos, Eric Heiden, G. Sukhatme, Chris Denniston, David Millard|1.75|2
758|Accelerating Weather Prediction Using Near-Memory Reconfigurable Fabric|Ongoing climate change calls for fast and accurate weather and climate modeling. However, when solving large-scale weather prediction simulations, state-of-the-art CPU and GPU implementations suffer from limited performance and high energy consumption. These implementations are dominated by complex irregular memory access patterns and low arithmetic intensity that pose fundamental challenges to acceleration. To overcome these challenges, we propose and evaluate the use of near-memory acceleration using a reconfigurable fabric with high-bandwidth memory (HBM). We focus on compound stencils that are fundamental kernels in weather prediction models. By using high-level synthesis techniques, we develop NERO, an field-programmable gate array+HBM-based accelerator connected through Open Coherent Accelerator Processor Interface to an IBM POWER9 host system. Our experimental results show that NERO outperforms a 16-core POWER9 system by \( 5.3\times \) and \( 12.7\times \) when running two different compound stencil kernels. NERO reduces the energy consumption by \( 12\times \) and \( 35\times \) for the same two kernels over the POWER9 system with an energy efficiency of 1.61 GFLOPS/W and 21.01 GFLOPS/W. We conclude that employing near-memory acceleration solutions for weather prediction modeling is promising as a means to achieve both high performance and high energy efficiency.|ACM Transactions on Reconfigurable Technology and Systems|2021|10.1145/3501804|Gagandeep Singh, C. Hagleitner, D. Diamantopoulos, S. Stuijk, O. Mutlu, Juan G'omez-Luna, H. Corporaal|1.75|2
764|Accelerating advection for atmospheric modelling on Xilinx and Intel FPGAs|Reconfigurable architectures, such as FPGAs, execute code at the electronics level, avoiding assumptions imposed by the general purpose black-box micro-architectures of CPUs and GPUs. Such tailored execution can result in increased performance and power efficiency, and as the HPC community moves towards exascale an important question is the role these hardware technologies can play in future supercomputers.In this paper we explore the porting of the PW advection kernel, an important code component used in a variety of atmospheric simulations and accounting for around 40% of the runtime of the popular Met Office NERC Cloud model (MONC). Building upon previous work which ported this kernel to an older generation of Xilinx FPGA, we target latest generation Xilinx Alveo U280 and Intel Stratix 10 FPGAs. Designing around the abstraction of an Application Specific Dataflow Machine (ASDM), we develop a design which is performance portable between vendors and explore implementation differences between the tool chains and compare kernel performance between FPGA hardware. This is followed by a more general performance comparison, scaling up the number of kernels on the Xilinx Alveo and Intel Stratix 10, against a 24 core Xeon Platinum Cascade Lake CPU and NVIDIA Tesla V100 GPU. When overlapping the transfer of data to and from the boards with compute, the FPGA solutions considerably outperform the CPU and, whilst falling short of the GPU in terms of performance, demonstrate power usage benefits, with the Alveo being especially power efficient. The result of this work is a comparison and set of design techniques that apply both to this specific atmospheric advection kernel on Xilinx and Intel FPGAs, and that are also of interest more widely when looking to accelerate HPC codes on a variety of reconfigurable architectures.|IEEE International Conference on Cluster Computing|2021|10.1109/Cluster48925.2021.00113|Nick Brown|1.75|2
796|Training machine learning models faster with Dask|"Machine learning (ML) relies on stochastic algorithms, all of which rely on gradient approximations with ""batch size"" examples. Growing the batch size as the optimization proceeds is a simple and usable method to reduce the training time, provided that the number of workers grows with the batch size. In this work, we provide a package that trains PyTorch models on Dask clusters, and can grow the batch size if desired. Our simulations indicate that for a particular model that uses GPUs for a popular image classification task, the training time can be reduced from about 120 minutes with standard SGD to 45 minutes with a variable batch size method."|SciPy|2021|10.25080/majora-1b6fd038-011|Scott Sievert, Joesph Holt|1.75|2
800|PICSAR-QED: a Monte Carlo module to simulate strong-field quantum electrodynamics in particle-in-cell codes for exascale architectures|Physical scenarios where the electromagnetic fields are so strong that quantum electrodynamics (QED) plays a substantial role are one of the frontiers of contemporary plasma physics research. Investigating those scenarios requires state-of-the-art particle-in-cell (PIC) codes able to run on top high-performance computing (HPC) machines and, at the same time, able to simulate strong-field QED processes. This work presents the PICSAR-QED library, an open-source, portable implementation of a Monte Carlo module designed to provide modern PIC codes with the capability to simulate such processes, and optimized for HPC. Detailed tests and benchmarks are carried out to validate the physical models in PICSAR-QED, to study how numerical parameters affect such models, and to demonstrate its capability to run on different architectures (CPUs and GPUs). Its integration with WarpX, a state-of-the-art PIC code designed to deliver scalable performance on upcoming exascale supercomputers, is also discussed and validated against results from the existing literature.|New Journal of Physics|2021|10.1088/1367-2630/ac4ef1|H. Vincenti, A. Sainte-Marie, N. Zaïm, J. Vay, L. Fedeli, Maxence Th'evenet, A. Huebl, A. Myers|1.75|2
815|Metrics and Design of an Instruction Roofline Model for AMD GPUs|Due to the recent announcement of the Frontier supercomputer, many scientific application developers are working to make their applications compatible with AMD (CPU-GPU) architectures, which means moving away from the traditional CPU and NVIDIA-GPU systems. Due to the current limitations of profiling tools for AMD GPUs, this shift leaves a void in how to measure application performance on AMD GPUs. In this article, we design an instruction roofline model for AMD GPUs using AMD’s ROCProfiler and a benchmarking tool, BabelStream (the HIP implementation), as a way to measure an application’s performance in instructions and memory transactions on new AMD hardware. Specifically, we create instruction roofline models for a case study scientific application, PIConGPU, an open source particle-in-cell simulations application used for plasma and laser-plasma physics on the NVIDIA V100, AMD Radeon Instinct MI60, and AMD Instinct MI100 GPUs. When looking at the performance of multiple kernels of interest in PIConGPU we find that although the AMD MI100 GPU achieves a similar, or better, execution time compared to the NVIDIA V100 GPU, profiling tool differences make comparing performance of these two architectures hard. When looking at execution time, GIPS, and instruction intensity, the AMD MI60 achieves the worst performance out of the three GPUs used in this work.|ACM Transactions on Parallel Computing|2021|10.1145/3505285|S. Chandrasekaran, M. Leinhauser, S. Bastrakov, M. Bussmann, A. Debus, R. Widera|1.75|2
830|Porting incompressible flow matrix assembly to FPGAs for accelerating HPC engineering simulations|Engineering is an important domain for supercomputing, with the Alya model being a popular code for undertaking such simulations. With ever increasing demand from users to model larger, more complex systems at reduced time to solution it is important to explore the role that novel hardware technologies, such as FPGAs, can play in accelerating these workloads on future exascale systems.In this paper we explore the porting of Alya’s incompressible flow matrix assembly kernel, which accounts for a large proportion of the model runtime, onto FPGAs. After describing in detail successful strategies for optimisation at the kernel level, we then explore sharing the workload between the FPGA and host CPU, mapping most appropriate parts of the kernel between these technologies, enabling us to more effectively exploit the FPGA. We then compare the performance of our approach on a Xilinx Alveo U280 against a 24-core Xeon Platinum CPU and Nvidia V100 GPU, with the FPGA significantly out-performing the CPU and performing comparably against the GPU, whilst drawing substantially less power. The result of this work is both an experience report describing appropriate dataflow optimisations which we believe can be applied more widely as a case-study across HPC codes, and a performance comparison for this specific workload that demonstrates the potential for FPGAs in accelerating HPC engineering simulations.|International Workshop on Heterogeneous High-performance Reconfigurable Computing|2021|10.1109/H2RC54759.2021.00007|Nick Brown|1.75|2
833|System-Level Modeling of GPU/FPGA Clusters for Molecular Dynamics Simulations|FPGA-accelerated molecular dynamics (MD) research dates back to almost two decades ago and is still being actively studied. MD on FPGA clusters, however, is still in its initial phase with only small systems built and limited performance studies. Given the cost of building accelerator clusters, and (as we show) the number of plausible architectures, a thorough study is needed. In particular, we investigate both FPGA and GPU/FPGA hybrid clusters. The latter are potentially attractive given the broad availability of GPU clusters and use of GPUs for MD, but the current inability of GPUs to scale for certain critical domains. In this work, we model four promising MD accelerator platforms, including FPGA-only systems with homogeneous and heterogeneous nodes, an existing FPGA-GPU hybrid system (the Cygnus supercomputer), and a synthesis of the commercially available Nvidia DGX-1/DGX-2 products with an FPGA cluster. The models are compared and evaluated, and we find that each of the platforms is preferable for some domains.|IEEE Conference on High Performance Extreme Computing|2021|10.1109/HPEC49654.2021.9622838|Vipin Sachdeva, W. Sherman, Sahan Bandara, Martin C. Herbordt, Tong Geng, Chunshu Wu|1.75|2
1237|Performance of second order particle-in-cell methods on modern many-core architectures|The emergence of modern many-core architectures that offer an extreme level of parallelism makes methods that were previously infeasible due to computational expense now achievable. Particle-in-Cell (PIC) codes often fail to fully leverage this increased performance \npotential due to their high use of memory bandwidth. The use of higher order PIC methods may offer a solution to this by improving simulation accuracy significantly for an increase in computational intensity when compared to their first order counterparts. This greater expense is accompanied with only a minor increase in the amount of memory throughput required during the simulation. \nIn this presentation we will show the performance of a second order PIC algorithm. Our implementation uses second order finite elements and particles that are represented with a collection of surrounding ghost particles. These ghost particles each have associated weights \nand offsets around the true particle position and therefore represent a charge distribution. \nWe test our PIC implementation against a first order algorithm on various modern compute architectures including Intel’s Knights Landing (KNL) and NVIDIA’s Tesla P100. \nOur preliminary results show the viability of second order methods for PIC applications on these architectures when compared to previous generations of many-core hardware. \nSpecifically, we see an order of magnitude improvement in performance for second order methods between the Pascal and Kepler GPU architectures, despite only a 4× improvement in theoretical peak performance between the architectures. Although these initial results \nshow a large increase in runtime over first order methods, we hope to be able to show improved scaling behaviour and increased simulation accuracy in the future.||2017|10.1109/cluster.2017.122|S. Jarvis, S. Wright, Dominic A. S. Brown, John P. Jones, M. Bettencourt|1.75|2
297|Fast Pencil Beam Dose Calculation for Proton Therapy Using a Double-Gaussian Beam Model|The highly conformal dose distributions produced by scanned proton pencil beams (PBs) are more sensitive to motion and anatomical changes than those produced by conventional radiotherapy. The ability to calculate the dose in real-time as it is being delivered would enable, for example, online dose monitoring, and is therefore highly desirable. We have previously described an implementation of a PB algorithm running on graphics processing units (GPUs) intended specifically for online dose calculation. Here, we present an extension to the dose calculation engine employing a double-Gaussian beam model to better account for the low-dose halo. To the best of our knowledge, it is the first such PB algorithm for proton therapy running on a GPU. We employ two different parameterizations for the halo dose, one describing the distribution of secondary particles from nuclear interactions found in the literature and one relying on directly fitting the model to Monte Carlo simulations of PBs in water. Despite the large width of the halo contribution, we show how in either case the second Gaussian can be included while prolonging the calculation of the investigated plans by no more than 16%, or the calculation of the most time-consuming energy layers by about 25%. Furthermore, the calculation time is relatively unaffected by the parameterization used, which suggests that these results should hold also for different systems. Finally, since the implementation is based on an algorithm employed by a commercial treatment planning system, it is expected that with adequate tuning, it should be able to reproduce the halo dose from a general beam line with sufficient accuracy.|Frontiers in Oncology|2015|10.3389/fonc.2015.00281|R. Jena, J. da Silva, R. Ansorge|1.7|2
1373|GPU Accelerated Lattice Boltzmann Simulation of Flow in Porous Media|A sparse lattice representation lattice Boltzmann method algorithm is implemented on Graphics Processing Units( GPU) to accelerate pore scale flow simuation. Prefomance testing shows that sparse lattice representation approach grately reduces memory requirement and maintains performance under low porosity compared with basic algorithm. Overall speedup reaches two orders of magnitude compared with serial code. Various factors including collision model,float number precision,and GPU that affect computing speed of the algorithm are invesgated independently. It indicates that MRT model runs as fast as LBGK model on new generation of GPU cards. While on old GPU cards,MRT model's computing speed matchs LBGK only when using single precision float.||2015|10.1007/s11242-015-0609-3|Zhu Lianhu|1.7|2
835|Cost-Efficient and Quality-of-Experience-Aware Player Request Scheduling and Rendering Server Allocation for Edge-Computing-Assisted Multiplayer Cloud Gaming|Prompted by the remarkable progress in both cloud computing and GPU virtualization, cloud gaming has been attracting more and more attention in the gaming industry. With the cloud gaming model, players do not need to download or install the game on local devices, and constantly upgrade their devices. Despite these advantages, cloud gaming faces several challenges for its success, including long response delay, poor game fairness, and high operational cost. To this end, this article proposes an edge computing-assisted multiplayer cloud gaming system named ECACG to improve multiplayer cloud gaming experiences and operating costs by offloading the game rendering task to the nearby edge server. Based on the ECACG, two decision processes are completed. One is player request scheduling and the other is rendering server allocation. The decision problem is formulated into a constrained multiobjective optimization model. A novel hybrid algorithm based on deep reinforcement learning and heuristic strategy is developed to solve the optimization problem. The effectiveness of the proposed ECACG is evaluated by simulation experiments based on the real-world parameters. The simulation results show that compared with the existing schemes, the proposed ECACG can achieve lower rental costs and better fairness, while providing the good-enough response delay for players.|IEEE Internet of Things Journal|2022|10.1109/JIOT.2021.3132849|Yongqiang Gao, Jiantao Zhou, Chaoyu Zhang, Zhulong Xie, Zhengwei Qi|1.6666666666666667|2
848|A Unified Software/Hardware Scalable Architecture for Brain-Inspired Computing Based on Self-Organizing Neural Models|The field of artificial intelligence has significantly advanced over the past decades, inspired by discoveries from the fields of biology and neuroscience. The idea of this work is inspired by the process of self-organization of cortical areas in the human brain from both afferent and lateral/internal connections. In this work, we develop a brain-inspired neural model associating Self-Organizing Maps (SOM) and Hebbian learning in the Reentrant SOM (ReSOM) model. The framework is applied to multimodal classification problems. Compared to existing methods based on unsupervised learning with post-labeling, the model enhances the state-of-the-art results. This work also demonstrates the distributed and scalable nature of the model through both simulation results and hardware execution on a dedicated FPGA-based platform named SCALP (Self-configurable 3D Cellular Adaptive Platform). SCALP boards can be interconnected in a modular way to support the structure of the neural model. Such a unified software and hardware approach enables the processing to be scaled and allows information from several modalities to be merged dynamically. The deployment on hardware boards provides performance results of parallel execution on several devices, with the communication between each board through dedicated serial links. The proposed unified architecture, composed of the ReSOM model and the SCALP hardware platform, demonstrates a significant increase in accuracy thanks to multimodal association, and a good trade-off between latency and power consumption compared to a centralized GPU implementation.|Frontiers in Neuroscience|2022|10.3389/fnins.2022.825879|Lyes Khacef, Joachim Schmidt, A. Upegui, Laurent Rodriguez, Benoît Miramond, Artem R. Muliukov, Quentin Berthet|1.6666666666666667|2
860|RateML: A Code Generation Tool for Brain Network Models|Whole brain network models are now an established tool in scientific and clinical research, however their use in a larger workflow still adds significant informatics complexity. We propose a tool, RateML, that enables users to generate such models from a succinct declarative description, in which the mathematics of the model are described without specifying how their simulation should be implemented. RateML builds on NeuroML’s Low Entropy Model Specification (LEMS), an XML based language for specifying models of dynamical systems, allowing descriptions of neural mass and discretized neural field models, as implemented by the Virtual Brain (TVB) simulator: the end user describes their model’s mathematics once and generates and runs code for different languages, targeting both CPUs for fast single simulations and GPUs for parallel ensemble simulations. High performance parallel simulations are crucial for tuning many parameters of a model to empirical data such as functional magnetic resonance imaging (fMRI), with reasonable execution times on small or modest hardware resources. Specifically, while RateML can generate Python model code, it enables generation of Compute Unified Device Architecture C++ code for NVIDIA GPUs. When a CUDA implementation of a model is generated, a tailored model driver class is produced, enabling the user to tweak the driver by hand and perform the parameter sweep. The model and driver can be executed on any compute capable NVIDIA GPU with a high degree of parallelization, either locally or in a compute cluster environment. The results reported in this manuscript show that with the CUDA code generated by RateML, it is possible to explore thousands of parameter combinations with a single Graphics Processing Unit for different models, substantially reducing parameter exploration times and resource usage for the brain network models, in turn accelerating the research workflow itself. This provides a new tool to create efficient and broader parameter fitting workflows, support studies on larger cohorts, and derive more robust and statistically relevant conclusions about brain dynamics.|Frontiers in Network Physiology|2022|10.3389/fnetp.2022.826345|Michiel van der Vlag, M. Woodman, Sandra Díaz-Pier, A. Morrison, Viktor Jirsa , J. Fousek, Aarón Pérez Martín|1.6666666666666667|2
890|A high throughput generative vector autoregression model for stochastic synapses|By imitating the synaptic connectivity and plasticity of the brain, emerging electronic nanodevices offer new opportunities as the building blocks of neuromorphic systems. One challenge for large-scale simulations of computational architectures based on emerging devices is to accurately capture device response, hysteresis, noise, and the covariance structure in the temporal domain as well as between the different device parameters. We address this challenge with a high throughput generative model for synaptic arrays that is based on a recently available type of electrical measurement data for resistive memory cells. We map this real-world data onto a vector autoregressive stochastic process to accurately reproduce the device parameters and their cross-correlation structure. While closely matching the measured data, our model is still very fast; we provide parallelized implementations for both CPUs and GPUs and demonstrate array sizes above one billion cells and throughputs exceeding one hundred million weight updates per second, above the pixel rate of a 30 frames/s 4K video stream.|Frontiers in Neuroscience|2022|10.3389/fnins.2022.941753|G. Molas, J. Nodin, T. Hennen, D. Bedau, R. Waser, D. J. Wouters, A. Elías|1.6666666666666667|2
901|Near-wall approximations to speed up simulations for atmosphere boundary layers in the presence of forests using lattice Boltzmann method on GPU|Forests play an important role in inﬂuencing the wind resource in atmospheric boundary layers and the fatigue life of wind turbines. Due to turbulence, a diﬃculty in the simulation of the forest eﬀects is that ﬂow statistical and ﬂuctuating content should be accurately resolved using a turbulence-resolved CFD method, which requires a large amount of computing time and resources. In this paper, we demonstrate a fast but accurate simulation platform that uses a lattice Boltzmann method with large eddy simulation on Graphic Processing Units (GPU). The simulation tool is the open-source program, GASCANS, developed at the University of Manchester. The simulation platform is validated based on canonical wall-bounded turbulent ﬂows. A forest is modelled in the form of body forces injected near the wall. Since a uniform cell size is applied throughout the computational domain, the averaged ﬁrst-layer cell height over the wall reaches to (cid:104) ∆ y + (cid:105) = 165. Simulation results agree well with previous experiments and numerical data obtained from ﬁnite volume methods. We demonstrate that good results are possible without the use of a wall-function, since the forest forces overwhelm wall friction. This is shown to hold as long as the forest region is resolved with several cells. In addition to the GPU speedup, the approximations also signiﬁcantly beneﬁt the computation eﬃciency. (This paper is currently submitted to peer-review journal)||2022|10.1080/19942060.2022.2132420|Xinyuan Shao, J. Niu, L. Davidson, X. Xue, M. Santasmasas, H. Yao, A. Revell|1.6666666666666667|2
903|Robotic Detection of a Human-Comprehensible Gestural Language for Underwater Multi-Human-Robot Collaboration|In this paper, we present a motion-based robotic communication framework that enables non-verbal communication among autonomous underwater vehicles (AUVs) and human divers. We design a gestural language for AUV-to-AUV communication which can be easily understood by divers observing the conversation - unlike typical radio frequency, light, or audio-based AUV communication. To allow AUVs to visually understand a gesture from another AUV, we propose a deep network (RRCommNet) which exploits a self-attention mechanism to learn to recognize each message by extracting maximally discriminative spatio-temporal features. We train this network on diverse simulated and real-world data. Our experimental evaluations, both in simulation and in closed-water robot trials, demonstrate that the proposed RRCommNet architecture is able to decipher gesture-based messages with an average accuracy of 88-94% on simulated data and 73-83% on real data (depending on the version of the model used). Further, by performing a message transcription study with human participants, we also show that the proposed language can be understood by humans with an overall transcription accuracy of 88 %. Finally, we discuss the inference runtime of RRCommNet on embedded GPU hardware, for real-time use on board AUVs in the field.|IEEE/RJS International Conference on Intelligent RObots and Systems|2022|10.1109/IROS47612.2022.9981450|Junaed Sattar, Michael Fulton, Sadman Sakib Enan|1.6666666666666667|2
917|Petascale Brownian dynamics simulations of highly resolved polymer chains with hydrodynamic interactions using modern GPUs|Brownian dynamics simulations of fairly long, highly detailed polymer chains, at the resolution of a single Kuhn step, remains computationally prohibitive even on the modern processors. This is especially true when the beads on the chain experience hydrodynamic interactions (HI), which requires the usage of methods like Cholesky decomposition of large matrices at every timestep. In this study, we perform Petascale BD simulations, with HI, of fairly long and highly resolved polymer chains on modern GPUs. Our results clearly highlight the inadequacies of the typical models that use beads connected by springs. In this manuscript, firstly, we present the details of a highly scalable, parallel hybrid code implemented on a GPU for BD simulations of chains resolved to a single Kuhn step. In this hybrid code using CUDA and MPI, we have incorporated HI using the Cholesky decomposition method. Next, we validate the GPU implementation extensively with theoretical expectations for polymer chains at equilibrium and in flow with results in the absence of HI. Further, our results in flow with HI show significantly different temporal variations of stretch, in both startup extensional and shear flows, relative to the conventional bead-spring models. In all cases investigated, the ensemble averaged chain stretch is much lower than bead-spring predictions. Also, quite remarkably, our GPU implementation shows a scaling of $\sim$$N^{1.2}$ and $\sim$$N^{2.2}$ of the computational times for shorter and longer chains in the most modern available GPU, respectively, which is significantly lower than the theoretically expected $\sim$$N^{3}$. We expect our methods and results to pave the way for further analysis of polymer physics in flow fields, with long and highly detailed chain models.||2022|10.1002/mats.202200008|Praphul Kumar, V. Krishna, Bharatkumar Sharma, I. S. Dalal|1.6666666666666667|2
928|GX: a GPU-native gyrokinetic turbulence code for tokamak and stellarator design|GX is a code for solving the nonlinear gyrokinetic system for low-frequency turbulence in magnetized plasmas, particularly tokamaks and stellarators. In GX, our primary motivation and target is a fast gyrokinetic solver that can be used for fusion reactor design and optimization along with wide-ranging physics exploration. This has led to several code and algorithm design decisions, specifically chosen to prioritize time to solution. First, we have used a discretization algorithm that is pseudo-spectral in the entire phase-space, including a Laguerre-Hermite pseudo-spectral formulation of velocity space, which allows for smooth interpolation between coarse gyrofluid-like resolutions and finer conventional gyrokinetic resolutions and efficient evaluation of a model collision operator. Additionally, we have built GX to natively target graphics processors (GPUs), which are among the fastest computational platforms available today. Finally, we have taken advantage of the reactor-relevant limit of small $\rho_*$ by using the radially-local flux-tube approach. In this paper we present details about the gyrokinetic system and the numerical algorithms used in GX to solve the system. We then present several numerical benchmarks against established gyrokinetic codes in both tokamak and stellarator magnetic geometries to verify that GX correctly simulates gyrokinetic turbulence in the small $\rho_*$ limit. Moreover, we show that the convergence properties of the Laguerre-Hermite spectral velocity formulation are quite favorable for nonlinear problems of interest. Coupled with GPU acceleration, which we also investigate with scaling studies, this enables GX to be able to produce useful turbulence simulations in minutes on one (or a few) GPUs. GX is open-source software that is ready for fusion reactor design studies.||2022|10.1063/5.0087131|W. Dorland, T. Qian, N. Mandell, I. Abel, R. Gaur, Maria D. Martin, P. Kim|1.6666666666666667|2
941|BrainPy: a flexible, integrative, efficient, and extensible framework towards general-purpose brain dynamics programming|The neural mechanisms underlying brain functions are extremely complicated. Brain dynamics modeling is an indispensable tool for elucidating these mechanisms by modeling the dynamics of the neural circuits that execute brain functions. To ease and facilitate brain dynamics modeling, a general-purpose programming framework is needed to enable users to freely define neural models across multiple scales; efficiently simulate, train, and analyze model dynamics; and conveniently extend new modeling approaches. By utilizing the advanced just-in-time (JIT) compilation, we developed BrainPy. BrainPy provides a rich infrastructure tailored for brain dynamics programming, which supports an integrated platform for brain dynamics model building, simulation, training, and analysis. Models in BrainPy can be JIT compiled into binary instructions for multiple devices (including CPU, GPU, and TPU) to achieve a high running performance comparable to native C or CUDA. Moreover, BrainPy features an extensible architecture allowing easy expansion of new infrastructure, utilities, and machine learning approaches.|bioRxiv|2022|10.1101/2022.10.28.514024|Si Wu, Xiaoyu Chen, Tianqiu Zhang, Chaoming Wang|1.6666666666666667|2
962|Task Offloading and Resource Allocation in CPU-GPU Heterogeneous Networks|With the massive use of GPU, task scheduling under CPU-GPU clusters has become an indispensable research topic. Unlike existing models, we propose an innovative framework that users offload their tasks in CPU-GPU heterogeneous Edge Clusters (ECs) instead of general-purpose CPU clusters. The framework takes full advantage of the GPU's powerful parallel computing capabilities. Specifically, we decompose each user task into sequential segments and parallel segments, which can be offloaded to CPUs and GPUs of the ECs, respectively. By dis-cretizing the GPU's computing capability, we formulate a Mixed Integer Nonlinear Programming (MINLP), which involves jointly optimizing the task offloading decision, the uplink transmission power of users, and computing resource allocation. To tackle this challenging problem, we propose a Joint Simulated Annealing and Convex Optimization (JSAC) based algorithm to minimize the total overhead consisting of delay and energy consumption. Our experimental simulation results demonstrate that the JSAC algorithm can make full use of GPU's powerful parallel computing capability via allocating GPU resources effectively. In particular, the JSAC algorithm achieves optimal performance in terms of system overhead, number of beneficial UEs, and speedup.|Global Communications Conference|2022|10.1109/GLOBECOM48099.2022.10000976|Wenxiang Liu, Yong Zhou, Chenyu Gong, Liantao Wu, Mulei Ma, Yang Yang|1.6666666666666667|2
1156|Accelerated GPU based SPECT Monte Carlo simulations|"Monte Carlo (MC) modelling is widely used in the field of single photon emission computed tomography (SPECT) as it is a reliable technique to simulate very high quality scans. This technique provides very accurate modelling of the radiation transport and particle interactions in a heterogeneous medium. Various MC codes exist for nuclear medicine imaging simulations. Recently, new strategies exploiting the computing capabilities of graphical processing units (GPU) have been proposed. This work aims at evaluating the accuracy of such GPU implementation strategies in comparison to standard MC codes in the context of SPECT imaging. GATE was considered the reference MC toolkit and used to evaluate the performance of newly developed GPU Geant4-based Monte Carlo simulation (GGEMS) modules for SPECT imaging. Radioisotopes with different photon energies were used with these various CPU and GPU Geant4-based MC codes in order to assess the best strategy for each configuration. Three different isotopes were considered: 99mTc, 111In and 131I, using a low energy high resolution (LEHR) collimator, a medium energy general purpose (MEGP) collimator and a high energy general purpose (HEGP) collimator respectively. Point source, uniform source, cylindrical phantom and anthropomorphic phantom acquisitions were simulated using a model of the GE infinia II 3/8"" gamma camera. Both simulation platforms yielded a similar system sensitivity and image statistical quality for the various combinations. The overall acceleration factor between GATE and GGEMS platform derived from the same cylindrical phantom acquisition was between 18 and 27 for the different radioisotopes. Besides, a full MC simulation using an anthropomorphic phantom showed the full potential of the GGEMS platform, with a resulting acceleration factor up to 71. The good agreement with reference codes and the acceleration factors obtained support the use of GPU implementation strategies for improving computational efficiency of SPECT imaging simulations."|Physics in Medicine and Biology|2016|10.1088/0031-9155/61/11/4001|M. Bardiès, Marie-Paule Garcia, J. Bert, D. Benoit, D. Visvikis|1.6666666666666667|2
254|A programmable hardware accelerator for simulating dynamical systems|The fast and energy-efficient simulation of dynamical systems defined by coupled ordinary/partial differential equations has emerged as an important problem. The accelerated simulation of coupled ODE/PDE is critical for analysis of physical systems as well as computing with dynamical systems. This paper presents a fast and programmable accelerator for simulating dynamical systems. The computing model of the proposed platform is based on multilayer cellular nonlinear network (CeNN) augmented with nonlinear function evaluation engines. The platform can be programmed to accelerate wide classes of ODEs/PDEs by modulating the connectivity within the multilayer CeNN engine. An innovative hardware architecture including data reuse, memory hierarchy, and near-memory processing is designed to accelerate the augmented multilayer CeNN. A dataflow model is presented which is supported by optimized memory hierarchy for efficient function evaluation. The proposed solver is designed and synthesized in 15nm technology for the hardware analysis. The performance is evaluated and compared to GPU nodes when solving wide classes of differential equations and the power consumption is analyzed to show orders of magnitude improvement in energy efficiency.|International Symposium on Computer Architecture|2017|10.1145/3079856.3080252|Yun Long, S. Mukhopadhyay, Duckhwan Kim, J. Kung|1.625|2
316|GPU-powered model analysis with PySB/cupSODA|Summary A major barrier to the practical utilization of large, complex models of biochemical systems is the lack of open‐source computational tools to evaluate model behaviors over high‐dimensional parameter spaces. This is due to the high computational expense of performing thousands to millions of model simulations required for statistical analysis. To address this need, we have implemented a user‐friendly interface between cupSODA, a GPU‐powered kinetic simulator, and PySB, a Python‐based modeling and simulation framework. For three example models of varying size, we show that for large numbers of simulations PySB/cupSODA achieves order‐of‐magnitude speedups relative to a CPU‐based ordinary differential equation integrator. Availability and implementation The PySB/cupSODA interface has been integrated into the PySB modeling framework (version 1.4.0), which can be installed from the Python Package Index (PyPI) using a Python package manager such as pip. cupSODA source code and precompiled binaries (Linux, Mac OS/X, Windows) are available at github.com/aresio/cupSODA (requires an Nvidia GPU; developer.nvidia.com/cuda‐gpus). Additional information about PySB is available at pysb.org. Contact paolo.cazzaniga@unibg.it or c.lopez@vanderbilt.edu Supplementary information Supplementary data are available at Bioinformatics online.|Bioinform.|2017|10.1093/bioinformatics/btx420|P. Cazzaniga, James C. Pino, Alexander L. R. Lubbock, D. Besozzi, Marco S. Nobile, G. Mauri, Carlos F. Lopez, L. A. Harris|1.625|2
453|GPU-accelerated atmospheric chemical kinetics in the ECHAM/MESSy (EMAC) Earth system model (version 2.52)|Abstract. This paper presents an application of GPU accelerators in Earth system modeling. We focus on atmospheric chemical kinetics, one of the most computationally intensive tasks in climate–chemistry model simulations. We developed a software package that automatically generates CUDA kernels to numerically integrate atmospheric chemical kinetics in the global climate model ECHAM/MESSy Atmospheric Chemistry (EMAC), used to study climate change and air quality scenarios. A source-to-source compiler outputs a CUDA-compatible kernel by parsing the FORTRAN code generated by the Kinetic PreProcessor (KPP) general analysis tool. All Rosenbrock methods that are available in the KPP numerical library are supported. Performance evaluation, using Fermi and Pascal CUDA-enabled GPU accelerators, shows achieved speed-ups of 4. 5 ×  and 20. 4 × , respectively, of the kernel execution time. A node-to-node real-world production performance comparison shows a 1. 75 ×  speed-up over the non-accelerated application using the KPP three-stage Rosenbrock solver. We provide a detailed description of the code optimizations used to improve the performance including memory optimizations, control code simplification, and reduction of idle time. The accuracy and correctness of the accelerated implementation are evaluated by comparing to the CPU-only code of the application. The median relative difference is found to be less than 0.000000001 % when comparing the output of the accelerated kernel the CPU-only code. The approach followed, including the computational workload division, and the developed GPU solver code can potentially be used as the basis for hardware acceleration of numerous geoscientific models that rely on KPP for atmospheric chemical kinetics applications.||2017|10.5194/GMD-10-3679-2017|Michail Alvanos, Theodoros Christoudias|1.625|2
394|Power distribution network design and optimization based on frequency dependent target impedance|In this paper, frequency dependent target impedance is proposed for optimal Power Distribution Network (PDN) design. Target impedance indicates how much and where decoupling capacitors should be located in the PDNs therefore it directly affects performance of the electrical systems or manufacturing cost. In this paper, frequency dependent target impedance considering IC's power current spectrum is proposed. Based on the proposed target impedance, GPU's VDD PDN is designed so that the hierarchical PDN impedance is maintained bellow the proposed target impedance in frequency domain. We simulated designed PDN with GPU chip's signal model in the time domain to verify the proposed target impedance. Simultaneous Switching Noise (SSN) in the GPU chip PDN was suppressed bellow the target which is 5% of the supply voltage based on proposed design. Compared to the conventional target impedance, proposed target impedance allowed more flexible design and at the same time satisfied the design criterion.|Electrical Design of Advanced Packaging and Systems Symposium|2015|10.1109/EDAPS.2015.7383675|Taisik Yang, Jonghyun Cho, Youngwoo Kim, Kiyeong Kim, Joungho Kim, Yun Ra, Kibum Kang, Woohyun Paik|1.6|2
571|Tsunami-HySEA: A GPU based model for the Italian candidate Tsunami Service Provider|Tsunami Service Providers (TSP), providing tsunami warnings in the framework of the systems coordinated by IOC/UNESCO worldwide, and other national tsunami warning centers, are striving to complement, or replace, decision matrices and pre-calculated tsunami scenario databases with FTRT (Faster Than Real Time) tsunami simulations. The aim is to increase the accuracy of tsunami forecast by assimilating the largest possible amount of data in quasi real time, and performing simulations in a few minutes wall-clock time, possibly including the coastal inundation stage. This strategy of direct real time computation, that could seem unfeasible a decade ago, it is now foreseeable thanks to the astonishingly recent increase in the computational power and bandwidth evolution of modern GPUs.||2015|10.5194/nhess-15-2019-2015|D. Melini, M. Castro, A. Piatanesi, S. Lorito, R. Tonini, I. Molinari, J. M. G. Vida, F. Romano, M. Asunción, J. Macías|1.6|2
600|Performance Evaluation of a Two-Dimensional Flood Model on Heterogeneous High-Performance Computing Architectures|This paper describes the implementation of a two-dimensional hydrodynamic flood model with two different numerical schemes on heterogeneous high-performance computing architectures. Both schemes were able to solve the nonlinear hyperbolic shallow water equations using an explicit upwind first-order approach on finite differences and finite volumes, respectively, and were conducted using MPI and CUDA. Four different test cases were simulated on the Summit supercomputer at Oak Ridge National Laboratory. Both numerical schemes scaled up to 128 nodes (768 GPUs) with a maximum 98.2x speedup of over 1 GPU. The lowest run time for the 10 day Hurricane Harvey event simulation at 5 meter resolution (272 million grid cells) was 50 minutes. GPUDirect communication proved to be more convenient than the standard communication strategy. Both strong and weak scaling are shown.|Platform for Advanced Scientific Computing Conference|2020|10.1145/3394277.3401852|M. Morales-Hernández, A. Kalyanapu, K. Evans, S. Kao, T. Dullo, S. Gangrade, Thomas M. Hines, Md Bulbul Sharif, S. Ghafoor|1.6|2
628|Deterministic Boltzmann Transport Equation Solver for Patient-Specific CT Dose Estimation: Comparison Against a Monte Carlo Benchmark for Realistic Scanner Configurations and Patient Models.|PURPOSE\nEpidemiological evidence suggests an increased risk of cancer related to CT scans, with children exposed to greater risk. The purpose of this work is to test the reliability of a linear Boltzmann Transport Equation (LBTE) solver for rapid and patient-specific CT dose estimation. This includes building a flexible LBTE framework for modeling modern clinical CT scanners and to validate the resulting dose maps across a range of realistic scanner configurations and patient models.\n\n\nMETHODS\nIn this study, computational tools were developed for modeling CT scanners, including a bowtie filter, overrange collimation, and tube current modulation. The LBTE solver requires discretization in the spatial, angular, and spectral dimensions, which may affect the accuracy of scanner modeling. To investigate these effects, this study evaluated the LBTE dose accuracy for different discretization parameters, scanner configurations, and patient models (male, female, adults, pediatric). The method used to validate the LBTE dose maps was the Monte Carlo code Geant4, which provided ground truth dose maps. LBTE simulations were implemented on a GeForce GTX 1080 graphic unit, while Geant4 was implemented on a distributed cluster of CPUs.\n\n\nRESULTS\nThe agreement between Geant4 and the LBTE solver quantifies the accuracy of the LBTE, which was similar across the different protocols and phantoms. The results suggest that 18 views per rotation provides sufficient accuracy, as no significant improvement in the accuracy was observed by increasing the number of projection views. Considering this discretization, the LBTE solver average simulation time was approximately 30 seconds. However, in the LBTE solver the phantom model was implemented with a lower voxel resolution with respect to Geant4, as it is limited by the memory of the GPU. Despite this discretization the results showed a good agreement between the LBTE and Geant4, with root mean square error of the deposited energy in organs of approximately 3.5% for most of the studied configurations.\n\n\nCONCLUSIONS\nThe LBTE solver is proposed as an alternative to Monte Carlo for patient-specific organ dose estimation. This study demonstrated accurate organ dose estimates for the rapid LBTE solver when considering realistic aspects of CT scanners and a range of phantom models. Future plans will combine the LBTE framework with deep-learning autosegmentation algorithms to provide near real-time patient-specific organ dose estimation.|Medical Physics (Lancaster)|2020|10.1002/mp.14494|P. Jordan, T. Wareing, Adam S. Wang, A. Maslowski, T. Schmidt, S. Principi|1.6|2
644|Analysis of the performance of a hybrid CPU/GPU 1D2D coupled model for real flood cases|\n Coupled 1D2D models emerged as an efficient solution for a two-dimensional (2D) representation of the floodplain combined with a fast one-dimensional (1D) schematization of the main channel. At the same time, high-performance computing (HPC) has appeared as an efficient tool for model acceleration. In this work, a previously validated 1D2D Central Processing Unit (CPU) model is combined with an HPC technique for fast and accurate flood simulation. Due to the speed of 1D schemes, a hybrid CPU/GPU model that runs the 1D main channel on CPU and accelerates the 2D floodplain with a Graphics Processing Unit (GPU) is presented. Since the data transfer between sub-domains and devices (CPU/GPU) may be the main potential drawback of this architecture, the test cases are selected to carry out a careful time analysis. The results reveal the speed-up dependency on the 2D mesh, the event to be solved and the 1D discretization of the main channel. Additionally, special attention must be paid to the time step size computation shared between sub-models. In spite of the use of a hybrid CPU/GPU implementation, high speed-ups are accomplished in some cases.|Journal of Hydroinformatics|2020|10.2166/hydro.2020.032|M. Morales-Hernández, P. Brufau, P. García-Navarro, I. Echeverribar|1.6|2
676|Inclusion of Building‐Resolving Capabilities Into the FastEddy® GPU‐LES Model Using an Immersed Body Force Method|As a first step toward achieving full physics urban weather simulation capabilities within the resident‐GPU large‐eddy simulation (LES) FastEddy® model, we have implemented and verified/validated a method for explicit representation of building effects. Herein, we extend the immersed body force method (IBFM) from Chan and Leach (2007, https://doi.org/10.1175/2006JAMC1321.1) to (i) be scale independent and (ii) control building surface temperatures. Through a specific drag‐like term in the momentum equations, the IBFM is able to enforce essentially zero velocities within the buildings, in turn resulting in a no‐slip boundary condition at the building walls. In addition, we propose similar forcing terms in the energy and mass conservation equations that allow an accurate prescription of the building temperature. The extended IBFM is computationally efficient and has the potential to be coupled to building energy models. The IBFM exhibits excellent agreement with laboratory experiments of an array of staggered cubes at a grid spacing of Δ=1  mm, demonstrating the applicability of the method beyond the atmospheric scale. In addition, the IBFM is validated at atmospheric scale through simulations of downtown Oklahoma City ( Δ=2  m) using data collected during the Joint Urban 2003 (JU03) field campaign. Our LES IBFM results for mean wind speed, turbulence kinetic energy, and SF6 transport and dispersion compare well to observations and produce turbulence spectra that are in good agreement with sonic anemometer data. Statistical performance metrics for the JU03 simulations are within the range of other LES models in the literature.|Journal of Advances in Modeling Earth Systems|2020|10.1029/2020MS002141|D. Muñoz‐Esparza, Scott Meech, J. Pinto, B. Kosović, C. García-Sánchez, R. Sharman, H. Shin, J. Knievel, S. Swerdlin, M. Steiner, J. Sauer|1.6|2
309|A Two Teraflop Swarm|We introduce the Xpuck swarm, a research platform with an aggregate raw processing power in excess of two teraflops. The swarm uses 16 e-puck robots augmented with custom hardware that uses the substantial CPU and GPU processing power available from modern mobile system-on-chip devices. The augmented robots, called Xpucks, have at least an order of magnitude greater performance than previous swarm robotics platforms. The platform enables new experiments that require high individual robot computation and multiple robots. Uses include online evolution or learning of swarm controllers, simulation for answering what-if questions about possible actions, distributed super-computing for mobile platforms, and real-world applications of swarm robotics that requires image processing, or SLAM. The teraflop swarm could also be used to explore swarming in nature by providing platforms with similar computational power as simple insects. We demonstrate the computational capability of the swarm by implementing a fast physics-based robot simulator and using this within a distributed island model evolutionary system, all hosted on the Xpucks.|Frontiers in Robotics and AI|2018|10.3389/frobt.2018.00011|S. Hauert, Simon Jones, A. Winfield, M. Studley|1.5714285714285714|2
1254|Graphics processing units-accelerated adaptive nonlocal means filter for denoising three-dimensional Monte Carlo photon transport simulations|Abstract. The Monte Carlo (MC) method is widely recognized as the gold standard for modeling light propagation inside turbid media. Due to the stochastic nature of this method, MC simulations suffer from inherent stochastic noise. Launching large numbers of photons can reduce noise but results in significantly greater computation times, even with graphics processing units (GPU)-based acceleration. We develop a GPU-accelerated adaptive nonlocal means (ANLM) filter to denoise MC simulation outputs. This filter can effectively suppress the spatially varying stochastic noise present in low-photon MC simulations and improve the image signal-to-noise ratio (SNR) by over 5 dB. This is equivalent to the SNR improvement of running nearly 3.5  ×   more photons. We validate this denoising approach using both homogeneous and heterogeneous domains at various photon counts. The ability to preserve rapid optical fluence changes is also demonstrated using domains with inclusions. We demonstrate that this GPU-ANLM filter can shorten simulation runtimes in most photon counts and domain settings even combined with our highly accelerated GPU MC simulations. We also compare this GPU-ANLM filter with the CPU version and report a threefold to fourfold speedup. The developed GPU-ANLM filter not only can enhance three-dimensional MC photon simulation results but also be a valuable tool for noise reduction in other volumetric images such as MRI and CT scans.|Journal of Biomedical Optics|2018|10.1117/1.JBO.23.12.121618|Yaoshen Yuan, Leiming Yu, Q. Fang, Zafer Doğan|1.5714285714285714|2
1377|Hardware-software model co-simulation for GPU IP development|"This Master's thesis project aims to explore the possibility of a mixed simulation \nenvironment in which parts of a software model for emulating a hardware design \nmay be swapped with their corresponding RTL description. More specifically, this \nwork focuses on the sofware model for Arm's next-generation Mali GPU, which \nis used to understand system on chip properties, including functionality and performance. A component of this model (written in C++) is substituted with its \nhardware model (written in SystemVerilog) to be able to run simulations in a system \ncontext at a faster emulation speed, and with higher accuracy in the results \ncompared to a pure-software model execution. For this, a ""co-simulation"" environment is developed, using SystemVerilog's DPI-C as the main communication \ninterface between C++ and SystemVerilog. The proposed environment contains \nnew software and hardware blocks to enable the desired objective without major \nmodifications in neither the software Mali model nor the substituted component. \nMetrics and results for characterizing this co-simulation environment are also provided, namely timing accuracy, data correctness and simulation time with respect \nto other previously available simulation options. These results hope to show that \nthe proposed environment may open new use-cases and improve development and \nverification time of hardware components in a system such as the Mali GPU."||2018|10.1016/j.microrel.2018.07.007|Jaime Gancedo Rodrigo|1.5714285714285714|2
346|MPI-ACC: Accelerator-Aware MPI for Scientific Applications|Data movement in high-performance computing systems accelerated by graphics processing units (GPUs) remains a challenging problem. Data communication in popular parallel programming models, such as the Message Passing Interface (MPI), is currently limited to the data stored in the CPU memory space. Auxiliary memory systems, such as GPU memory, are not integrated into such data movement standards, thus providing applications with no direct mechanism to perform end-to-end data movement. We introduce MPI-ACC, an integrated and extensible framework that allows end-to-end data movement in accelerator-based systems. MPI-ACC provides productivity and performance benefits by integrating support for auxiliary memory spaces into MPI. MPI-ACC supports data transfer among CUDA, OpenCL and CPU memory spaces and is extensible to other offload models as well. MPI-ACC's runtime system enables several key optimizations, including pipelining of data transfers, scalable memory management techniques, and balancing of communication based on accelerator and node architecture. MPI-ACC is designed to work concurrently with other GPU workloads with minimum contention. We describe how MPI-ACC can be used to design new communication-computation patterns in scientific applications from domains such as epidemiology simulation and seismology modeling, and we discuss the lessons learned. We present experimental results on a state-of-the-art cluster with hundreds of GPUs; and we compare the performance and productivity of MPI-ACC with MVAPICH, a popular CUDA-aware MPI solution. MPI-ACC encourages programmers to explore novel application-specific optimizations for improved overall cluster utilization.|IEEE Transactions on Parallel and Distributed Systems|2016|10.1109/TPDS.2015.2446479|K. Murthy, Xiaosong Ma, K. Bisset, James Dinan, Wu-chun Feng, R. Thakur, J. Mellor-Crummey, Feng Ji, Milind Chabbi, Lokendra S. Panwar, P. Balaji, Ashwin M. Aji|1.5555555555555556|2
1092|Parallel Hyperspectral Coded Aperture for Compressive Sensing on GPUs|The application of compressive sensing (CS) to hyperspectral images is an active area of research over the past few years, both in terms of the hardware and the signal processing algorithms. However, CS algorithms can be computationally very expensive due to the extremely large volumes of data collected by imaging spectrometers, a fact that compromises their use in applications under real-time constraints. This paper proposes four efficient implementations of hyperspectral coded aperture (HYCA) for CS, two of them termed P-HYCA and P-HYCA-FAST and two additional implementations for its constrained version (CHYCA), termed P-CHYCA and P-CHYCA-FAST on commodity graphics processing units (GPUs). HYCA algorithm exploits the high correlation existing among the spectral bands of the hyperspectral data sets and the generally low number of endmembers needed to explain the data, which largely reduces the number of measurements necessary to correctly reconstruct the original data. The proposed P-HYCA and P-CHYCA implementations have been developed using the compute unified device architecture (CUDA) and the cuFFT library. Moreover, this library has been replaced by a fast iterative method in the P-HYCA-FAST and P-CHYCA-FAST implementations that leads to very significant speedup factors in order to achieve real-time requirements. The proposed algorithms are evaluated not only in terms of reconstruction error for different compressions ratios but also in terms of computational performance using two different GPU architectures by NVIDIA: 1) GeForce GTX 590; and 2) GeForce GTX TITAN. Experiments are conducted using both simulated and real data revealing considerable acceleration factors and obtaining good results in the task of compressing remotely sensed hyperspectral data sets.|IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing|2016|10.1109/JSTARS.2015.2436440|G. Martín, Vítor Silva, J. Bioucas-Dias, S. Bernabé, Antonio J. Plaza, J. Nascimento|1.5555555555555556|2
1164|Distributed multi-scale muscle simulation in a hybrid MPI–CUDA computational environment|We present Mexie, an extensible and scalable software solution for distributed multi-scale muscle simulations in a hybrid MPI–CUDA environment. Since muscle contraction relies on the integration of physical and biochemical properties across multiple length and time scales, these models are highly processor and memory intensive. Existing parallelization efforts for accelerating multi-scale muscle simulations imply the usage of expensive large-scale computational resources, which produces overwhelming costs for the everyday practical application of such models. In order to improve the computational speed within a reasonable budget, we introduce the concept of distributed calculations of multi-scale muscle models in a mixed CPU–GPU environment. The concept is applied to a two-scale muscle model, in which a finite element macro model is coupled with the microscopic Huxley kinetics model. Finite element calculations of a continuum macroscopic model take place strictly on the CPU, while numerical solutions of the partial differential equations of Huxley’s cross-bridge kinetics are calculated on both CPUs and GPUs. We present a modular architecture of the solution, along with an internal organization and a specific load balancer that is aware of memory boundaries in such a heterogeneous environment. Solution was verified on both benchmark and real-world examples, showing high utilization of involved processing units, ensuring high scalability. Speed-up results show a boost of two orders of magnitude over any previously reported distributed multi-scale muscle models. This major improvement in computational feasibility of multi-scale muscle models paves the way for new discoveries in the field of muscle modeling and future clinical applications.|International Conference on Advances in System Simulation|2016|10.1177/0037549715620299|M. Ivanovic, A. Kaplarevic-Malisic, R. Gilbert, S. Mijailovich, B. Stojanovic|1.5555555555555556|2
298|Remote-scope promotion: clarified, rectified, and verified|Modern accelerator programming frameworks, such as OpenCL, organise threads into work-groups. Remote-scope promotion (RSP) is a language extension recently proposed by AMD researchers that is designed to enable applications, for the first time, both to optimise for the common case of intra-work-group communication (using memory scopes to provide consistency only within a work-group) and to allow occasional inter-work-group communication (as required, for instance, to support the popular load-balancing idiom of work stealing). We present the first formal, axiomatic memory model of OpenCL extended with RSP. We have extended the Herd memory model simulator with support for OpenCL kernels that exploit RSP, and used it to discover bugs in several litmus tests and a work-stealing queue, that have been used previously in the study of RSP. We have also formalised the proposed GPU implementation of RSP. The formalisation process allowed us to identify bugs in the description of RSP that could result in well-synchronised programs experiencing memory inconsistencies. We present and prove sound a new implementation of RSP that incorporates bug fixes and requires less non-standard hardware than the original implementation. This work, a collaboration between academia and industry, clearly demonstrates how, when designing hardware support for a new concurrent language feature, the early application of formal tools and techniques can help to prevent errors, such as those we have found, from making it into silicon.|Conference on Object-Oriented Programming Systems, Languages, and Applications|2015|10.1145/2814270.2814283|Mark Batty, A. Donaldson, John Wickerson, Bradford M. Beckmann|1.5|2
301|Towards scalable adaptive mesh refinement on future parallel architectures|In the march towards exascale, supercomputer architectures are undergoing a significant change. Limited by power consumption and heat dissipation, future supercomputers are likely to be built around a lower-power many-core model. This shift in supercomputer design will require sweeping code changes in order to take advantage of the highly-parallel architectures. Evolving or rewriting legacy applications to perform well on these machines is a significant challenge. \n \nMini-applications, small computer programs that represent the performance characteristics of some larger application, can be used to investigate new programming models and improve the performance of the legacy application by proxy. These applications, being both easy to modify and representative, are essential for establishing a path to move legacy applications into the exascale era. \n \nThe focus of the work presented in this thesis is the design, development and employment of a new mini-application, CleverLeaf, for shock hydro- dynamics with block-structured adaptive mesh refinement (AMR). We report on the development of CleverLeaf, and show how the fresh start provided by a mini-application can be used to develop an application that is flexible, accurate, and easy to employ in the investigation of exascale architectures. \n \nWe also detail the development of the first reported resident parallel block-structured AMR library for Graphics Processing Units (GPUs). Extending the SAMRAI library using the CUDA programming model, we develop datatypes that store data only in GPU memory, as well the necessary operators for moving and interpolating data on an adaptive mesh. We show that executing AMR simulations on a GPU is up to 4.8⇥ faster than a CPU, and demonstrate scalability on over 4,000 nodes using a combination of CUDA and MPI. \n \nFinally, we show how mini-applications can be employed to improve the performance of production applications on existing parallel architectures by selecting the optimal application configuration. Using CleverLeaf, we identify the most appropriate configurations on three contemporary supercomputer architectures. Selecting the best parameters for our application can reduce run-time by up to 82% and reduce memory usage by up to 32%.||2015|10.1109/icpp.2015.15|D. Beckingsale|1.5|2
426|GPU accelerated ray launching for high-fidelity virtual test drives of VANET applications|Due to the complexity of Vehicular Ad Hoc Networks, future driving assistance systems need to be validated through virtual test drives in a simulated environment. An accurate modeling of the vehicle-to-vehicle communication channel is crucial to enable a precise evaluation of such network-aware applications. Since existing ray-based methods cause long computation times, a new parallel GPU-based ray-launching simulation method is presented. The algorithmic improvements allow a high utilization of the GPU computing power, which results in significantly faster simulations while achieving high accuracy. The validation of the simulation results against real-world measurements showed a high level of agreement.|International Symposium on High Performance Computing Systems and Applications|2015|10.1109/HPCSim.2015.7237048|M. Schiller, A. Knoll, T. Eibert, M. Mocker|1.5|2
460|Swing up Control of Inverted Pendulum on a Cart with Collision by Monte Carlo Model Predictive Control|Monte Carlo Model Predictive Control (MCMPC) is a kind of sample-based MPC methods, which does not require gradient information of cost function. This feature allows us to apply it to systems with non differentiable cost function or discontinuous event taking full advantage of recent parallel computing such as GPU. In this paper, we consider the problem of swing-up stabilization of a cart type inverted pendulum focusing on this feature of MCMPC. The first application is the problem considering the unwinding phenomenon. By applying MCMPC, it is possible to avoid the unwinding phenomenon by directly implementing the feature of the rotation group. The resultant controller thereby is inherently discontinuous. The second application is swing-up stabilization with a model considering collision of the cart with walls. In this case, thanks to the advantage of MCMPC being capable of handling discontinuous events, swinging up can speed up by exploiting the energy by the collision. these are verified by simulations and experiment.|Annual Conference of the Society of Instrument and Control Engineers of Japan|2019|10.23919/SICE.2019.8859912|Shintaro Nakatani, H. Date|1.5|2
689|SeisSol on Distributed Multi-GPU Systems: CUDA Code Generation for the Modal Discontinuous Galerkin Method|We present a GPU implementation of the high order Discontinuous Galerkin (DG) scheme in SeisSol, a software package for simulating seismic waves and earthquake dynamics. Our particular focus is on providing a performance portable solution for heterogeneous distributed multi-GPU systems. We therefore redesigned SeisSol’s code generation cascade for GPU programming models. This includes CUDA source code generation for the performance-critical small batched matrix multiplications kernels. The parallelisation extends the existing MPI+X scheme and supports SeisSol’s cluster-wise Local Time Stepping (LTS) algorithm for ADER time integration. We performed a Roofline model analysis to ensure that the generated batched matrix operations achieve the performance limits posed by the memory-bandwidth roofline. Our results also demonstrate that the generated GPU kernels outperform the corresponding cuBLAS subroutines by 2.5 times on average. We present strong and weak scaling studies of our implementation on the Marconi100 supercomputer (with 4 Nvidia Volta V100 GPUs per node) on up to 256 GPUs , which revealed good parallel performance and efficiency in case of time integration using global time stepping. However, we show that directly mapping the LTS method from CPUs to distributed GPU environments results in lower hardware utilization. Nevertheless, due to the algorithmic advantages of local time stepping, the method still reduces time-to-solution by a factor of 1.3 on average in contrast to the GTS scheme.|International Conference on High Performance Computing in Asia-Pacific Region|2021|10.1145/3432261.3436753|M. Bader, Ravil Dorozhinskii|1.5|2
735|Fast Numerical Simulation of Allen-Cahn Equation|Simulation speed depends on code structures. Hence, it is crucial how to build a fast algorithm. We solve the Allen–Cahn equation by an explicit finite difference method, so it requires grid calculations implemented by many for-loops in the simulation code. In terms of programming, many for-loops make the simulation speed slow. We propose a model architecture containing a pad and a convolution operation on the Allen–Cahn equation for fast computation while maintaining accuracy. Also, the GPU operation is used to boost up the speed more. In this way, the simulation of other differential equations can be improved. In this paper, various numerical simulations are conducted to confirm that the Allen–Cahn equation follows motion by mean curvature and phase separation in two-dimensional and three-dimensional spaces. Finally, we demonstrate that our algorithm is much faster than an unoptimized code and the CPU operation.|Mathematical Problems in Engineering|2021|10.1155/2021/5263989|Yongho Kim, Yongho Choi|1.5|2
747|Megaverse: Simulating Embodied Agents at One Million Experiences per Second|We present Megaverse, a new 3D simulation platform for reinforcement learning and embodied AI research. The efficient design of our engine enables physics-based simulation with high-dimensional egocentric observations at more than 1,000,000 actions per second on a single 8-GPU node. Megaverse is up to 70x faster than DeepMind Lab in fully-shaded 3D scenes with interactive objects. We achieve this high simulation performance by leveraging batched simulation, thereby taking full advantage of the massive parallelism of modern GPUs. We use Megaverse to build a new benchmark that consists of several single-agent and multi-agent tasks covering a variety of cognitive challenges. We evaluate model-free RL on this benchmark to provide baselines and facilitate future research. The source code is available at https://www.megaverse.info|International Conference on Machine Learning|2021|10.1109/cog52621.2021.9619096|Brennan Shacklett, Aleksei Petrenko, V. Koltun, Erik Wijmans|1.5|2
749|FAME: Fast Algorithms for Maxwell's Equations for Three-dimensional Photonic Crystals|In this article, we propose the Fast Algorithms for Maxwell’s Equations (FAME) package for solving Maxwell’s equations for modeling three-dimensional photonic crystals. FAME combines the null-space free method with fast Fourier transform (FFT)-based matrix-vector multiplications to solve the generalized eigenvalue problems (GEPs) arising from Yee’s discretization. The GEPs are transformed into a null-space free standard eigenvalue problem with a Hermitian positive-definite coefficient matrix. The computation times for FFTbased matrix-vector multiplications with matrices of dimension 7 million are only 0.33 and 3.6 × 10−3 seconds using MATLAB with an Intel Xeon CPU and CUDA C++ programming with a single NVIDIA Tesla P100 GPU, respectively. Such multiplications significantly reduce the computational costs of the conjugate gradient method for solving linear systems. We successfully use FAME on a single P100 GPU to solve a set of GEPs with matrices of dimension more than 19 million, in 127 to 191 seconds per problem. These results demonstrate the potential of our proposed package to enable large-scale numerical simulations for novel physical discoveries and engineering applications of photonic crystals.|ACM Transactions on Mathematical Software|2021|10.1145/3446329|Wen-Wei Lin, Sheng Wang, Jia-Wei Lin, Xing-Long Lyu, Tsung-Ming Huang, Tie-xiang Li|1.5|2
756|Device-Level Parallel-in-Time Simulation of MMC-Based Energy System for Electric Vehicles|The device-level electromagnetic transient (EMT) simulation with the nonlinear behaviour model (NBM) of insulated-gate bipolar transistors (IGBTs) and diodes can provide an accurate insight into the power converters from the perspective of thermal performance and energy efficiency. However, device-level simulation is rarely implemented in electric vehicles (EVs) due to its extreme computation complexity natively introduced by the device models. To solve this problem, an interpolation strategy is designed based on the parallel-in-time algorithm for the device-level simulation of the modular multilevel converter (MMC) connected with the induction machine in EV applications. The MMC is mathematically separated as multiple submodules with the same attributes, which can be processed in a parallel manner in the graphics processing unit (GPU). By implementing the device-level simulation in the different time-step in GPU, the interpolation strategy provides the precise initial values for the nonlinear solution process iteratively. The accuracy of the proposed simulation scheme is validated by commercial simulation tools at the device level. In addition, the system-level simulation of EVs is carried out at different driving cycles, and the results demonstrate a significant reduction in simulation time.|IEEE Transactions on Vehicular Technology|2021|10.1109/TVT.2021.3081534|Ning Lin, C. Lyu, V. Dinavahi|1.5|2
783|Performance Analysis of CP2K Code for Ab Initio Molecular Dynamics|Using a realistic molecular catalyst system, we conduct scaling studies of ab initio molecular dynamics simulations using the popular CP2K code on both Intel Xeon CPU and NVIDIA V100 GPU architectures. Additional performance improvements were gained by finding more optimal process placement and affinity settings. Statistical methods were employed to understand performance changes in spite of the variability in runtime for each molecular dynamics timestep. Ideal conditions for CPU runs were found when running at least four MPI ranks per node, bound evenly across each socket. This study also showed that fully utilizing processing cores, with one OpenMP thread per core, performed better than when reserving cores for the system. The CPU-only simulations scaled at 70% or more of the ideal scaling up to 10 compute nodes, after which the returns began to diminish more quickly. Simulations on a single 40-core node with two NVIDIA V100 GPUs for acceleration achieved over 3.7× speedup compared to the fastest single 36-core node CPU-only version. These same GPU runs showed a 13% speedup over the fastest time achieved across five CPU-only nodes.|Journal of Chemical Information and Modeling|2021|10.1021/acs.jcim.1c01538|Ying Wai Li, R. Robey, Dewi Yokelson, N. Tkachenko, Pavel A. Dub|1.5|2
844|Batched Sparse Iterative Solvers for Computational Chemistry Simulations on GPUs|This paper presents batched iterative solvers for GPU architectures. We elaborate on the design of the batched functionality aiming for optimal performance while still giving the user some flexibility in terms of choosing a sparse matrix format, a preconditioner optimized for the distinct items of the batch, and an application-specific stopping criterion that is evaluated for each problem in the batch, individually. Performance results for benchmark problems coming from PeleLM simulations reveal the potential of the batched iterative solvers for computational chemistry simulations, and their advantage compared to the current vendor-provided batched solutions.|ACM SIGPLAN Symposium on Scala|2021|10.1109/ScalA54577.2021.00010|Isha Aggarwal, C. Woodward, Pratik Nayak, Aditya Kashi, H. Anzt, C. Balos|1.5|2
987|Accelerated SPIRiT Parallel MR Image Reconstruction Based on Joint Sparsity and Sparsifying Transform Learning|Iterative self-consistent parallel imaging reconstruction (SPIRiT) was an autocalibrating model for parallel magnetic resonance imaging reconstruction, which is often formulated as a SPIRiT reconstruction problem with some regularization terms. Some methods based on the operator splitting and alternating direction method of multipliers (ADMM) have been employed to solve the formulated regularized SPIRiT problem. In this paper, we propose to combine the sparsifying transform learning and joint sparsity with Cartesian SPIRiT parallel magnetic resonance imaging, and solve the resulting reconstruction problem by using the variable splitting and ADMM techniques. Simulation experiments on four in vivo data sets demonstrate that the proposed algorithm achieves a better image reconstruction quality than that of other competing methods. In addition, the proposed algorithm is very suitable for graphics processing unit (GPU) parallel computing, and its accelerated version, achieved by using a GPU, is very fast, requiring only 6.7 s to reconstruct a 200 × 200 pixel image with 8 channels.|IEEE Transactions on Computational Imaging|2023|10.1109/TCI.2023.3252260|Y. Liu, Jizhong Duan, Junfeng Wang|1.5|2
1011|Massively Distributed Finite-Volume Flux Computation|Designing large-scale geological carbon capture and storage projects and ensuring safe long-term CO2 containment – as a climate change mitigation strategy – requires fast and accurate numerical simulations. These simulations involve solving complex PDEs governing subsurface fluid flow using implicit finite-volume schemes widely based on Two-Point Flux Approximation (TPFA). This task is computationally and memory expensive, especially when performed on highly detailed geomodels. In most current HPC architectures, memory hierarchy and data management mechanisms are insufficient to overcome the challenges of large scale numerical simulations. Therefore, it is crucial to design algorithms that can exploit alternative and more balanced paradigms, such as dataflow and in-memory computing. This work introduces an algorithm for TPFA computations that exploits a dataflow architecture, such as Cerebras CS-2, which helps to significantly minimize memory bottlenecks. Our implementation achieves two orders of magnitude speedup compared to multiple reference implementations running on latest generations of NVIDIA GPUs.|SC Workshops|2023|10.1145/3624062.3624252|François P. Hamon, M. Araya-Polo, M. Jacquelin, R. Settgast, R. Sai|1.5|2
1060|Fast Compressed Sensing of 3D Radial T1 Mapping with Different Sparse and Low-Rank Models|Knowledge of the relative performance of the well-known sparse and low-rank compressed sensing models with 3D radial quantitative magnetic resonance imaging acquisitions is limited. We use 3D radial T1 relaxation time mapping data to compare the total variation, low-rank, and Huber penalty function approaches to regularization to provide insights into the relative performance of these image reconstruction models. Simulation and ex vivo specimen data were used to determine the best compressed sensing model as measured by normalized root mean squared error and structural similarity index. The large-scale compressed sensing models were solved by combining a GPU implementation of a preconditioned primal-dual proximal splitting algorithm to provide high-quality T1 maps within a feasible computation time. The model combining spatial total variation and locally low-rank regularization yielded the best performance, followed closely by the model combining spatial and contrast dimension total variation. Computation times ranged from 2 to 113 min, with the low-rank approaches taking the most time. The differences between the compressed sensing models are not necessarily large, but the overall performance is heavily dependent on the imaged object.|Journal of Imaging|2023|10.3390/jimaging9080151|V. Kolehmainen, N. Hänninen, Antti Paajanen, O. Nykänen, M. Nissi, Matti Hanhela|1.5|2
1068|Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach|The eikonal equation has become an indispensable tool for modeling cardiac electrical activation accurately and efficiently. In principle, by matching clinically recorded and eikonal-based electrocardiograms (ECGs), it is possible to build patient-specific models of cardiac electrophysiology in a purely non-invasive manner. Nonetheless, the fitting procedure remains a challenging task. The present study introduces a novel method, Geodesic- BP, to solve the inverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machine learning frameworks, allowing us to optimize the parameters of the eikonal equation to reproduce a given ECG. We show that Geodesic-BP can reconstruct a simulated cardiac activation with high accuracy in a synthetic test case, even in the presence of modeling inaccuracies. Furthermore, we apply our algorithm to a publicly available dataset of a biventricular rabbit model, with promising results. Given the future shift towards personalized medicine, Geodesic-BP has the potential to help in future functionalizations of cardiac models meeting clinical time constraints while maintaining the physiological accuracy of state-ofthe- art cardiac models.|IEEE transactions on bio-medical engineering|2023|10.1109/TBME.2023.3331876|Simone Pezzuto, G. Haase, Jan Verhulsdonk, Alexander Effland, T. Grandits|1.5|2
1080|SEECHIP: A Scalable and Energy-Efficient Chiplet-based GPU Architecture Using Photonic Links|The continuous increase in GPU performance benefits a wide range of high-performance computing (HPC) applications. Slower growth of transistor density and limited size of chip die are now posing significant challenges to scale GPUs. The chiplet technology provides a potential solution to surpass these limitations. However, the performance of these chiplet-based GPUs is often constrained by the metallic-based interconnects between the chiplets. Emerging technologies such as photonic interconnect can overcome the limitations of metallic interconnects, offering several superior properties, such as high bandwidth density and low energy consumption. In this paper, we propose SEECHIP: a Scalable and Energy-Efficient CHIPlet-based GPU architecture using photonic links. SEECHIP introduces a novel photonic inter-chiplet network that supports both unicast and broadcast communication, providing the same transmission bandwidth at both the sending and receiving ends. In addition, we propose a tailored hierarchical memory architecture, which is more suitable for the parallelization of general-purpose HPC applications. Simulation results using 14 benchmarks show that SEECHIP can achieve and reduction in execution time and energy consumption, respectively, as compared to other GPUs with metallic or photonic interconnects. Simulation results also show that SEECHIP has good scalability compared with the other GPUs.|International Conference on Parallel Processing|2023|10.1145/3605573.3605626|Fei Dai, H. Zhang, Yawen Chen, Haibo Zhang, Zhiyi Huang|1.5|2
1096|A low-cost unity-based virtual training simulator for laparoscopic partial nephrectomy using HTC Vive|Laparoscopic education and surgery assessments increase the success rates and lower the risks during actual surgeries. Hospital residents need a secure setting, and trainees require a safe and controlled environment with cost-effective resources where they may hone their laparoscopic abilities. Thus, we have modeled and developed a surgical simulator to provide the initial training in Laparoscopic Partial Nephrectomy (LPN—a procedure to treat kidney cancer or renal masses). To achieve this, we created a virtual simulator using an open-source game engine that can be used with a commercially available, reasonably priced virtual reality (VR) device providing visual and haptic feedback. In this study, the proposed simulator’s design is presented, costs are contrasted, and the simulator’s performance is assessed using face and content validity measures. CPU- and GPU-based computers can run the novel simulation with a soft body deformation based on simplex meshes. With a reasonable trade-off between price and performance, the HTC Vive’s controlled soft body effect, physics-based deformation, and haptic rendering offer the advantages of an excellent surgical simulator. The trials show that the medical volunteers who performed the initial LPN procedures for newbie surgeons received positive feedback.|PeerJ Computer Science|2023|10.7717/peerj-cs.1627|Fareeha Rasheed, Waheed Iqbal, Faisal Bukhari, Hafiza Ayesha Hoor Chaudhry, Muhammad Asif|1.5|2
1125|Direct molecular gas dynamics simulations of re-entry vehicles via the Boltzmann equation|This work explores the feasibility of performing three-dimensional molecular gas dynamics simulations of hypersonic flows such as re-entry vehicles through directly solving the six-dimensional nonlinear Boltzmann equation closed with the BGK (Bhatnagar-Gross-Krook) collision model. Through the combination of high-order unstructured spatial discretizations and conservative discrete velocity models as well as their efficient implementation on large-scale GPU computing architectures, we demonstrate the ability to simulate unsteady and non-equilibrium three-dimensional high-speed flows at a feasible computational cost through a unified numerical framework. We present the results of high-order simulations of the Apollo capsule at realistic re-entry conditions from the AS-202 mission flight path, including the steady non-equilibrium flow in the high-altitude regime at a Mach number of 22.7 and a Reynolds number of 43,000 as well as the unsteady turbulent flow in the low-altitude regime at a Mach number of 8 and a Reynolds number of 550,000. The results show the validity of the approach over the entire range of a typical re-entry trajectory from the rarefied to the continuum limit, the ability to directly resolve strong shocks profiles without numerical shock capturing techniques, and the ability of resolving small-scale unsteady flow structures in the inertial range.||2023|10.1016/j.jcp.2023.112146|T. Dzanic, Luigi Martinelli|1.5|2
1328|Parallel Methods For Classical and Disordered Spin Models|This book presents two novel parallel techniques employed for the study of both classical and disordered spin models. The first technique is a new parallel multi-core and multi-CPU algorithm for the computation of the partition function of a strip lattice in the Potts model. The second technique is an adaptive parallel GPU exchange Monte Carlo algorithm for the simulation of 3D lattices in the random field Ising model. Both techniques can provide substantial speedup over the state of the art methods available and contribute to the research and understanding of such fascinating interaction models.||2017|10.1007/698_2017_179|Navarro Guerrero, C. Alejandro|1.5|2
1374|Numerical Simulations of Gaseous Detonation Propagation Using Different Supercomputing Architechtures|The aim of the present study is to calculate the process of detonation combustion of gas mixtures in engines. Development and verification of 3D transient mathematical model of chemically reacting gas mixture flows incorporating hydrogen was performed. Development of a computational model based on the mathematical one for parallel computing on supercomputers incorporating CPU and GPU units was carried out. Investigation of the influence of computational grid size on simulation precision and computational speed was performed. Investigation of calculation runtime acceleration was carried out subject to variable number of parallel threads on different architectures and implying different strategies of parallel computation.||2017|10.1142/S0219876217500384|L. Stamov, V. Nikitin, N. Smirnov, V. V. Tyrenkova, V. A. Nerchenko|1.5|2
1450|Development of Computational Tools for Computerized Training of Cryosurgery|Cryosurgery is the destruction of undesired tissue by freezing. Modern cryosurgery is performed by strategically inserting a number of cooling probes (cryoprobes) shaped like long hypodermic needles into the target region. Minimally invasive cryosurgery presents unique challenges to the clinician associated with the use of ultrasound (US) as the imaging modality, the selection of the cryoprobe configuration, the accurate placement of the cryoprobes, and the monitoring of the procedure through imaging and temperature measurements. Currently the cost of training clinicians to perform cryosurgery is exceptionally high because the procedure itself is very complex, and the learning process consists of a long residency period. The work presented in this thesis is part of an ongoing project between the Biothermal Technology Laboratory and the Computational Engineering and Robotics Laboratory at Carnegie Mellon University, to develop a computerized training platform that will teach clinicians how to perform the minimally invasive cryosurgery effectively and efficiently. This work uses prostate cryosurgery as a developmental model for verification and benchmarking of all algorithms. Towards that goal, this work provides many of the tools needed to create a virtual training environment. A cryosurgical training framework was presented to enable the diverse functionality of such a computerized training system. Next, a method for generating synthetic ultrasound images from a small library of samples was presented. This method is based off of a texture synthesis technique called “Image Analogies”. The algorithm generates realistic 3D ultrasound images from a small library of samples. Then, an improvement was made to an efficient bioheat transfer simulator. The implementation used GPU computing to achieve a 15x increase in performance over the previous state of the art approach. The enhanced bioheat simulator was leveraged in a cryosurgically relevant ultrasound simulator. The ultrasound simulator uses nonlinear ray-tracing and a novel energy propagation step to simulate imaging artifacts associated with cryoprobe insertion and the thermal field in real-time. Finally, an enhancement to the cryoprobe placement algorithm “bubble packing” was performed to enable the addition of geometric constraints. These geometric constraints allow more clinically relevant plans to be generated. It is the hope of the author that these tools will be the foundation of a fully immersive surgical simulator.||2015|10.1177/1533034615580694|R. Keelan|1.5|2
1603|Micromagnetic Simulations of Magnetic Thin Films using MuMax3|The field of spintronics uses the spin property of the electron, in addition to to its charge to define new phenomena in magnetism and electronics. It has received increased attention in the last years due to the numerous applications that it inspires, as well as the small scale (nanometre range) of said applications. A powerful tool developed in the last two decades, micromagnetic simulations can now shed light into the intricacies of magnetic textures that emerge in or at the interface between various material systems. The present work makes use of the GPU-accelerated micromagnetic simulation software MuMax3 to investigate the magnetic textures and non-trivial magnetization response of two selected systems. Firstly, the phenomenon of exchange bias is modeled and the antiferromagnetic phase is implemented in the context of the chosen simulation software. The material system considered is Co/CoO. The model takes the material and geometrical parameters as inputs and confirms the emergent shift in the hysteresis loop, with a modest exchange bias field of Hbias = 0.0149 T. As a second application, the behaviour of a magnetic material with strong anisotropy, as well as a strong temperature dependence of the anisotropy constants is investigated. The temperature dependence of the magnetization is modelled and the Curie temperature of the sample is identified at TC = 165K. The model is successful in exhibiting the strong anisotropy present in the sample by performing M-H simulations for three different temperatures T ∈ {0 K, 10 K, 50 K} and implicitly, three anisotropy regimes. The coercivity, as well as the squareness of the hysteresis loop are in close agreement with the expected values. The simulation also proves the switching of the easy and hard directions of magnetization at a temperature of 50 K.||2019|10.1016/j.jcrysgro.2019.125207|Mihai Andrei Frantiu|1.5|2
1609|The Actuator Line Model in Lattice Boltzmann Frameworks: Numerical Sensitivity and Computational Performance|The growing use of large-eddy simulations for the modelling of wind farms makes the need for efficient numerical frameworks more essential than ever. GPU-accelerated implementations of the Lattice Boltzmann Method (LBM) have shown to provide significant performance gains over classical Navier-Stokes-based computational fluid dynamics. Yet, their use in the field of wind energy remains limited to date. In this fundamental study the cumulant LBM is scrutinised for actuator line simulations of wind turbines. The numerical sensitivity of the method in a simple uniform inflow is investigated with respect to spatial and temporal resolution as well as the width of the actuator line’s regularisation kernel. Comparable accuracy and slightly better stability properties are shown in relation to a standard Navier-Stokes implementation. The results indicate the overall suitability of the cumulant LBM for wind turbine wake simulations. The potential of the LBM for future wind energy applications is clarified by means of a brief comparison of computational performance.|Journal of Physics: Conference Series|2019|10.1088/1742-6596/1256/1/012022|S. Ivanell, K. Nilsson, H. Asmuth, H. Olivares-Espinosa|1.5|2
285|Radiative Heat Transfer Calculation on 16384 GPUs Using a Reverse Monte Carlo Ray Tracing Approach with Adaptive Mesh Refinement|Modeling thermal radiation is computationally challenging in parallel due to its all-to-all physical and resulting computational connectivity, and is also the dominant mode of heat transfer in practical applications such as next-generation clean coal boilers, being modeled by the Uintah framework. However, a direct all-to-all treatment of radiation is prohibitively expensive on large computers systems whether homogeneous or heterogeneous. DOE Titan and the planned DOE Summit and Sierra machines are examples of current and emerging GPU-based heterogeneous systems where the increased processing capability of GPUs over CPUs exacerbates this problem. These systems require that computational frameworks like Uintah leverage an arbitrary number of on-node GPUs, while simultaneously utilizing thousands of GPUs within a single simulation. We show that radiative heat transfer problems can be made to scale within Uintah on heterogeneous systems through a combination of reverse Monte Carlo ray tracing (RMCRT) techniques combined with AMR, to reduce the amount of global communication. In particular, significant Uintah infrastructure changes, including a novel lock and contention-free, thread-scalable data structure for managing MPI communication requests and improved memory allocation strategies were necessary to achieve excellent strong scaling results to 16384 GPUs on Titan.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2016|10.1109/IPDPSW.2016.93|M. Berzins, Daniel Sunderland, A. Humphrey, T. Harman|1.4444444444444444|2
227|Fast Projection Algorithm for LIM-Based Simultaneous Algebraic Reconstruction Technique and Its Parallel Implementation on GPU|Simultaneous algebraic reconstruction technique (SART) is a well-known iterative method in X-ray computed tomography, which provides better image quality than analytical methods when dealing with incomplete or noisy data. The disadvantage of SART is the slow speed compared with the analytical methods. Since forward projection and backprojection are two major time-consuming operations in iterative reconstruction, we propose an algorithm for fast forward projection and improved backprojection for the line integral model-based SART. Using the proposed algorithm, the SART method was implemented on a GPU platform with NVIDIA’s parallel computing architecture. Both computer simulations and physical phantom experiments were carried out, and their results show that our approach is highly efficient and accurate. The computation time for the system matrix using our proposed projector is 10 times faster than that using the Siddon’s projector, and our improved backprojection algorithm is 1.5 times faster than Li’s method in determining the minimum bounding interval. The GPU-based SART using our proposed projection algorithm can obtain about 7.4 times reconstruction speed-up compared with that using the traditional projection approach, while preserving the accuracy of the results.|IEEE Access|2018|10.1109/ACCESS.2018.2829861|Yuhe Zhang, Guohua Cao, Guohua Geng, Baodong Liu, Xu Dong, Shunli Zhang|1.4285714285714286|2
409|CPU–GPU Parallel Framework for Real‐Time Interactive Cutting of Adaptive Octree‐Based Deformable Objects|A software framework taking advantage of parallel processing capabilities of CPUs and GPUs is designed for the real‐time interactive cutting simulation of deformable objects. Deformable objects are modelled as voxels connected by links. The voxels are embedded in an octree mesh used for deformation. Cutting is performed by disconnecting links swept by the cutting tool and then adaptively refining octree elements near the cutting tool trajectory. A surface mesh used for visual display is reconstructed from disconnected links using the dual contour method. Spatial hashing of the octree mesh and topology‐aware interpolation of distance field are used for collision. Our framework uses a novel GPU implementation for inter‐object collision and object self collision, while tool‐object collision, cutting and deformation are assigned to CPU, using multiple threads whenever possible. A novel method that splits cutting operations into four independent tasks running in parallel is designed. Our framework also performs data transfers between CPU and GPU simultaneously with other tasks to reduce their impact on performances. Simulation tests show that when compared to three‐threaded CPU implementations, our GPU accelerated collision is 53–160% faster; and the overall simulation frame rate is 47–98% faster.|Computer graphics forum (Print)|2018|10.1111/cgf.13162|Zhenkuan Pan, Weizhong Zhang, Shiyu Jia, Xiaokang Yu|1.4285714285714286|2
1288|Exploring Execution Schemes for Agent-Based Traffic Simulation on Heterogeneous Hardware|Microscopic traffic simulation is associated with substantial runtimes, limiting the feasibility of large-scale evaluation of traffic scenarios. Even though today heterogeneous hardware comprised of CPUs, graphics processing units (GPUs) and fused CPU-GPU devices is inexpensive and widely available, common traffic simulators still rely purely on CPU-based execution, leaving substantial acceleration potentials untapped. A number of existing works have considered the execution of traffic simulations on accelerators, but have relied on simplified models of road networks and driver behaviour tailored to the given hardware platform. Thus, the existing approaches cannot directly benefit from the vast body of research on the validity of common traffic simulation models. In this paper, we explore the performance gains achievable through the use of heterogeneous hardware when relying on typical traffic simulation models used in CPU-based simulators. We propose a partial offloading approach that relies either on a dedicated GPU or a fused CPU-GPU device. Further, we present a traffic simulation running fully on a manycore GPU and discuss the challenges of this approach. Our results show that a CPU-based parallelisation closely approaches the results of partial offloading, while full offloading substantially outperforms the other approaches. We achieve a speedup of up to 28.7x over the sequential execution on a CPU.|IEEE International Symposium on Distributed Simulation and Real-Time Applications|2018|10.1109/DISTRA.2018.8601016|D. Eckhoff, Philipp Andelfinger, Jiajian Xiao, Wentong Cai, A. Knoll|1.4285714285714286|2
1333|Performance Analysis Strategies for Task-based Applications on Hybrid Platforms. (Stratégies d'analyse de performance pour les applications basées sur tâches sur plates-formes hybrides)|Programming paradigms in High-Performance Computing have been shifting toward task-based models that are capable of adapting readily to heterogeneous and scalable supercomputers. The performance of task-based applications heavily depends on the runtime scheduling heuristics and on its ability to exploit computing and communication resources. Unfortunately, the traditional performance analysis strategies are unfit to fully understand task-based runtime systems and applications: they expect a regular behavior with communication and computation phases, while task-based applications demonstrate no clear phases. Moreover, the finer granularity of task-based applications typically induces a stochastic behavior that leads to irregular structures that are difficult to analyze. In this thesis, we propose performance analysis strategies that exploit the combination of application structure, scheduler, and hardware information. We show how our strategies can help to understand performance issues of task-based applications running on hybrid platforms. Our performance analysis strategies are built on top of modern data analysis tools, enabling the creation of custom visualization panels that allow understanding and pinpointing performance problems incurred by bad scheduling decisions and incorrect runtime system and platform configuration. By combining simulation and debugging we are also able to build a visual representation of the internal state and the estimations computed by the scheduler when scheduling a new task. We validate our proposal by analyzing traces from a Cholesky decomposition implemented with the StarPU task-based runtime system and running on hybrid (CPU/GPU) platforms. Our case studies show how to enhance the task partitioning among the multi-(GPU, core) to get closer to theoretical lower bounds, how to improve MPI pipelining in multi-(node, core, GPU) to reduce the slow start in distributed nodes and how to upgrade the runtime system to increase MPI bandwidth. By employing simulation and debugging strategies, we also provide a workflow to investigate, in depth, assumptions concerning the scheduler decisions. This allows us to suggest changes to improve the runtime system scheduling and prefetch mechanisms.||2018|10.1002/cpe.4472|Vinícius Garcia Pinto|1.4285714285714286|2
1509|GPU parameter tuning for tall and skinny dense linear least squares problems|ABSTRACT Linear least squares problems (LLSPs) routinely arise in many scientific and engineering problems. One of the fastest ways to solve LLSPs involves performing calculations in parallel on graphics processing units (GPUs). However, GPU algorithms are typically designed for one GPU architecture and may be suboptimal or unusable on another GPU. To design optimal algorithms for any GPU with little need for modifying code, tuneable parameters can simplify the transition of GPU algorithms to different GPU architectures. In this paper, we investigate the benefits of using derivative-free optimization (DFO) and simulation optimization (SO) to systematically optimize tuneable parameters for a GPU or hybrid CPU/GPU LLSP solvers. Computational experiments show that both DFO and SO can be effective tools for determining optimal tuning parameters that can speed up the performance of the popular LLSP solver MAGMA by about 1.8x, compared to MAGMA's default parameters for large tall and skinny matrices. Using DFO solvers, we were able to identify optimal parameters after enumerating an order of magnitude fewer parameter combinations than with direct enumeration. Additionally, the proposed approach is faster than a state-of-the-art autotuner and provides better tuning parameters.|Optim. Methods Softw.|2018|10.1080/10556788.2018.1527331|N. Ploskas, N. Sahinidis, Benjamin Sauk|1.4285714285714286|2
595|Real‐Time Glints Rendering With Pre‐Filtered Discrete Stochastic Microfacets|Many real‐life materials have a sparkling appearance. Examples include metallic paints, sparkling fabrics and snow. Simulating these sparkles is important for realistic rendering but expensive. As sparkles come from small shiny particles reflecting light into a specific direction, they are very challenging for illumination simulation. Existing approaches use a four‐dimensional hierarchy, searching for light‐reflecting particles simultaneously in space and direction. The approach is accurate, but extremely expensive. A separable model is much faster, but still not suitable for real‐time applications. The performance problem is even worse when illumination comes from environment maps, as they require either a large sample count per pixel or pre‐filtering. Pre‐filtering is incompatible with the existing sparkle models, due to the discrete multi‐scale representation. In this paper, we present a GPU‐friendly, pre‐filtered model for real‐time simulation of sparkles and glints. Our method simulates glints under both environment maps and point light sources in real time, with an added cost of just 10 ms per frame with full high‐definition resolution. Editing material properties requires extra computations but is still real time, with an added cost of 10 ms per frame.|Computer graphics forum (Print)|2020|10.1111/cgf.14007|Hong Deng, Beibei Wang, Nicolas Holzschuch|1.4|2
655|Learning Multiple‐Scattering Solutions for Sphere‐Tracing of Volumetric Subsurface Effects|Accurate subsurface scattering solutions require the integration of optical material properties along many complicated light paths. We present a method that learns a simple geometric approximation of random paths in a homogeneous volume with translucent material. The generated representation allows determining the absorption along the path as well as a direct lighting contribution, which is representative of all scatter events along the path. A sequence of conditional variational auto‐encoders (CVAEs) is trained to model the statistical distribution of the photon paths inside a spherical region in the presence of multiple scattering events. A first CVAE learns how to sample the number of scatter events, occurring on a ray path inside the sphere, which effectively determines the probability of this ray to be absorbed. Conditioned on this, a second model predicts the exit position and direction of the light particle. Finally, a third model generates a representative sample of photon position and direction along the path, which is used to approximate the contribution of direct illumination due to in‐scattering. To accelerate the tracing of the light path through the volumetric medium toward the solid boundary, we employ a sphere‐tracing strategy that considers the light absorption and can perform a statistically accurate next‐event estimation. We demonstrate efficient learning using shallow networks of only three layers and no more than 16 nodes. In combination with a GPU shader that evaluates the CVAEs’ predictions, performance gains can be demonstrated for a variety of different scenarios. We analyze the approximation error that is introduced by the data‐driven scattering simulation and shed light on the major sources of error.|Computer graphics forum (Print)|2020|10.1111/cgf.142623|Ludwig Leonard Mendez, Kevin Hoehlein, R. Westermann|1.4|2
683|DockIT: a tool for interactive molecular docking and molecular complex construction|SUMMARY\nDockIT is a tool that has a unique set of physical and graphical features for interactive molecular docking. It enables the user to bring a ligand and a receptor into a docking pose by controlling relative position and orientation, either with a mouse and keyboard, or with a haptic device. Atomic interactions are modelled using molecular dynamics-based force-fields with the force on the ligand being felt on a haptic device. Real-time calculation and display of intermolecular hydrogen bonds and multipoint collision detection either using maximum force or maximum atomic overlap, mean that together with the ability to monitor selected intermolecular atomic distances, the user can find physically feasible docking poses that satisfy distance constraints derived from experimental methods. With these features and the ability to output and reload docked structures it can be used to accurately build up large multi-component molecular systems in preparation for molecular dynamics simulation.\n\n\nAVAILABILITY AND IMPLEMENTATION\nDockIT is available free of charge for non-commercial use at http://www.haptimol.co.uk/downloads.htm. It requires a windows computer with GPU that supports OpenCL 1.2 and OpenGL 4.0. It may be used with a mouse and keyboard, or a haptic device from 3DSystems.|Bioinform.|2020|10.1093/bioinformatics/btaa1059|S. Laycock, S. Hayward, Georgios Iakovou, Mousa Alhazzazi|1.4|2
728|Study of the accuracy and applicability of the difference scheme for solving the diffusion-convection problem at large grid Péclet numbers|The work is devoted to the study of a difference scheme for solving the diffusion-convection problem at large grid Peclet numbers. The suspension transport problem numerical solving is carried out using the improved Upwind Leapfrog difference scheme. Its difference operator is a linear combination of the operators of Upwind and Standard Leapfrog difference schemes, while the modified scheme is obtained from schemes with optimal weighting coefficients. At certain values of the weighting coefficients, this combination leads to mutual compensation of approximation errors, and the resulting scheme gets better properties than the original schemes. In addition, it includes a cell filling function that allows simulating problems in areas with complex geometry. Computational experiments were carried out to solve the suspension transport problem, which arises, for example, during the propagation of suspended matter plumes in an aquatic environment and changes in the bottom topography due to the deposition of suspended soil particles into the sediment during soil unloading into a reservoir (dumping). The results of modeling the suspension transport problem at various values of the grid Peclet number are presented. The algorithm implementation was carried out using the software and hardware architecture of parallel computing: on a central processing unit (Central Processing Unit - CPU) and on a graphics accelerator (Graphics Processing Unit - GPU). The solution to the applied problem has shown its efficiency on the CPU with small computational grids and, if it is necessary to decrease the space steps, then the GPU solution is preferable. It was found that, when using the modified Upwind Leapfrog scheme, an increase in the speed of the water flow does not lead to a loss of solution accuracy due to dissipative sources and is accompanied by an insignificant increase in computational labor costs.|Computational Continuum Mechanics|2020|10.7242/1999-6691/2020.13.4.34|A. Sukhinov, I. Y. Kuznetsova, Y. Belova, A. Chistyakov, E. Protsenko|1.4|2
1576|Computing methods for parallel processing and analysis on complex networks|Nowadays to solve some problems is required to model complex systems to simulate and \nunderstand its behavior. \nA good example of one of those complex systems is the Facebook Social Network, this \nsystem represents people and their relationships, Other example, the Internet composed \nby a vast number of servers, computers, modems and routers, All Science field (physics, \neconomics political, and so on) have complex systems which are complex because of the \nbig volume of data required to represent them and their fast change on their structure \nAnalyze the behavior of these complex systems is important to create simulations or \ndiscover dynamics over it with main goal of understand how it works. \nSome complex systems cannot be easily modeled; We can begin by analyzing their \nstructure, this is possible creating a network model, Mapping the problem´s entities and \nthe relations between them. \nSome popular analysis over the structure of a network are: \n• The Community Detection – discover how their entities are grouped \n• Identify the most important entities – measure the node´s influence over the \nnetwork \n• Features over whole network like – the diameter, number of triangles, clustering \ncoefficient, and the shortest path between two entities. \nMultiple algorithms have been created to give a result to these analyses over the network \nmodel although if they are executed by one machine take a lot of time to complete the task \nor may not be executed due to machine limitation resources. \nAs more demanding applications have been appearing to process the algorithms of these \ntype of analysis, several parallel programming models and different kind of hardware \narchitecture have been created to deal with the big input of data, reduce the time \nexecution, save power consumption and enhance the efficiency in the computation in each \nmachine also taking in mine the application requirements. \nParallelize these algorithms are a challenge due to: \n• We need to analyze data dependence to implement a parallel version of the \nalgorithm always taking in mine the scalability and the performance of the code. \n• Create a implementation of the algorithm for one parallel programming model like \nMapReduce (Apache Hadoop), RDD (Apache Spark), Pregel(Apache Giraph) these \noriented to bigdata or HPC models how MPI + OpenMP , OmpSS or CUDA. \n• Distribute the data input over the processing platform for each node or offload it \ninto accelerators such as GPU or FPGA and so on. \n• Store the data input and store the result of the processing requires techniques of \nDistribute file systems(HDFS), distribute NoSQL Data Bases (Object Data Bases, \nGraph Data Bases, Document Data Bases) or traditional relational Data \nBases(oracle, SQL server). \nIn this Master Thesis, we decided create Graph processing using Apache bigdata Tools \nmainly creating testing over MareNostrum III and the Amazon cloud for some Community \nDetection Algorithms using SNAP Graphs with ground-truth communities. \nCreating a comparative between their parallel computational time execution and scalability.||2015|10.1016/j.trpro.2015.09.006|Luis Andrés, Vázquez Benítez|1.4|2
292|Fast Segmentation From Blurred Data in 3D Fluorescence Microscopy|We develop a fast algorithm for segmenting 3D images from linear measurements based on the Potts model (or piecewise constant Mumford-Shah model). To that end, we first derive suitable space discretizations of the 3D Potts model, which are capable of dealing with 3D images defined on non-cubic grids. Our discretization allows us to utilize a specific splitting approach, which results in decoupled subproblems of moderate size. The crucial point in the 3D setup is that the number of independent subproblems is so large that we can reasonably exploit the parallel processing capabilities of the graphics processing units (GPUs). Our GPU implementation is up to 18 times faster than the sequential CPU version. This allows to process even large volumes in acceptable runtimes. As a further contribution, we extend the algorithm in order to deal with non-negativity constraints. We demonstrate the efficiency of our method for combined image deconvolution and segmentation on simulated data and on real 3D wide field fluorescence microscopy data.|IEEE Transactions on Image Processing|2017|10.1109/TIP.2017.2716843|M. Storath, M. Unser, Dennis Rickert, A. Weinmann|1.375|2
355|GPGPU Performance Estimation with Core and Memory Frequency Scaling|Graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, simple and accurate performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is important to decide the best frequency configuration for energy saving. We reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Over a 2 x range of both core and memory frequencies among 20 GPU kernels, our model achieves accurate results (4.83 % error on average) with real hardware. Compared to the cycle-level simulators, our model only needs simple micro-benchmarks to extract a set of hardware parameters and kernel performance counters to produce such high accuracy without kernel source analysis.|International Conference on Parallel and Distributed Systems|2017|10.1109/PADSW.2018.8645000|Xiaowen Chu, Qiang Wang|1.375|2
1189|Spatial convolution for mirror image suppression in Fourier domain optical coherence tomography.|We developed a spatial convolution approach for mirror image suppression in phase-modulated Fourier domain optical coherence tomography, and demonstrated it in vivo for small animal imaging. Utilizing the correlation among neighboring A-scans, the mirror image suppression process was simplified to a three-parameter convolution. By adjusting the three parameters, we can implement different Fourier domain sideband windows, which is important but complicated in existing approaches. By properly selecting the window size, we validated the spatial convolution approach on both simulated and experimental data, and showed that it is versatile, fast, and effective. The new approach reduced the computational cost by 32% and improved the mirror image suppression by 10%. We adapted the spatial convolution approach to a GPU accelerated system for ultrahigh-speed processing in 0.1 ms. The advantage of the ultrahigh speed was demonstrated in vivo for small animal imaging in a mouse model. The fast scanning and processing speed removed respiratory motion artifacts in the in vivo imaging.|Optics Letters|2017|10.1364/OL.42.000506|Ping Yu, Lixin Ma, Miao Zhang|1.375|2
459|Using a GPU to Accelerate a Longwave Radiative Transfer Model with Efficient CUDA-Based Methods|Climatic simulations rely heavily on high-performance computing. As one of the atmospheric radiative transfer models, the rapid radiative transfer model for general circulation models (RRTMG) is used to calculate the radiative transfer of electromagnetic radiation through a planetary atmosphere. Radiation physics is one of the most time-consuming physical processes, so the RRTMG presents large-scale and long-term simulation challenges to the development of efficient parallel algorithms that fit well into multicore clusters. This paper presents a method for improving the calculative efficiency of radiation physics, an RRTMG long-wave radiation scheme (RRTMG_LW) that is accelerated on a graphics processing unit (GPU). First, a GPU-based acceleration algorithm with one-dimensional domain decomposition is proposed. Then, a second acceleration algorithm with two-dimensional domain decomposition is presented. After the two algorithms were implemented in Compute Unified Device Architecture (CUDA) Fortran, a GPU version of the RRTMG_LW, namely G-RRTMG_LW, was developed. Results demonstrated that the proposed acceleration algorithms were effective and that the G-RRTMG_LW achieved a significant speedup. In the case without I/O transfer, the 2-D G-RRTMG_LW on one K40 GPU obtained a speed increase of 18.52× over the baseline performance on a single Intel Xeon E5-2680 CPU core.|Applied Sciences|2019|10.3390/app9194039|Yuzhu Wang, Yuan Zhao, Xiaohui Ji, Jinrong Jiang, Wei Li, Albert Y. Zomaya|1.3333333333333333|2
854|Optimizing Huffman Decoding for Error-Bounded Lossy Compression on GPUs|More and more HPC applications require fast and effective compression techniques to handle large volumes of data in storage and transmission. Not only do these applications need to compress the data effectively during simulation, but they also need to perform decompression efficiently for post hoc analysis. SZ is an error-bounded lossy compressor for scientific data, and cuSZ is a version of SZ designed to take advantage of the GPU's power. At present, cuSZ's compression performance has been optimized significantly while its decompression still suffers considerably lower performance because of its sophisticated loss-less compression step-a customized Huffman decoding. In this work, we aim to significantly improve the Huffman decoding performance for cuSZ, thus improving the overall decompression performance in turn. To this end, we first investigate two state-of-the-art GPU Huffman decoders in depth. Then, we propose a deep architectural optimization for both algorithms. Specifically, we take full advantage of CUDA GPU architectures by using shared memory on decoding/writing phases, online tuning the amount of shared memory to use, improving memory access patterns, and reducing warp divergence. Finally, we evaluate our optimized decoders on an Nvidia V100 GPU using eight representative scientific datasets. Our new decoding solution obtains an average speedup of 3.64× over cuSZ's Huffman decoder and improves its overall decompression performance by 2.43× on average.|IEEE International Parallel and Distributed Processing Symposium|2022|10.1109/ipdps53621.2022.00075|F. Cappello, Jiannan Tian, Dingwen Tao, Cody Rivera, S. Di, Xiaodong Yu|1.3333333333333333|2
858|Regularized lattice Boltzmann method parallel model on heterogeneous platforms|As an improved method of lattice Boltzmann method (LBM), regularized lattice Boltzmann method (RLBM) has been applied to simulate fluid flow. Nevertheless, the performance of RLBM needs to be considered when simulating actual problems. The rise of multicore platforms, especially the popularity of graphics processor units (GPUs), has provided possible implementation solutions for parallel computing. In this article, an RLBM parallel model on the CPU/GPU heterogeneous platforms is proposed. To solve the problem of possible GPU memory shortage, the CPU controls the startup of the kernel function in the RLBM algorithm and participates in the calculation. Due to the characteristics of the algorithm, the entire flow field is divided into CPU computing areas and GPU computing areas according to the given flow field division rules. OpenMP and CUDA are, respectively, applied to CPU and GPU for parallel computing. The startup of the kernel function takes a short time, and the CPU and GPU can be approximately regarded as performing calculations simultaneously. Since the data exchange between the CPU and GPU has a significant impact on performance, a buffer is set at the boundary of the CPU and GPU to reduce the frequency of data exchange. The buffer size determines the number of program iterations before exchanging data. The performance of the algorithm is measured by MFLUPS, and the algorithm is applied to the 3D lid‐driven cavity flow. The obtained results show that the MFLUPS of the algorithm is 25 times that of CPU MFLUPS, and it adds an increment equivalent to CPU MFLUPS compared with GPU MFLUPS. And the algorithm is extended to a multi‐GPU version, which is also applied to the 3D lid‐driven cavity flow. The results obtained show that the MFLUPS of the multi‐GPU version is 1.8 times that of the single GPU.|Concurrency and Computation|2022|10.1002/cpe.6875|Wei Song, Yu Li, Zhixiang Liu|1.3333333333333333|2
893|A Low Memory Footprint Quantized Neural Network for Depth Completion of Very Sparse Time-of-Flight Depth Maps|Sparse active illumination enables precise time-of-flight depth sensing as it maximizes signal-to-noise ratio for low power budgets. However, depth completion is required to produce dense depth maps for 3D perception. We address this task with realistic illumination and sensor resolution constraints by simulating ToF datasets for indoor 3D perception with challenging sparsity levels. We propose a quantized convolutional encoder-decoder network for this task. Our model achieves optimal depth map quality by means of input pre-processing and carefully tuned training with a geometry-preserving loss function. We also achieve low memory footprint for weights and activations by means of mixed precision quantization-at-training techniques. The resulting quantized models are comparable to the state of the art in terms of quality, but they require very low GPU times and achieve up to 14-fold memory size reduction for the weights w.r.t. their floating point counterpart with minimal impact on quality metrics.|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)|2022|10.1109/CVPRW56347.2022.00302|V. Cambareri, Fabien Cardinaux, P. Zanuttigh, Gianluca Agresti, C. I. Ugwu, Adriano Simonetto, Xiao-Yan Jiang|1.3333333333333333|2
898|Precise and Fast Segmentation of Offshore Farms in High-Resolution SAR Images Based on Model Fusion and Half-Precision Parallel Inference|In aquaculture, using high-resolution synthetic aperture radar (SAR) images to precisely segment offshore farms is helpful for reasonable layout planning and statistics of breeding density. However, conventional segmentation methods tend to have low accuracy and slow inference speed. Therefore, we propose a novel, precise, and fast segmentation scheme for offshore farms in high-resolution SAR images based on model fusion and half-precision parallel inference. Specifically, we propose several new high-performance improved UNet++ methods and reasonably fuse the test results. At the same time, a simulated annealing strategy and a morphological closing operation are introduced to improve the segmentation accuracy. In addition, we find that resizing the images to 256 × 256 pixels is better than 512 × 512 pixels for this task, which not only has higher segmentation accuracy but can also increase the inference speed by nearly 13%. Furthermore, a novel half-precision parallel inference strategy is proposed, which can fully utilize the GPU and increase the inference speed by 72.6%. Compared with some state-of-the-art methods, the proposed scheme that merges two improved UNet++ achieves superior accuracy with a frequency weighted intersection over union of 0.9876 and a single image inference time of 0.0218 s on the high-resolution SAR offshore farm dataset.|IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing|2022|10.1109/jstars.2022.3181355|Deyan Lan, Chuang Yu, Xin Xia, Xin Liu, Yunpeng Liu, Shuhang Wu|1.3333333333333333|2
930|Automatic Differentiation of C++ Codes on Emerging Manycore Architectures with Sacado|Automatic differentiation (AD) is a well-known technique for evaluating analytic derivatives of calculations implemented on a computer, with numerous software tools available for incorporating AD technology into complex applications. However, a growing challenge for AD is the efficient differentiation of parallel computations implemented on emerging manycore computing architectures such as multicore CPUs, GPUs, and accelerators as these devices become more pervasive. In this work, we explore forward mode, operator overloading-based differentiation of C++ codes on these architectures using the widely available Sacado AD software package. In particular, we leverage Kokkos, a C++ tool providing APIs for implementing parallel computations that is portable to a wide variety of emerging architectures. We describe the challenges that arise when differentiating code for these architectures using Kokkos, and two approaches for overcoming them that ensure optimal memory access patterns as well as expose additional dimensions of fine-grained parallelism in the derivative calculation. We describe the results of several computational experiments that demonstrate the performance of the approach on a few contemporary CPU and GPU architectures. We then conclude with applications of these techniques to the simulation of discretized systems of partial differential equations.|ACM Transactions on Mathematical Software|2022|10.1145/3560262|E. Phipps, R. Pawlowski, C. Trott|1.3333333333333333|2
1037|Regularized Maximum Likelihood Image Synthesis and Validation for ALMA Continuum Observations of Protoplanetary Disks|Regularized Maximum Likelihood (RML) techniques are a class of image synthesis methods that achieve better angular resolution and image fidelity than traditional methods like CLEAN for sub-mm interferometric observations. To identify best practices for RML imaging, we used the GPU-accelerated open source Python package MPoL, a machine learning-based RML approach, to explore the influence of common RML regularizers (maximum entropy, sparsity, total variation, and total squared variation) on images reconstructed from real and synthetic Atacama Large millimeter/submillimeter Array (ALMA) continuum observations of protoplanetary disks. We tested two different cross-validation (CV) procedures to characterize their performance and determine optimal prior strengths, and found that CV over a coarse grid of regularization strengths easily identifies a range of models with comparably strong predictive power. To evaluate the performance of RML techniques against a ground truth image, we used MPoL on a synthetic protoplanetary disk data set and found that RML methods successfully resolve structures at fine spatial scales present in the original simulation. We used ALMA DSHARP observations of the protoplanetary disk around HD 143006 to compare the performance of MPoL and CLEAN, finding that RML imaging improved the spatial resolution of the image by up to a factor of 3 without sacrificing sensitivity. We provide general recommendations for building an RML workflow for image synthesis of ALMA protoplanetary disk observations, including effective use of CV. Using these techniques to improve the imaging resolution of protoplanetary disk observations will enable new science, including the detection of protoplanets embedded in disks.|Publications of the Astronomical Society of the Pacific|2022|10.1088/1538-3873/acdf84|J. Jennings, T. Quinn, Robert C. Frazier, Hannah M. Grzybowski, I. Czekala, Yina Jian, Kadri M. Nizam, R. Loomis, B. Zawadzki|1.3333333333333333|2
1390|Rapid design and simulation of functional digital materials|"Digital fabrication aims to bring the programmability of the digital world into the physical world and has the potential to radically transform the way we make things. We are are developing a novel digital fabrication technique where a small basis set of discrete part types, called ""digital materials"", are reversibly joined into large assemblies with embedded functionality. Objects constructed this way may be programmed with exotic functional behavior based on the composition of their constituent parts. In this thesis I build an end-to end computer-aided design (CAD), simulation, and manufacturing (CAM) pipeline for digital materials that respects the discretization of the parts in its underlying software representation. I propagate the same abstract geometric ""cell"" representation of parts from the design workflow into simulation and path planning. I develop a dynamic model for simulating anisotropic, multimaterial assemblies of cells with embedded mechanical and electronic functionality based on local interactions. I demonstrate the similarities between my mechanical model and the Timoshenko Beam Element. I note an advantage of my model for simulating flexural joints is its non-linear treatment of angular displacements allowing for large angular deformations to be simulated without costly remeshing. I implement this model in software and demonstrate its potential for parallelization by calculating each cell-cell interaction in a separate core of the GPU. I compare my simulation results with a professional multiphysics software package. I demonstrate that my tool facilitates rapid exploration of the design space around functional digital materials with several examples. Thesis Supervisor: Prof. Neil Gershenfeld Title: Director, MIT Center for Bits and Atoms"||2016|10.1115/msec2016-8627|Amanda Ghassaei|1.3333333333333333|2
1540|HAWS: Accelerating GPU Wavefront Execution through Selective Out-of-order Execution|Graphics Processing Units (GPUs) have become an attractive platform for accelerating challenging applications on a range of platforms, from High Performance Computing (HPC) to full-featured smartphones. They can overcome computational barriers in a wide range of data-parallel kernels. GPUs hide pipeline stalls and memory latency by utilizing efficient thread preemption. But given the demands on the memory hierarchy due to the growth in the number of computing cores on-chip, it has become increasingly difficult to hide all of these stalls. \n \nIn this article, we propose a novel Hint-Assisted Wavefront Scheduler (HAWS) to bypass long-latency stalls. HAWS starts by enhancing a compiler infrastructure to identify potential opportunities that can bypass memory stalls. HAWS includes a wavefront scheduler that can continue to execute instructions in the shadow of a memory stall, executing instructions speculatively, guided by compiler-generated hints. HAWS increases utilization of GPU resources by aggressively fetching/executing speculatively. Based on our simulation results on the AMD Southern Islands GPU architecture, at an estimated cost of 0.4% total chip area, HAWS can improve application performance by 14.6% on average for memory intensive applications.|ACM Transactions on Architecture and Code Optimization (TACO)|2019|10.1145/3291050|Leiming Yu, Xiang Gong, D. Kaeli, Xun Gong|1.3333333333333333|2
1562|Emerald: Graphics Modeling for SoC Systems|Mobile systems-on-chips (SoCs) have become ubiquitous computing platforms, and, in recent years, they have become increasingly heterogeneous and complex. A typical SoC includes CPUs, graphics processor units (GPUs), image processors, video encoders/decoders, AI engines, digital signal processors (DSPs) and 2D engines among others [33], [70], [71]. One of the most significant SoC units in terms of both off-chip memory bandwidth and SoC die area is the GPU. In this paper, we present Emerald, a simulator that builds on existing tools to provide a unified model for graphics and GPGPU applications. Emerald enables OpenGL (v4.5) and OpenGL ES (v3.2) shaders to run on GPGPU-Sim's timing model and is integrated with gem5 and Android to simulate full SoCs. Emerald thus provides a platform for studying system-level SoC interactions while including the impact of graphics. We present two case studies using Emerald. First, we use Emerald's full-system mode to highlight the importance of system-wide interactions by studying and analyzing memory organization and scheduling schemes for SoC systems. Second, we use Emerald's standalone mode to evaluate a novel mechanism for balancing the graphics shading work assigned to each GPU core.|International Symposium on Computer Architecture|2019|10.1145/3307650.3322221|Ayub A. Gubran, Tor M. Aamodt|1.3333333333333333|2
222|Modelling the multi-conjugate adaptive optics system of the European Extremely Large Telescope|MAORY is the Multi-Conjugate Adaptive Optics Module for the E-ELT. The baseline design assumes six sodium Laser Guide Stars and three Natural Guide Stars for wavefront sensing. Three deformable mirrors, including the telescope adaptive mirror M4, are optically conjugated to different altitudes in the atmosphere to achieve compensation of the atmospheric turbulence effects over an extended Field of View. In preparation for the project phase-B we are analyzing different critical aspects of such a system. We are developing a versatile and modular end-to-end simulation code that makes use of GPUs to obtain high-fidelity modelling of the system performance and, in parallel, a semplified code for the analysis of the effects induced by the temporal variation of the sodium layer where the artificial laser guide stars are generated. An overview of the work in progress will be||2015|10.1051/0004-6361/201525610|E. Diolaiti, M. Patti, L. Schreiber, G. Bregoli, R. C. Butler, I. Foppiani, M. Lombini, C. Arcidiacono, P. Ciliegi|1.3|2
1459|Procedural generation of multiple stable, small-scale solar systems using 3D N-Body simulation.|Owing to mankind’s constant pursuit of knowledge, we have been seeking to understand the vast universe around us. Thanks to the field of GPU Computing, we have in the recent decade been able to do simulations on the universe with an efficiency like never before. This allows us to create simulation models of the universe, which we can use to learn more about it. In this thesis, we develop a galaxy generator that generates a small galaxy with a user-specified number of solar systems and internal bodies. One of the central goals of the galaxy generator is to generate galaxies that can remain in equilibrium for as long as possible. We do various tests with the galaxy generator in order to discover and understand what makes a galaxy stable. We find that the integration method and the type of N-Body algorithm has a significant impact on how the simulation is run, and choosing the correct one for the situation is crucial. We develop an approximate, specialized NBody algorithm and integration method for both CPU and GPU that is able to calculate the movement of the bodies fast by using the center of mass as an approximation. Results show that the proposed N-Body algorithm is able to significantly speed up computation over the All-Pairs GPU N-Body algorithm. We also show that the proposed algorithm is suitable for GPU execution, as the GPU-based algorithm is several times faster than the CPUbased one. When analyzing the accuracy of the approximated algorithm, we found that the error becomes significant over time. We quantify the error and try to explain the reason for the observed error. Finally, we perform several tests on the proposed integration method, and find out that it shows promise as a way to integrate distant forces less frequently than nearby forces.||2015|10.1109/isbi.2015.7163849|Joakim Hommeland|1.3|2
248|Time Warp on the GPU: Design and Assessment|The parallel execution of discrete-event simulations on commodity GPUs has been shown to achieve high event rates. Most previous proposals have focused on conservative synchronization, which typically extracts only limited parallelism in cases of low event density in simulated time. We present the design and implementation of an optimistic fully GPU-based parallel discrete-event simulator based on the Time Warp synchronization algorithm. The optimistic simulator implementation is compared with an otherwise identical implementation using conservative synchronization. Our evaluation shows that in most cases, the increase in parallelism when using optimistic synchronization significantly outweighs the increased overhead for state keeping and rollbacks. To reduce the cost of state keeping, we show how XORWOW, the default pseudo-random number generator in CUDA, can be reversed based solely on its current state. Since the optimal configuration of multiple performance-critical simulator parameters depends on the behavior of the simulation model, these parameters are adapted dynamically based on performance measurements and heuristic optimization at runtime. We evaluate the simulator using the PHOLD benchmark model and a simplified model of peer-to-peer networks using the Kademlia protocol. On a commodity GPU, the optimistic simulator achieves event rates of up to 81.4 million events per second and a speedup of up to 3.6 compared with conservative synchronization.|SIGSIM Principles of Advanced Discrete Simulation|2017|10.1145/3064911.3064912|Philipp Andelfinger, Xinhu Liu|1.25|2
354|RLEX: Saftey and Data Quality in Reinforcement Learning-based and Adaptive Systems|1. ABSTRACT The marriage of data management and machine learning is one of the most significant trends in recent database research. Industry and academia have built systems to improve the efficiency of model training [3,5,7,20], studied problems in learning on normalized data [16,19], designed libraries for in-database learning [2,8, 10], explored how to apply data cleaning before learning [15]. As database researchers, it is natural to think machine learning as simply a form of high-dimensional aggregate query processing. However , unlike traditional SQL analytics, learning systems potentially create feedback loops where the learned models affect the future training data. For example, if a model recommends Pop music to European users–future data would have fewer examples of Euro-pean preferences on other genres. To address such problems, we argue that learning systems need to move beyond the supervised learning paradigm and add support for Reinforcement Learning (RL) – drawing inspiration from recent successes of Google DeepMind on AlphaGo project [1]. Rather than predicting a single label for a single feature vector, RL algorithms learn stateful policies that output a sequence of predictions (called actions). As the name implies, these policies are learned through iterative trial-and-error (called exploration); optimizing an aggregate reward (utility of the predictions). In the music recommendation example, one could imagine a system that learns to recommend songs based on those songs previously heard by a user to maximize the total aggregate time spent on the platform. Such is the promise of RL, and in principle, RL subsumes all supervised learning models. However, this generality comes at a steep price. RL algorithms can spend significant amounts of time exploring before learning a policy of value. It is further the case that many state-of-the-art RL algorithms have to store the entire execution trace of the system for learning [4]. While these algorithms do exploit parallelism [17] and GPUs [18], much of these results are restricted to simulated problems, where exploration is relatively inexpensive, and the data is by definition clean. Transferring these results to real systems requires addressing challenges in geo-distribution, data quality, physical design in these execution traces. We propose RLEX a DBMS system for managing RL execution traces which builds on our work on the SampleClean project [14] and reliable RL in surgical robotics [11]. Anatomy of a Modern RL Algorithm: Especially when Neu-ral Networks are involved, the current state-of-the-art in RL is to use a technique called experience …|Conference on Innovative Data Systems Research|2017|10.14778/3115404.3115414|S. Krishnan|1.25|2
746|Parallel Fine-Grained Comparison of Long DNA Sequences in Homogeneous and Heterogeneous GPU Platforms With Pruning|The parallelization of Smith-Waterman (SW) sequence comparison tools for long DNA sequences has been a big challenge over the years, requesting the use of several devices and sophisticated optimizations. Pruning is one of these optimizations, which can reduce considerably the amount of computation. This article proposes MultiBP, a sequence comparison solution in multiple GPUs with block pruning. Two MultiBP strategies are proposed. In static score-sharing, workload is statically distributed to the GPUs, and the best score is sent to neighbor GPUs to simulate a global view. In the dynamic strategy, execution is divided into cycles and workload is dynamically assigned, according to the GPUs processing rate. MultiBP was integrated to MASA-CUDAlign and tested in homogeneous and heterogeneous platforms, with different NVidia GPU architectures. The best results in our homogeneous and heterogeneous platforms were mostly obtained by the static and dynamic approaches, respectively. We also show that our decision module is able to select the best strategy in most cases. Finally, the comparison of the human and chimpanzee chromosomes 1 in a cluster with 512 V100 NVidia GPUs took 11 minutes and obtained the impressive rate of 82,822 GCUPS (Billions of Cells Updated per Second) which is, to our knowledge, the best performance for SW tools in GPUs.|IEEE Transactions on Parallel and Distributed Systems|2021|10.1109/TPDS.2021.3084069|J. P. Navarro, A. C. Melo, Marco Figueiredo, George Teodoro, E. Sandes|1.25|2
798|Probabilistic Occlusion Culling using Confidence Maps for High-Quality Rendering of Large Particle Data|Achieving high rendering quality in the visualization of large particle data, for example from large-scale molecular dynamics simulations, requires a significant amount of sub-pixel super-sampling, due to very high numbers of particles per pixel. Although it is impossible to super-sample all particles of large-scale data at interactive rates, efficient occlusion culling can decouple the overall data size from a high effective sampling rate of visible particles. However, while the latter is essential for domain scientists to be able to see important data features, performing occlusion culling by sampling or sorting the data is usually slow or error-prone due to visibility estimates of insufficient quality. We present a novel probabilistic culling architecture for super-sampled high-quality rendering of large particle data. Occlusion is dynamically determined at the sub-pixel level, without explicit visibility sorting or data simplification. We introduce confidence maps to probabilistically estimate confidence in the visibility data gathered so far. This enables progressive, confidence-based culling, helping to avoid wrong visibility decisions. In this way, we determine particle visibility with high accuracy, although only a small part of the data set is sampled. This enables extensive super-sampling of (partially) visible particles for high rendering quality, at a fraction of the cost of sampling all particles. For real-time performance with millions of particles, we exploit novel features of recent GPU architectures to group particles into two hierarchy levels, combining fine-grained culling with high frame rates.|IEEE Transactions on Visualization and Computer Graphics|2021|10.1109/TVCG.2021.3114788|Marco Agus, P. Rautek, Mohamed Ibrahim, M. Hadwiger, G. Reina|1.25|2
1502|Portable implementation model for CFD simulations. Application to hybrid CPU/GPU supercomputers|ABSTRACT Nowadays, high performance computing (HPC) systems experience a disruptive moment with a variety of novel architectures and frameworks, without any clarity of which one is going to prevail. In this context, the portability of codes across different architectures is of major importance. This paper presents a portable implementation model based on an algebraic operational approach for direct numerical simulation (DNS) and large eddy simulation (LES) of incompressible turbulent flows using unstructured hybrid meshes. The strategy proposed consists in representing the whole time-integration algorithm using only three basic algebraic operations: sparse matrix–vector product, a linear combination of vectors and dot product. The main idea is based on decomposing the nonlinear operators into a concatenation of two SpMV operations. This provides high modularity and portability. An exhaustive analysis of the proposed implementation for hybrid CPU/GPU supercomputers has been conducted with tests using up to 128 GPUs. The main objective consists in understanding the challenges of implementing CFD codes on new architectures.||2017|10.1080/10618562.2017.1390084|A. Gorobets, G. Oyarzun, A. Oliva, R. Borrell|1.25|2
1580|The Performance Improvement of the Lagrangian Particle Dispersion Model (LPDM) Using Graphics Processing Unit (GPU) Computing|Abstract : The Lagrangian Particle Dispersion Model (LPDM) simulates the ensemble average transport of aerosols and gases in turbulent wind conditions. Many atmospheric transport and dispersion models, such as LPDM, can take multiple hours to complete execution, which makes them not useful for rapid release and planning purposes. This report documents the implementation process of integrating the LPDM code with graphics processing unit (GPU) computing technology to improve its performance. As a result, the execution time of the GPU-accelerated LPDM application was much faster than the original LPDM application.||2017|10.1016/b978-0-12-802381-5.00049-x|Leelinda P. Dawson|1.25|2
471|Supporting efficient execution of continuous space agent‐based simulation on GPU|Using agent‐based simulation (ABS) to analyze complex adaptive systems gains growing popularity over the past decades. One of the fundamental issues in ABS is to increase the execution speed. In this paper, we identify two common modules that widely exist in ABS applications, namely, the agent management module and the agent interaction module. Improving the efficiency of these two common modules can significantly speed up the ABS execution in general. GPU architecture, programming model, and memory hierarchy are studied. Effective strategies on GPU are proposed when we design the two modules. The first contribution of this work is to propose an AgentPool data structure to handle agent creation and deletion on GPU. The second contribution is an efficient agent interaction module, which is designed by carefully utilizing the GPU memory hierarchy. To demonstrate effectiveness and generality, the proposed strategies are applied to a range of ABS applications, including game‐of‐life, flocking boids, prey‐and‐predator, and the social force‐based crowd simulation. The simulation results demonstrate that the proposed strategies achieve better performance than the commonly used CPU and GPU ABS framework, namely, Mason and FLAME, for ABS applications using continuous space. Copyright © 2016 John Wiley & Sons, Ltd.|Concurrency and Computation|2016|10.1002/cpe.3808|Xiaosong Li, S. Turner, Wentong Cai|1.2222222222222223|2
1482|An Accelerated Explicit Method and GPU Parallel Computing for Thermal Stress and Welding Deformation of Automotive Parts|An accelerated explicit method and GPU parallel computing program of finite element method (FEM) are developed for simulating transient thermal stress and welding deformation in large scale models. In the accelerated explicit method, a two-stage computation scheme is employed. The first computation stage is based on a dynamic explicit method considering the characteristics of the welding mechanical process by controlling both the temperature increment and time scaling parameter. In the second computation stage, a static equilibrium computation scheme is implemented after thermal loading to obtain a static solution of transient thermal stress and welding deformation. It has been demonstrated that the developed GPU parallel computing program has a good scalability for large scale models of more than 20 million degrees of freedom (DOFs). The validity of the accelerated explicit method is verified by comparing the transient thermal deformation and residual stresses with those computed by the implicit FEM and ...||2016|10.1142/S175882511650023X|N. Ma, S. Yuan|1.2222222222222223|2
522|Passively parallel regularized stokeslets|Stokes flow, discussed by G.G. Stokes in 1851, describes many microscopic biological flow phenomena, including cilia-driven transport and flagellar motility; the need to quantify and understand these flows has motivated decades of mathematical and computational research. Regularized stokeslet methods, which have been used and refined over the past 20 years, offer significant advantages in simplicity of implementation, with a recent modification based on nearest-neighbour interpolation providing significant improvements in efficiency and accuracy. Moreover this method can be implemented with the majority of the computation taking place through built-in linear algebra, entailing that state-of-the-art hardware and software developments in the latter, in particular multicore and GPU computing, can be exploited through minimal modifications (‘passive parallelism’) to existing Matlab computer code. Hence, and with widely available GPU hardware, significant improvements in the efficiency of the regularized stokeslet method can be obtained. The approach is demonstrated through computational experiments on three model biological flows: undulatory propulsion of multiple Caenorhabditis elegans, simulation of progression and transport by multiple sperm in a geometrically confined region, and left–right symmetry breaking particle transport in the ventral node of the mouse embryo. In general an order-of-magnitude improvement in efficiency is observed. This development further widens the complexity of biological flow systems that are accessible without the need for extensive code development or specialist facilities. This article is part of the theme issue ‘Stokes at 200 (part 2)’.|Philosophical Transactions of the Royal Society A|2020|10.1098/rsta.2019.0528|M. Gallagher, David. J. Smith|1.2|2
602|OpenABLext: An automatic code generation framework for agent‐based simulations on CPU‐GPU‐FPGA heterogeneous platforms|The execution of agent‐based simulations (ABSs) on hardware accelerator devices such as graphics processing units (GPUs) has been shown to offer great performance potentials. However, in heterogeneous hardware environments, it can become increasingly difficult to find viable partitions of the simulation and provide implementations for different hardware devices. To automate this process, we present OpenABLext, an extension to OpenABL, a model specification language for ABSs. By providing a device‐aware OpenCL backend, OpenABLext enables the co‐execution of ABS on heterogeneous hardware platforms consisting of central processing units, GPUs, and field programmable gate arrays (FPGAs). We present a novel online dispatching method that efficiently profiles partitions of the simulation during run‐time to optimize the hardware assignment while using the profiling results to advance the simulation itself. In addition, OpenABLext features automated conflict resolution based on user‐specified rules, supports graph‐based simulation spaces, and utilizes an efficient neighbor search algorithm. We show the improved performance of OpenABLext and demonstrate the potential of FPGAs in the context of ABS. We illustrate how co‐execution can be used to further lower execution times. OpenABLext can be seen as an enabler to tap the computing power of heterogeneous hardware platforms for ABS.|Concurrency and Computation|2020|10.1002/cpe.5807|D. Eckhoff, Philipp Andelfinger, Jiajian Xiao, Wentong Cai, P. Richmond, A. Knoll|1.2|2
643|A GPU-based 2D shallow water quality model|\n In this study, a 2D shallow water flow solver integrated with a water quality model is presented. The interaction between the main water quality constituents included is based on the Water Quality Analysis Simulation Program. Efficiency is achieved by computing with a combination of a Central Processing Unit (CPU) and a Graphics Processing Unit (GPU) device. This technique is intended to provide robust and accurate simulations with high computation speedups with respect to a single-core CPU in real events. The proposed numerical model is evaluated in cases that include the transport and reaction of water quality components over irregular bed topography and dry–wet fronts, verifying that the numerical solution in these situations conserves the required properties (C-property and positivity). The model can operate in any steady or unsteady form allowing an efficient assessment of the environmental impact of water flows. The field data from an unsteady river reach test case are used to show that the model is capable of predicting the measured temporal distribution of dissolved oxygen and water temperature, proving the robustness and computational efficiency of the model, even in the presence of noisy signals such as wind speed.||2020|10.2166/hydro.2020.030|M. Morales-Hernández, Geovanny Gordillo, I. Echeverribar, J. Fernández-Pato, P. García-Navarro|1.2|2
673|CFD code adaptation to the FPGA architecture|For the last years, we observe the intensive development of accelerated computing platforms. Although current trends indicate a well-established position of GPU devices in the HPC environment, FPGA (Field-Programmable Gate Array) aspires to be an alternative solution to offload the CPU computation. This paper presents a systematic adaptation of four various CFD (Computational Fluids Dynamic) kernels to the Xilinx Alveo U250 FPGA. The goal of this paper is to investigate the potential of the FPGA architecture as the future infrastructure able to provide the most complex numerical simulations in the area of fluid flow modeling. The selected kernels are customized to a real-scientific scenario, compatible with the EULAG (Eulerian/semi-Lagrangian) fluid solver. The solver is used to simulate thermo-fluid flows across a wide range of scales and is extensively used in numerical weather prediction. The proposed adaptation is focused on the analysis of the strengths and weaknesses of the FPGA accelerator, considering performance and energy efficiency. The proposed adaptation is compared with a CPU implementation that was strongly optimized to provide realistic and objective benchmarks. The performance results are compared with a set of server CPUs containing various Intel generations, including Intel SkyLake-based CPUs as Xeon Gold 6148 and Xeon Platinum 8168, as well as Intel Xeon E5-2695 CPU based on the IvyBridge architecture. Since all the kernels belong to the group of memory-bound algorithms, our main challenge is to saturate global memory bandwidth and provide data locality with the intensive BRAM (Block RAM) reusing. Our adaptation allows us to reduce the performance per watt up to 80% compared to the CPUs.|The international journal of high performance computing applications|2020|10.1177/1094342020972461|Lukasz Kuczynski, K. Rojek, Kamil Halbiniak|1.2|2
1593|A Parallel-Computing Algorithm for High-Energy Physics Particle Tracking and Decoding Using GPU Architectures|"Real-time data processing is one of the central processes of particle physics experiments which require large computing resources. The LHCb (Large Hadron Collider beauty) experiment will be upgraded to cope with a particle bunch collision rate of 30 million times per second, producing <inline-formula> <tex-math notation=""LaTeX"">$10^{9}$ </tex-math></inline-formula> particles/s. 40 Tbits/s need to be processed in real-time to make filtering decisions to store data. This poses a computing challenge that requires exploration of modern hardware and software solutions. We present <italic>Compass</italic>, a particle tracking algorithm and a parallel raw input decoding optimized for GPUs. It is designed for highly parallel architectures, data-oriented, and optimized for fast and localized data access. Our algorithm is configurable, and we explore the trade-off in computing and physics performance of various configurations. A CPU implementation that delivers the same physics performance as our GPU implementation is presented. We discuss the achieved physics performance and validate it with Monte Carlo simulated data. We show a computing performance analysis comparing consumer and server-grade GPUs, and a CPU. We show the feasibility of using a full GPU decoding and particle tracking algorithm for high-throughput particle trajectories reconstruction, where our algorithm improves the throughput up to <inline-formula> <tex-math notation=""LaTeX"">$7.4\times $ </tex-math></inline-formula> compared to the LHCb baseline."|IEEE Access|2020|10.1109/ACCESS.2019.2927261|D. Bruch, Javier García-Blas, D. C. Pérez, José Daniel García Sánchez, P. F. Declara, N. Neufeld|1.2|2
497|Airborne ultrasound phased arrays|This work shows the development and the characterization of an air-coupled ultrasonic phased array \nusing a 3D-printed waveguide. The waveguide allows an element distance of λ/2 with existing ultrasonic transducers exceeding λ/2 dimension. With an element distance of λ/2, a grating-lobe-free radiation characteristic is realized. In addition to the improved radiation characteristic, the spatial separation of transducers and the acoustically active aperture is achieved. This allows the free arrangement of the ultrasonic transducers in the design space and the use of larger and more powerful ultrasonic transducers, since the transducer size is not limited by the element arrangement. \n \nApart from the design of the waveguide, phased array electronics is presented which enables transmit and receive beamforming of ultrasonic signals. Furthermore, the required signal processing architecture for receive beamforming is presented. The signal processing uses GPU-acceleration to achieve an immediate evaluation of the received signals. \n \nDifferent applications are demonstrated with the built up system. The first application is a gas flow metering based on the the time-of-flight-principle. By exploiting the electronic beam steering, the measurable velocity range is extended compared to conventional single transducers. \n \nFurthermore, the imaging based on pulse echo localization is demonstrated. This method is state of the art in medical imaging as well as in sonar systems under water. In this thesis, this method is transferred to air-coupled ultrasound. For this, a characterization with different measurement series is carried out. It was possible to demonstrate ranges up to 6 m. The achievable lateral resolution for separating two \nadjacent objects is at least 12°. Furthermore, successive objects can be resolved with an axial resolution \nof 200 mm. \n \nIn another experiment the beamforming of sound waves is made visible with the help of schlieren photography. The necessary image processing is presented. The schlieren images are used for the comparison between simulation and the real beamforming behavior. \n \nThe developed platform enables the evaluation of additional ultrasonic applications. These include non-destructive testing using Lamb waves, which are the subject of current research at the Measurement and Sensor Technology Group at the Technische Universitat Darmstadt. Furthermore, the field monitoring for robotic applications is already being investigated using the array and compared with established sensor \nsystems. \n \nIn addition to these ultrasound applications, the developed platform is a model system for various questions of signal processing and communication technology.||2019|10.3390/rs11222674|A. Jäger|1.1666666666666667|2
1441|A Validated Physical Model For Real-Time Simulation of Soft Robotic Snakes|In this work we present a framework that is capable of accurately representing soft robotic actuators in a multiphysics environment in real-time. We propose a constraint-based dynamics model of a 1-dimensional pneumatic soft actuator that accounts for internal pressure forces, as well as the effect of actuator latency and damping under inflation and deflation and demonstrate its accuracy a full soft robotic snake with the composition of multiple 1D actuators. We verify our model’s accuracy in static deformation and dynamic locomotion open-loop control experiments. To achieve real-time performance we leverage the parallel computation power of GPUs to allow interactive control and feedback.|IEEE International Conference on Robotics and Automation|2019|10.1109/ICRA.2019.8794375|Kenny Erleben, Renato Gasoto, Xuan Liu, Yinan Sun, Jie Fu, M. Macklin, C. Onal|1.1666666666666667|2
1532|Exploring Memory Persistency Models for GPUs|Given its high integration density, high speed, byte addressability, and low standby power, non-volatile or persistent memory is expected to supplement/replace DRAM as main memory. Through persistency programming model (which defines durability ordering of stores) and durable transaction constructs, the programmer can provide recoverable data structure (RDS) which allows programs to recover to a consistent state after a failure. While persistency models have been well studied for CPUs, they have been neglected for graphics processing units (GPUs). Considering the importance of GPUs as a dominant accelerator for high performance computing, we investigate persistency models for GPUs. GPU applications exhibit substantial differences with CPUs applications, hence in this paper we adapt, re-architect, and optimize CPU persistency models for GPUs. We design a pragma-based compiler scheme for expressing persistency model for GPUs. We identify that the thread hierarchy in GPUs offers intuitive scopes to form epochs and durable transactions. We find that undo logging produces significant performance overheads. We propose to use idempotency analysis to reduce both logging frequency and the size of logs. Through both real-system and simulation evaluations, we show low overheads of our proposed architecture support.|International Conference on Parallel Architectures and Compilation Techniques|2019|10.1109/PACT.2019.00032|Yan Solihin, Huiyang Zhou, Zhen Lin, Mohammad A. Alshboul|1.1666666666666667|2
1585|KleinPAT: optimal mode conflation for time-domain precomputation of acoustic transfer|We propose a new modal sound synthesis method that rapidly estimates all acoustic transfer fields of a linear modal vibration model, and greatly reduces preprocessing costs. Instead of performing a separate frequencydomain Helmholtz radiation analysis for each mode, our method partitions vibration modes into chords using optimal mode conflation, then performs a single time-domain wave simulation for each chord. We then perform transfer deconflation on each chord’s time-domain radiation field using a specialized QR solver, and thereby extract the frequency-domain transfer functions of each mode. The precomputed transfer functions are represented for fast far-field evaluation, e.g., using multipole expansions. In this paper, we propose to use a single scalar-valued Far-field Acoustic Transfer (FFAT) cube map. We describe a GPU-accelerated vector wavesolver that achieves high-throughput acoustic transfer computation at accuracy sufficient for sound synthesis. Our implementation, KleinPAT, can achieve hundredto thousand-fold speedups compared to existing Helmholtz-based transfer solvers, thereby enabling large-scale generation of modal sound models for audio-visual applications.|ACM Transactions on Graphics|2019|10.1145/3306346.3322976|Jui-Hsien Wang, Doug L. James|1.1666666666666667|2
331|A Novel Performance Prediction Model for Mobile GPUs|With the fast-growing development of mobile devices, the application of high-end three-dimensional (3-D) graphics is expanding to include usage in mobile platforms. Recent mobile application processors are equipped with a multicore CPU and a mobile GPU on a single chip, thereby enabling the incorporation of high-end 3-D graphics into mobile devices. The problem is that such features consume large amounts of power. Thus, previous studies focused on estimating the power consumption of mobile GPUs, but such research has been unfamiliar with user perspective. To address this deficiency, the current work developed a novel performance prediction model for mobile GPUs on Adreno. The model uses both the instruction throughput of a unified shader and GFLOPS. The utilization of the Adreno GPUs was adjusted to its maximum value to ensure that their performance is unaffected by dynamic voltage and frequency scaling and throttling functions. The model was validated using GFXBench under real game application environments. The simulation results provided the computational rates of each hardware unit of the Adreno GPUs and the rate of increase in the instruction processing of the unified shader. To verify the accuracy of the model, we compared the difference rates of the prediction results between those derived from the proposed model and those using Snapdragon profiler. The average error rate was 3.32% with three applications running on four different mobile devices.|IEEE Access|2018|10.1109/ACCESS.2018.2816040|Jinyoung Lee, Jae-Ho Nah, W. Park, Youngsik Kim, Juwon Yun, Yeong-Kyu Lim, Cheong-Ghil Kim|1.1428571428571428|2
1289|Highly-Scalable and Generalized Sensor Structures for Efficient Physically-Based Simulation of Multi-Modal Sensor Networks|Multi-modal sensor networks are used in various fields of application to allow for highly-automated and autonomous systems. Due to the increasing number of sensors and overall system complexity, we propose the use of Virtual Testbeds (VTBs) which allow for comprehensive virtual system tests throughout the development process. Here, VTBs are used as a software environment in which “Digital Twins” of real technical systems are modeled and simulated within their operational environment. In this contribution, we describe a highly-scalable sensor framework used by those VTBs to enable efficient physically-based simulations of various sensor types (e.g. camera, LiDAR, RADAR and ultrasonic sensors). In this context, we present generalized sensor structures that are acting on the same data base to reduce redundancy and ensure consistency across different sensor types and manufacturers. Furthermore, those highly-scalable sensor structures are combined with modern GPUs to achieve efficient and interactive simulations of sensor-enabled applications from various domains.|International Conference on Information Control Systems & Technologies|2018|10.1109/ICSENST.2018.8603563|Jörn Thieling, J. Roßmann|1.1428571428571428|2
1409|Stress and Distortion Simulation of Additive Manufacturing Process by High Performance Computing|Numerical simulation is an efficient way to better understand the thermal and mechanical evolution during metal additive manufacturing (AM) and to design and optimize the process. However, with today’s computational tools, pass-bypass thermal-mechanical numerical simulation of the metal AM process is extremely time-consuming. In this study, a new finite element code recently developed in house at Oak Ridge National Lab was used for additive manufacturing simulation. Our new code effectively utilizes GPU based high-performance computers to allow for realistic simulation of the transient thermal and mechanical response of materials during additive manufacturing. A benchmark study on a cylinder model by powder bed selective laser melting was carried out and distortion profile was compared to the experimental measurements. The accuracy and efficiency of the code was also demonstrated by analyzing a wire and arc additive manufacturing (WAAM) model which consists of a base plate and four deposited layers.|Volume 6A: Materials and Fabrication|2018|10.1115/PVP2018-85045|B. Carlson, Zhili Feng, Jian Chen, Hui-juan Huang, Hui-Ping Wang, G. Frederick, P. Crooker|1.1428571428571428|2
1498|Parallel implementation of implicit finite element model with cohesive zones and collision response using CUDA|The aim of this work is to efficiently implement the Park‐Paulino‐Roesler cohesive zone model with the objective of creating realistic high‐resolution simulations of material deformation, fracture, and postfracture behavior. Intrinsically, unstructured meshes can create more realistic fracture patterns in bulk material than structured meshes. Implicit methods, stable for much larger time steps, have greater potential to model both fracture and postfracture behavior without sacrificing speed of execution. Several technical contributions are presented, including (i) GPU‐accelerated implementation of the Park‐Paulino‐Roesler cohesive zone model, (ii) efficient creation of sparse matrix structure, and (iii) comparison of different unloading/reloading relations when using an implicit scheme. A potential‐based collision response scheme was implemented that allows one to model the interaction of fragmented material. Several test simulations are carried out to demonstrate the flexibility of the model and its ability to reproduce different materials under various loading conditions. Benchmarking results show that most of the computational time is spent by the third‐party solver library, meaning that other operations do not require optimization. The library is made available as open source.||2018|10.1002/nme.5825|R. Taylor, R. Sarracino, I. Gribanov|1.1428571428571428|2
276|Fastron: An Online Learning-Based Model and Active Learning Strategy for Proxy Collision Detection|We introduce the Fastron, a configuration space (C-space) model to be used as a proxy to kinematic-based collision detection. The Fastron allows iterative updates to account for a changing environment through a combination of a novel formulation of the kernel perceptron learning algorithm and an active learning strategy. Our simulations on a 7 degree-of-freedom arm indicate that proxy collision checks may be performed at least 2 times faster than an efficient polyhedral collision checker and at least 8 times faster than an efficient high-precision collision checker. The Fastron model provides conservative collision status predictions by padding C-space obstacles, and proxy collision checking time does not scale poorly as the number of workspace obstacles increases. All results were achieved without GPU acceleration or parallel computing.|Conference on Robot Learning|2017|10.1109/isi.2017.8004883|Nikhil Das, Naman Gupta, Michael C. Yip|1.125|2
388|Improving Uintah's Scalability Through the Use of Portable Kokkos-Based Data Parallel Tasks|The University of Utah's Carbon Capture Multidisciplinary Simulation Center (CCMSC) is using the Uintah Computational Framework to predict performance of a 1000 MWe ultra-supercritical clean coal boiler. The center aims to utilize the Intel Xeon Phi-based DOE systems, Theta and Aurora, through the Aurora Early Science Program by using the Kokkos C++ library to enable node-level performance portability. This paper describes infrastructure advancements and portability improvements made possible by the integration of Kokkos within Uintah. This integration marks a step towards consolidating Uintah's MPI+PThreads and MPI+CUDA hybrid parallelism approaches into a single MPI+Kokkos approach. Scalability results are presented that compare serial and data parallel task execution models for a challenging radiative heat transfer calculation, central to the center's predictive boiler simulations. These results demonstrate both good strong-scaling characteristics to 256 Knights Landing (KNL) processors on the NSF Stampede system, and show the KNL-based calculation to compete with prior GPU-based results for the same calculation.|Practice and Experience in Advanced Research Computing|2017|10.1145/3093338.3093388|John K. Holmen, M. Berzins, Daniel Sunderland, A. Humphrey|1.125|2
1574|Modeling Path Dependent Derivatives Using CUDA Parallel Platform|The pricing of derivative securities with path dependence is governed by stochastic differential equations which rarely have a closed-form, analytic solution. These complex derivatives can be valued used simulation methods known as Monte Carlo methods, which converge slowly and thus require significant computational cost. This thesis demonstrates how the use of the GPU (Graphics Process Unit) can drastically lower the computational cost of these methods. The Longstaff-Schwartz Least Squares Monte Carlo Method is implemented to price American options, and suggestions are made for improving the efficiency of the algorithm. A model for valuing Guaranteed Lifetime Withdrawal Benefit (GLWB) options using Monte Carlo methods is also proposed and implemented in CUDA’s parallel environment. Finally, the sensitivity of the GLWB option to various factors and the ramifications for insurance companies who sell this guarantee is discussed.||2017|10.3390/w9020099|L. Sterle|1.125|2
194|GPU computing using CUDA in the deployment of smart grids|This paper underscores the use of CUDA-based GPUs as high performance parallel computers for the purpose of real time analysis in a smart grid setting. In a smart grid, with the influx of new, renewable, distributed generation technologies, the network is more complex and requires more computationally intensive means of simulation and analysis. To show its usefulness, a power flow analysis case study will be programmed in CUDA C++ and its performance benchmarked against a sequential CPU counterpart. The results show that the GPU performs better than single-threaded CPU programs, in terms of execution time. A lack of optimization in GPU programs decreases the potential performance benefits, however, as system size increases, the scalability advantages afforded by the CUDA model are evident. The results also show that performance is GPU-platform dependent, i.e. dependent on GPU architecture and power.|Sai|2016|10.1109/SAI.2016.7556141|D. J. Sooknanan, A. Joshi|1.1111111111111112|2
195|Probabilistic multi-sensor fusion based on signed distance functions|In this paper, we present an approach for the probabilistic fusion of 3D sensor measurements. Our fusion algorithm is based on truncated signed distance functions. It explicitly considers the measurement noise by modeling the surface using random variables. Furthermore, our proposed surface model provides an explicit estimation of the spatial uncertainty. The approach can be implemented on a GPU to achieve a high update performance and enable online updates of the model. The approach was evaluated in simulation and using real sensor data. In our experiments, we confirmed that it accurately estimates surfaces from noisy sensor data and that it provides a corresponding estimate of the uncertainty. We could also show that the approach is able to fuse measurements from sensors with different noise characteristics.|IEEE International Conference on Robotics and Automation|2016|10.1109/ICRA.2016.7487333|Kai M. Wurm, Dong Chen, Georg von Wichert, Vincent Dietrich, Philipp Ennen|1.1111111111111112|2
200|Real-World-Time Simulation of Memory Consolidation in a Large-Scale Cerebellar Model|We report development of a large-scale spiking network model of the cerebellum composed of more than 1 million neurons. The model is implemented on graphics processing units (GPUs), which are dedicated hardware for parallel computing. Using 4 GPUs simultaneously, we achieve realtime simulation, in which computer simulation of cerebellar activity for 1 s completes within 1 s in the real-world time, with temporal resolution of 1 ms. This allows us to carry out a very long-term computer simulation of cerebellar activity in a practical time with millisecond temporal resolution. Using the model, we carry out computer simulation of long-term gain adaptation of optokinetic response (OKR) eye movements for 5 days aimed to study the neural mechanisms of posttraining memory consolidation. The simulation results are consistent with animal experiments and our theory of posttraining memory consolidation. These results suggest that realtime computing provides a useful means to study a very slow neural process such as memory consolidation in the brain.|Frontiers in Neuroanatomy|2016|10.3389/fnana.2016.00021|Tadashi Yamazaki, Masato Gosui|1.1111111111111112|2
430|High-performance simulation of fracture in idealized ‘brick and mortar’ composites using adaptive Monte Carlo minimization on the GPU|Simulation of the nonlinear mechanical response of materials with explicit representation of microstructural features is extremely challenging. These models typically involve a very large number of degrees of freedom, and are prone to convergence difficulties when searching for roots to nonlinear equilibrium equations. We focus on an idealized material model that is motivated by the microstructure of synthetic nacre: individual ‘bricks’ (representing ceramic platelets) interact through nonlinear cohesive springs (representing a small volume fraction of polymer that bonds the platelets). The model simulates composite fracture through rupture of the cohesive springs. The problem is cast in terms of energy minimization and is essentially described by ‘nearest neighbor’ interactions. The principal focus of this paper is to illustrate the computational gains achievable by the strategic marriage of robust, global Monte Carlo minimization algorithms to the graphics processing unit architecture, and to describe how they were realized on the Nvidia GPU. Results comparing the computation times for graphics processing unit and central processing unit implementations demonstrate that a new adaptive version of the simulated annealing algorithm yields a speedup of approximately 5 times, whereas the graphics processing unit implementation yields a speed-up of about 16 times over conventional four-core central processing unit implementations. The resulting speed enhancement for adaptive graphics processing unit minimization of a factor of 80 enables a far broader range of simulations than has previously been possible. Simulations involving as many as 300,000 bricks can be performed in hours, as compared to weeks required by central processing unit implementation. Many aspects of this approach are translatable to other physical problems involving energy minimization in systems with large numbers of degrees of freedom.|The international journal of high performance computing applications|2016|10.1177/1094342015593395|J. Pro, R. K. Lim, M. Begley, L. Petzold, M. Utz|1.1111111111111112|2
469|Green computing on graphics processing units|To answer the question ‘How much energy is consumed for a numerical simulation running on Graphic Processing Unit?’, an experimental protocol is here established. The current provided to a graphic processing unit (GPU) during computation is directly measured using amperometric clamps. Signal processing on the intensity of the current of the power supplied to a GPU, with noise reduction technique, gives precise timing of GPU states, which allow establishing an energy consumption model of the GPU. Energy consumption of each operation, memory copy, vector addition, and element wise product is precisely measured to tune and validate the energy consumption model. The accuracy of the proposed energy consumption model compared to measurements is finally illustrated on a conjugate gradient method for a problem discretized by a finite element method. Copyright © 2015 John Wiley & Sons, Ltd.|Concurrency and Computation|2016|10.1002/cpe.3692|Abal-Kassim Cheik Ahamed, A. Suzuki, F. Magoulès|1.1111111111111112|2
1455|Artillery Survivability Model|Abstract : This work investigates the use of modern simulation techniques for evaluating artillery movement doctrine. A simulation called the Artillery Survivability Model was created as a proof of principle. The simulation incorporates the most salient features relating to artillery survivability according to our small-scale survey of expert opinion on this subject. It consists of a 3D agent based simulation that incorporates AI technology that is novel to this domain, including terrain analysis, advanced movement planning, and GPU-based particle filters to represent enemy anticipation of friendly artillery behavior. The simulation has been created with the popular game engine Unity 3D, and has two different modes. The first is the experiment mode, which is executed from command line without rendering any image, and runs up to 50 times faster than the real-time simulation. Therefore, it is a suitable platform to perform multiple runs for experimenting. The experiment mode also enables users to set their own design of experiment by manipulating an editable CSV file. The second one is a real-time mode that renders a 3D virtual environment of a restricted battlefield where the survivability movements of an artillery company are visualized. This mode provides detailed visualization of the simulation and enables future experimental uses of the simulation as a training tool.||2016|10.1016/j.jbiomech.2016.01.039|Y. Temiz|1.1111111111111112|2
180|Parallel Thermal Analysis of 3-D Integrated Circuits With Liquid Cooling on CPU-GPU Platforms|In this brief, we propose an efficient parallel finite difference-based thermal simulation algorithm for 3-D-integrated circuits (ICs) using generalized minimum residual method (GMRES) solver on CPU-graphic processing unit (GPU) platforms. First, the new method starts from basic physics-based heat equations to model 3-D-ICs with intertier liquid cooling microchannels and directly solves the resulting partial differential equations. Second, we develop a new parallel GPU-GMRES solver to compute the resulting thermal systems on a CPU-GPU platform. We also explore different preconditioners (implicit and explicit) and study their performances on thermal circuits and other types of matrices. Experimental results show the proposed GPU-GMRES solver can deliver orders of magnitudes speedup over the parallel LU-based solver and up to 4× speedup over CPU-GMRES for both dc and transient thermal analyzes on a number of thermal circuits and other published problems.|IEEE Transactions on Very Large Scale Integration (VLSI) Systems|2015|10.1109/TVLSI.2014.2309617|S. Tan, Kai He, Xuexin Liu, Wenjian Yu, Zao Liu, Kuangya Zhai|1.1|2
329|Evaluation of the Training Process of three different Prognostic Approaches based on the Gaussian Process|Data-driven prognostic approaches like Gaussian Process combined with Unscented Kalman Filter (GPUKF) are promising methods for predicting the Remaining Useful Lifetime (RUL) of a degrading component. Whereas the Gaussian Process (GP) is appropriate to derive a suitable degradation model by means of a set of training data, the Unscented Kalman Filter (UKF) employs this model to determine the prediction and its uncertainty. Since a degradation process is highly stochastic, it is assumed that by applying more sets of training data the accuracy and precision of the GPUKF is increased. In order to examine the performance enhancement two different approaches are investigated in this paper: First, a single GP is trained with all available data sets. The second approach combines several GPs (each created with a data set of one degradation process) by extending the GPUKF with a Multiple Model Method. The development of a third prognostic approach aims at the investigation of the UKF as a suitable tool for the prognostic algorithm. Therefore, a third method applies a Particle Filter in combination with the GP. For the evaluation of the aforementioned prognosis algorithms according to their precision and accuracy a set of prevalent performance metrics like the Prognostic Horizon and the Mean Average Percentage Error of a prediction is analyzed. The validity of the determined results is increased by considering the variance of certain metrics over several units under test. Moreover, particular focus is set on the examination of the performance change caused by the use of more training data sets. In order to quantify this process known metrics are extended. The evaluation is based on simulated data sets, which are generated by an exponential degradation model. Christian Preusche et. al. This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 United States License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. The analysis of the implemented algorithms indicates that the applied metrics are in a comparable range. However, the three approaches reveal a different behaviour concerning the convergence of the performance values according to the number of training data. In particular cases there is even a decline in accuracy and precision attend by a rising number of training data.||2015|10.1017/s1368980015002499|C. Preusche, C. Anger, U. Klingauf|1.1|2
1158|Ray Tracing for Simulation of Millimeter-Wave Whole Body Imaging Systems|A ray tracing algorithm for modeling millimeter waves in a whole body imaging system is presented. Ray tracing is a well-known method for approximating high-frequency wave behavior and is well suited for implementation on graphics processing units (GPUs), presenting computational speed advantages over conventional full-wave modeling techniques. This method leverages the NVIDIA OptiX engine to ensure computational efficiency. Numerical results in this work are compared with conventional two-dimensional method of moments solutions to assess accuracy and computational times are compared with a three-dimensional GPU implementation of the modified equivalent current approximation.|IEEE Transactions on Antennas and Propagation|2015|10.1109/TAP.2015.2486801|Zhongliang Chen, B. Gonzalez-Valdes, J. Martinez, Luis Tirado, C. Rappaport, Kathryn Williams|1.1|2
199|A Performance Study of CUDA UVM versus Manual Optimizations in a Real-World Setup: Application to a Monte Carlo Wave-Particle Event-Based Interaction Model|The performance of a Monte Carlo model for the simulation of electromagnetic wave propagation in particle-filled atmospheres has been conducted for different CUDA versions and design approaches. The proposed algorithm exhibits a high degree of parallelism, which allows favorable implementation in a GPU. Practical implementation aspects of the model have been also explained and their impact assessed, such as the use of the different types of memories present in a GPU. A number of setups have been chosen in order to compare performance for manually optimized versus Unified Virtual Memory (UVM ) implementations for different CUDA versions. Features and relative performance impact of the different options have been discussed, extracting practical hints and rules useful to speed up CUDA programs.|IEEE Transactions on Parallel and Distributed Systems|2016|10.1109/TPDS.2015.2463813|Jose M. Nadal-Serrano, Marisa López-Vallejo|1.0|2
255|PI-FLAME: A parallel immune system simulator using the FLAME graphic processing unit environment|Agent-based models (ABMs) are increasingly being used to study population dynamics in complex systems, such as the human immune system. Previously, Folcik et al. (The basic immune simulator: an agent-based model to study the interactions between innate and adaptive immunity. Theor Biol Med Model 2007; 4: 39) developed a Basic Immune Simulator (BIS) and implemented it using the Recursive Porous Agent Simulation Toolkit (RePast) ABM simulation framework. However, frameworks such as RePast are designed to execute serially on central processing units and therefore cannot efficiently handle large model sizes. In this paper, we report on our implementation of the BIS using FLAME GPU, a parallel computing ABM simulator designed to execute on graphics processing units. To benchmark our implementation, we simulate the response of the immune system to a viral infection of generic tissue cells. We compared our results with those obtained from the original RePast implementation for statistical accuracy. We observe that our implementation has a 13× performance advantage over the original RePast implementation.|International Conference on Advances in System Simulation|2017|10.1177/0037549716673724|Shailesh Tamrakar, P. Richmond, R. D'Souza|1.0|2
259|Efficiency Enhancement of PIC-MCC Modeling for Magnetron Sputtering Simulations Using GPU Parallelization|A particle-in-cell Monte Carlo collision (PIC-MCC) model is presented, parallelized to be suitable for magnetron sputtering simulations on general purpose graphics processing units (GPGPUs). To reduce the large computation time generally required to calculate particle-electromagnetic field interactions, a numerical algorithm is optimized to obtain the best performance with GPGPU parallelization. The efficiency and accuracy of the GPGPU parallelized PIC-MCC model are examined by comparing the calculated results and the corresponding computational time with analytical solutions and the computation time of the serial code, respectively. The erosion and deposition rates during sputtering predicted using this model are in good agreement with experimental results. The newly developed PIC-MCC model with GPGPU parallelization is thereby shown to be both efficient and accurate.|IEEE Transactions on Plasma Science|2016|10.1109/TPS.2016.2593491|Jinpil Lee, Junkyeong Bae, Jihun Kim, I. Sohn|1.0|2
272|Efficient synthetic traffic models for large, complex SoCs|"The interconnect or network on chip (NoC) is an increasingly important component in processors. As systems scale up in size and functionality, the ability to efficiently model larger and more complex NoCs becomes increasingly important to the design and evaluation of such systems. Recent work proposed the ""SynFull"" methodology that performs statistical analysis of a workload's NoC traffic to create compact traffic generators based on Markov models. While the models generate synthetic traffic, the traffic is statistically similar to the original trace and can be used for fast NoC simulation. However, the original SynFull work only evaluated multi-core CPU scenarios with a very simple cache coherence protocol (MESI). We find the original SynFull methodology to be insufficient when modeling the NoC of a more complex system on a chip (SoC). We identify and analyze the shortcomings of SynFull in the context of a SoC consisting of a heterogeneous architecture (CPU and GPU), a more complex cache hierarchy including support for full coherence between CPU, GPU, and shared caches, and heterogeneous workloads. We introduce new techniques to address these shortcomings. Furthermore, the original SynFull methodology can only model a NoC with N nodes when the original application analysis is performed on an identically-sized N-node system, but one typically wants to model larger future systems. Therefore, we introduce new techniques to enable SynFull-like analysis to be extrapolated to model such larger systems. Finally, we present a novel synthetic memory reference model to replace SynFull's fixed latency model; this allows more realistic evaluation of the memory subsystem and its interaction with the NoC. The result is a robust NoC simulation methodology that works for large, heterogeneous SoC architectures."|International Symposium on High-Performance Computer Architecture|2016|10.1109/HPCA.2016.7446073|G. Loh, Natalie D. Enright Jerger, Matthew Poremba, Onur Kayiran, Jieming Yin|1.0|2
320|Extending the Solvation-Layer Interface Condition Continum Electrostatic Model to a Linearized Poisson-Boltzmann Solvent.|We extend the linearized Poisson-Boltzmann (LPB) continuum electrostatic model for molecular solvation to address charge-hydration asymmetry. Our new solvation-layer interface condition (SLIC)/LPB corrects for first-shell response by perturbing the traditional continuum-theory interface conditions at the protein-solvent and the Stern-layer interfaces. We also present a GPU-accelerated treecode implementation capable of simulating large proteins, and our results demonstrate that the new model exhibits significant accuracy improvements over traditional LPB models, while reducing the number of fitting parameters from dozens (atomic radii) to just five parameters, which have physical meanings related to first-shell water behavior at an uncharged interface. In particular, atom radii in the SLIC model are not optimized but uniformly scaled from their Lennard-Jones radii. Compared to explicit-solvent free-energy calculations of individual atoms in small molecules, SLIC/LPB is significantly more accurate than standard parametrizations (RMS error 0.55 kcal/mol for SLIC, compared to RMS error of 3.05 kcal/mol for standard LPB). On parametrizing the electrostatic model with a simple nonpolar component for total molecular solvation free energies, our model predicts octanol/water transfer free energies with an RMS error 1.07 kcal/mol. A more detailed assessment illustrates that standard continuum electrostatic models reproduce total charging free energies via a compensation of significant errors in atomic self-energies; this finding offers a window into improving the accuracy of Generalized-Born theories and other coarse-grained models. Most remarkably, the SLIC model also reproduces positive charging free energies for atoms in hydrophobic groups, whereas standard PB models are unable to generate positive charging free energies regardless of the parametrized radii. The GPU-accelerated solver is freely available online, as is a MATLAB implementation.|Journal of Chemical Theory and Computation|2017|10.1021/acs.jctc.6b00832|J. Bardhan, M. Knepley, A. Molavi Tabrizi, Ali Mehdizadeh Rahimi, C. D. Cooper, S. Goossens|1.0|2
330|High-Performance Agent-Based Modeling Applied to Vocal Fold Inflammation and Repair|Fast and accurate computational biology models offer the prospect of accelerating the development of personalized medicine. A tool capable of estimating treatment success can help prevent unnecessary and costly treatments and potential harmful side effects. A novel high-performance Agent-Based Model (ABM) was adopted to simulate and visualize multi-scale complex biological processes arising in vocal fold inflammation and repair. The computational scheme was designed to organize the 3D ABM sub-tasks to fully utilize the resources available on current heterogeneous platforms consisting of multi-core CPUs and many-core GPUs. Subtasks are further parallelized and convolution-based diffusion is used to enhance the performance of the ABM simulation. The scheme was implemented using a client-server protocol allowing the results of each iteration to be analyzed and visualized on the server (i.e., in-situ) while the simulation is running on the same server. The resulting simulation and visualization software enables users to interact with and steer the course of the simulation in real-time as needed. This high-resolution 3D ABM framework was used for a case study of surgical vocal fold injury and repair. The new framework is capable of completing the simulation, visualization and remote result delivery in under 7 s per iteration, where each iteration of the simulation represents 30 min in the real world. The case study model was simulated at the physiological scale of a human vocal fold. This simulation tracks 17 million biological cells as well as a total of 1.7 billion signaling chemical and structural protein data points. The visualization component processes and renders all simulated biological cells and 154 million signaling chemical data points. The proposed high-performance 3D ABM was verified through comparisons with empirical vocal fold data. Representative trends of biomarker predictions in surgically injured vocal folds were observed.|Frontiers in Physiology|2018|10.3389/fphys.2018.00304|J. JáJá, Nuttiiya Seekhao, C. Shung, N. Y. Li-Jessen, L. Mongeau|1.0|2
338|Simulating molecular docking with haptics|Intermolecular binding underlies various metabolic and regulatory processes of the \ncell, and the therapeutic and pharmacological properties of drugs. Molecular docking \nsystems model and simulate these interactions in silico and allow the study of the \nbinding process. In molecular docking, haptics enables the user to sense the interaction \nforces and intervene cognitively in the docking process. Haptics-assisted docking \nsystems provide an immersive virtual docking environment where the user can interact \nwith the molecules, feel the interaction forces using their sense of touch, identify \nvisually the binding site, and guide the molecules to their binding pose. Despite a \nforty-year research e�ort however, the docking community has been slow to adopt this \ntechnology. Proprietary, unreleased software, expensive haptic hardware and limits \non processing power are the main reasons for this. Another signi�cant factor is the \nsize of the molecules simulated, limited to small molecules. \nThe focus of the research described in this thesis is the development of an interactive \nhaptics-assisted docking application that addresses the above issues, and enables \nthe rigid docking of very large biomolecules and the study of the underlying interactions. \nNovel methods for computing the interaction forces of binding on the CPU \nand GPU, in real-time, have been developed. The force calculation methods proposed \nhere overcome several computational limitations of previous approaches, such as precomputed \nforce grids, and could potentially be used to model molecular \nexibility \nat haptic refresh rates. Methods for force scaling, multipoint collision response, and \nhaptic navigation are also reported that address newfound issues, particular to the \ninteractive docking of large systems, e.g. force stability at molecular collision. The \ni \nii \nresult is a haptics-assisted docking application, Haptimol RD, that runs on relatively \ninexpensive consumer level hardware, (i.e. there is no need for specialized/proprietary \nhardware).||2015|10.1016/j.jmgm.2015.06.003|Georgios Iakovou|1.0|2
363|Efficient implementation of quantum materials simulations on distributed CPU-GPU systems|We present a scalable implementation of the Linearized Augmented Plane Wave method for distributed memory systems, which relies on an efficient distributed, block-cyclic setup of the Hamiltonian and overlap matrices and allows us to turn around highly accurate 1000+ atom all-electron quantum materials simulations on clusters with a few hundred nodes. The implementation runs efficiently on standard multi-core CPU nodes, as well as hybrid CPU-GPU nodes. The key for the latter is a novel algorithm to solve the generalized eigenvalue problem for dense, complex Hermitian matrices on distributed hybrid CPU-GPU systems. Performance tests for Li-intercalated CoO2 supercells containing 1501 atoms demonstrate that high-accuracy, transferable quantum simulations can now be used in throughput materials search problems. While our application can benefit and get scalable performance through CPU-only libraries like ScaLAPACK or ELPA2, our new hybrid solver enables the efficient use of GPUs and shows that a hybrid CPU-GPU architecture scales to a desired performance using substantially fewer cluster nodes, and notably, is considerably more energy efficient than the traditional multi-core CPU only systems for such complex applications.|International Conference for High Performance Computing, Networking, Storage and Analysis|2015|10.1145/2807591.2807654|J. Dongarra, S. Tomov, T. Schulthess, R. Solcà, A. Kozhevnikov, A. Haidar|1.0|2
393|A GPU Implementation of an Explicit Compact FDTD Algorithm with a Digital Impedance Filter for Room Acoustics Applications|In recent years, computational engineering has undergone great changes due to the development of the graphics processing unit (GPU) technology. For example, in room acoustics, the wave-based methods, that formerly were considered too expensive for 3-D impulse response simulations, are now chosen to exploit the parallel nature of GPU devices considerably reducing the execution time of the simulations. There exist contributions related to this topic that have explored the performance of different GPU algorithms; however, the computational analysis of a general explicit model that incorporates algorithms with different neighboring orders and a general frequency dependent impedance boundary model has not been properly developed. In this paper, we present a GPU implementation of a complete room acoustic model based on a family of explicit finite-difference time-domain (FDTD) algorithms. We first develop a strategy for implementing a frequency independent (FI) impedance model which is free from thread divergences and then, we extend the model adding a digital impedance filter (DIF) boundary subroutine able to compute the acoustic pressure of different nodes such as corners or edges without an additional performance penalty. Both implementations are validated and deeply analyzed by performing different 3-D numerical experiments. Finally, we define a performance metric which is able to objectively measure the computing throughput of a FDTD implementation using a simple number. The robustness of this metric allows us to compare algorithms even if these have been run in different GPU cards or have been formulated with other explicit models.|IEEE/ACM Transactions on Audio Speech and Language Processing|2015|10.1109/TASLP.2015.2434212|C. Spa, E. Hernández, Anton Rey|1.0|2
399|Statistical pattern based modeling of GPU memory access streams|Recent research studies have shown that modern GPU performance is often limited by the memory system performance. Optimizing memory hierarchy performance requires GPU designers to draw design insights based on the cache & memory behavior of end-user applications. Unfortunately, it is often difficult to get access to end-user workloads due to the confidential or proprietary nature of the software/data. Furthermore, the efficiency of early design space exploration of cache & memory systems is often limited due to either the slow speed of detailed simulation techniques or limited scope of state-of-the-art cache analytical models. To enable efficient GPU memory system exploration, we present a novel methodology and framework that statistically models the GPU memory access stream locality. The proposed G-MAP (GPU Memory Access Proxy) framework models the regularity in codelocalized memory access patterns of GPGPU applications and the parallelism in GPU's execution model to create miniaturized memory proxies. We evaluate G-MAP using 18 GPGPU benchmarks and show that G-MAP proxies can replicate cache/memory performance of original applications with over 90% accuracy across over 5000 different L1/L2 cache, prefetcher and memory configurations.|Design Automation Conference|2017|10.1145/3061639.3062320|Jiajun Wang, Reena Panda, L. John, Xinnian Zheng, A. Gerstlauer|1.0|2
402|A Novel Implementation of 2D3V Particle-in-Cell (PIC) Algorithm for Kepler GPU Architecture|The implementation of 2D-3v (2D in space and 3D in velocity space) PIC-MCC (Particle-In-Cell Monte Carlo Collision) method described in this paper involves the computational solution of Vlasov-Poisson equations, which provides the spatial and temporal evolution of the charged-particle velocity distribution functions in plasmas under the effect of self-consistent electromagnetic (EM) fields and collisions. Stringent numerical constraints associated with a PIC code makes it computationally prohibitive on CPUs in case of large problem sizes (total number of particles, number of grid points and simulation time-scale). We present the design and implementation of a Graphics Processing Unit (GPU) based 2D-3v PIC code using the CUDA C APIs for Kepler architecture. Several parallelization and optimization techniques have been presented in this paper with special emphasis on shuffle intrinsic specific to Nvidia Kepler architecture (or later), which significantly improves the performance compared to existing GPU implementations in the literature. On a test bed comprising of a serial implementation on Xeon E5 CPU and parallel implementations on Nvidia Tesla K40 graphics card, we have achieved a speedup of up to 60x in double precision mode. Effect of important numerical parameters on speedup has been investigated. Finally, we compare the performance of our best parallel implementation on different GPUs (Kepler as well as Maxwell) and analyze the effect of hardware architecture on the performance of the PIC code.|International Conference on High Performance Computing|2017|10.1109/HiPC.2017.00050|Harshil Shah, Siddharth Kamaria, Miral Shah, B. Chaudhury, Riddhesh Markandeya|1.0|2
407|Lightweight Hardware Transactional Memory for GPU Scratchpad Memory|Graphics Processing Units (GPUs) have become the accelerator of choice for data-parallel applications, enabling the execution of thousands of threads in a Single Instruction - Multiple Thread (SIMT) fashion. Using OpenCL terminology, GPUs offer a global memory space shared by all the threads in the GPU, as well as a local memory space shared by only a subset of the threads. Programmers can use local memory as a scratchpad to improve the performance of their applications due to its lower latency as compared to global memory. In the SIMT execution model, data locking mechanisms used to protect shared data limit scalability. To take full advantage of the lower latency that local memory affords, and to provide an efficient synchronization mechanism, we propose GPU-LocalTM as a lightweight and efficient transactional memory (TM) for GPU local memory. To minimize the storage resources required for TM support, GPU-LocalTM allocates transactional metadata in the existing memory resources. Additionally, GPU-LocalTM implements different conflict detection mechanisms that can be used to match the characteristics of the application. For the workloads studied in our simulation-based evaluation, GPU-LocalTM provides from 1.1X up to 100X speedup over serialized critical sections.|IEEE transactions on computers|2018|10.1109/TC.2017.2776908|O. Plata, D. Kaeli, Alejandro Villegas, R. Asenjo, A. Navarro|1.0|2
411|Interactive Large‐Scale Procedural Forest Construction and Visualization Based on Particle Flow Simulation|Interactive visualization of large forest scenes is challenging due to the large amount of geometric detail that needs to be generated and stored, particularly in scenarios with a moving observer such as forest walkthroughs or overflights. Here, we present a new method for large‐scale procedural forest generation and visualization at interactive rates. We propose a hybrid approach by combining geometry‐based and volumetric modelling techniques with gradually transitioning level of detail (LOD). Nearer trees are constructed using an extended particle flow algorithm, in which particle trails outline the tree ramification in an inverse direction, i.e. from the leaves towards the roots. Reduced geometric representation of a tree is obtained by subsampling the trails. For distant trees, a new volumetric rendering technique in pixel‐space is introduced, which avoids geometry formation altogether and enables visualization of vast forest areas with millions of unique trees. We demonstrate that a GPU‐based implementation of the proposed method provides interactive frame rates in forest overflight scenarios, where new trees are constructed and their LOD adjusted on the fly.|Computer graphics forum (Print)|2018|10.1111/cgf.13304|D. Strnad, Štefan Kohek|1.0|2
463|Effects of Numerical Precision on Simulation of Massive Spatial Data on GPU Based on Modified Planar Rotator Model|The present research builds on a recently proposed spatial prediction method for gridded data, based on a suitably modified planar rotator (MPR) spin model from statistical physics. This approach maps the measured data onto interacting spins and, exploiting spatial correlations between them, which are similar to those present in geostatistical data, predicts the data at unmeasured locations. Due to the short-range nature of the spin pair interactions in the MPR model, parallel implementation of the prediction algorithm on graphical processing units (GPUs), can achieve impressive speedups, compared to the CPU implementation. In this work we study the effects of reduced computing precision as well as GPU-based hardware intrinsic functions on the speedup and accuracy of the MPR-based prediction and explore which aspects of the simulation can potentially benefit the most from the reduced precision. It is found that, particularly for massive data sets, a thoughtful precision setting of the GPU implementation can significantly increase the computational effciency, while incurring little to no degradation in the prediction accuracy.||2019|10.1007/s11004-019-09835-3|M. Borovsk'y, M. vZukovivc, M. Lach|1.0|2
491|Real-World Oceanographic Simulations on the GPU using a Two-Dimensional Finite-Volume Scheme|In this work, we take a modern high-resolution finite-volume scheme for solving the rotational shallow-water equations and extend it with features required to run real-world ocean simulations. Our contributions include a spatially varying north vector and Coriolis term required for large scale domains, moving wet-dry fronts, a static land mask, bottom shear stress, wind forcing, boundary conditions for nesting in a global model, and an efficient model reformulation that makes it well-suited for massively parallel implementations. Our model order is verified using a grid convergence test, and we show numerical experiments using three different sections along the coast of Norway based on data originating from operational forecasts run at the Norwegian Meteorological Institute. Our simulation framework shows perfect weak scaling on a modern P100 GPU, and is capable of providing tidal wave forecasts that are very close to the operational model at a fraction of the cost. All source code and data used in this work are publicly available under open licenses.|arXiv.org|2019|10.1007/978-3-030-17747-8_10|H. Holm, A. Brodtkorb|1.0|2
550|Full-GPU Reservoir Simulation Delivers on its Promise for Giant Carbonate Fields|Summary Simulation of carbonate fields presents challenges due to the underlying multi-scale heterogeneities and consequent stiff nature of the flow equations. This paper highlights the principles of a full-GPU (Graphics Processing Unit) reservoir simulator, currently approaching feature parity with traditional CPU-based codes. The approach exhibits fine-grained parallelism beyond that of CPU-based and hybrid CPU-GPU solutions; consequent performance improvements enable modeling of giant carbonate fields with limited computing resources. Additionally, large black-oil models are memory-bound, and GPU bandwidth has shown significant progress with every generational release of new hardware. Performance will keep improving without changes in the code base, which has not been observed with CPU codes in almost two decades. Computational performance of a full-GPU black-oil reservoir simulator is benchmarked against legacy and modern parallel CPU simulators, for two giant gas and oil carbonate reservoirs. Results for the gas reservoir indicate a ∼7.3x chip-to-chip speed improvement (one GPU vs. to 16 CPU cores), and ∼5.5x for the oil reservoir, both against the fastest reference simulator. These results suggest that full-GPU codes are ready to simulate complex carbonate models of commercial grade, with exceptional performance, which should encourage the industry to pursue research and development efforts geared towards this approach.|Third EAGE WIPIC Workshop: Reservoir Management in Carbonates|2019|10.3997/2214-4609.201903118|R. Gandham, A. Cominelli, A. Vidyasagar, K. Mukundakrishnan, L. Patacchini, P. Panfili, F. Caresani|1.0|2
581|A massively scalable distributed multigrid framework for nonlinear marine hydrodynamics|The focus of this article is on the parallel scalability of a distributed multigrid framework, known as the DTU Compute GPUlab Library, for execution on graphics processing unit (GPU)-accelerated supercomputers. We demonstrate near-ideal weak scalability for a high-order fully nonlinear potential flow (FNPF) time domain model on the Oak Ridge Titan supercomputer, which is equipped with a large number of many-core CPU-GPU nodes. The high-order finite difference scheme for the solver is implemented to expose data locality and scalability, and the linear Laplace solver is based on an iterative multilevel preconditioned defect correction method designed for high-throughput processing and massive parallelism. In this work, the FNPF discretization is based on a multi-block discretization that allows for large-scale simulations. In this setup, each grid block is based on a logically structured mesh with support for curvilinear representation of horizontal block boundaries to allow for an accurate representation of geometric features such as surface-piercing bottom-mounted structures—for example, mono-pile foundations as demonstrated. Unprecedented performance and scalability results are presented for a system of equations that is historically known as being too expensive to solve in practical applications. A novel feature of the potential flow model is demonstrated, being that a modest number of multigrid restrictions is sufficient for fast convergence, improving overall parallel scalability as the coarse grid problem diminishes. In the numerical benchmarks presented, we demonstrate using 8192 modern Nvidia GPUs enabling large-scale and high-resolution nonlinear marine hydrodynamics applications.|The international journal of high performance computing applications|2019|10.1177/1094342019826662|S. L. Glimberg, Luke N. Olson, A. Engsig-Karup|1.0|2
586|Task‐based parallel strategies for computational fluid dynamic application in heterogeneous CPU/GPU resources|Parallel applications executing in contemporary heterogeneous clusters are complex to code and optimize. The task‐based programming model is an alternative to handle the coding complexity. This model consists of splitting the problem domain into tasks with dependencies through a directed acyclic graph, and submit the set of tasks to a runtime scheduler that maps each task dynamically to resources. We consider that computational fluid dynamics applications are typical in scientific computing but not enough exploited by designs that employ the task‐based programming model. This article presents task‐based parallel strategies for a simple CFD application that targets heterogeneous multi‐CPU/multi‐GPU computing resources. We design, develop, evaluate, and compare the performance of three parallel strategies (naive, ghost‐cells, and arrow) of a task‐based heterogeneous (CPU and GPU) application that simulates the flow of an incompressible Newtonian fluid with constant viscosity. All implementations rely on the StarPU runtime, and we use the StarVZ toolkit to conduct comprehensive performance analysis. Results indicate that the ghost cell strategy provides the best speedup (77×) considering the simulation time when the GPU resources still have available memory. However, the arrow strategy achieves better results when the simulation data increases.|Concurrency and Computation|2020|10.1002/cpe.5772|M. Serpa, P. Navaux, L. Schnorr, Lucas Leandro Nesi|1.0|2
619|Hierarchical Device-Level Modular Multilevel Converter Modeling for Parallel and Heterogeneous Transient Simulation of HVDC Systems|System-level electromagnetic transient (EMT) simulation of large-scale power converters with high-order nonlinear semiconductor switch models remains a challenge albeit it is essential for design preview. In this work, a multi-layer hierarchical modeling methodology is proposed for high-performance computing of the modular multilevel converter involving device-level IGBT/diode models. The computational burden induced by converter scale and model complexity is dramatically alleviated following the proposal of topological reconfiguration and network equivalence, which create a substantial number of identical circuit units that facilitate massively parallel processing on the graphics processing unit (GPU), using the kernel-based single-instruction multi-threading computing architecture. As the DC system brings significant inhomogeneity which dilutes parallelism, heterogeneous computing is investigated and the computational tasks are properly assigned to CPU and GPU to fully exploit their respective features. The separation of nonlinear device-level models from the rest of the system enables multi-rate implementation for further efficiency enhancement since the two parts allow distinct time-steps. A remarkable acceleration of over 50 times is achieved by the hybrid CPU/GPU platform over conventional CPU simulation, and the validity of the proposed modeling and computing method is confirmed by commercial EMT tools ANSYS/Simplorer and PSCAD/EMTDC.|IEEE Open Journal of Power Electronics|2020|10.1109/OJPEL.2020.3016296|Ning Lin, V. Dinavahi, Ruimin Zhu|1.0|2
654|Tinker-HP : Accelerating Molecular Dynamics Simulations of Large Complex Systems with Advanced Point Dipole Polarizable Force Fields using GPUs and Multi-GPUs systems|We present the extension of the Tinker-HP package (Lagard\`ere et al., Chem. Sci., 2018,9, 956-972) to the use of Graphics Processing Unit (GPU) cards to accelerate molecular dynamics simulations using polarizable many-body force fields. The new high-performance module allows for an efficient use of single- and multi-GPU architectures ranging from research laboratories to modern pre-exascale supercomputer centers. After detailing an analysis of our general scalable strategy that relies on OpenACC and CUDA, we discuss the various capabilities of the package. Among them, the multi-precision possibilities of the code are discussed. If an efficient double precision implementation is provided to preserve the possibility of fast reference computations, we show that a lower precision arithmetic is preferred providing a similar accuracy for molecular dynamics while exhibiting superior performances. As Tinker-HP is mainly dedicated to accelerate simulations using new generation point dipole polarizable force field, we focus our study on the implementation of the AMOEBA model and provide illustrative benchmarks of the code for single- and multi-cards simulations on large biosystems encompassing up to millions of atoms.The new code strongly reduces time to solution and offers the best performances ever obtained using the AMOEBA polarizable force field. Perspectives toward the strong-scaling performance of our multi-node massive parallelization strategy, unsupervised adaptive sampling and large scale applicability of the Tinker-HP code in biophysics are discussed. The present software has been released in phase advance on GitHub in link with the High Performance Computing community COVID-19 research efforts and is free for Academics (see https://github.com/TinkerTools/tinker-hp).|arXiv.org|2020|10.1021/acs.jctc.9b01204|O. Adjoua, A. Durocher, Pengyu Y. Ren, Thibaut Very, I. Dupays, Théo Jaffrelot Inizan, Frédéric Célerse, Zhi Wang, Louis Lagardère, Luc-Henri Jolly, Jean‐Philip Piquemal, J. Ponder|1.0|2
660|Massively Parallel Circuit Setup in GPU-SPICE|SPICE simulations are the industry standard to analyze circuits for decades. However, they are computationally complex as each circuit is simulated at the transistor-level where individual transistor is modeled with dozens of sophisticated equations. This limits the practicality of SPICE simulations to relatively small circuits. However, this is in a direct conflict with the ever-increasing demands of circuit designers in which SPICE simulations for large circuits (e.g., DSPs, AES, etc.) at full accuracy are inevitably required to fulfill new industrial standards like automotive safety ISO 26262 with tool confidence level 1. To accelerate SPICE simulation without sacrificing accuracy, state-of-the-art approaches have started to employ GPUs to parallelize the LU-factorization and device linearization phases. Instead of focusing on these phases, this article demonstrates for the first time that when large circuits come into play, a new and equally important performance bottleneck emerges at the circuit setup phase. Speeding up the circuit setup phase in SPICE is our key focus in this paper. Our two implementations demonstrate that our GPU-based circuit setup reduces the analysis time from 4.5 days to merely 89 seconds for a 256-bit multiplier, which consists of more than 1M transistors. Our achieved speedup is 4396x compared to the baseline (open-source NGSPICE) and more than 2x compared to commercial (HSPICE and Spectre) SPICE circuit setup.|IEEE transactions on computers|2020|10.1109/TC.2020.3032343|Fu Lam Florian Diep, H. Amrouch, Victor M. van Santen, J. Henkel|1.0|2
696|Multi-GPU SNN Simulation with Perfect Static Load Balancing|We present a SNN simulator which scales to millions of neurons, billions of synapses, and 8 GPUs. This is made possible by 1) a novel, cache-aware spike transmission algorithm 2) a model parallel multi-GPU distribution scheme and 3) a static, yet very effective load balancing strategy. The simulator further features an easy to use API and the ability to create custom models. We compare the proposed simulator against two state of the art ones on a series of benchmarks using three wellestablished models. We find that our simulator is faster, consumes less memory, and scales linearly with the number of GPUs.|arXiv.org|2021|10.1109/ijcnn52387.2021.9533921|Antonis A. Argyros, Dennis Bautembach, I. Oikonomidis|1.0|2
712|GPU Acceleration of the HemeLB Code for Lattice Boltzmann Simulations in Sparse Complex Geometries|We present an implementation and scaling analysis of a GPU-accelerated kernel for HemeLB, a high-performance Lattice Boltzmann code for sparse complex geometries. We describe the structure of the GPU implementation and we study the scalability of HemeLB on a GPU cluster under normal operating conditions and with real-world application cases. We investigate the effect of CUDA block size and GPU over-subscription on the single-GPU performance, and we present a strong-scaling analysis of multi-GPU parallel simulations using two different hardware models (P100 and V100) and a variety of large cerebral aneurysm geometries. We find that HemeLB-GPU achieves single-GPU speedups of $50\times$ (P100) and $100\times$ (V100) compared to a single CPU core, with good scalability up to 32 GPUs. We also discuss strategies to improve both the kernel performance as well as the scalability of HemeLB-GPU to a larger number of GPUs. The GPU implementation supports the LBGK collision kernel, boundary conditions for walls and inlets/outlets, and several lattice types (D3Q15, D3Q19, D3Q27), and it integrates seamlessly with the existing infrastructure in HemeLB for graph partitioning and parallelization via MPI. It is expected that the GPU implementation will enable users of the HemeLB code to make better utilization of heterogeneous high-performance computing systems for large-scale lattice Boltzmann simulations.|IEEE Access|2021|10.1109/ACCESS.2021.3073667|Benjamin T. Shealy, Mehrdad Yousefi, A. Srinath, Melissa C. Smith, U. Schiller|1.0|2
714|GPU Acceleration of 3D Agent-Based Biological Simulations|Researchers in biology are faced with the tough challenge of developing high-performance computer simulations of their increasingly complex agent-based models. BioDynaMo is an open-source agent-based simulation platform that aims to alleviate researchers from the intricacies that go into the development of high-performance computing. Through a high-level interface, researchers can implement their models on top of BioDynaMo's multi-threaded core execution engine to rapidly develop simulations that effectively utilize parallel computing hardware. In biological agent-based modeling, the type of operations that are typically the most compute-intensive are those that involve agents interacting with their local neighborhood. In this work, we investigate the currently implemented method of handling neighborhood interactions of cellular agents in BioDynaMo, and ways to improve the performance to enable large-scale and complex simulations. We propose to replace the kd-tree implementation to find and iterate over the neighborhood of each agent with a uniform grid method that allows us to take advantage of the massively parallel architecture of graphics processing units (GPUs). We implement the uniform grid method in both CUDA and OpenCL to address GPUs from all major vendors and evaluate several techniques to further improve the performance. Furthermore, we analyze the performance of our implementations for models with a varying density of neighboring agents. As a result, the performance of the mechanical interactions method improved by up to two orders of magnitude in comparison to the multithreaded baseline version. The implementations are open-source and publicly available on Github.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2021|10.1109/IPDPSW52791.2021.00040|Ahmad Hesam, Lukas Breitwieser, Z. Al-Ars, F. Rademakers|1.0|2
731|Modeling and Simulation of Tsunami Impact: A Short Review of Recent Advances and Future Challenges|Tsunami modeling and simulation has changed in the past few years more than it has in decades, especially with respect to coastal inundation. Among other things, this change is supported by the approaching era of exa-scale computing, whether via GPU or more likely forms of hybrid computing whose presence is growing across the geosciences. For reasons identified in this review, exa-scale computing efforts will impact the on-shore, highly turbulent régime to a higher degree than the 2D shallow water equations used to model tsunami propagation in the open ocean. This short review describes the different approaches to tsunami modeling from generation to impact and underlines the limits of each model based on the flow régime. Moreover, from the perspective of a future comprehensive multi-scale modeling infrastructure to simulate a full tsunami, we underline the current challenges associated with this approach and review the few efforts that are currently underway to achieve this goal. A table of existing tsunami software packages is provided along with an open Github repository to allow developers and model users to update the table with additional models as they are published and help with model discoverability.|Géosciences|2020|10.20944/preprints202010.0394.v2|S. Marras, K. Mandli|1.0|2
739|Multi-GPU SNN Simulation with Static Load Balancing|We present a SNN simulator which scales to millions of neurons, billions of synapses, and 8 GPUs. This is made possible by 1) a novel, cache-aware spike transmission algorithm 2) a model parallel multi-GPU distribution scheme and 3) a static, yet very effective load balancing strategy. The simulator further features an easy to use API and the ability to create custom models. We compare the proposed simulator against two state of the art ones on a series of benchmarks using three well-established models. We find that our simulator is faster, consumes less memory, and scales linearly with the number of GPUs.|IEEE International Joint Conference on Neural Network|2021|10.1109/IJCNN52387.2021.9533921|Antonis A. Argyros|1.0|2
757|Refactoring the MPS/University of Chicago Radiative MHD (MURaM) model for GPU/CPU performance portability using OpenACC directives|The MURaM (Max Planck University of Chicago Radiative MHD) code is a solar atmosphere radiative MHD model that has been broadly applied to solar phenomena ranging from quiet to active sun, including eruptive events such as flares and coronal mass ejections. The treatment of physics is sufficiently realistic to allow for the synthesis of emission from visible light to extreme UV and X-rays, which is critical for a detailed comparison with available and future multi-wavelength observations. This component relies critically on the radiation transport solver (RTS) of MURaM; the most computationally intensive component of the code. The benefits of accelerating RTS are multiple fold: A faster RTS allows for the regular use of the more expensive multi-band radiation transport needed for comparison with observations, and this will pave the way for the acceleration of ongoing improvements in RTS that are critical for simulations of the solar chromosphere. We present challenges and strategies to accelerate a multi-physics, multi-band MURaM using a directive-based programming model, OpenACC in order to maintain a single source code across CPUs and GPUs. Results for a 2883 test problem show that MURaM with the optimized RTS routine achieves 1.73x speedup using a single NVIDIA V100 GPU over a fully subscribed 40-core Intel Skylake CPU node and with respect to the number of simulation points (in millions) per second, a single NVIDIA V100 GPU is equivalent to 69 Skylake cores. We also measure parallel performance on up to 96 GPUs and present weak and strong scaling results.|Platform for Advanced Scientific Computing Conference|2021|10.1145/3468267.3470576|Cena Miller, D. Przybylski, Eric Wright, S. Chandrasekaran, R. Loft, S. Suresh, M. Rempel, S. Su|1.0|2
766|Wave-PIM: Accelerating Wave Simulation Using Processing-in-Memory|Wave simulations are used in many applications: medical imaging, oil and gas exploration, earthquake hazard mitigation, and defense systems, among others. Most of these applications require repeated solutions of the wave equation on supercomputers. Minimizing time to solution and energy consumption are very beneficial in this domain. Data movement overhead is one of the key bottlenecks that affect energy consumption. Processing-in-Memory (PIM) has been a popular choice for deploying data-intensive applications, as a result of its high parallelism, low off-chip data movement and low energy consumption. In this paper, we propose an ISA-based, digital PIM system, to accelerate wave simulations. We fully explore the parallelism inside the algorithm, based on the size of the model and the availability of the PIM resources. We also examine the interconnections to optimize the inter-block data transfer. Our Wave-PIM can achieve an average of 41.98 × speedup, as well as 12.66 × energy savings, compared to the three state-of-the-art GPU platforms.|International Conference on Parallel Processing|2021|10.1145/3472456.3472512|Ruihao Li, Dimitrios Gourounas, A. Fathi, Dimitar Trenev, L. John, Karan Govil, Bagus Hanindhito, A. Gerstlauer|1.0|2
782|WarpDrive: Extremely Fast End-to-End Deep Multi-Agent Reinforcement Learning on a GPU|Deep reinforcement learning (RL) is a powerful framework to train decision-making models in complex environments. However, RL can be slow as it requires repeated interaction with a simulation of the environment. In particular, there are key system engineering bottlenecks when using RL in complex environments that feature multiple agents with high-dimensional state, observation, or action spaces. We present WarpDrive, a flexible, lightweight, and easy-to-use open-source RL framework that implements end-to-end deep multi-agent RL on a single GPU (Graphics Processing Unit), built on PyCUDA and PyTorch. Using the extreme parallelization capability of GPUs, WarpDrive enables orders-of-magnitude faster RL compared to common implementations that blend CPU simulations and GPU models. Our design runs simulations and the agents in each simulation in parallel. It eliminates data copying between CPU and GPU. It also uses a single simulation data store on the GPU that is safely updated in-place. WarpDrive provides a lightweight Python interface and flexible environment wrappers that are easy to use and extend. Together, this allows the user to easily run thousands of concurrent multi-agent simulations and train on extremely large batches of experience. Through extensive experiments, we verify that WarpDrive provides high-throughput and scales almost linearly to many agents and parallel environments. For example, WarpDrive yields 2.9 million environment steps/second with 2000 environments and 1000 agents (at least 100x higher throughput compared to a CPU implementation) in a benchmark Tag simulation. As such, WarpDrive is a fast and extensible multi-agent RL platform to significantly accelerate research and development.|arXiv.org|2021|10.2139/ssrn.3900018|Tian Lan, Stephan Zheng, Sunil Srinivasa|1.0|2
829|FSATOOL 2.0: An integrated molecular dynamics simulation and trajectory data analysis program|Molecular dynamics simulation is important in the computational study of the biomolecules. In this paper, we upgrade our previous FSATOOL to version 2.0. It is no longer a plugin as before. Besides the existed enhanced sampling and Markov state model analysis module, FSATOOL 2.0 has three new features now. First, it contains a molecular dynamics simulation engine on both CPU and GPU device. The engine works with an embedded enhanced sampling module. Second, it can do the free energy calculation by various practical methods, including the weighted histogram analysis method and Gaussian mixture model. Third, it has many subroutines to process the trajectory data, such as principal component analysis, time‐structure based independent component analysis, contact analysis, and Φ‐value analysis. Most importantly, all these calculations are integrated into one package. The trajectory data format is compatible with all the modules. With a proper input parameter file, users can do the molecular dynamics simulation and data analysis work by only a few simplified commands. The capabilities and theoretical backgrounds of FSATOOL 2.0 are introduced in the paper.|Journal of Computational Chemistry|2021|10.1002/jcc.26772|Mincong Wu, Changjun Chen, Zirui Shu, Jun-min Liao|1.0|2
852|Demonstrator for dependable edge-based cyber-physical systems|Dependable cyber-physical systems (CPS) are increasingly used in various application fields, such as urban mobility, smart city, industrial IoT and telecommunication. Beside functional requirements, dependable CPS systems have to meet several extra-functional requirements such as reliability, availability, fault-tolerance and performance. The complexity of modern CPS systems significantly increased since the extensive use of distributed services, redundant architectures and advanced safety mechanisms. In addition, several new technologies have emerged in the edge, such as embedded GPU-s, AI acceleration and virtualisation tools, which enhance the extra-functional properties, such as latency and performance of dependable CPS systems. Because of the increased complexity and the cutting edge technologies, evaluating the extra-functional requirements becomes difficult for modern CPS systems. Consequently, several new analysis techniques have also been developed. We developed an open-source demonstrator for dependable edge-based CPS systems in the field of smart city and urban mobility. With the demonstrator, the researchers can compare and evaluate different technologies, safety mechanisms and analysis techniques. The demonstrator consists of several emerging technologies such as hardware accelerators, load-balance mechanisms, containerisation and container deployment tools. The architecture of the demonstrator was developed following the edge computing paradigm and model-driven engineering approach. The demonstrator contains distributed redundant and fault-tolerant services. We also developed a hardware-in-the-loop (HIL) test environment to simulate various environmental scenarios and evaluate extra-functional properties.|Latin-American Symposium on Dependable Computing|2021|10.1109/ladc53747.2021.9672569|András Vörös, Richárd Szabó, S. Nagy, Máté Levente Vajda|1.0|2
873|Large‐scale environmental data science with ExaGeoStatR|Parallel computing in exact Gaussian process (GP) calculations becomes necessary for avoiding computational and memory restrictions associated with large‐scale environmental data science applications. The exact evaluation of the Gaussian log‐likelihood function requires O(n2) storage and O(n3) operations, where n is the number of geographical locations. Thus, exactly computing the log‐likelihood function with a large number of locations requires exploiting the power of existing parallel computing hardware systems, such as shared‐memory, possibly equipped with GPUs, and distributed‐memory systems, to solve this exact computational complexity. In this article, we present ExaGeoStatR, a package for exascale geostatistics in R that supports a parallel computation of the exact maximum likelihood function on a wide variety of parallel architectures. Furthermore, the package allows scaling existing GP methods to a large spatial/temporal domain. Prohibitive exact solutions for large geostatistical problems become possible with ExaGeoStatR. Parallelization in ExaGeoStatR depends on breaking down the numerical linear algebra operations in the log‐likelihood function into a set of tasks and rendering them for a task‐based programming model. The package can be used directly through the R environment on parallel systems without the user needing any C, CUDA, or MPI knowledge. Currently, ExaGeoStatR supports several maximum likelihood computation variants such as exact, diagonal super tile and tile low‐rank approximations, and mixed‐precision. ExaGeoStatR also provides a tool to simulate large‐scale synthetic datasets. These datasets can help assess different implementations of the maximum log‐likelihood approximation methods. Herein, we show the implementation details of ExaGeoStatR, analyze its performance on various parallel architectures, and assess its accuracy using synthetic datasets with up to 250K observations. The experimental analysis covers the exact computation of ExaGeoStatR to demonstrate the parallel capabilities of the package. We provide a hands‐on tutorial to analyze a sea surface temperature real dataset. The performance evaluation involves comparisons with the popular packages GeoR, fields, and bigGP for exact Gaussian likelihood evaluation. The approximation methods in ExaGeoStatR are not considered in this article since they were analyzed in previous studies.|Environmetrics|2019|10.1002/env.2770|Ying Sun, H. Ltaief, Sameh Abdulah, M. Genton, Yuxiao Li, Jian Cao, D. Keyes|1.0|2
882|Performance portable ice-sheet modeling with MALI|High-resolution simulations of polar ice sheets play a crucial role in the ongoing effort to develop more accurate and reliable Earth system models for probabilistic sea-level projections. These simulations often require a massive amount of memory and computation from large supercomputing clusters to provide sufficient accuracy and resolution; therefore, it has become essential to ensure performance on these platforms. Many of today’s supercomputers contain a diverse set of computing architectures and require specific programming interfaces in order to obtain optimal efficiency. In an effort to avoid architecture-specific programming and maintain productivity across platforms, the ice-sheet modeling code known as MPAS-Albany Land Ice (MALI) uses high-level abstractions to integrate Trilinos libraries and the Kokkos programming model for performance portable code across a variety of different architectures. In this article, we analyze the performance portable features of MALI via a performance analysis on current CPU-based and GPU-based supercomputers. The analysis highlights not only the performance portable improvements made in finite element assembly and multigrid preconditioning within MALI with speedups between 1.26 and 1.82x across CPU and GPU architectures but also identifies the need to further improve performance in software coupling and preconditioning on GPUs. We perform a weak scalability study and show that simulations on GPU-based machines perform 1.24–1.92x faster when utilizing the GPUs. The best performance is found in finite element assembly, which achieved a speedup of up to 8.65x and a weak scaling efficiency of 82.6% with GPUs. We additionally describe an automated performance testing framework developed for this code base using a changepoint detection method. The framework is used to make actionable decisions about performance within MALI. We provide several concrete examples of scenarios in which the framework has identified performance regressions, improvements, and algorithm differences over the course of 2 years of development.|The international journal of high performance computing applications|2022|10.1177/10943420231183688|Jerry Watkins, S. Price, Luca Bertagna, M. Perego, Max Carlson, Kyle Shan, M. Hoffman, Carolyn Kao, I. Tezaur|1.0|2
910|Fast parallel algorithm for three-dimensional distance-driven model in iterative computed tomography reconstruction|The projection matrix model is used to describe the physical relationship between reconstructed object and projection. Such a model has a strong influence on projection and backprojection, two vital operations in iterative computed tomographic reconstruction. The distance-driven model (DDM) is a state-of-the-art technology that simulates forward and back projections. This model has a low computational complexity and a relatively high spatial resolution; however, it includes only a few methods in a parallel operation with a matched model scheme. This study introduces a fast and parallelizable algorithm to improve the traditional DDM for computing the parallel projection and backprojection operations. Our proposed model has been implemented on a GPU (graphic processing unit) platform and has achieved satisfactory computational efficiency with no approximation. The runtime for the projection and backprojection operations with our model is approximately 4.5 s and 10.5 s per loop, respectively, with an image size of 256×256×256 and 360 projections with a size of 512×512. We compare several general algorithms that have been proposed for maximizing GPU efficiency by using the unmatched projection/backprojection models in a parallel computation. The imaging resolution is not sacrificed and remains accurate during computed tomographic reconstruction.||2015|10.1088/1674-1056/24/2/028703|Ai-Long 爱龙 Cai 蔡, L. Li 李, L. Wang 王, H. Zhang 张, Bin 镔 Yan 闫, J. Chen 陈, X. Xi 席, J. Li 李|1.0|2
920|An SBR Based Ray Tracing Channel Modeling Method for THz and Massive MIMO Communications|Terahertz (THz) communication and the application of massive multiple-input multiple-output (MIMO) technology are significant for the sixth generation (6G) communication systems. In this paper, we employ the shooting and bouncing ray (SBR) method integrated with GPU acceleration technology to model THz and massive MIMO channel. The results of ray tracing (RT) simulation in this paper, i.e., angle of departure (AoD), angle of arrival (AoA), and power delay profile (PDP) under the frequency band supported by the commercial RT software Wireless Insite (WI) are in agreement with those produced by WI. Based on the Kirchhoff scattering effect on material surfaces and atmospheric absorption loss showing at THz frequency band, the modified propagation models of Fresnel reflection coefficients and free-space attenuation are consistent with the measured results. For massive MIMO, the channel capacity and the stochastic power distribution are analyzed. The results indicate the applicability of SBR method for building deterministic models of THz and massive MIMO channels with extensive functions and acceptable accuracy.|IEEE Vehicular Technology Conference|2022|10.1109/VTC2022-Fall57202.2022.10012996|Yinghua Wang, Chenghai Wang, Jie Huang, Yuxiao Li, Jialing Huang, Yifan Jin, Zizhen Zhou, Hao Cao, Yuanzhe Wang|1.0|2
944|Titan: a scheduler for foundation model fine-tuning workloads|The recent breakthrough of foundation model (FM) research raises a new trend to acquire efficient DL models by fine-tuning FMs with low-resource datasets. Current GPU clusters are mainly established to develop DL models by training from scratch. How to tailor a GPU cluster scheduler for FM fine-tuning workloads is still not explored. We propose Titan, a scheduler to improve the efficiency of FM fine-tuning workloads based on their three distinct features. (1) It takes full advantage of the fixed model structure to estimate the job duration accurately and configure the fine-tuning workload efficiently. (2) The multi-task adaptivity of FMs enables multiple fine-tuning workloads to share the same model parameters, which can significantly reduce the GPU resource consumption. (3) It considers the pipeline parallelism of FM fine-tuning workloads and concurrently executes the parameter transmission and gradient computation to hide the overhead of context switch. Preliminary simulation result validates the efficiency of Titan.|ACM Symposium on Cloud Computing|2022|10.1145/3542929.3563460|Peng Sun, Yonggang Wen, Tianwei Zhang, Wei Gao|1.0|2
948|Towards a Dynamic Composability Approach for using Heterogeneous Systems in Remote Sensing|Influenced by the advances in data and computing, the scientific practice increasingly involves machine learning and artificial intelligence driven methods which requires specialized capabilities at the system-, science- and service-level in addition to the conventional large-capacity supercomputing approaches. The latest distributed architectures built around the composability of data-centric applications led to the emergence of a new ecosystem for container coordination and integration. However, there is still a divide between the application development pipelines of existing supercomputing environments, and these new dynamic environments that disaggregate fluid resource pools through accessible, portable and re-programmable interfaces. New approaches for dynamic composability of heterogeneous systems are needed to further advance the data-driven scientific practice for the purpose of more efficient computing and usable tools for specific scientific domains. In this paper, we present a novel approach for using composable systems in the intersection between scientific computing, artificial intelligence (AI), and remote sensing domain. We describe the architecture of a first working example of a composable infrastructure that federates Expanse, an NSF-funded supercomputer, with Nautilus, a Kubernetes-based GPU geo-distributed cluster. We also summarize a case study in wildfire modeling, that demonstrates the application of this new infrastructure in scientific workflows: a composed system that bridges the insights from edge sensing, AI and computing capabilities with a physics-driven simulation.|IEEE International Conference on e-Science|2022|10.1109/eScience55777.2022.00047|T. DeFanti, M. Tatineni, Christopher Irving, Adrien Trouillaud, Ismael Pérez, John Graham, Michael L. Norman, D. Mishin, Shawn M. Strande, I. Altintas, L. Smarr|1.0|2
957|Developments in SRW Code and Sirepo Framework Supporting Simulation of Time-Dependent Coherent X-ray Scattering Experiments|Physical optics simulations for beamlines and experiments are essential for the effective use of synchrotron light source facilities such as NSLS-II at BNL. The SRW software package supports such source-to-detector simulations for coherent X-ray scattering and imaging experiments through its Python interface and Sirepo browser-based graphical user interface. This allows one to define custom sample models, assess the feasibility of an experiment, and estimate most appropriate beamline settings before using valuable beamtime. We discuss the recent use of general-purpose GPU resources and coherent mode decomposition algorithms in SRW to accelerate physical optics simulations with partially coherent X-rays. To illustrate these new capabilities, we describe simulations of typical time series of partially coherent scattering images used in X-ray Photon Correlation Spectroscopy (XPCS) experiments; aiming to characterize the nanoscale dynamics of a disordered sample, representing a solution of nanoparticles undergoing Brownian diffusion.|Journal of Physics: Conference Series|2022|10.1088/1742-6596/2380/1/012126|R. Nagler, A. Fluerasu, Ruizi Li, A. He, Himanshu Goel, M. Rakitin, O. Chubar, P. Moeller, L. Wiegart|1.0|2
959|GPU-Parallelized Iterative LQR with Input Constraints for Fast Collision Avoidance of Autonomous Vehicles|Collision avoidance in emergency situations is a crucial and challenging task in motion planning for autonomous vehicles. Especially in the field of optimization-based planning using nonlinear model predictive control, many efforts to achieve real-time performance are still ongoing. Among various approaches, the iterative linear quadratic regulator (iLQR) is known as an efficient means of nonlinear optimization. Additionally, parallel computing architectures, such as GPUs, are more widely applied in autonomous vehicles. In this paper, we propose 1) a parallel computing framework for iLQR with input constraints considering the characteristics of the problem and 2) a proper environmental formulation that can be covered with single-precision floating-point computation of the GPU. The GPU-accelerated framework was tested on a real-time simulation-in-the-loop system using CarMaker and ROS at a 20 Hz sampling rate on a low-performance mobile computer and was compared against the same framework realized with a CPU.|IEEE/RJS International Conference on Intelligent RObots and Systems|2022|10.1109/IROS47612.2022.9982026|Yeongseok Lee, Kyung-Soo Kim, Min-Yyeong Cho|1.0|2
983|Shield Model Predictive Path Integral: A Computationally Efficient Robust MPC Method Using Control Barrier Functions|Model Predictive Path Integral (MPPI) control is a type of sampling-based model predictive control that simulates thousands of trajectories and uses these trajectories to synthesize optimal controls on-the-fly. In practice, however, MPPI encounters problems limiting its application. For instance, it has been observed that MPPI tends to make poor decisions if unmodeled dynamics or environmental disturbances exist, preventing its use in safety-critical applications. Moreover, the multi-threaded simulations used by MPPI require significant onboard computational resources, making the algorithm inaccessible to robots without modern GPUs. To alleviate these issues, we propose a novel (Shield-MPPI) algorithm that provides robustness against unpredicted disturbances and achieves real-time planning using a much smaller number of parallel simulations on regular CPUs. The novel Shield-MPPI algorithm is tested on an aggressive autonomous racing platform both in simulation and in hardware. The results show that the proposed controller greatly reduces the number of constraint violations compared to state-of-the-art robust MPPI variants and stochastic Model Predictive Control methods.|IEEE Robotics and Automation Letters|2023|10.1109/LRA.2023.3315211|Chuchu Fan, P. Tsiotras, Ji Yin, Charles Dawson|1.0|2
984|UniQ: A Unified Programming Model for Efficient Quantum Circuit Simulation|Quantum circuit simulation is critical for verifying quantum computers. Given exponential complexity in the simulation, existing simulators use different architectures to accelerate the simulation. However, due to the variety of both simulation methods and modern architectures, it is challenging to design a high-performance yet portable simulator. In this work, we propose UniQ, a unified programming model for multiple simulation methods on various hardware architectures. We provide a unified application abstraction to describe different applications, and a unified hierarchical hardware abstraction upon different hardware. Based on these abstractions, UniQ can perform various circuit transformations without being aware of either concrete application or architecture detail, and generate high-performance execution schedules on different platforms without much human effort. Evaluations on CPU, GPU, and Sunway platforms show that UniQ can accelerate quantum circuit simulation by up to 28.59× (4.47× on average) over state-of-the-art frameworks, and successfully scale to 399,360 cores on 1,024 nodes.|International Conference for High Performance Computing, Networking, Storage and Analysis|2022|10.1109/SC41404.2022.00054|Jidong Zhai, Haojie Wang, Chen Zhang, Zeyu Song, Zixuan Ma, Lei Xie|1.0|2
986|A Comparison of Multithreading, Vectorization, and GPU Computing for the Acceleration of Cardiac Electrophysiology Models|Realistic simulation of cardiac electrophysiology requires both high resolution and computationally expensive models of membrane dynamics. Optimization of membrane models can therefore have a large impact on time, hardware, and energy usage. We tested both CPU-based and GPU-based optimization techniques for a human heart model with Ten Tusscher-Panfilov 2006 dynamics. Compared to a multithreaded code running on 64 CPU cores, the tested NVIDIA Tesla P100 GPU proved about 3 times faster. Effective use of the CPU's SIMD capabilities allowed a similar performance gain. GPU performance was bounded by the data transfer rate between GPU and main memory. Optimal SIMD use required explicit vectorization and an adapted data structure. We conclude that on mixed CPU-GPU systems the best results are obtained by optimizing both CPU and GPU code and using a runtime system that balances CPU and GPU load.|2022 Computing in Cardiology (CinC)|2022|10.22489/CinC.2022.399|M. Potse, Denis Barthou, Chiheb Sakka, Olivier Aumage, Amina Guermouche, Emmanuelle Saillard, Y. Coudière|1.0|2
991|Identifying and overcoming the sampling challenges in relative binding free energy calculations of a model protein:protein complex|Relative alchemical binding free energy calculations are routinely used in drug discovery projects to optimize the affinity of small molecules for their drug targets. Alchemical methods can also be used to estimate the impact of amino acid mutations on protein:protein binding affinities, but these calculations can involve sampling challenges due to the complex networks of protein and water interactions frequently present in protein:protein interfaces. We investigate these challenges by extending a GPU-accelerated open-source relative free energy calculation package (Perses) to predict the impact of amino acid mutations on protein:protein binding. Using the well-characterized model system barnase:barstar, we describe analyses for identifying and characterizing sampling problems in protein:protein relative free energy calculations. We find that mutations with sampling problems often involve charge-changes, and inadequate sampling can be attributed to slow degrees of freedom that are mutation-specific. We also explore the accuracy and efficiency of current state-of-the-art approaches—alchemical replica exchange and alchemical replica exchange with solute tempering—for overcoming relevant sampling problems. By employing sufficiently long simulations, we achieve accurate predictions (RMSE 1.61, 95% CI: [1.12, 2.11] kcal/mol), with 86% of estimates within 1 kcal/mol of the experimentally-determined relative binding free energies and 100% of predictions correctly classifying the sign of the changes in binding free energies. Ultimately, we provide a model workflow for applying protein mutation free energy calculations to protein:protein complexes, and importantly, catalog the sampling challenges associated with these types of alchemical transformations. Our free open-source package (Perses) is based on OpenMM and available at https://github.com/choderalab/perses.|bioRxiv|2023|10.1101/2023.03.07.530278|J. Chodera, Sukrit Singh, Dominic A. Rufa, Mike Henry, Laura E. Rosen, Iván Pulido, Ivy Zhang, Kevin Hauser|1.0|2
1006|Evaluating the Impact of Transition Delay Faults in GPUs|This work proposes a method to evaluate the effects of transition delay faults (TDFs) in GPUs. The method takes advantage of low-level (i.e., RT- and gate-level) descriptions of a GPU to evaluate the effects of transition delay faults in GPUs, thus paving the way to model them as errors at the instruction level, which can contribute to the resilience evaluations of large and complex applications. For this purpose, the paper describes a setup that efficiently simulates transition delay faults. The results allow us to compare their effects with stuck-at-faults (SAFs) and perform an error classification correlating these faults as instruction-level errors. We resort to an open-source model of a GPU (FlexGripPlus) and a set of workloads for the evaluation. The experimental results show that, according to the application code style, TDFs can compromise the operation of an application from 1.3 to 11.63 times less than SAFs. Moreover, for all the analyzed applications, a considerable percentage of sites of the Integer (5.4% to 51.7%), Floating-point (0.9% to 2.4%), and Special Function unit (17.0% to 35.6%) can become critical if affected by a SAF or TDF. Finally, a correlation between the fault's impact from both fault models and the instructions executed by the applications reveals that SAFs in the functional units are more prone (from 45.6% to 60.4%) to propagate errors at the software level for all units than TDFs (from 17.9% to 58.8%).|International Conference on VLSI Design|2023|10.1109/VLSID57277.2023.00077|J. E. R. Condia, M. Reorda|1.0|2
1014|Exact and approximate simulation of large quantum circuits on a single GPU|We benchmark the performances of Qrack, an open-source software library for the high-performance classical simulation of (gate-model) quantum computers. Qrack simulates, in the Schrödinger picture, the exact quantum state of $n$ qubits evolving under the application of a circuit composed of elementary quantum gates. Moreover, Qrack can also run approximate simulations in which a tunable reduction of the quantum state fidelity is traded for a significant reduction of the execution time and memory footprint. In this work, we give an overview of both simulation methods (exact and approximate), highlighting the main physics-based and software-based techniques. Moreover, we run computationally heavy benchmarks on a single GPU, executing large quantum Fourier transform circuits and large random circuits. Compared with other classical simulators, we report competitive execution times for the exact simulation of Fourier transform circuits with up to 27 qubits. We also demonstrate the approximate simulation of all amplitudes of random circuits acting on 54 qubits with 7 layers at average fidelity higher than 4%, a task commonly considered hard without super-computing resources.|International Conference on Quantum Computing and Engineering|2023|10.1109/QCE57702.2023.00109|Nathan Shammah, A. Mari, Benn Bollay, W. Zeng, D. Strano, Aryan Blaauw|1.0|2
1015|Critical Assessment of Self-Consistency Checks in the All-Atom Molecular Dynamics Simulation of Intrinsically Disordered Proteins.|All atom simulations can be used to quantify conformational properties of Intrinsically Disordered Proteins (IDP). However, simulations must satisfy convergence checks to ensure observables computed from simulation are reliable and reproducible. While absolute convergence is purely a theoretical concept requiring infinitely long simulation, a more practical, yet rigorous, approach is to impose Self Consistency Checks (SCCs) to gain confidence in the simulated data. Currently there is no study of SCCs in IDPs, unlike their folded counterparts. In this paper, we introduce different criteria for self-consistency checks for IDPs. Next, we impose these SCCs to critically assess the performance of different simulation protocols using the N terminal domain of HIV Integrase and the linker region of SARS-CoV-2 Nucleoprotein as two model IDPs. All simulation protocols begin with all-atom implicit solvent Monte Carlo (MC) simulation and subsequent clustering of MC generated conformations to create the representative structures of the IDPs. These representative structures serve as the initial structure for subsequent molecular dynamics (MD) runs with explicit solvent. We conclude that generating multiple short (∼3 μs) MD simulation trajectories─all starting from the most representative MC generated conformation─and merging them is the protocol of choice due to (i) its ability to satisfy multiple SCCs, (ii) consistently reproducing experimental data, and (iii) the efficiency of running independent trajectories in parallel by harnessing multiple cores available in modern GPU clusters. Running one long trajectory (greater than 20 μs) can also satisfy the first two criteria but is less desirable due to prohibitive computation time. These findings help resolve the challenge of identifying a usable starting configuration, provide an objective measure of SCC, and establish rigorous criteria to determine the minimum length (for one long simulation) or number of trajectories needed in all-atom simulation of IDPs.|Journal of Chemical Theory and Computation|2023|10.1021/acs.jctc.2c01140|Kari Gaalswyk, Austin Haider, K. Ghosh|1.0|2
1026|A Multimodel Edge Computing Offloading Framework for Deep-Learning Application Based on Bayesian Optimization|With the rapid development of the Internet of Things (IoT), data generated by IoT devices are also increasing exponentially. The edge computing has alleviated the problems of limited network and transmission delay when processing tasks of IoT devices in traditional cloud computing. And with the popularity of deep-learning, more and more terminal devices are embedded with artificial intelligence (AI) processors for higher processing capability at the edge. However, the problems of deep-learning task offloading in a heterogeneous edge computing environment have not been fully investigated. In this article, a multimodel edge computing offloading framework is proposed, using NVIDIA Jetson edge devices (Jetson TX2, Jetson Xavier NX, and Jetson Nano) and GeForce RTX GPU servers (RTX3080 and RTX2080) to simulate the edge computing environment, and make binary computational offloading decisions for face detection tasks. We also introduce a Bayesian optimization algorithm, namely, modified tree-structured Parzen estimator (MTPE), to reduce the total cost of edge computation within a time slot including response time and energy consumption, and ensure the accuracy requirements of face detection. In addition, we employ the Lyapunov model to obtain the harvesting energy between time slots to keep the energy queue stable. Experiments reveal that MTPE algorithm can achieve the globally optimal solution in fewer iterations. The total cost of multimodel edge computing framework is reduced by an average of 17.94% compared to a single-model framework. In contrast to the double deep Q-network (DDQN), our proposed algorithm can decrease the computational consumption by 23.01% for obtaining the offloading decision.|IEEE Internet of Things Journal|2023|10.1109/JIOT.2023.3280162|Hong Zhang, Liqiang Wang, Zidi Zhao, Haijun Huang|1.0|2
1033|Monte Carlo Simulation of Diffuse Optical Spectroscopy for 3D Modeling of Dental Tissues|Three-dimensional precise models of teeth are critical for a variety of dental procedures, including orthodontics, prosthodontics, and implantology. While X-ray-based imaging devices are commonly used to obtain anatomical information about teeth, optical devices offer a promising alternative for acquiring 3D data of teeth without exposing patients to harmful radiation. Previous research has not examined the optical interactions with all dental tissue compartments nor provided a thorough analysis of detected signals at various boundary conditions for both transmittance and reflectance modes. To address this gap, a GPU-based Monte Carlo (MC) method has been utilized to assess the feasibility of diffuse optical spectroscopy (DOS) systems operating at 633 nm and 1310 nm wavelengths for simulating light-tissue interactions in a 3D tooth model. The results show that the system’s sensitivity to detect pulp signals at both 633 nm and 1310 nm wavelengths is higher in the transmittance compared with that in the reflectance mode. Analyzing the recorded absorbance, reflectance, and transmittance data verified that surface reflection at boundaries can improve the detected signal, especially from the pulp region in both reflectance and transmittance DOS systems. These findings could ultimately lead to more accurate and effective dental diagnosis and treatment.|Italian National Conference on Sensors|2023|10.3390/s23115118|Yu Chen, Mousa Moradi|1.0|2
1038|SNS-Toolbox: An Open Source Tool for Designing Synthetic Nervous Systems and Interfacing Them with Cyber–Physical Systems|One developing approach for robotic control is the use of networks of dynamic neurons connected with conductance-based synapses, also known as Synthetic Nervous Systems (SNS). These networks are often developed using cyclic topologies and heterogeneous mixtures of spiking and non-spiking neurons, which is a difficult proposition for existing neural simulation software. Most solutions apply to either one of two extremes, the detailed multi-compartment neural models in small networks, and the large-scale networks of greatly simplified neural models. In this work, we present our open-source Python package SNS-Toolbox, which is capable of simulating hundreds to thousands of spiking and non-spiking neurons in real-time or faster on consumer-grade computer hardware. We describe the neural and synaptic models supported by SNS-Toolbox, and provide performance on multiple software and hardware backends, including GPUs and embedded computing platforms. We also showcase two examples using the software, one for controlling a simulated limb with muscles in the physics simulator Mujoco, and another for a mobile robot using ROS. We hope that the availability of this software will reduce the barrier to entry when designing SNS networks, and will increase the prevalence of SNS networks in the field of robotic control.|Biomimetics|2023|10.3390/biomimetics8020247|William R. P. Nourse, N. Szczecinski, Clayton Jackson, R. Quinn|1.0|2
1045|Automatic tool for real-time estimation of QFN-related heat transfer in multi-layer PCB by using SPICE simulations|Power hungry electronic components such as CPU, GPUs as well as voltage regulators heat up during operation. Several sensing applications require ambient air temperature and humidity measurements. Nevertheless, ambient air temperature and humidity measures with a surface mounting technology devices is challenging due to the thermal influence of nearby components. In this context, thermal design of Printed Circuit Boards (PCB) becomes a critical step to ensure the reliability of electronic systems and to preserve those components that are vulnerable to heat-accelerated failure mechanisms. Although PCB thermal analysis increasingly relies on software embedding complex but accurate fluid dynamics or finite elements solvers, simulation times during the design phase result very long and are not suitable for rapid prototyping processes. To bridge this gap, this paper proposes an automatic tool for the rapid simulation of heat transfer pathways inside a PCB when quad-flat no-lead (QFN) packages are employed. The proposed tool exploits a resistive networks-based model able to adapt to the metal/dielectric/soldermask composition of the analyzed area. It is made possible by integrating image processing algorithms to identify thermal connections between the analyzed elements, allowing a multi-layer reconstruction even with irregularly shaped metal areas. The proposed tool has been tested on a PCB and the results compared with the ones from professional software for FEM thermal analysis. The proposed modeling system can ensure optimal accuracy on the chip area (error compared to $\mathrm{F E M}\lt 1^{\circ} \mathrm{C}$), and within areas of 9 $\mathrm{cm}^{2}$, resulting $\sim 91$ times faster than the equivalent FEM in estimating heating trend on the same board.|International Workshop on Advances in Sensors and Interfaces|2023|10.1109/IWASI58316.2023.10164511|G. Mezzina, D. Venuto, C. L. Saragaglia, Alberto Fakhri Brunetti, G. Matarrese|1.0|2
1054|Hybrid Barnes – Hut/Multipole algorithm application to vortex particles velocities calculation and integral equation solution|The approximate fast algorithm is developed that makes it possible to calculate velocities of the vortex particles in two-dimensional flow simulations by using vortex methods. It is adapted to efficient solution of the boundary integral equation arising at each time step of simulation. The proposed method can be considered as generalization of the classical Barnes – Hut method, taking into account some ideas of the Fast Multipole Method. Some model problems are considered that can be solved by using vortex methods, the developed fast algorithm is implemented for multi-cores CPUs and (with some modifications) for GPUs. The scalability of the code is rather high; the developed algorithms can run with millions of vortex particles taking only tens or hundreds of milliseconds per time step; linear systems with dimension of about 104 can be solved on multicore CPU in tens milliseconds.|Journal of Physics: Conference Series|2023|10.1088/1742-6596/2543/1/012003|Alexandra Kolganova, E. Ryatina, I. Marchevsky|1.0|2
1056|A Full-Wave Reference Simulator for Computing Surface Reflectance|Computing light reflection from rough surfaces is an important topic in computer graphics. Reflection models developed based on geometric optics fail to capture wave effects such as diffraction and interference, while existing models based on physical optics approximations give erroneous predictions under many circumstances (e.g. when multiple scattering from the surface cannot be ignored). We present a scalable 3D full-wave simulator for computing reference solutions to surface scattering problems, which can be used to evaluate and guide the development of approximate models for rendering. We investigate the range of validity for some existing wave optics based reflection models; our results confirm these models for low-roughness surfaces but also show that prior rendering methods do not accurately predict the scattering behavior of some types of surfaces. Our simulator is based on the boundary element method (BEM) and accelerated using the adaptive integral method (AIM), and is implemented to execute on modern GPUs. We demonstrate the simulator on domains up to 60 × 60 × 10 wavelengths, involving surface samples with significant height variations. Furthermore, we propose a new system for efficiently computing BRDF values for large numbers of incident and outgoing directions at once, by combining small simulations to characterize larger areas. Our simulator will be released as an open-source toolkit for computing surface scattering.|ACM Transactions on Graphics|2023|10.1145/3592414|Yunchen Yu, E. Michielssen, Mengqi Xia, Steve Marschner, B. Walter|1.0|2
1058|Visual Imitation Learning for robotic fresh mushroom harvesting|Imitation Learning holds significant promise in enabling the automation of complex robotic manipulations tasks which are impossible to explicitly program. Mushroom harvesting is a task of high difficulty requiring weeks of intense training even for humans to master. In this work we present an end-to-end Imitation Learning pipeline that learns to apply the series of motions, namely reaching, grasping, twisting, and pulling the mushroom directly from pixel-level information. Mushroom harvesting experiments are carried out within a simulated environment that models the mushroom dynamics based on von Mises yielding theory with parameters obtained through expert picker demonstration wearing gloves with force sensors. We test the robustness of our technique by performing randomization on the camera extrinsic and intrinsic parameters as well as on the mushroom sizes. We also evaluate on different kinds of visual input namely grayscale and depth maps. Overall, our technique shows significant promise in automating mushroom harvesting directly from visual input while being remarkably lean in terms of computation intensity. Our models can be trained on a standard Laptop GPU in under one hour while inference of an action takes less than 1.5ms on a Laptop CPU. A brief overview of our experiments in video format is available at: https://bit.ly/41kCH7T|2023 31st Mediterranean Conference on Control and Automation (MED)|2023|10.1109/MED59994.2023.10185745|Myrto Iglezou, Konstantinos Vasios, P. Chatzakos, Vishwanathan Mohan, Antonios Porichis|1.0|2
1110|ProSpar-GP: scalable Gaussian process modeling with massive non-stationary datasets|Gaussian processes (GPs) are a popular class of Bayesian nonparametric models, but its training can be computationally burdensome for massive training datasets. While there has been notable work on scaling up these models for big data, existing methods typically rely on a stationary GP assumption for approximation, and can thus perform poorly when the underlying response surface is non-stationary, i.e., it has some regions of rapid change and other regions with little change. Such non-stationarity is, however, ubiquitous in real-world problems, including our motivating application for surrogate modeling of computer experiments. We thus propose a new Product of Sparse GP (ProSpar-GP) method for scalable GP modeling with massive non-stationary data. The ProSpar-GP makes use of a carefully-constructed product-of-experts formulation of sparse GP experts, where different experts are placed within local regions of non-stationarity. These GP experts are fit via a novel variational inference approach, which capitalizes on mini-batching and GPU acceleration for efficient optimization of inducing points and length-scale parameters for each expert. We further show that the ProSpar-GP is Kolmogorov-consistent, in that its generative distribution defines a valid stochastic process over the prediction space; such a property provides essential stability for variational inference, particularly in the presence of non-stationarity. We then demonstrate the improved performance of the ProSpar-GP over the state-of-the-art, in a suite of numerical experiments and an application for surrogate modeling of a satellite drag simulator.||2023|10.1080/00401706.2023.2166124|Simon Mak, Kevin Li|1.0|2
1138|FeReX: A Reconfigurable Design of Multi-bit Ferroelectric Compute-in-Memory for Nearest Neighbor Search|Rapid advancements in artificial intelligence have given rise to transformative models, profoundly impacting our lives. These models demand massive volumes of data to operate effectively, exacerbating the data-transfer bottleneck inherent in the conventional von-Neumann architecture. Compute-in-memory (CIM), a novel computing paradigm, tackles these issues by seamlessly embedding in-memory search functions, thereby obviating the need for data transfers. However, existing non-volatile memory (NVM)-based accelerators are application specific. During the similarity based associative search operation, they only support a single, specific distance metric, such as Hamming, Manhattan, or Euclidean distance in measuring the query against the stored data, calling for reconfigurable in-memory solutions adaptable to various applications. To overcome such a limitation, in this paper, we present FeReX, a reconfigurable associative memory (AM) that accommodates various distance metrics including Hamming, Manhattan, and Euclidean distances. Leveraging multi-bit ferroelectric field-effect transistors (FeFETs) as the proxy and a hardware-software co-design approach, we introduce a constrained satisfaction problem (CSP)-based method to automate AM search input voltage and stored voltage configurations for different distance based search functions. Device-circuit co-simulations first validate the effectiveness of the proposed FeReX methodology for reconfigurable search distance functions. Then, we benchmark FeReX in the context of k-nearest neighbor (KNN) and hyperdimensional computing (HDC), which highlights the robustness of FeReX and demonstrates up to 250x speedup and 10^4 energy savings compared with GPU.||2024|10.23919/date58400.2024.10546615|Can Li, Jianyi Yang, Cheung Liu, Xunzhao Yin, T. Kampfe, Mohsen Imani, Cheng Zhuo, Zhicheng Xu, Ruibin Mao, Chao Li|1.0|2
1148|Simple and efficient GPU parallelization of existing H-Matrix accelerated BEM code|In this paper, we demonstrate how GPU-accelerated BEM routines can be used in a simple black-box fashion to accelerate fast boundary element formulations based on Hierarchical Matrices (H-Matrices) with ACA (Adaptive Cross Approximation). In particular, we focus on the expensive evaluation of the discrete weak form of boundary operators associated with the Laplace and the Helmholtz equation in three space dimensions. The method is based on offloading the CPU assembly of elements during the ACA assembly onto a GPU device and to use threading strategies across ACA blocks to create sufficient workload for the GPU. The proposed GPU strategy is designed such that it can be implemented in existing code with minimal changes to the surrounding application structure. This is in particular interesting for existing legacy code that is not from the ground-up designed with GPU computing in mind. Our benchmark study gives realistic impressions of the benefits of GPU-accelerated BEM simulations by using state-of-the-art multi-threaded computations on modern high-performance CPUs as a reference, rather than drawing synthetic comparisons with single-threaded codes. Speed-up plots illustrate that performance gains up to a factor of 5.5 could be realized with GPU computing under these conditions. This refers to a boundary element model with about 4 million unknowns, whose H-Matrix weak form associated with a real-valued (Laplace) boundary operator is set up in only 100 minutes harnessing the two GPUs instead of 9 hours when using the 20 CPU cores at disposal only. The benchmark study is followed by a particularly demanding real-life application, where we compute the scattered high-frequency sound field of a submarine to demonstrate the increase in overall application performance from moving to a GPU-based ACA assembly.|arXiv.org|2017|10.1007/978-3-319-28832-1_9|Boris Dilba, Kerstin Vater, T. Betcke|1.0|2
1192|GPU-Powered Multi-Swarm Parameter Estimation of Biological Systems: A Master-Slave Approach|In silico investigation of biological systems requires the knowledge of numerical parameters that cannot be easily measured in laboratory experiments, leading to the Parameter Estimation (PE) problem, in which the unknown parameters are automatically inferred by means of optimization algorithms exploiting the available experimental data. Here we present MS 2 PSO, an efficient parallel and distributed implementation of a PE method based on Particle Swarm Optimization (PSO) for the estimation of reaction constants in mathematical models of biological systems, considering as target for the estimation a set of discrete-time measurements of molecular species amounts. In particular, such PE method accounts for the availability of experimental data typically measured under different experimental conditions, by considering a multi-swarm PSO in which the best particles of the swarms can migrate. This strategy allows to infer a common set of reaction constants that simultaneously fits all target data used in the PE. To the aim of efficiently tackling the PE problem, MS 2 PSO embeds the execution of cupSODA, a deterministic simulator that relies on Graphics Processing Units to achieve a massive parallelization of the simulations required in the fitness evaluation of particles. In addition, a further level of parallelism is realized by exploiting the Master-Slave distributed programming paradigm. We apply MS 2 PSO for the PE of synthetic biochemical models with 10, 20 and 30 parameters to be estimated, and compare the performances obtained with different GPUs and different configurations (i.e., numbers of processes) of the Master-Slave.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2018|10.1109/PDP2018.2018.00115|L. Rundo, P. Cazzaniga, A. Tangherloni, S. Spolaor, Marco S. Nobile|1.0|2
1348|An efficient sparse matrix‐vector multiplication on CUDA‐enabled graphic processing units for finite element method simulations|Finite element method (FEM) is a well‐developed method to solve real‐world problems that can be modeled with differential equations. As the available computational power increases, complex and large‐size problems can be solved using FEM, which typically involves multiple degrees of freedom (DOF) per node, high order of elements, and an iterative solver requiring several sparse matrix‐vector multiplication operations. In this work, a new storage scheme is proposed for sparse matrices arising from FEM simulations with multiple DOF per node. A sparse matrix‐vector multiplication kernel and its variants using the proposed scheme are also given for CUDA‐enabled GPUs. The proposed scheme and the kernels rely on the mesh connectivity data from FEM discretization and the number of DOF per node. The proposed kernel performance was evaluated on seven test matrices for double‐precision floating point operations. The performance analysis showed that the proposed GPU kernel outperforms the ELLPACK (ELL) and CUSPARSE Hybrid (HYB) format GPU kernels by an average of 42% and 32%, respectively, on a Tesla K20c card. Copyright © 2016 John Wiley & Sons, Ltd.||2017|10.1002/nme.5346|A. Altınkaynak|1.0|2
1355|GPU-accelerated simulations of the chemotactic response of amoeba Dictyostelium discoideum|Firstly, the design and implementation of a generic framework that easies the creation of gpu-accelerated physical simulations. Secondly, it tests a model for the chemotactic response and locomotion of the amoeba Dictyostelium discoideum, using the aforementioned framework. The framework has proven itself very useful during the testing of the model, although some improvements could be done. The model used in this project yielded satisfactory results, but should be completed in order to provide more interesting insights.||2016|10.1007/s00500-015-2005-x|D. Vidal|1.0|2
1483|Fast parallel algorithm for three-dimensional distance-driven model in iterative computed tomography reconstruction|The projection matrix model is used to describe the physical relationship between reconstructed object and projection. Such a model has a strong influence on projection and backprojection, two vital operations in iterative computed tomographic reconstruction. The distance-driven model (DDM) is a state-of-the-art technology that simulates forward and back projections. This model has a low computational complexity and a relatively high spatial resolution; however, it includes only a few methods in a parallel operation with a matched model scheme. This study introduces a fast and parallelizable algorithm to improve the traditional DDM for computing the parallel projection and backprojection operations. Our proposed model has been implemented on a GPU (graphic processing unit) platform and has achieved satisfactory computational efficiency with no approximation. The runtime for the projection and backprojection operations with our model is approximately 4.5 s and 10.5 s per loop, respectively, with an image size of 256×256×256 and 360 projections with a size of 512×512. We compare several general algorithms that have been proposed for maximizing GPU efficiency by using the unmatched projection/backprojection models in a parallel computation. The imaging resolution is not sacrificed and remains accurate during computed tomographic reconstruction.||2015|10.1088/1674-1056/24/2/028703|Lei Li, Jianxin Li, Ailong Cai, Linyuan Wang, Xiaoqi Xi, Jianlin Chen, Bin Yan, Hanming Zhang|1.0|2
1584|Hydrodynamic Simulations using GPGPU Architectures|Simulating the flow of different fluids can be a highly computational intensive process, which requires large amounts of resources. Recently there has been a lot of research effort directed towards GPU processing, which can greatly increase the performance of different applications, such as Smoothed Particle Hydrodynamics (SPH), which is most commonly used for hydrodynamic simulations. Smoothed particle hydrodynamics (SPH) is a numerical method commonly used in Computational Fluid Dynamics (CFD). It is a method that can simulate particle flow and interaction with structures and highly deformable bodies. It replaces the fluid with a set of particles that carry properties such as mass, speed and position that move according to the governing dynamics. The dynamics of fluids are based on the Navier-Stokes equations. These describe the physical properties of continuous fields in the fluid. SPH approximates these equations using an integral interpolant that is then solved numerically. This article addresses the current state of technologies available that can be used to speed up the algorithm and proposes a set of optimizations that can be achieved by using different frameworks. We also draw conclusions regarding the equilibrium between performance and accuracy, using different numerical algorithms, frameworks and hardware optimizations.|arXiv.org|2019|10.1007/978-3-030-11009-3_9|E. Slusanschi, Elena Simona Apostol, C. Leordeanu, Adrian Coman|1.0|2
349|Multi-scale Visualization of Molecular Architecture Using Real-Time Ambient Occlusion in Sculptor|The modeling of large biomolecular assemblies relies on an efficient rendering of their hierarchical architecture across a wide range of spatial level of detail. We describe a paradigm shift currently under way in computer graphics towards the use of more realistic global illumination models, and we apply the so-called ambient occlusion approach to our open-source multi-scale modeling program, Sculptor. While there are many other higher quality global illumination approaches going all the way up to full GPU-accelerated ray tracing, they do not provide size-specificity of the features they shade. Ambient occlusion is an aspect of global lighting that offers great visual benefits and powerful user customization. By estimating how other molecular shape features affect the reception of light at some surface point, it effectively simulates indirect shadowing. This effect occurs between molecular surfaces that are close to each other, or in pockets such as protein or ligand binding sites. By adding ambient occlusion, large macromolecular systems look much more natural, and the perception of characteristic surface features is strongly enhanced. In this work, we present a real-time implementation of screen space ambient occlusion that delivers realistic cues about tunable spatial scale characteristics of macromolecular architecture. Heretofore, the visualization of large biomolecular systems, comprising e.g. hundreds of thousands of atoms or Mega-Dalton size electron microscopy maps, did not take into account the length scales of interest or the spatial resolution of the data. Our approach has been uniquely customized with shading that is tuned for pockets and cavities of a user-defined size, making it useful for visualizing molecular features at multiple scales of interest. This is a feature that none of the conventional ambient occlusion approaches provide. Actual Sculptor screen shots illustrate how our implementation supports the size-dependent rendering of molecular surface features.|PLoS Comput. Biol.|2015|10.1371/journal.pcbi.1004516|M. Wahle, W. Wriggers|0.9|2
361|GREEN Cache: Exploiting the Disciplined Memory Model of OpenCL on GPUs|As various graphics processing unit architectures are deployed across broad computing spectrum from a hand-held or embedded device to a high-performance computing server, OpenCL becomes the de facto standard programming environment for general-purpose computing on graphics processing units. Unlike its CPU counterpart, OpenCL has several distinct features such as its disciplined memory model, which is partially inherited from conventional 3D graphics programming models. On the other hand, due to ever increasing memory bandwidth pressure and low power requirement, the capacity of on-chip caches in GPUs keeps increasing overtime. Given such trends, we believe that we have interesting programming model/architecture co-optimization opportunities, in particular, how to energy-efficiently utilize large on-chip caches for GPUs. In this paper, as a showcase, we study the characteristics of the OpenCL memory model and propose a technique called GPU Region-aware energy-efficient non-inclusive cache hierarchy, or GREEN cache hierarchy. With the GREEN cache, our simulation results show that we can save 56 percent of dynamic energy in the L1 cache, 39 percent of dynamic energy in the L2 cache, and 50 percent of leakage energy in the L2 cache with practically no performance degradation and off-chip access increases.|IEEE transactions on computers|2015|10.1109/TC.2015.2395435|Jaekyu Lee, Hyesoon Kim, Dong Hyuk Woo, M. Azimi|0.9|2
374|Nyami: a synthesizable GPU architectural model for general-purpose and graphics-specific workloads|Graphics processing units (GPUs) continue to grow in popularity for general-purpose, highly parallel, high-throughput systems. This has forced GPU vendors to increase their focus on general purpose workloads, sometimes at the expense of the graphics-specific workloads. Using GPUs for general-purpose computation is a departure from the driving forces behind programmable GPUs that were focused on a narrow subset of graphics rendering operations. Rather than focus on purely graphics-related or general-purpose use, we have designed and modeled an architecture that optimizes for both simultaneously to efficiently handle all GPU workloads. In this paper, we present Nyami, a co-optimized GPU architecture and simulation model with an open-source implementation written in Verilog. This approach allows us to more easily explore the GPU design space in a synthesizable, cycle-precise, modular environment. An instruction-precise functional simulator is provided for co-simulation and verification. Overall, we assume a GPU may be used as a general-purpose GPU (GPGPU) or a graphics engine and account for this in the architecture's construction and in the options and modules selectable for synthesis and simulation. To demonstrate Nyami's viability as a GPU research platform, we exploit its flexibility and modularity to explore the impact of a set of architectural decisions. These include sensitivity to cache size and associativity, barrel and switch-on-stall multithreaded instruction scheduling, and software vs. hardware implementations of rasterization. Through these experiments, we gain insight into commonly accepted GPU architecture decisions, adapt the architecture accordingly, and give examples of the intended use as a GPU research tool.|IEEE International Symposium on Performance Analysis of Systems and Software|2015|10.1109/ISPASS.2015.7095803|Philip Dexter, A. Carpenter, Timothy N. Miller, Jeffrey T. Bush|0.9|2
1320|Enhancing the Simulation of Membrane System on the GPU for the N-Queens Problem|Previous approaches using active membrane systems to solve the N-queens problem deﬁned manymembranes with just one rule inside them. This resultedin many communication rules utilised to communicate between membranes, which made communications betweenthe cores and the threads a very time-consuming process.The proposed approach reduces unnecessary membranesand communication rules by deﬁning two membranes withmany objects and rules inside each membrane. With thisstructure, objects and rules can evolve concurrently in parallel, which makes the model suitable for implementationon a Graphics processing unit (GPU). The speedup usinga GPU with global memory for N=10 is 10.6 times, butusing tiling and shared memory, it is 33 times.||2015|10.1049/CJE.2015.10.012|Chandren Ravie, Maroosi Ali|0.9|2
376|Performance optimization of Sparse Matrix‐Vector Multiplication for multi‐component PDE‐based applications using GPUs|Simulations of many multi‐component PDE‐based applications, such as petroleum reservoirs or reacting flows, are dominated by the solution, on each time step and within each Newton step, of large sparse linear systems. The standard solver is a preconditioned Krylov method. Along with application of the preconditioner, memory‐bound Sparse Matrix‐Vector Multiplication (SpMV) is the most time‐consuming operation in such solvers. Multi‐species models produce Jacobians with a dense block structure, where the block size can be as large as a few dozen. Failing to exploit this dense block structure vastly underutilizes hardware capable of delivering high performance on dense BLAS operations. This paper presents a GPU‐accelerated SpMV kernel for block‐sparse matrices. Dense matrix‐vector multiplications within the sparse‐block structure leverage optimization techniques from the KBLAS library, a high performance library for dense BLAS kernels. The design ideas of KBLAS can be applied to block‐sparse matrices. Furthermore, a technique is proposed to balance the workload among thread blocks when there are large variations in the lengths of nonzero rows. Multi‐GPU performance is highlighted. The proposed SpMV kernel outperforms existing state‐of‐the‐art implementations using matrices with real structures from different applications. Copyright © 2016 John Wiley & Sons, Ltd.|Concurrency and Computation|2016|10.1002/cpe.3874|D. Keyes, J. Dongarra, A. Abdelfattah, H. Ltaief|0.8888888888888888|2
412|Modeling parameterized geometry in GPU-based Monte Carlo particle transport simulation for radiotherapy|Monte Carlo (MC) particle transport simulation on a graphics-processing unit (GPU) platform has been extensively studied recently due to the efficiency advantage achieved via massive parallelization. Almost all of the existing GPU-based MC packages were developed for voxelized geometry. This limited application scope of these packages. The purpose of this paper is to develop a module to model parametric geometry and integrate it in GPU-based MC simulations. In our module, each continuous region was defined by its bounding surfaces that were parameterized by quadratic functions. Particle navigation functions in this geometry were developed. The module was incorporated to two previously developed GPU-based MC packages and was tested in two example problems: (1) low energy photon transport simulation in a brachytherapy case with a shielded cylinder applicator and (2) MeV coupled photon/electron transport simulation in a phantom containing several inserts of different shapes. In both cases, the calculated dose distributions agreed well with those calculated in the corresponding voxelized geometry. The averaged dose differences were 1.03% and 0.29%, respectively. We also used the developed package to perform simulations of a Varian VS 2000 brachytherapy source and generated a phase-space file. The computation time under the parameterized geometry depended on the memory location storing the geometry data. When the data was stored in GPU’s shared memory, the highest computational speed was achieved. Incorporation of parameterized geometry yielded a computation time that was ~3 times of that in the corresponding voxelized geometry. We also developed a strategy to use an auxiliary index array to reduce frequency of geometry calculations and hence improve efficiency. With this strategy, the computational time ranged in 1.75–2.03 times of the voxelized geometry for coupled photon/electron transport depending on the voxel dimension of the auxiliary index array, and in 0.69–1.23 times for photon only transport.|Physics in Medicine and Biology|2016|10.1088/0031-9155/61/15/5851|X. Jia, Y. Chi, Z. Tian|0.8888888888888888|2
440|SOP‐GPU: influence of solvent‐induced hydrodynamic interactions on dynamic structural transitions in protein assemblies|Hydrodynamic interactions (HI) are incorporated into Langevin dynamics of the Cα‐based protein model using the Truncated Expansion approximation (TEA) to the Rotne–Prager–Yamakawa diffusion tensor. Computational performance of the obtained GPU realization demonstrates the model's capability for describing protein systems of varying complexity (102–105 residues), including biological particles (filaments, virus shells). Comparison of numerical accuracy of the TEA versus exact description of HI reveals similar results for the kinetics and thermodynamics of protein unfolding. The HI speed up and couple biomolecular transitions through cross‐communication among protein domains, which result in more collective displacements of structure elements governed by more deterministic (less variable) dynamics. The force‐extension/deformation spectra from nanomanipulations in silico exhibit sharper force signals that match well the experimental profiles. Hence, biomolecular simulations without HI overestimate the role of tension/stress fluctuations. Our findings establish the importance of incorporating implicit water‐mediated many‐body effects into theoretical modeling of dynamic processes involving biomolecules. © 2016 The Authors. Journal of Computational Chemistry Published by Wiley Periodicals, Inc.|Journal of Computational Chemistry|2016|10.1002/jcc.24368|V. Barsegov, Andrey Alekseenko, O. Kononova, Y. Kholodov, K. Marx|0.8888888888888888|2
1163|Improving performance of GPU code using novel features of the NVIDIA kepler architecture|Graphics processing unit (GPU) computing is a popular approach to simulating complex models and performing massive calculations. GPUs have attracted a great deal of interest because they offer both high performance and energy efficiency. Efficient General‐Purpose computation on Graphics Processing Units requires good parallelism, memory coalescing, regular memory access, small overhead on data exchange between the CPU and the GPU, and few explicit global synchronizations, which are usually gained from optimizing the algorithms. Besides these advantages, the proper use of some novel features provided on NVIDIA GPUs can offer further improvement. In this paper, we modify an existing optimized GPU application to illustrate the potential performance gains of these features and to demonstrate the performance trade offs of different implementation choices. The paper focuses on the challenges of reducing interactions between CPU and GPU and reducing the use of explicit synchronization. We explain how to achieve these objectives using two features of the Kepler architecture, warp shuffle, and dynamic parallelism. We find that a judicious use of these two techniques, eliminating repeated operations and synchronizations, results in significantly better performance. We describe various pitfalls encountered in optimizing our code to use these two features and how these were addressed. In particular, we identify a subtle data race condition when using dynamic parallelism under certain circumstances, and present our solution. Finally, we present a technique to trade off the allocation of various device resources to find the parameters that offer the best performance. Copyright © 2016 John Wiley & Sons, Ltd.|Concurrency and Computation|2016|10.1002/cpe.3744|Yuanzhe Li, L. Schwiebert, Eyad Hailat, J. Potoff, J. Mick|0.8888888888888888|2
308|Using Criticality of GPU Accesses in Memory Management for CPU-GPU Heterogeneous Multi-Core Processors|Heterogeneous chip-multiprocessors with CPU and GPU integrated on the same die allow sharing of critical memory system resources among the CPU and GPU applications. Such architectures give rise to challenging resource scheduling problems. In this paper, we explore memory access scheduling algorithms driven by criticality of GPU accesses in such systems. Different GPU access streams originate from different parts of the GPU rendering pipeline, which behaves very differently from the typical CPU pipeline requiring new techniques for GPU access criticality estimation. We propose a novel queuing network model to estimate the performance-criticality of the GPU access streams. If a GPU application performs below the quality of service requirement (e.g., frame rate in 3D scene rendering), the memory access scheduler uses the estimated criticality information to accelerate the critical GPU accesses. Detailed simulations done on a heterogeneous chip-multiprocessor model with one GPU and four CPU cores running heterogeneous mixes of DirectX, OpenGL, and CPU applications show that our proposal improves the GPU performance by 15% on average without degrading the CPU performance much. Extensions proposed for the mixes containing GPGPU applications, which do not have any quality of service requirement, improve the performance by 7% on average for these mixes.|ACM Transactions on Embedded Computing Systems|2017|10.1145/3126540|Mainak Chaudhuri, S. Rai|0.875|2
1168|Building Cognition from Spiking Neurons : Nengo and the Neural Engineering Framework|There are two primary objectives for this full-day tutorial. First, we will demonstrate how complex cognitive architectures can be implemented using spiking neurons, including both low-level sensorimotor systems and highlevel systems such as working memory, symbol-like manipulations, and cognitive control. Second, we will identify the class of algorithms that neurons are particularly efficient at implementing. These algorithms are slightly different from the typical algorithms that are efficient to implement on traditional computers, and result in different and unexpected capabilities. The basis of this tutorial is the Neural Engineering Framework (NEF; Eliasmith and Anderson, 2003) and the freely available software toolkit Nengo (Bekolay et al., 2014; <http://nengo.ca>). The NEF provides a method for taking algorithms and deriving the optimal method for organizing neurons to approximate those algorithms. These neurons can be of any type (rate-based, spiking, compartmental, etc.), and can be constrained to match neurophysiological parameters. Nengo is a software toolkit that implements the NEF, providing a Python scripting library for defining these models, an interactive user interface for visualizing the models, and support for running these models on a variety of hardware, including GPUs and specialized neuromorphic processors. These tools have previously been used to build models of working memory (Singh and Eliasmith, 2004; Duggins et al., 2017), word associations (Kajić et al., 2017), adaptive arm control (DeWolf et al., 2016), action planning (Blouw et al., 2016), action selection (Stewart et al., 2012), and many other phenomena. They were also used to create Spaun, the first, largest, and still only, spiking-neuron-based brain simulation capable of performing multiple different cognitive tasks (Eliasmith et al., 2012). During the workshop, participants will gain hands-on experience with using Nengo to define neural models by specifying the desired algorithms. They will be able to adjust neural parameters so as to adjust these algorithms given neurophysiological constraints. Participants will also be exposed to the techniques used to take standard symbolic algorithms and convert them into algorithms suitable for implementation using neurons. Earlier versions of this tutorial have been presented at ICCM (2009) and CogSci (2010, 2011, and 2012). However, since then, Nengo has been extensively redesigned, providing both a more flexible and integrated user interface and the capability of interfacing with more powerful computing hardware, allowing for much larger models to be run more quickly. For example, the original Spaun model (2.5 million neurons) required 2.5 hours to produce 1 second of simulation. With the new version of Nengo, we can now run that same model requiring only 20 seconds for 1 second of simulation, using modern GPUs. We have also redesigned our API to make the definition of cognitive models easier. We believe these extensive changes mean that it is appropriate to present this new version of Nengo at ICCM.||2017|10.1109/iscas.2017.8050810|T. Stewart|0.875|2
1252|A GPU-based Parallelization Approach to conduct Spatially-Explicit Uncertainty and Sensitivity Analysis in the Application Domain of Landscape Assessment|This paper illustrates a CUDA GPU-based concept to accelerate the computationally intensive calculations of performing spatially-explicit uncertainty and sensitivity analysis in multi-criteria decision-making models. Uncertainty and sensitivity analysis is a two-step approach to validating the robustness of spatial- and non-spatial model solutions. The uncertainty analysis quantifies the variability of model outcomes, while the sensitivity analysis accounts for the contributions of model inputs to the overall model output variability. The proposed solution is applicable for large-scale spatial problems that incorporate millions of alternatives and hundreds of thousands of simulation runs. Furthermore, this GPU-based concept represents a low-cost approach in comparison to high-performance computing that incorporates super computers. Additionally, the concept allows the integration of different decision rules (e.g. simple additive weighting, ideal point, ordered weighting averaging, or analytical hierarchy process) in order to evaluate the performance of the alternatives involved. The proposed approach was tested on a landscape assessment example in order to identify the variability of the model outcomes with respect to the criteria ‘Compactness’, ‘Mean Patch Area’, ‘Relief Energy’ and ‘Variety’ that define landscape diversity.||2017|10.1553/GISCIENCE2017_01_S44|Karl-Heinrich Anders, Christoph Erlacher, P. Jankowski, T. Blaschke, Gernot Paulus|0.875|2
1314|A GPU Implementation of a Shooting and Bouncing Ray Tracing Method for Radio Wave Propagation|─ Shooting and bouncing ray tracing method (SBR) is widely adopted in radio wave propagation simulations. Compared with the center-ray tube model, the lateral-ray tube model is more accurate but more time consuming. As a result, we use graphics processing unit (GPU) to accelerate the lateral-ray tube model. In this paper, we proposed a GPU-Based shooting and bouncing lateral-ray tube tracing method that is applied to predicting the radio wave propagation. The numerical experiment demonstrates that the GPU-based SBR can significantly improve the computational efficiency of lateral-ray tube model about 16 times faster, while providing the same accuracy as the CPU-based SBR. The most efficient mode of transferring the data of triangle faces is also discussed. Index Terms ─ Compute unified device architecture (CUDA), graphics processing unit (GPU), radio wave propagation, ray tracing, shooting and bouncing ray (SBR).||2017|10.23919/ursigass.2017.8105251|Yougang Gao, Ming Zhao, Xiaohe Tang, D. Shi, Chu Wang|0.875|2
224|Weighted time lag plot defect parameter extraction and GPU-based BTI modeling for BTI variability|Recent MOSFET devices exhibit a strong variability in their Bias Temperature Instability (BTI) induced degradation (e.g., Vth-shift). For identical stress patterns, each device exhibits unique degradation behavior. As BTI variability increases with shrinking device geometries, modeling BTI variability becomes essential. The challenge of modeling BTI variability is the significant time required to characterize a representative set of devices to properly calibrate the BTI variability model. In addition, (SPICE) circuit simulations under BTI variability are extremely time consuming. Both challenges originate from unique uncorrelated BTI behavior in each device. Each device features a unique set of defects with a unique state (occupied/unoccupied) in each defect. In this work, we tackle the characterization challenge by processing the data acquired from our parallel measurement setup with lightweight and fast defect extraction. Our novel weighted time lag plot defect parameter extraction, removes uncorrelated voltage noise and categorizes correlated noise (i.e., Random Telegraph Noise (RTN)) and discrete voltage steps (i.e., BTI). After the measurement data is processed, capture time, emission time and induced degradation of each defect can be extracted. After defect parameters are extracted, we can fit a bi-variate log-normal defect distribution and calibrate our BTI model. To employ a BTI variability model in circuit simulation, it must be able to model thousands of MOSFETs. Circuits consist of thousands of devices, each with unique behavior, resulting in computationally intensive modeling. Our GPU-based BTI variability model employs massive parallelism (beyond 1000 processing cores) found in graphic cards to model thousands of MOSFETs in seconds. Therefore, our novel defect parameter extraction methodology allows lightweight, yet accurate characterization of our model, while our model itself enables circuit simulations in large circuits as it models 100,000 MOSFETs in just 119s.|IEEE International Reliability Physics Symposium|2018|10.1109/IRPS.2018.8353659|J. Diaz-Fortuny, J. Martín-Martínez, H. Amrouch, Victor M. van Santen, R. Castro-López, F. Fernández, R. Rodríguez, J. Henkel, M. Nafría, E. Roca|0.8571428571428571|2
1208|Accelerating GPU betweenness centrality|Graphs that model social networks, numerical simulations, and the structure of the Internet are enormous and cannot be manually inspected. A popular metric used to analyze these networks is Betweenness Centrality (BC), which has applications in community detection, power grid contingency analysis, and the study of the human brain. However, these analyses come with a high computational cost that prevents the examination of large graphs of interest. Recently, the use of Graphics Processing Units (GPUs) has been promising for efficient processing of unstructured data sets. Prior GPU implementations of BC suffer from large local data structures and inefficient graph traversals that limit scalability and performance. Here we present a hybrid GPU implementation that provides good performance on graphs of arbitrary structure rather than just scale-free graphs as was done previously. Our methods achieve up to 13× speedup on high-diameter graphs and an average of 2.71× speedup overall compared to the best existing GPU algorithm. We also observe near linear speedup when running BC on 192 GPUs.|Communications of the ACM|2018|10.1145/3230485|David A. Bader, Adam McLaughlin|0.8571428571428571|2
1219|DM-HEOM: A Portable and Scalable Solver-Framework for the Hierarchical Equations of Motion|Computing the Hierarchical Equations of Motion (HEOM) is by itself a challenging problem, and so is writing portable production code that runs efficiently on a variety of architectures while scaling from PCs to supercomputers. We combined both challenges to push the boundaries of simulating quantum systems, and to evaluate and improve methodologies for scientific software engineering. Our contributions are threefold: We present the first distributed memory implementation of the HEOM method (DM-HEOM), we describe an interdisciplinary development workflow, and we provide guidelines and experiences for designing distributed, performance-portable HPC applications with MPI-3, OpenCL and other state-of-the-art programming models. We evaluated the resulting code on multi- and many-core CPUs as well as GPUs, and demonstrate scalability on a Cray XC40 supercomputer for the PS I molecular light harvesting complex.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2018|10.1109/IPDPSW.2018.00149|Tobias Kramer, T. Steinke, A. Reinefeld, M. Noack|0.8571428571428571|2
1226|Energy‐based tuning of metaheuristics for molecular docking on multi‐GPUs|Virtual Screening (VS) methods simulate molecular interactions in silico to look for the best chemical compound that interacts with a given molecular target. VS is becoming increasingly popular to accelerate the drug discovery process and constitute hard optimization problems with a huge computational cost. To deal with these two challenges, we have created METADOCK, an application that (1) enables a wide range of metaheuristics through a parametrized schema and (2) promotes the use of a multi‐GPU environment within a heterogeneous cluster. Metaheuristics provide approximate solutions in a reasonable time frame, but, given the stochastic nature of real‐life procedures, the energy budget goes hand in hand with acceleration to validate the proposed solution. This paper evaluates energy trade‐offs and correlations with performance for a set of metaheuristics derived from METADOCK. We establish a solid inference from minimal power to maximal performance in GPUs, and from there, to optimal energy consumption. This way, ideal heuristics can be chosen according not only to best accuracy and performance but also to energy requirements. Our study starts with a preselection of parameterized metaheuristic functions, building blocks where we will find optimal patterns from power criteria while preserving parallelism through a GPU execution. We then establish a methodology to figure out the best instances of the parameterized kernels based on energy patterns obtained, which are analyzed from different viewpoints, ie, performance, average power, and total energy consumed. We also compare the best workload distributions for optimal performance and power efficiency among Pascal and Maxwell GPUs on popular Titan models. Our experimental results demonstrate that the most power efficient GPU can be overloaded in order to reduce the total amount of energy required by as much as 20%, finding unique scenarios where Maxwell does it better in execution time, but with Pascal always ahead in performance per watt, reaching peaks of up to 40%.|Concurrency and Computation|2018|10.1002/cpe.4684|M. Ujaldón, J. Cecilia, Baldomero Imbernón, Jesús Pérez Serrano|0.8571428571428571|2
1232|Faster than the Speed of Life: Accelerating Developmental Biology Simulations with GPUs and FPGAs|Life scientists are faced with the tough challenge of developing high-performance computer simulations of their increasingly complex models. BioDynaMo is an open-source biological simulation platform that aims to alleviate them from the intricacies that go into development. Life scientists are able to base their models on top of BioDynaMo’s highly optimized core execution engine. At the core of all biological simulations is the mechanical interactions between possibly millions of objects. In this work we investigate the currently implemented method of handling mechanical interactions, and ways to improve the performance in order to enable large-scale and complex simulations. We propose to replace the existing kd-tree implementation for neighborhood operations with a uniform grid method that allows us to take advantage of architectures of hardware accelerators, such as GPUs and FPGAs. As a result, the multi-threaded uniform grid implementation accounts for a 14× speedup with respect to the serial baseline version. Accelerating the mechanical interactions through hardware acceleration proved to perform best on a GPU, with a resulting speedup of 134×.||2018|10.1016/j.jpdc.2018.04.014|Ahmad Hesam|0.8571428571428571|2
1253|Inexact descent methods for elastic parameter optimization|Elastic parameter optimization has revealed its importance in 3D modeling, virtual reality, and additive manufacturing in recent years. Unfortunately, it is known to be computationally expensive, especially if there are many parameters and data samples. To address this challenge, we propose to introduce the inexactness into descent methods, by iteratively solving a forward simulation step and a parameter update step in an inexact manner. The development of such inexact descent methods is centered at two questions: 1) how accurate/inaccurate can the two steps be; and 2) what is the optimal way to implement an inexact descent method. The answers to these questions are in our convergence analysis, which proves the existence of relative error thresholds for the two inexact steps to ensure the convergence. This means we can simply solve each step by a fixed number of iterations, if the iterative solver is at least linearly convergent. While the use of the inexact idea speeds up many descent methods, we specifically favor a GPU-based one powered by state-of-the-art simulation techniques. Based on this method, we study a variety of implementation issues, including backtracking line search, initialization, regularization, and multiple data samples. We demonstrate the use of our inexact method in elasticity measurement and design applications. Our experiment shows the method is fast, reliable, memory-efficient, GPU-friendly, flexible with different elastic models, scalable to a large parameter space, and parallelizable for multiple data samples.|ACM Transactions on Graphics|2018|10.1145/3272127.3275021|Huamin Wang, Guo-Zi Yang, Wei Li, Ruigang Yang|0.8571428571428571|2
1527|GPU-Accelerated Boussinesq Model Using Compute Unified Device Architecture FORTRAN|ABSTRACT Kim, B.; Oh, C.; Yi, Y., and Kim, D.-H., 2018. GPU-Accelerated of Boussinesq model using compute unified device architecture FORTRAN. In: Shim, J.-S.; Chun, I., and Lim, H.S. (eds.), Proceedings from the International Coastal Symposium (ICS) 2018 (Busan, Republic of Korea). Journal of Coastal Research, Special Issue No. 85, pp. 1176–1180. Coconut Creek (Florida), ISSN 0749-0208. Graphic Processing Units (GPU) have a number of arithmetic units and their associated structures specialized for graphic processes make the computational performances much faster than CPU (Central Processing Units). In these days, many numerical models implemented by FORTRAN have been applied on real field scale problems, which requires huge computational resources and simulation time as well. In this study, a GPU version of Boussinesq equation model was implemented using the Compute Unified Device Architecture (CUDA) FORTRAN. The computed results of the GPU-CUDA FORTRAN Boussinesq model were verified by comparing with the computed result of a CPU based Boussinesq model that had been already verified for many benchmark tests. Exact agreements except round off magnitude have been observed from the comparison. The GPU-CUDA FORTRAN Boussinesq model showed about 20 times faster computational time compared with the CPU based code. In addition, as the computational domain becomes larger, the computational efficiency of GPU-CUDA FORTRAN version over the CPU version more increased.|Journal of Coastal Research|2018|10.2112/SI85-236.1|Dae‐Hong Kim, Youngmin Yi, Chanyoung Oh, Boram Kim|0.8571428571428571|2
499|On-Line Recursive Decomposition of Intramuscular EMG Signals Using GPU-Implemented Bayesian Filtering|Objective: Real-time intramuscular electromyography (iEMG) decomposition, which is needed in biofeedback studies and interfacing applications, is a complex procedure that involves identifying the motor neuron spike trains from a streaming iEMG recording. Methods: We have previously proposed a sequential decomposition algorithm based on a Hidden Markov Model of EMG, which used Bayesian filter to estimate unknown parameters of motor unit (MU) spike trains, as well as their action potentials (MUAPs). Here, we present a modification of this original model in order to achieve a real-time performance of the algorithm as well as a parallel computation implementation of the algorithm on Graphics Processing Unit (GPU). Specifically, the Kalman filter previously used to estimate the MUAPs, is replaced by a least-mean-square filter. Additionally, we introduce a number of heuristics that help to omit the most improbable decomposition scenarios while searching for the best solution. Then, a GPU-implementation of the proposed algorithm is presented. Results: Simulated iEMG signals containing up to 10 active MUs, as well as five experimental fine-wire iEMG signals acquired from the tibialis anterior muscle, were decomposed in real time. The accuracy of decompositions depended on the level of muscle activation, but in all cases exceeded 85${\%}$. Conclusion: The proposed method and implementation provide an accurate, real-time interface with spinal motor neurons. Significance: The presented real time implementation of the decomposition algorithm substantially broadens the domain of its application.|IEEE Transactions on Biomedical Engineering|2019|10.1109/TBME.2019.2948397|Konstantin Akhmadeev, Tianyi Yu, Y. Aoustin, D. Farina, E. Le Carpentier|0.8333333333333334|2
501|Exploration of formal verification in GPU hardware IP|Today, digital circuits are part of every ones daily life in form of mobile phones, computers, television, smart cards etc. The advent of new technologies such as internet of things, 5G etc. are continuously making the digital circuits more and more complex in design. With this increase in complexity comes the possibility of more bugs in the system. Finding and fixing these bugs is of paramount importance as it can lead to huge financial losses or can even be fatal especially in safety critical applications. Over the last few decades, traditional simulation based verification has been used as the default methodology to verify digital hardware designs. Although it has certain drawbacks such as: \n \n • It is not exhaustive in nature meaning that it is very tough to cover all the possible input test vector as part of input stimulus being applied to the test bench. \n • And due to its non-exhaustive nature, there will always be a possibility of missing bugs in the design especially the corner cases that can lower the design quality. \n \nRecently formal verification has been evolved as an attractive and more comprehensive alternative to simulation based verification. In this master thesis work, model checking (alternatively property based verification) has been applied as a preferred formal verification technique on a module inside the Arm Mali GPU hardware IP in order to try and hit the corner case design bugs, if any. The module or DUT chosen for investigation in this thesis work is FSDC. This is a new module and mainly consists of control path logic which suits formal verification. The DUT has already been extensively verified by simulation based verification. The result shown later for this thesis work highlights that: \n \n • Formal verification can be applied to a complete module as an alternative verification methodology. \n • After the application of formal verification on DUT, 3 design bugs were discovered which were missed by the simulation verification. \n \nFinally, based on above results and various challenges faced during this thesis work such as state space exploration problem or applying formal verification on complex parts of the design, it can be concluded that formal verification complements and at least provides a partial solution to the limitations of simulation based verification.||2019|10.1145/3316781.3323478|Nishant Gupta|0.8333333333333334|2
506|NeuDATool: An open source neutron data analysis tools, supporting GPU hardware acceleration, and across-computer cluster nodes parallel|Empirical potential structure refinement (EPSR) is a neutron scattering data analysis algorithm and a software package. It was developed by the British spallation neutron source (ISIS) Disordered Materials Group in 1980s, and aims to construct the most-probable atomic structures of disordered liquids. It has been extensively used during the past decades, and has generated reliable results. However, it is programmed in Fortran and implements a shared-memory architecture with OpenMP. With the extensive construction of supercomputer clusters and the widespread use of graphics processing unit (GPU) acceleration technology, it is now necessary to rebuild the EPSR with these techniques in the effort to improve its calculation speed. In this study, an open source framework NeuDATool is proposed. It is programmed in the object-oriented language C++, can be paralleled across nodes within a computer cluster, and supports GPU acceleration. The performance of NeuDATool has been tested with water and amorphous silica neutron scattering data. The test shows that the software could reconstruct the correct microstructure of the samples, and the calculation speed with GPU acceleration could increase by more than 400 times compared with CPU serial algorithm at a simulation box consists about 100 thousand atoms. NeuDATool provides another choice for scientists who are familiar with C++ programming and want to define specific models and algorithms for their analyses.||2019|10.1063/1674-0068/CJCP2005077|He Cheng, Taisen Zuo, Guisheng Jiao, Zehua Han, Changli Ma|0.8333333333333334|2
521|Jily: Cost-Aware AutoScaling of Heterogeneous GPU for DNN Inference in Public Cloud|Recently, a large number of DNN inference services have emerged in public clouds, making the low-cost deployment of DNN inference services a hot research topic. Previous studies have failed to take into account GPU heterogeneity and batch processing, both of which will seriously affect the financial cost as well as the latency. In this paper, we study the problem of DNN inference service deployment in public cloud, considering both GPU heterogeneity and batch processing. The goal is to minimize the financial costs under the constraint of latency. We propose Jily, an autoscaling scheduler for DNN inference services to minimize the cost while satisfying the given latency SLO. Jily finds the optimal heterogeneous GPU instance provisioning through a DNN inference model profiler, a latency estimator, a workload predictor and a cost-aware scaler. Simulation results demonstrate that Jily can reduce average cost by up to 28% compared to a state-of-the-art autoscaling approach. Further, Jily has been proved to have good versatility and robustness under different batching mechanisms and latency SLO constrains.|IEEE International Performance, Computing, and Communications Conference|2019|10.1109/IPCCC47392.2019.8958770|Zhaoxing Wang, Qiuyang Liu, Jizhong Han, Xuehai Tang|0.8333333333333334|2
545|Simulation for Reliability, Hardware Security, and Ising Computing in VLSI Chip Design|"Author(s): Cook, Chase William | Advisor(s): Tan, Sheldon X.-D | Abstract: The continued scaling of VLSI circuits has provided a wealth of opportunities andchallenges to the VLSI circuit design area. Both these challenges and opportunities, however,require new simulation tools that can enable their solution or exploitation as classicalmethods typically dealt with problem domains with smaller scales or less complexity. Inthis dissertation, simulation methods are presented to address the emerging VLSI designtopics of Electromigration induced aging and Ising computing and are then applied to theapplication areas of hardware security and graph partitioning respectively.The Electromigration aging effect in VLSI circuits is a long-term reliability issueaffecting current carrying metal wires leading to IR drop degradation. Typically, simpleanalytical equations can determine a wire’s effective age or if it will be affected by the EMaging effect at all. However, these classical methods are overly conservative and can lead toover design or unnecessary design iterations. Furthermore, it is expected that the EM agingeffect will become more severe in future Integrated Cirucits (ICs) due to increasing currentdensities and the prevalance of polycrystaline copper atom structures seen at small wiredimensions. For this reason, more comprehensive simulation techniques that can efficientlysimulate the EM effect with less conservative results can help mitigate overdesign andincrease design margins while reducing design iterations.The area of Hardware Security is becoming increasingly important as the chipsupply chain becomes more globalized and the integrity of chips becomes more diffiuclt toverify. Utilizing the accurate simulation techniques for EM, we can utilize this reliabilityeffect to demonstrate how a reliability based attack could be perpatrated. Furthermore, wecan utilize this aging effect as a defense mechanism to help us validate the integrity of anIC and detect counterfeit chips in the component supply chain market.Ising computing is an emerging method of solving combinatorial optimization problemsby simulating the interactions of so-called spin glasses and their interactions. Borrowingconcepts from quantum computing, this methods mimics the quantum interaction betweenspin glasses in such a way that finding a ground state of these spin glass models leadsto the solution of a particular problem. In this dissertation, effective methods of simulatingthe spin glass interactions using General Purpose Graphics Processing Units (GPGPUs)and finding their ground state are developed.In addition to the GPU based Ising model simulations, important combinatorialproblems can be mapped to the Ising model. In this dissertation the Ising solver is appliedto graph partitioning which can be utilized in VLSI design and many other domains as well.Specifically, solvers for the maxcut problem and the balanced min-cut partitioning problemare developed."||2019|10.1016/j.vlsi.2019.01.011|Chase Cook|0.8333333333333334|2
580|Computational Fluid Dynamics Simulation on the Heat Sink Performance of a Graphics Processing Unit Thermal Management|The article focuses on the numerical investigation of temperature distribution in a central processing units (CPU) case with different time interval such as t = 100 s, 200 s, 300 s, 400 s, 500 s. Heat sink performance of a graphics processing unit (GPU) thermal management and impacts of different shape and velocity on the thermal performance are considered. In this study, three heat sink models are designed (A, B, and C) based on the volume area of heat sink. This study emphasizes the heat transfer phenomena caused by a GPU in a computer case in both steady state and transient state. A CFD software STAR – CCM + is used to carry out to study the fluid flow and heat transfer simulation of graphics card heat sink in a computer case and the same time an enhanced method of reducing the temperature of GPU is proposed. The results show that heat sink B with the least volume area, has the fastest rate of heat exchange followed by heat sink C and heat sink A. Likewise, the result indicates an inverse relationship between the volume and the total surface of the heat sink and the final temperature of the graphics card chip. As the total volume and surface of the heat sink increases, the rate of heat transfer increases via faster rate of conduction between graphics card chip to heat sink meanwhile the cooling of the heat sink is aided by wind inlet via convection.|Jurnal Kejuruteraan|2019|10.17576/jkukm-2019-31(1)-17|Jie Suang Ng, M. F. Abdullah, Z. Harun, W. M. Mahmood, E. Lotfy|0.8333333333333334|2
1425|A Framework for Mesoscopic Traffic Simulation in GPU|Much progress has recently been made to enhance mesoscopic traffic simulation with the focus on advanced modeling features and traffic management support capabilities. Subsequently, the computational performance needs to be improved in order to make mesoscopic traffic simulation stay effective to real-time applications. This paper presents a framework for mesoscopic traffic simulation which offloads both the demand and supply components to GPU. The simulation algorithm divides the simulation flow into different steps and designs multiple kernels to handle the steps. A high level of data parallelism is achieved by assigning the GPU threads to the appropriate components of the traffic network. Several optimization options using an innovative data structure and improved warp execution are also deployed to harness the GPU performance while preserving simulation correctness. The performance of the framework is evaluated in a real network showing the speedup of up to nearly 5 times in the demand simulation and more than 4 times in the supply simulation compared to the sequential simulation.|IEEE Transactions on Parallel and Distributed Systems|2019|10.1109/TPDS.2019.2896636|Gary S. H. Tan, V. Vu|0.8333333333333334|2
1550|Probabilistic hypergraph grammars for efficient molecular optimization|We present an approach to make molecular optimization more efficient. We infer a hypergraph replacement grammar from the ChEMBL database, count the frequencies of particular rules being used to expand particular nonterminals in other rules, and use these as conditional priors for the policy model. Simulating random molecules from the resulting probabilistic grammar, we show that conditional priors result in a molecular distribution closer to the training set than using equal rule probabilities or unconditional priors. We then treat molecular optimization as a reinforcement learning problem, using a novel modification of the policy gradient algorithm - batch-advantage: using individual rewards minus the batch average reward to weight the log probability loss. The reinforcement learning agent is tasked with building molecules using this grammar, with the goal of maximizing benchmark scores available from the literature. To do so, the agent has policies both to choose the next node in the graph to expand and to select the next grammar rule to apply. The policies are implemented using the Transformer architecture with the partially expanded graph as the input at each step. We show that using the empirical priors as the starting point for a policy eliminates the need for pre-training, and allows us to reach optima faster. We achieve competitive performance on common benchmarks from the literature, such as penalized logP and QED, with only hundreds of training steps on a budget GPU instance.|arXiv.org|2019|10.1007/978-3-030-13435-8_17|Mark Harley, E. Kraev|0.8333333333333334|2
1588|Quantitative predictions in small-animal X-ray fluorescence tomography.|X-ray fluorescence (XRF) tomography from nanoparticles (NPs) shows promise for high-spatial-resolution molecular imaging in small-animals. Quantitative reconstruction algorithms aim to reconstruct the true distribution of NPs inside the small-animal, but so far there has been no feasible way to predict signal levels or evaluate the accuracy of reconstructions in realistic scenarios. Here we present a GPU-based computational model for small-animal XRF tomography. The unique combination of a highly accelerated Monte Carlo tool combined with an accurate small-animal phantom allows unprecedented realistic full-body simulations. We use this model to simulate our experimental system to evaluate the quantitative performance and accuracy of our reconstruction algorithms on large-scale organs as well as mm-sized tumors. Furthermore, we predict the detection limits for sub-mm tumors at realistic NP concentrations. The computational model will be a valuable tool for optimizing next-generation experimental arrangements and reconstruction algorithms.|Biomedical Optics Express|2019|10.1364/BOE.10.003773|H. Hertz, Jakob C. Larsson, K. Shaker|0.8333333333333334|2
1613|Improvement of Control Performance of Sampling Based Model Predictive Control using GPU|This paper presents the application of Graphics Processing Unit (GPU) to improve the control performance of sampling based predictive control algorithms. As an example problem, obstacle avoidance situation with parked cars in a street is modeled as a non-linear model predictive control problem. Car dynamics and non-linear constraints are considered to achieve collision avoidance. The control input must be optimized in every control step in real-time considering the non-linear constraints. Sampling based approach is used to solve this problem and one of the major limitations to this approach is the computational cost involved. In this paper, the sampling-based optimization algorithm was adapted to utilize the parallel computing capabilities of GPU using CUDA. The generated input sequence and the computational speeds were compared with a CPU based program for the same case. The proposed method is implemented in a simulation experiment with car dynamics simulator to verify its performance in terms of path tracking. Finally, a general relationship between sample size and GPU acceleration of its calculation speed is also discussed.|2019 IEEE Intelligent Vehicles Symposium (IV)|2019|10.1109/IVS.2019.8813807|Tatsuya Suzuki, H. Okuda, Arun Muraleedharan|0.8333333333333334|2
236|Simulation of parallel similarity measure computations for large data sets|The paper presents our approach to implementation of similarity measure for big data analysis in a parallel environment. We describe the algorithm for parallelisation of the computations. We provide results from a real MPI application for computations of similarity measures as well as results achieved with our simulation software. The simulation environment allows us to model parallel systems of various sizes with various components such as CPUs, GPUs, network interconnects, and model parallel applications in a meta language. The simulations allow us to determine in details how computations will be performed on a particular hardware. They also allow to predict the shapes of time curves beyond the area where empirical results can be obtained due to limited computational resources such as memory capacity.|International Conference on Cybernetics|2015|10.1109/CYBConf.2015.7175980|P. Rosciszewski, Mariusz R. Matuszek, P. Czarnul, J. Szymański|0.8|2
531|flatspin: A large-scale artificial spin ice simulator|"We present flatspin, a novel simulator for systems of interacting mesoscopic spins on a lattice, also known as artificial spin ice (ASI). Our magnetic switching criteria enables ASI dynamics to be captured in a dipole model. Through GPU acceleration, flatspin can simulate realistic dynamics of millions of magnets within practical time frames. We demonstrate flatspin's versatility through the reproduction of a diverse set of established experimental results from the literature. In particular, magnetization details of ""pinwheel"" ASI during field-driven reversal have been reproduced, for the first time, by a dipole model. The simulation framework enables quick exploration and investigation of new ASI geometries and properties at unprecedented speeds."|Physical review B|2020|10.1103/PhysRevB.106.064408|J. Leliaert, E. Folven, G. Tufte, Anders Strømberg, O. R. Lykkebø, Johannes H. Jensen, Magnus Själander, A. Penty|0.8|2
560|Evaluation of Quantitative, Efficient Image Reconstruction for VersaPET, a Compact PET System.|PURPOSE\nPreviously we developed a high-resolution PET system-VersaPET -- characterized by a block geometry with relatively large axial and transaxial inter-block gaps and a compact geometry susceptible to parallax blurring effects. In this work, we report the qualitative and quantitative evaluation of a graphic processing unit (GPU)-accelerated maximum-likelihood by expectation-maximization (MLEM) image reconstruction framework for VersaPET which features accurate system geometry and projection space point-spread-function (PSF) modeling.\n\n\nMETHODS\nWe combined the ray-tracing module from STIR (an open-source PET image reconstruction package) with VersaPET's exact block geometry for the geometric system matrix. PSF modeling of crystal penetration and scattering was achieved by a custom Monte-Carlo simulation for projection space blurring in all dimensions. We also parallelized the reconstruction in GPU taking advantage of the system's symmetry for PSF computation. To investigate the effects of PSF width, we generated and studied multiple kernels between one that reflects the true LYSO density in the MC simulation and another that reflects geometry only (no PSF). GATE simulations of hot and cold-sphere phantoms with spheres of different sizes, real microDerenzo phantom and human blood vessel data were used to characterize the quantitative and qualitative performances of the reconstruction.\n\n\nRESULTS\nReconstruction with an accurate system geometry effectively improved image quality compared to STIR (version 3.0) which assumes an idealized system geometry. Reconstructions of GATE-simulated hot-sphere phantom data showed that all PSF kernels achieved superior performance in contrast recovery and bias reduction compared to using no PSF, but may introduce edge artifact and lumped background noise pattern depending on the width of PSF kernels. Cold-sphere phantom simulation results also indicated improvement in contrast recovery and quantification with PSF modeling (compared to no PSF) for 5 mm and 10 mm cold spheres. Real microDerenzo phantom images with the PSF kernel that reflects the true LYSO density showed degraded resolving power of small sectors that could be resolved more clearly by underestimated PSF kernels, which is consistent with recent literature despite differences in scanner geometries and in approaches to system model estimation. The human vessel results resemble those of the hot-sphere phantom simulation with the PSF kernel that reflects the true LYSO density achieving the highest peak in the time activity curve (TAC) and similar lumped noise pattern.\n\n\nCONCLUSIONS\nWe fully evaluated a practical MLEM reconstruction framework that we developed for VersaPET in terms of qualitative and quantitative performance. Different PSF kernels may be adopted for improving the results of specific imaging tasks but the underlying reasons for the variation in optimal kernel for the real and simulation studies requires further study.|Medical Physics (Lancaster)|2020|10.1002/mp.14158|P. Vaska, Shouyi Wei|0.8|2
583|Distributed Interactive Visualization Using GPU-Optimized Spark|With the advent of advances in imaging and computing technologies, large-scale data acquisition and processing have become commonplace in many science and engineering disciplines. Conventional workflows for large-scale data processing usually rely on in-house or commercial software that are designed for domain-specific computing tasks. Recent advances in MapReduce, which was originally developed for batch processing textual data via a simplified programming model of the map and reduce functions, have expanded its applications to more general tasks in big-data processing, such as scientific computing, and biomedical image processing. However, as shown in previous work, volume rendering and visualization using MapReduce is still considered challenging and impractical owing to the disk-based, batch-processing nature of its computing model. In this article, contrary to this common belief, we show that the MapReduce computing model can be effectively used for interactive visualization. Our proposed system is a novel extension of Spark, one of the most popular open-source MapReduce frameworks, which offers GPU-accelerated MapReduce computing. To minimize CPU-GPU communication and overcome slow, disk-based shuffle performance, the proposed system supports GPU in-memory caching and MPI-based direct communication between compute nodes. To allow for GPU-accelerated in-situ visualization using raster graphics in Spark, we leveraged the CUDA-OpenGL interoperability, resulting in faster processing speeds by several orders of magnitude compared to conventional MapReduce systems. We demonstrate the performance of our system via several volume processing and visualization tasks, such as direct volume rendering, iso-surface extraction, and numerical simulations with in-situ visualization.|IEEE Transactions on Visualization and Computer Graphics|2020|10.1109/tvcg.2020.2990894|Sumin Hong, W. Jeong, Junyoung Choi|0.8|2
599|Simulation of Fire with a Gas Kinetic Scheme on Distributed GPGPU Architectures|The simulation of fire is a challenging task due to its occurrence on multiple space-time scales and the non-linear interaction of multiple physical processes. Current state-of-the-art software such as the Fire Dynamics Simulator (FDS) implements most of the required physics, yet a significant drawback of this implementation is its limited scalability on modern massively parallel hardware. The current paper presents a massively parallel implementation of a Gas Kinetic Scheme (GKS) on General Purpose Graphics Processing Units (GPGPUs) as a potential alternative modeling and simulation approach. The implementation is validated for turbulent natural convection against experimental data. Subsequently, it is validated for two simulations of fire plumes, including a small-scale table top setup and a fire on the scale of a few meters. We show that the present GKS achieves comparable accuracy to the results obtained by FDS. Yet, due to the parallel efficiency on dedicated hardware, our GKS implementation delivers a reduction of wall-clock times of more than an order of magnitude. This paper demonstrates the potential of explicit local schemes in massively parallel environments for the simulation of fire.|De Computis|2020|10.3390/computation8020050|Stephan Lenz, M. Geier, M. Krafczyk|0.8|2
601|GPU-accelerated Time Simulation of Systems with Adaptive Voltage and Frequency Scaling|Timing validation of systems with adaptive voltage-and frequency scaling (AVFS) requires an accurate timing model under multiple operating points. Simulating such a model at gate level is extremely time-consuming, and the state-of-the-art compromises both accuracy and compute efficiency.This paper presents a method for dynamic gate delay modeling on graphics processing unit (GPU) accelerators which is based on polynomial approximation with offline statistical learning using regression analysis. It provides glitch-accurate switching activity information for gates and designs under varying supply voltages with negligible memory and performance impact. Parallelism from the evaluation of operating conditions, gates and stimuli is exploited simultaneously to utilize the high arithmetic computing throughput of GPUs. This way, large-scale design space exploration of AVFS-based systems is enabled. Experimental results demonstrate the efficiency and accuracy of the presented approach showing speedups of three orders of magnitude over conventional time simulation that supports static delays only.|Design, Automation and Test in Europe|2020|10.23919/DATE48585.2020.9116256|E. Schneider, H. Wunderlich|0.8|2
605|An Automated Workflow for Hemodynamic Computations in Cerebral Aneurysms|In recent years, computational fluid dynamics (CFD) has become a valuable tool for investigating hemodynamics in cerebral aneurysms. CFD provides flow-related quantities, which have been shown to have a potential impact on aneurysm growth and risk of rupture. However, the adoption of CFD tools in clinical settings is currently limited by the high computational cost and the engineering expertise required for employing these tools, e.g., for mesh generation, appropriate choice of spatial and temporal resolution, and of boundary conditions. Herein, we address these challenges by introducing a practical and robust methodology, focusing on computational performance and minimizing user interaction through automated parameter selection. We propose a fully automated pipeline that covers the steps from a patient-specific anatomical model to results, based on a fast, graphics processing unit- (GPU-) accelerated CFD solver and a parameter selection methodology. We use a reduced order model to compute the initial estimates of the spatial and temporal resolutions and an iterative approach that further adjusts the resolution during the simulation without user interaction. The pipeline and the solver are validated based on previously published results, and by comparing the results obtained for 20 cerebral aneurysm cases with those generated by a state-of-the-art commercial solver (Ansys CFX, Canonsburg PA). The automatically selected spatial and temporal resolutions lead to results which closely agree with the state-of-the-art, with an average relative difference of only 2%. Due to the GPU-based parallelization, simulations are computationally efficient, with a median computation time of 40 minutes per simulation.|Comput. Math. Methods Medicine|2020|10.1155/2020/5954617|Y. Murayama, S. Rapaka, Takashi Suzuki, H. Takao, C. Nita, Viorel Mihalef, Puneet S. Sharma, L. Itu, T. Redel|0.8|2
631|Deterministic Atomic Buffering|Deterministic execution for GPUs is a desirable property as it helps with debuggability and reproducibility. It is also important for safety regulations, as safety critical workloads are starting to be deployed onto GPUs. Prior deterministic architectures, such as GPUDet, attempt to provide strong determinism for all types of workloads, incurring significant performance overheads due to the many restrictions that are required to satisfy determinism. We observe that a class of reduction workloads, such as graph applications and neural architecture search for machine learning, do not require such severe restrictions to preserve determinism. This motivates the design of our system, Deterministic Atomic Buffering (DAB), which provides deterministic execution with low area and performance overheads by focusing solely on ordering atomic instructions instead of all memory instructions. By scheduling atomic instructions deterministically with atomic buffering, the results of atomic operations are isolated initially and made visible in the future in a deterministic order. This allows the GPU to execute deterministically in parallel without having to serialize its threads for atomic operations as opposed to GPUDet. Our simulation results show that, for atomic-intensive applications, DAB performs 4× better than GPUDet and incurs only a 23% slowdown on average compared to a non-deterministic GPU architecture. We also characterize the bottlenecks and provide insights for future optimizations.|Micro|2020|10.1109/MICRO50266.2020.00083|Matthew D. Sinclair, Joseph Devietti, Timothy G. Rogers, Tor M. Aamodt, Shaylin Cattell, Yuan Chou, Christopher Ng, Jeremy Intan|0.8|2
638|Towards Real-Time Magnetic Dosimetry Simulations for Inductive Charging Systems|The exposure of a human by magneto-quasistatic fields from wireless charging systems is to be determined from magnetic field measurements in near real-time. This requires a fast linear equations solver for the discrete Poisson system of the Co-Simulation Scalar Potential Finite Difference (Co-Sim. SPFD) scheme. Here, the use of the AmgX library on NVIDIA GPUs is presented for this task. It enables solving the equation system resulting from an ICNIRP recommended human voxel model resolution of 2 mm in less than 0.5 seconds on a single NVIDIA Tesla V100 GPU.|arXiv.org|2020|10.1108/COMPEL-03-2021-0084|N. Haussmann, M. Clemens, R. Mease, M. Zang, B. Schmuelling, M. Bolten|0.8|2
651|System simulation with PULP virtual platform and SystemC|Driven by performance, power and energy requirements compute platforms evolved from single-core homogeneous into highly parallel heterogeneous architectures. These architectures use different CPUs, GPUs, accelerators, interconnects, memories, etc., and target diverse applications creating avast design space. Moreover, ever-growing security concerns leverage the adoption of open source silicon designs and tools, specially those with collaborative research and engineering efforts from industry and academia to develop and maintain for the long term. In this context, RISC-V based solutions are an outstanding choice and virtual platforms are essential for a fast design cycle. However, simulation frameworks often do not provide off-the-shelf interoperability hindering the reusability of existing models. In this paper, we present for the first time a coupling of the PULP virtual platform, which provides manycore RISC-V accelerators in a feature-rich simulation environment, with SystemC/TLM-2.0, a de facto standard in industry that is also largely used in the academia. Furthermore, we evaluate the coupling of simulation engines and demonstrate its usefulness in a case study.|Workshop on Rapid Simulation and Performance Evaluation|2020|10.1145/3375246.3375256|Germain Haugou, Éder F. Zulian, Matthias Jung, N. Wehn, C. Weis|0.8|2
682|Hardware-accelerated Simulation-based Inference of Stochastic Epidemiology Models for COVID-19|Epidemiology models are central to understanding and controlling large-scale pandemics. Several epidemiology models require simulation-based inference such as Approximate Bayesian Computation (ABC) to fit their parameters to observations. ABC inference is highly amenable to efficient hardware acceleration. In this work, we develop parallel ABC inference of a stochastic epidemiology model for COVID-19. The statistical inference framework is implemented and compared on Intel’s Xeon CPU, NVIDIA’s Tesla V100 GPU, Google’s V2 Tensor Processing Unit (TPU), and the Graphcore’s Mk1 Intelligence Processing Unit (IPU), and the results are discussed in the context of their computational architectures. Results show that TPUs are 3×, GPUs are 4×, and IPUs are 30× faster than Xeon CPUs. Extensive performance analysis indicates that the difference between IPU and GPU can be attributed to higher communication bandwidth, closeness of memory to compute, and higher compute power in the IPU. The proposed framework scales across 16 IPUs, with scaling overhead not exceeding 8% for the experiments performed. We present an example of our framework in practice, performing inference on the epidemiology model across three countries and giving a brief overview of the results.|ACM Journal on Emerging Technologies in Computing Systems|2020|10.1145/3471188|S. Kulkarni, C. A. Moritz, Seth Nabarro, M. M. Krell|0.8|2
692|Accelerating Simulation-based Inference with Emerging AI Hardware|Developing models of natural phenomena by capturing their underlying complex interactions is a core tenet of various scientific disciplines. These models are useful as simulators and can help in understanding the natural processes being studied. One key challenge in this pursuit has been to enable statistical inference over these models, which would allow these simulation-based models to learn from real-world observations. Recent efforts, such as Approximate Bayesian Computation (ABC), show promise in performing a new kind of inference to leverage these models. While the scope of applicability of these inference algorithms is limited by the capabilities of contemporary computational hardware, they show potential of being greatly parallelized. In this work, we explore hardware accelerated simulation-based inference over probabilistic models, by combining massively parallelized ABC inference algorithm with the cutting-edge AI chip solutions that are uniquely suited for this purpose. As a proof-of-concept, we demonstrate inference over a probabilistic epidemiology model used to predict the spread of COVID-19. Two hardware acceleration platforms are compared - the Tesla V100 GPU and the Graphcore Mark1 IPU. Our results show that while both of these platforms outperform multi-core CPUs, the Mk1 IPUs are 7.5x faster than the Tesla V100 GPUs for this workload.|International Conference on Rebooting Computing|2020|10.1109/ICRC2020.2020.00003|M. M. Krell, A. Tsyplikhin, S. Kulkarni, C. A. Moritz|0.8|2
1172|Efficient scaling of a hydrodynamics simulation using compiler-based accelerator technology|In the realm of numerical modeling, there is often a requirement to run simulations with higher spatial and temporal resolutions. Increasing resolution can improve the accuracy of simulations with a corresponding increase in the run time. These run times can become impractical for conventional sequentially coded simulations. Parallelizing simulation code offers great possibilities for improved run-time performance, but it can be very difficult to achieve the necessary speedups when software development and debugging time are considered. This is especially the case when extensive modifications to legacy source code are needed to incorporate parallel processing capabilities. To address this problem, we used a directive-driven compiler to parallelize a hydrodynamics simulation targeting GPU hardware. This compiler-level parallel development environment enabled us to parallelize an existing legacy program with minimal alterations to the original source code. Using compiler directives and testing methodologies, we were able to achieve more than a 15x speedup and reduce the run time of high resolution simulations from months to days while verifying that results were equivalent to the sequential legacy version of the simulation.|Spring Simulation Multiconference|2015|10.1016/j.fob.2015.10.003|Phil Moore, Behzad Torkian, Jordan A. Bradshaw|0.8|2
1305|Numerical Simulation of Melting with Natural Convection Based on Lattice Boltzmann Method and Performed with CUDA Enabled GPU|A new solver is developed to numerically simulate the melting phase change with natural convection. This solver was implemented on a single Nvidia GPU based on the CUDA technology in order to simulate the melting phase change in a 2D rectangular enclosure. The Rayleigh number is of the order of magnitude of 10 8 and Prandlt is 50. The hybrid thermal lattice Boltzmann method (HTLBM) is employed to simulate the natural convection in the liquid phase, and the enthalpy formulation is used to simulate the phase change aspect. The model is validated by experimental data and published analytic results. The simulation results manifest a strong convection in the melted phase and a different flow pattern from the reference results with low Rayleigh number. In addition, the computational performance is estimated for single precision arithmetic, and this solver yields 703.31MLUPS and 61.89GB/s device to device data throughput on a Nvidia Tesla C2050 GPU.||2015|10.4208/CICP.2014.M350|K. Johannes, F. Kuznik, W. Gong|0.8|2
1331|A Fading Channel Simulator Implementation Based on GPU Computing Techniques|Channel simulators are powerful tools that permit performance tests of the individual parts of a wireless communication system. This is relevant when new communication algorithms are tested, because it allows us to determine if they fulfill the communications standard requirements. One of these tests consists of evaluating the system performance when a communication channel is considered. \nIn this sense, it is possible to model the channel as an FIR filter with time-varying random coefficients. If the number of coefficients is increased, then a better approach to real scenarios can be achieved; however, in that case, the computational complexity is increased. In order to address this issue, a design methodology for computing the time-varying coefficients of the fading channel simulators using consumer-designed graphic processing units (GPUs) is proposed. With the use of GPUs and the proposed methodology, it is possible for nonspecialized users in parallel computing to accelerate their simulation \ndevelopments when compared to conventional software. Implementation results show that the proposed approach allows the easy generation of communication channels while reducing the processing time. Finally, GPU-based implementation takes precedence when compared with the CPU-based implementation, due to the scattered nature of \nthe channel.||2015|10.1155/2015/237061|A. Atoche, J. V. Castillo, J. O. Aguilar, R. Carrasco-Alvarez|0.8|2
1415|Solving the Savage–Hutter equations for granular avalanche flows with a second‐order Godunov type method on GPU|In this article, we apply Davis's second‐order predictor‐corrector Godunov type method to numerical solution of the Savage–Hutter equations for modeling granular avalanche flows. The method uses monotone upstream‐centered schemes for conservation laws (MUSCL) reconstruction for conservative variables and Harten–Lax–van Leer contact (HLLC) scheme for numerical fluxes. Static resistance conditions and stopping criteria are incorporated into the algorithm. The computation is implemented on graphics processing unit (GPU) by using compute unified device architecture programming model. A practice of allocating memory for two‐dimensional array in GPU is given and computational efficiency of two‐dimensional memory allocation is compared with one‐dimensional memory allocation. The effectiveness of the present simulation model is verified through several typical numerical examples. Numerical tests show that significant speedups of the GPU program over the CPU serial version can be obtained, and Davis's method in conjunction with MUSCL and HLLC schemes is accurate and robust for simulating granular avalanche flows with shock waves. As an application example, a case with a teardrop‐shaped hydraulic jump in Johnson and Gray's granular jet experiment is reproduced by using specific friction coefficients given in the literature. Copyright © 2014 John Wiley & Sons, Ltd.||2015|10.1002/fld.3988|Li Yuan, J. Zhai, W. Liu, Xinting Zhang|0.8|2
268|Improving Reaction Kernel Performance in Lattice Microbes: Particle-Wise Propensities and Run-Time Generated Code|The reaction kernel for MPD-RDME, the GPU-accelerated reaction-diffusion master equation solver found in Lattice Microbes uses a large number of kinetic parameters to describe a biochemical network. Many of these parameters are required to compute the system's total reaction propensity, which is used to stochastically evaluate whether a reaction event takes place. In this paper, we examine two techniques for accelerating performance by modifying the total propensity calculation. The first technique is to use a particle-based approach to compute propensities from discrete particles and particle pairs. We find this technique results in a dramatic improvement in performance for a complex model, approximately 60 times faster. The second technique uses run-time generated source code to automatically create executable code tailored for the biological model being simulated. The removal of all memory reads for constant parameters increases performance for less complex models.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2016|10.1109/IPDPSW.2016.118|Michael J. Hallock, Z. Luthey-Schulten|0.7777777777777778|2
270|Adaptive Optics Simulation for the World's Largest Telescope on Multicore Architectures with Multiple GPUs|We present a high performance comprehensive implementation of a multi-object adaptive optics (MOAO) simulation on multicore architectures with hardware accelerators in the context of computational astronomy. This implementation will be used as an operational testbed for simulating the design of new instruments for the European Extremely Large Telescope project (E-ELT), the world's biggest eye and one of Europe's highest priorities in ground-based astronomy. The simulation corresponds to a multi-step multi-stage procedure, which is fed, near real-time, by system and turbulence data coming from the telescope environment. Based on the PLASMA library powered by the OmpSs dynamic runtime system, our implementation relies on a task-based programming model to permit an asynchronous out-of-order execution. Using modern multicore architectures associated with the enormous computing power of GPUs, the resulting data-driven compute-intensive simulation of the entire MOAO application, composed of the tomographic reconstructor and the observing sequence, is capable of coping with the aforementioned real-time challenge and stands as a reference implementation for the computational astronomy community.|Platform for Advanced Scientific Computing Conference|2016|10.1145/2929908.2929920|A. Charara, E. Gendron, H. Ltaief, D. Gratadour|0.7777777777777778|2
373|MrBayes tgMC3++: A High Performance and Resource-Efficient GPU-Oriented Phylogenetic Analysis Method|MrBayes is a widespread phylogenetic inference tool harnessing empirical evolutionary models and Bayesian statistics. However, the computational cost on the likelihood estimation is very expensive, resulting in undesirably long execution time. Although a number of multi-threaded optimizations have been proposed to speed up MrBayes, there are bottlenecks that severely limit the GPU thread-level parallelism of likelihood estimations. This study proposes a high performance and resource-efficient method for GPU-oriented parallelization of likelihood estimations. Instead of having to rely on empirical programming, the proposed novel decomposition storage model implements high performance data transfers implicitly. In terms of performance improvement, a speedup factor of up to 178 can be achieved on the analysis of simulated datasets by four Tesla K40 cards. In comparison to the other publicly available GPU-oriented MrBayes, the tgMC3++ method (proposed herein) outperforms the tgMC3 (v1.0), nMC3 (v2.1.1) and oMC3 (v1.00) methods by speedup factors of up to 1.6, 1.9 and 2.9, respectively. Moreover, tgMC3++ supports more evolutionary models and gamma categories, which previous GPU-oriented methods fail to take into analysis.|IEEE/ACM Transactions on Computational Biology & Bioinformatics|2016|10.1109/TCBB.2015.2495202|Guoguang Zhao, Weifeng Shi, Donghong Sun, T. Hamada, Cheng Ling, Jingyang Gao|0.7777777777777778|2
439|A real-time GPU-based coupled fluid-structure simulation with haptic interaction|Time series scientific simulation on supercomputers generates huge amounts of data at each time step. In the big data era, these data become impossible to be stored anymore, so simultaneous analysis of these data is strongly demanded. In order to realize such an on-the-fly intuitive analysis of simulation, this paper showcases a multimodal visualization and steering system with visual and haptic interfaces for a real-time GPU-based coupled fluid-structure simulation. Since the nature of touching sense of human beings requires extremely fast and continuous refreshing to form authentic feeling, parallel techniques were utilized to speed up haptic updating to around one millisecond. A middle-layer interface for a haptic device was developed to realize the ease of use of this device. Furthermore, a model of palpation was proposed to allow users to touch, push and sense the dynamic fluid motion inside a deformable tube.|International Conference on Interaction Sciences|2016|10.1109/ICIS.2016.7550842|S. Fukuma, S. Mori, Jiachao Zhang, Shunpei Yuasa|0.7777777777777778|2
1367|Structural and dynamic modeling of molecular systems at different length scales|The continuous growth of computing power, both in terms of hardware and software resources, has made the computational (in-silico) approach to complex scientific problems a very profitable tool, which provides useful information to support, interpret or in some cases even reproduce the experimental datum from first principles. Methods have become cheaper and faster in the last two decades, thanks also to the development of more efficient algorithms, able to extract in full the computational power contained in novel hardware solutions (e.g. parallel computing and GPUs-based hardware), and to provide relatively easy-to-use software packages for diverse applications. Nowadays the computational approach is employed in several scientific areas, covering many different applied disciplines such as medicine, engineering, chemistry, physics, materials science and many others. \nIn particular in this thesis work, some of the main approaches of computational chemistry, namely quantum mechanics, classical molecular dynamics and hybrid methods, are applied to the study of biomolecules and macromolecules, in order to investigate different aspects like structure, dynamics, energetics and in particular flexibility. In addition to the aforementioned methods we also explore a fluido-dynamic approach to describe and simulate microfluidic systems, focusing the attention on the reactivity of the systems studied. All these approaches are size-dependent and because they have different computational costs, their application should be limited to a reasonable size of the studied system. The profound difference in terms of cost/accuracy are discussed, providing a link between the different methodologies scales, in order to exemplify how information gathered at smaller length scale can be considered as an accurate starting point to perform simulations at larger spatial scales, in what is nowadays know popularly as multiscale modeling. The connection between the high accuracy/high cost and low accuracy/low cost methods is commented upon, to illustrate how a multiscale modeling approach can allow, in specific cases, to augment at the same time the accuracy of the data calculated and the size of the system simulated.||2016|10.1016/j.entcs.2016.09.021|Mauro Torsello|0.7777777777777778|2
218|Fast GPU-Based Seismogram Simulation From Microseismic Events in Marine Environments Using Heterogeneous Velocity Models|A novel approach is presented for fast generation of synthetic seismograms due to microseismic events, using heterogeneous marine velocity models. The partial differential equations for the three-dimensional (3-D) elastic wave equation have been numerically solved using the Fourier domain pseudo-spectral method which is parallelizable on the graphics processing unit (GPU) cards, thus making it faster compared to traditional CPU based computing platforms. Due to computationally expensive forward simulation of large geological models, several combinations of individual synthetic seismic traces are used for specified microseismic event locations, in order to simulate the effect of realistic microseismic activity patterns in the subsurface. We here explore the patterns generated by few hundreds of microseismic events with different source mechanisms using various combinations, both in event amplitudes and origin times, using the simulated pressure and three component particle velocity fields via 1-D, 2-D and 3-D seismic visualizations.|IEEE Transactions on Computational Imaging|2017|10.1109/TCI.2017.2654127|Saptarshi Das, M. Hobson, Xi Chen|0.75|2
240|A graphics tracing framework for exploring CPU+GPU memory systems|Modern SoCs contain CPU and GPU cores to execute both general purpose and highly-parallel graphics workloads. While the primary use of the GPU is for rendering graphics, the effects of graphics workloads on the overall system have received little attention. The primary reason for this is the lack of efficient tools and simulators for modern graphics applications. In this work, we present GLTraceSim, a new graphics memory tracing and replay framework for studying the memory behavior of graphics workloads and how they interact in heterogeneous CPU/GPU memory systems. GLTraceSim efficiently generates GPU memory access traces and their corresponding, synchronized, CPU render thread memory traces. The resulting traces can then be replayed in both high-level models and detailed full-system simulators. We evaluate GLTraceSim on a range of graphics workloads from browsers to games. Our results show that GLTraceSim can efficiently generate graphics memory traces, and use these traces to study graphics performance in heterogeneous CPU/GPU memory systems. We show that understanding the impact of graphics workloads is essential, as they can cause slowdowns in co-running CPU applications of 26–59%, depending on the memory technology.|IEEE International Symposium on Workload Characterization|2017|10.1109/IISWC.2017.8167756|Trevor E. Carlson, Erik Hagersten, Andreas Sembrant, D. Black-Schaffer|0.75|2
246|GPUpd: A Fast and Scalable Multi-GPU Architecture Using Cooperative Projection and Distribution|Graphics Processing Unit (GPU) vendors have been scaling singleGPU architectures to satisfy the ever-increasing user demands for faster graphics processing. However, as it gets extremely difficult to further scale single-GPU architectures, the vendors are aiming to achieve the scaled performance by simultaneously using multiple GPUs connected with newly developed, fast inter-GPU networks (e.g., NVIDIA NVLink, AMD XDMA). With fast inter-GPU networks, it is now promising to employ split frame rendering (SFR) which improves both frame rate and single-frame latency by assigning disjoint regions of a frame to different GPUs. Unfortunately, the scalability of current SFR implementations is seriously limited as they suffer from a large amount of redundant computation among GPUs. This paper proposes GPUpd, a novel multi-GPU architecture for fast and scalable SFR. With small hardware extensions, GPUpd introduces a new graphics pipeline stage called Cooperative Projection & Distribution (C-PD) where all GPUs cooperatively project 3D objects to 2D screen and effciently redistribute the objects to their corresponding GPUs. C-PD not only eliminates the redundant computation among GPUs, but also incurs minimal inter-GPU network traffic by transferring object IDs instead of mid-pipeline outcomes between GPUs. To further reduce the redistribution overheads, GPUpd minimizes inter-GPU synchronizations by implementing batching and runahead-execution of draw commands. Our detailed cycle-level simulations with 8 real-world game traces show that GPUpd achieves a geomean speedup of $4.98 \times$ in single-frame latency with 16 GPUs, whereas the current SFR implementations achieve only $3.07 \times$ geomean speedup which saturates on 4 or more GPUs. CCS CONCEPTS • Computing methodologies $\rightarrow$ Graphics processors; • Computer systems organization $\rightarrow$ Distributed architectures;|Micro|2017|10.1145/3123939.3123968|Minsoo Rhu, Hanjun Kim, Jangwoo Kim, Youngsok Kim, Jae-Eon Jo, Hanhwi Jang|0.75|2
291|Real-time simulation of hydraulic components for interactive control of soft robots|In this work we propose a new method for online motion planning in the task-space for hydraulic actuated soft robots. Our solution relies on the interactive resolution of an inverse kinematics problem, that takes into account the properties (mass, stiffness) of the deformable material used to build the robot. An accurate modeling of the mechanical behavior of hydraulic components is based on a novel GPU parallel method for the real-time computation of fluid weight distribution. The efficiency of the method is further increased by a novel GPU parallel leveraging mechanism. Our complete solution has been integrated within the open-source SOFA framework. In our results, we validate our simulation with a fabricated silicone cylinder and we demonstrate the usage of our approach for direct control of hydraulic soft robots.|IEEE International Conference on Robotics and Automation|2017|10.1109/ICRA.2017.7989575|E. Coevoet, Christian Duriez, Alejandro Rodríguez|0.75|2
607|ExaGeoStat: A High Performance Unified Framework for Geostatistics on Manycore Systems|We present ExaGeoStat, a high performance framework for geospatial statistics in climate and environment modeling. In contrast to simulation based on partial differential equations derived from first-principles modeling, ExaGeoStat employs a statistical model based on the evaluation of the Gaussian log-likelihood function, which operates on a large dense covariance matrix. Generated by the parametrizable Matern covariance function, the resulting matrix is symmetric and positive definite. The computational tasks involved during the evaluation of the Gaussian log-likelihood function become daunting as the number n of geographical locations grows, as O(n2) storage and O(n3) operations are required. While many approximation methods have been devised from the side of statistical modeling to ameliorate these polynomial complexities, we are interested here in the complementary approach of evaluating the exact algebraic result by exploiting advances in solution algorithms and many-core computer architectures. Using state-of-the-art high performance dense linear algebra libraries associated with various leading edge parallel architectures (Intel KNLs, NVIDIA GPUs, and distributed-memory systems), ExaGeoStat raises the game for statistical applications from climate and environmental science. ExaGeoStat provides a reference evaluation of statistical parameters, with which to assess the validity of the various approaches based on approximation. The framework takes a first step in the merger of large-scale data analytics and extreme computing for geospatial statistical applications, to be followed by additional complexity reducing improvements from the solver side that can be implemented under the same interface. Thus, a single uncompromised statistical model can ultimately be executed in a wide variety of emerging exascale environments.|arXiv.org|2017|10.1002/cpe.4187|Ying Sun, H. Ltaief, Sameh Abdulah, M. Genton, D. Keyes|0.75|2
698|A Python Framework for Fast Modelling and Simulation of Cellular Nonlinear Networks and other Finite-difference Time-domain Systems|This paper introduces and evaluates a freely available cellular nonlinear network simulator optimized for the effective use of GPUs, to achieve fast modelling and simulations. Its relevance is demonstrated for several applications in nonlinear complex dynamical systems, such as slow-growth phenomena as well as for various image processing applications such as edge detection. The simulator is designed as a Jupyter notebook written in Python and functionally tested and optimized to run on the freely available cloud platform Google Collaboratory. Although the simulator, in its actual form, is designed to model the FitzHugh Nagumo Reaction-Diffusion cellular nonlinear network, it can be easily adapted for any other type of finite-difference time-domain model. Four implementation versions are considered, namely using the PyCUDA, NUMBA respectively CUPY libraries (all three supporting GPU computations) as well as a NUMPY-based implementation to be used when GPU is not available. The specificities and performances for each of the four implementations are analyzed concluding that the PyCUDA implementation ensures a very good performance being capable to run up to 14000 Mega cells per seconds (each cell referring to the basic nonlinear dynamic system composing the cellular nonlinear network).|Computer Science in Cars Symposium|2021|10.1109/CSCS52396.2021.00043|R. Dogaru, I. Dogaru|0.75|2
701|Efficiently Solving Partial Differential Equations in a Partially Reconfigurable Specialized Hardware|Scientific computations with a wide range of applications in domains such as developing vaccines, forecasting the weather, predicting natural disasters, simulating aerodynamics of spacecraft, and exploring oil resources, create the main workloads of supercomputers. The key integration of such scientific computations is modeling physical phenomena that are done with the aid of partial differential equations (PDEs). Solving PDEs on supercomputers, even with those equipped with GPUs, consumes a large amount of power and yet is not as fast as desired. The main reason behind such slow processing is data dependency. The key challenge is that software techniques cannot resolve these dependencies, therefore, such applications cannot benefit from the parallelism provided by processors such as GPUs. Our key insight to address this challenge is that although we cannot resolve the dependencies, we can reduce their negative impacts by using hardware/software co-optimization. To this end, we propose breaking down the data-dependent operations into two groups of operations: a majority of parallelizable and the minority of data-dependent operations. We execute these two groups in the desired order: first, we put together all parallelizable operations and execute them all, subsequently; then, we switch to execute the small data-dependent part. As long as the data-dependent part is small, we can accelerate them by using fast hardware mechanisms. Besides, our proposed hardware mechanisms guarantee quickly switching between the two groups of operations. To follow the same order of execution, dictated by our software mechanism, and implemented in hardware, we also propose a new low-overhead compression format – sparsity is another attribute of PDEs that require compression. Furthermore, the core generic architecture of our proposed hardware allows the execution of other applications including sparse matrix-vector multiplication (SpMV) and graph algorithms. The key feature of the proposed hardware is partial reconfigurability, which on one hand, facilitates the execution of data-dependent computations, and on the other hand, allows executing broad application without changing the entire configuration. Our evaluations show that compared to GPUs, we achieve an average speedup of 15.6× for scientific computations while consuming 14× less energy.|IEEE transactions on computers|2021|10.1109/TC.2021.3060700|Bahar Asgari, T. Krishna, Hyesoon Kim, S. Yalamanchili, Ramyad Hadidi|0.75|2
715|Polynomial, piecewise-Linear, Step (PLS): A Simple, Scalable, and Efficient Framework for Modeling Neurons|Biological neurons can be modeled with different levels of biophysical/biochemical details. The accuracy with which a model reflects the actual physiological processes and ultimately the information function of a neuron, can range from very detailed to a schematic phenomenological representation. This range exists due to the common problem: one needs to find an optimal trade-off between the level of details needed to capture the necessary information processing in a neuron and the computational load needed to compute 1 s of model time. An increase in modeled network size or model-time, for which the solution should be obtained, makes this trade-off pivotal in model development. Numerical simulations become incredibly challenging when an extensive network with a detailed representation of each neuron needs to be modeled over a long time interval to study slow evolving processes, e.g., development of the thalamocortical circuits. Here we suggest a simple, powerful and flexible approach in which we approximate the right-hand sides of differential equations by combinations of functions from three families: Polynomial, piecewise-Linear, Step (PLS). To obtain a single coherent framework, we provide four core principles in which PLS functions should be combined. We show the rationale behind each of the core principles. Two examples illustrate how to build a conductance-based or phenomenological model using the PLS-framework. We use the first example as a benchmark on three different computational platforms: CPU, GPU, and mobile system-on-chip devices. We show that the PLS-framework speeds up computations without increasing the memory footprint and maintains high model fidelity comparable to the fully-computed model or with lookup-table approximation. We are convinced that the full range of neuron models: from biophysical to phenomenological and even to abstract models, may benefit from using the PLS-framework.|Frontiers in Neuroinformatics|2021|10.3389/fninf.2021.642933|M. Colonnese, R. Tikidji-Hamburyan|0.75|2
737|Quality-Oriented Task Allocation and Scheduling in Transcoding Servers With Heterogeneous Processors|Dynamically adaptive streaming over HTTP requires a large-scale server to transcode various bitrate versions in which different preset parameters can be used to provide different video qualities at each resolution. When transcoding servers contain a heterogeneous mix of CPUs and GPUs, the task scheduler must choose a processor and preset parameter for each transcoding task to meet the transcoding deadlines while achieving the best possible video quality. We apply regression analysis to sample variable-bit-rate videos to provide accurate (mean absolute percentage error values from 1.3% to 13.9%) model for predicting bitrate, transcoding time and video quality at each resolution on different processors. We build this into a greedy allocation and scheduling algorithm which first satisfies deadlines with low video quality, and then redistributes the workload to improve that quality while continuing to meet the deadlines. This scheme was both simulated and implemented on a testbed server. It satisfies all deadlines while outperforming standard algorithms by between 3.12% and 15.59% in terms of popularity-weighted video quality divided by bitrate.|IEEE transactions on circuits and systems for video technology (Print)|2021|10.1109/TCSVT.2021.3074158|Minseok Song, Dayoung Lee|0.75|2
753|GraspME - Grasp Manifold Estimator|In this paper, we introduce a Grasp Manifold Estimator (GraspME) to detect grasp affordances for objects directly in 2D camera images. To perform manipulation tasks autonomously it is crucial for robots to have such graspability models of the surrounding objects. Grasp manifolds have the advantage of providing continuously infinitely many grasps, which is not the case when using other grasp representations such as predefined grasp points. For instance, this property can be leveraged in motion optimization to define goal sets as implicit surface constraints in the robot configuration space. In this work, we restrict ourselves to the case of estimating possible end-effector positions directly from 2D camera images. To this extend, we define grasp manifolds via a set of keypoints and locate them in images using a Mask R-CNN [1] backbone. Using learned features allows to generalize to different view angle, with potentially noisy images, and objects that were not part of the training set. We rely on simulation data only and perform experiments on simple and complex objects, including unseen ones. Our framework achieves an inference speed of 11.5 fps on a GPU, an average precision for keypoint estimation of 94.5% and a mean pixel distance of only 1.29. This shows that we can estimate the objects very well via bounding boxes and segmentation masks as well as approximate the correct grasp manifold’s keypoint coordinates.|IEEE International Symposium on Robot and Human Interactive Communication|2021|10.1109/RO-MAN50785.2021.9515479|Jim Mainprice, Janik M. Hager, Ruben Bauer, Marc Toussaint|0.75|2
768|Numerical Aspects of Particle-in-Cell Simulations for Plasma-Motion Modeling of Electric Thrusters|The present work is focused on a detailed description of an in-house, particle-in-cell code developed by the authors, whose main aim is to perform highly accurate plasma simulations on an off-the-shelf computing platform in a relatively short computational time, despite the large number of macro-particles employed in the computation. A smart strategy to set up the code is proposed, and in particular, the parallel calculation in GPU is explored as a possible solution for the reduction in computing time. An application on a Hall-effect thruster is shown to validate the PIC numerical model and to highlight the strengths of introducing highly accurate schemes for the electric field interpolation and the macroparticle trajectory integration in the time. A further application on a helicon double-layer thruster is presented, in which the particle-in-cell (PIC) code is used as a fast tool to analyze the performance of these specific electric motors.|Aerospace|2021|10.3390/AEROSPACE8050138|D. Del Gatto, C. Curcio, R. Savino, A. Capozzoli, G. Gallo, Adriano Isoldi, A. Liseno|0.75|2
769|Efficient Rendering of Ocular Wavefront Aberrations using Tiled Point‐Spread Function Splatting|Visual aberrations are the imperfections in human vision, which play an important role in our everyday lives. Existing algorithms to simulate such conditions are either not suited for low‐latency workloads or limit the kinds of supported aberrations. In this paper, we present a new simulation method that supports arbitrary visual aberrations and runs at interactive, near real‐time performance on commodity hardware. Furthermore, our method only requires a single set of on‐axis phase aberration coefficients as input and handles the dynamic change of pupil size and focus distance at runtime. We first describe a custom parametric eye model and parameter estimation method to find the physical properties of the simulated eye. Next, we talk about our parameter sampling strategy which we use with the estimated eye model to establish a coarse point‐spread function (PSF) grid. We also propose a GPU‐based interpolation scheme for the kernel grid which we use at runtime to obtain the final vision simulation by extending an existing tile‐based convolution approach. We showcase the capabilities of our eye estimation and rendering processes using several different eye conditions and provide the corresponding performance metrics to demonstrate the applicability of our method for interactive environments.|Computer graphics forum (Print)|2021|10.1111/cgf.14267|István Csoba, Roland Kunkli|0.75|2
780|Trigger Happy: Assessing the Viability of Trigger-Based In Situ Analysis|Triggers are an emerging strategy for optimizing execution time for in situ analysis. However, their performance characteristics are complex, making it difficult to decide if a particular trigger-based approach is viable. With this study, we propose a cost model for trigger-based in situ analysis that can assess viability, and we also validate the model's efficacy. Then, once the cost model is established, we apply the model to inform the space of viable approaches, considering variation in simulation code, trigger techniques, and analyses, as well as trigger inspection and fire rates. Real-world values are needed both to validate the model and to use the model to inform the space of viable approaches. We obtain these values by surveying science application teams and by performing runs as large as 2,040 GPUs and 32 billion cells.|IEEE Symposium on Large Data Analysis and Visualization|2021|10.1109/LDAV53230.2021.00010|S. Sane, C. Harrison, Lawrence Livermore, Terece L. Turton, H. Childs, S. Brink, Matthew Larsen|0.75|2
789|Real-Time Interactive Simulations of Complex Ionic Cardiac Cell Models in 2D and 3D Heart Structures with GPUs on Personal Computers|Cardiac modeling in heart structures to study arrhythmia mechanisms has required running software on supercomputers, limiting such studies to groups with cluster access and skilled personnel. We present WebGL programs to run and visualize simulations of complex ionic models in 2D and 3D cardiac geometries, in real time, interactively using the multi-core GPU of a single computer. We use Abubu.js, a library we developed for solving partial differential equations such as those describing crystal growth and fluid flow to simulate complex ionic cell models. By combining this library with JavaScript, we allow direct real-time interactions with simulations. We implemented: 1) modification of model parameters and equations at any time, with direct access to the code while it runs; 2) electrode stimulation by mouse click anywhere in the 2D/3D tissue; 3) saving the system state at any time to re-initiate dynamics from a saved state; and 4) rotation/visualization of 3D structures at any angle. As examples of this modeling platform, we implemented phenomenological and human ventricular cell models (OVVR, 41 variables). In 2D we illustrate dynamics in an annulus, disk, and square tissue; in 2D and 3D porcine ventricles, we show functional/anatomical reentry, spiral wave dynamics in different regimes, early afterdepolarizations (EADs) and real-time model parameter variation effects.|2021 Computing in Cardiology (CinC)|2021|10.23919/cinc53138.2021.9662759|E. Cherry, F. Fenton, A. Kaboudian|0.75|2
792|Improving Barnes-Hut t-SNE Algorithm in Modern GPU Architectures with Random Forest KNN and Simulated Wide-Warp|The t-Distributed Stochastic Neighbor Embedding (t-SNE) is a widely used technique for dimensionality reduction but is limited by its scalability when applied to large datasets. Recently, BH-tSNE was proposed; this is a successful approximation that transforms a step of the original algorithm into an N-Body simulation problem that can be solved by a modified Barnes-Hut algorithm. However, this improvement still has limitations to process large data volumes (millions of records). Late studies, such as t-SNE-CUDA, have used GPUs to implement highly parallel BH-tSNE. In this research we have developed a new GPU BH-tSNE implementation that produces the embedding of multidimensional data points into three-dimensional space. We examine scalability issues in two of the most expensive steps of GPU BH-tSNE by using efficient memory access strategies , recent acceleration techniques , and a new approach to compute the KNN graph structure used in BH-tSNE with GPU. Our design allows up to 460% faster execution when compared to the t-SNE-CUDA implementation. Although our SIMD acceleration techniques were used in a modern GPU setup, we have also verified a potential for applications in the context of multi-core processors.|ACM Journal on Emerging Technologies in Computing Systems|2021|10.1145/3447779|B. H. Meyer, A. Pozo, Wagner M. Nunan Zola|0.75|2
793|Approximate Image-Space Multidirectional Occlusion Shading Model for Direct Volume Rendering|Understanding and perceiving three-dimensional scientific visualizations, such as volume rendering, benefit from visual cues produced by the shading models. The conventional approaches are local shading models since they are computationally inexpensive and straightforward to implement. However, the local shading models do not always provide proper visual cues since non-local information is not sufficiently taken into account for the shading. Global illumination models achieve better visual cues, but they are often computationally expensive. It has been shown that alternative illumination models, such as ambient occlusion, multidirectional shading, and shadows, provide decent perceptual cues. Although these models improve upon local shading models, they still require expensive preprocessing, extra GPU memory, and a high computational cost, which cause a lack of interactivity during the transfer function manipulations and light position changes. In this paper, we proposed an approximate image-space multidirectional occlusion shading model for the volume rendering. Our model was computationally less expensive compared to the global illumination models and did not require preprocessing. Moreover, interactive transfer function manipulations and light position changes were achievable. Our model simulated a wide range of shading behaviors, such as ambient occlusion and soft and hard shadows, and can be effortlessly applied to existing rendering systems such as direct volume rendering. We showed that the suggested model enhanced the visual cues with modest computational costs.|Applied Sciences|2021|10.3390/app11125717|Seokyeon Kim, Y. Jang|0.75|2
801|CNN+LSTM Accelerated Turbulent Flow Simulation with Link-Wise Artificial Compressibility Method|The simulation of turbulent flow, the most common form of fluid, is indispensable in computational fluid dynamics (CFD). The synthetic eddy method (SEM) generates the turbulent inflow and is adopted as the inlet boundary condition of simulation. However, SEM is time-consuming and can significantly slow down the simulation process which occupies 58% of the whole computational time. This is highly inefficient especially since SEM is only used as the inlet. In this paper we propose an efficient alternative. In particular, we leverage CNN+LSTM to replace SEM to obtain the turbulence statistics and combine it with link-wise artificial compressibility method (LW-ACM), which is a fast numerical method of CFD. We validate the predicted results by CNN+LSTM and prove that our model can provide the correct turbulence statistics even after a long time. Experiment results show that our CNN+LSTM module achieves over 15 × speedup compared with SEM, which greatly reduces the time consumption of turbulent inflow generation (from 58% to 7%). As a result, the whole time of turbulent flow simulation is more than halved. Compared with a newly released GPU-accelerated standard lattice Boltzmann method solver, our combination of CNN+LSTM and LW-ACM is about 8.6 × faster. Among all studies reported to date, our work is the fastest implementation for simulating turbulent channel flow, an important step for the field of fast CFD analysis.|International Conference on Parallel Processing|2021|10.1145/3472456.3472525|Canqun Yang, Sijiang Fan, Xiaowei Guo, Jiawei Fei, A. Revell|0.75|2
805|Early Evaluation of Fugaku A64FX Architecture Using Climate Workloads|The Energy Exascale Earth System Model (E3SM) Project is an ongoing, state-of-the-science Earth system modeling, simulation, and prediction project that targets efficient utilization of U.S. Department of Energy’s (DOE) supercomputers to meet the science needs of the nation and the mission needs of DOE. This work focuses on our early evaluation of the A64FX architecture on Fugaku supercomputer using E3SM benchmarks. We will present results that track hardware trends, facilitate architecture comparison and the specific impact on our workload using an atmospheric model benchmark. We have two variants of the code written in Fortran and C++/Kokkos respectively which were used to collect data on a variety of CPU and GPU platforms. Furthermore, we have conducted a comparative evaluation of the compilers on the A64FX architecture and found GNU to be the best performer for our workload. Our experience so far indicates that Fugaku/A64FX shows promising energy efficiency (performance/Watt) with further performance gains possible through architecture-aware optimization efforts.|IEEE International Conference on Cluster Computing|2021|10.1109/Cluster48925.2021.00107|M. Taylor, S. Sreepathi|0.75|2
813|CORSIKA 8|The CORSIKA 8 project is an international collaboration of scientists working together to deliver the most modern, flexible, robust and efficient framework for the simulation of ultra-high energy secondary particle cascades in matter. The main application is for cosmic ray air shower simulations, but it can also be applied to other problems in astro(particle)-physics, particle physics and nuclear physics. Besides a comprehensive and state-of-the-art collection of physics models as well as algorithms relevant for the field, also all possible interfaces to hardware acceleration (e.g. GPU) and parallelization (vectorization, multi-threading, multi-core) will be provided. We present the status and roadmap of this project. This code will soon be available for novel explorative studies and phenomonological research, and at the same time for massive productions runs for experiments.|EPJ Web of Conferences|2021|10.1051/epjconf/202125103038|Ralf Ulrich, Maximilian Reininghaus, Antonio Augusto Alves, Remy Prechelt|0.75|2
820|OpenACC Acceleration of an Agent-Based Biological Simulation Framework|Computational biology has increasingly turned to agent-based modeling to explore complex biological systems. Biological diffusion (diffusion, decay, secretion, and uptake) is a key driver of biological tissues. GPU computing can vastly accelerate the diffusion and decay operators in the partial differential equations used to represent biological transport in an agent-based biological modeling system. In this article, we utilize OpenACC to accelerate the diffusion portion of PhysiCell, a cross-platform agent-based biosimulation framework. We demonstrate an almost 40× speedup on the state-of-the-art NVIDIA Ampere 100 GPU compared to a serial run on AMD’s EPYC 7742. We also demonstrate 9× speedup on the 64-core AMD EPYC 7742 multicore platform. By using OpenACC for both the CPUs and the GPUs, we maintain a single source code base, thus creating a portable yet performant solution. With the simulator’s most significant computational bottleneck significantly reduced, we can continue cancer simulations over much longer times.|Computing in science & engineering (Print)|2021|10.1109/MCSE.2022.3226602|Robert Searles, P. Macklin, Karla Morris, Matt Stack, S. Chandrasekaran, Jeffrey C. Carver|0.75|2
865|DeepABM: Scalable and Efficient Agent-Based Simulations Via Geometric Learning Frameworks - a Case Study For Covid-19 Spread and Interventions|We introduce DeepABM, a computational framework for agent-based modeling that leverages geometric message passing for simulating action and interactions over large agent populations. Using DeepABM allows scaling simulations to large agent populations in real-time and running them efficiently on GPU architectures. Using the DeepABM framework, we build DeepABM-COVID simulator to provide support for various non-pharmaceutical interventions (quarantine, exposure notification, vaccination, testing) for the COVID-19 pandemic, and can scale to populations of representative size in real-time on a GPU. DeepABM-COVID can model 200 million interactions (over 100,000 agents across 180 time-steps) in 90 seconds, and is made available online to help researchers with modeling and analysis of various interventions. We explain various components of the framework and discuss results from one research study to evaluate the impact of delaying the second dose of the COVID-19 vaccine in collaboration with clinical and public health experts.|Online World Conference on Soft Computing in Industrial Applications|2021|10.1109/WSC52266.2021.9715507|J. Subramanian, Ramesh Raskar, S. Romero-Brufau, Ayush Chopra, K. Pasupathy, E. Gel, Balaji Krishnamurthy, T. Kingsley|0.75|2
1141|Multiple-GPU-Based Frequency-Dependent Finite-Difference Time Domain Formulation Using MATLAB Parallel Computing Toolbox|A parallel frequency-dependent, finite-difference time domain method is used to simulate electromagnetic waves propagating in dispersive media. The method is accomplished by using a singleprogram-multiple-data mode and tested on up to eight NVidia Tesla GPUs. The speedup using different numbers of GPUs is compared and presented in tables and graphics. The results provide recommendations for partitioning data from a 3-D computational model to achieve the best GPU performance.||2017|10.2528/PIERM17071704|William J. Mccollough, Wenyi Shao|0.75|2
1260|DEM GPU studies of industrial scale particle simulations for granular flow civil engineering applications|The use of the Discrete Element Method (DEM) for industrial civil engineering industrial applications is currently limited due to the computational demands when large numbers of particles are considered. The graphics processing unit (GPU) with its highly parallelized hardware architecture shows potential to enable solution of civil engineering problems using discrete granular approaches. We demonstrate in this study the pratical utility of a validated GPU-enabled DEM modeling environment to simulate industrial scale granular problems. As illustration, the flow discharge of storage silos using 8 and 17 million particles is considered. DEM simulations have been performed to investigate the influence of particle size (equivalent size for the 20/40-mesh gravel) and induced shear stress for two hopper shapes. The preliminary results indicate that the shape of the hopper significantly influences the discharge rates for the same material. Specifically, this work shows that GPU-enabled DEM modeling environments can model industrial scale problems on a single portable computer within a day for 30 seconds of process time.||2017|10.1051/EPJCONF/201714003071|D. Wilke, P. Pizette, N. Govender, N. Abriak|0.75|2
1190|Structured Grid-Based Parallel Simulation of a Simple DEM Model on Heterogeneous Systems|Here we present different preliminary parallel grid-based implementations of a simple particle system with the purpose to evaluate its performances on multi- and many-core computational devices. The system is modeled by means of the Discrete Element Method and the Extended Cellular Automata formalism, while OpenMP and OpenCL are used for parallelization. In particular, both the 3.1 and 4.5 OpenMP specifications have been considered, the latter also able to run on many-core computational devices like GPUs. The results of a first test simulation performed by considering a cubic domain with about 316,000 particles have shown a clear advantage of OpenCL on the considered Tesla K40 Nvidia GPU, while the OpenMP 3.1 implementation has performed better than the corresponding OpenMP 4.5 on the considered Intel Xeon E5-2650 16-thread CPU.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2018|10.1109/PDP2018.2018.00099|D. D'Ambrosio, W. Spataro, A. Rango, F. Maio, Pietro Napoli, A. D. Renzo|0.7142857142857143|2
1196|COBRA: A Bayesian approach to pulsar searching|We introduce COBRA, a GPU-accelerated Bayesian analysis package for performing pulsar searching, that uses candidates from traditional search techniques to set the prior used for the periodicity of the source, and performs a blind search in all remaining parameters. COBRA incorporates models for both isolated and accelerated systems, as well as both Keplerian and relativistic binaries, and exploits pulse phase information to combine search epochs coherently, over time, frequency or across multiple telescopes. We demonstrate the efficacy of our approach in a series of simulations that challenge typical search techniques, including highly aliased signals, and relativistic binary systems. In the most extreme case, we simulate an 8 h observation containing 24 orbits of a pulsar in a binary with a 30 M companion. Even in this scenario we show that we can build up from an initial low-significance candidate, to fully recovering the signal. We also apply the method to survey data of three pulsars from the globular cluster 47Tuc: PSRs J0024−7204D, J0023−7203J and J0024−7204R. This final pulsar is in a 1.6 h binary, the shortest of any pulsar in 47Tuc, and additionally shows significant scintillation. By allowing the amplitude of the source to vary as a function of time, however, we show that we are able to obtain optimal combinations of such noisy data. We also demonstrate the ability of COBRA to perform high-precision pulsar timing directly on the single pulse survey data, and obtain a 95 per cent upper limit on the eccentricity of PSR J0024−7204R of b < 0.0007.||2018|10.1093/MNRAS/STX2724|M. Kramer, P. Torne, L. Lentati, D. Champion, E. Barr|0.7142857142857143|2
1210|Transient Thermal Analysis for M.2 SSD Thermal Throttling: Detailed CFD Model vs Network-Based Model|Solid State Drive (SSD) technology continues to advance toward smaller footprints with higher bandwidth and adoption of new I/O interfaces in the PC market segment. Power performance requirements are tightening in the design process to address specific requirement along with the development of SSD technology. To meet this aggressive requirement of performance, one major issue is thermal throttling. As the NAND and ASIC junction temperatures approach their safe operating limits, performance throttling is triggered and thus power consumption would drop accordingly. Therefore, robust thermal understanding on system level as well as reliable and fast thermal prediction are becoming essential in the process of system thermal design to optimize performance in a quick turnaround manner. In this paper, we present two different modeling approaches on the system level to model and simulate M.2 2280 SSD thermal throttling behavior in a typical laptop working environment. One approach is to establish a detailed three dimensional CFD (computational fluid dynamics) model using traditional CFD tools. In this model, the motherboard is enclosed in a case or chassis. Major heat sources of components and packages on the motherboard are considered including CPU, GPU, M.2 SSD, DRAM etc. Advanced cooling solutions like heat pipe and blowers are also modeled. In order to accurately capture thermal behavior of the SSD, detailed structure and geometry of NAND, PMIC and ASIC packages are included. Both natural and force convection as well as radiation are considered in this model. Both steady state and transient simulation results are presented in this paper. Further, the simulation results are validated with experimental data to predict thermal throttling behavior. The experiment is carried out with the SSD running in a laptop and temperatures of NAND and platform are logged during the test. In this paper, a second approach to generate accurate thermal models is presented for electronic parts. The thermal model of an electronic part is extracted from its detailed geometry configuration and material properties, so multiple thermal models can form a thermal network for complex steady-state and transient analyses of a system design. The extracted thermal model has the following advantages,1.It can accurately predict both static and dynamic thermal behaviors of the electronic parts;2.It can accurately predict the temperature at any probing node pre-defined in the electronic part;3.It is independent of boundary condition and can accurately predict the thermal behavior regardless of the environment and cooling conditions.With the accurate dynamic thermal models, a large thermal system can be decoupled into multiple domains such as air flows, chassis, heat sinks, PCB boards, packages, etc. The whole system can be consequently reconstructed as an integrated model-based network, and thermal simulation can be performed using fast network simulators. In comparison to the traditional CFD or FEM tools, the network-based approach improves efficiency in both thermal system construction and simulation. This approach is demonstrated through thermal simulation of the SSD drive within a laptop environment under natural convection in its working condition. The simulated system includes packages, M.2 PCB, motherboard, heat sink, and chassis.|Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems|2018|10.1109/ITHERM.2018.8419631|Ning Ye, Nathan Ai, Shay Braha, Nir Amir, Hedan Zhang, Ernold Thompson, Hainan Wang, C. Kao|0.7142857142857143|2
1233|Reliability Estimations of Large Circuits in Massively-Parallel GPU-SPICE|SPICE simulations for reliability have special requirements. We present GPU-SPICE to serve these special requirements. First, our GPU-SPICE employs the massive parallelism found in GPUs to enable circuit simulations beyond $200K$ transistors. This is necessary to study reliability in micro-architecture components (e.g., multipliers, adders), as reliability estimations require full analogue SPICE simulations (instead of STA or other heuristics). Secondly, our GPU-SPICE can update transistor parameters during the circuit simulation, a feature necessary to model reliability degradation, which constantly reacts to circuit activity (e.g., Bias Temperature Instability reacting to $V_{gs}$ changes by increasing/decreasing $\Delta V_{th}$ in each transistor). Lastly, our GPU-SPICE is open-source software, this ensures that it easily can be employed, adapted and extended by other researchers. Due to the massive parallelism in a GPU and performance optimizations (convergence criteria, CUDA memory management, etc.), our GPU-SPICE is up to 218x faster than its single-threaded baseline NGSPICE.|IEEE International Symposium on On-Line Testing and Robust System Design|2018|10.1109/IOLTS.2018.8474096|H. Amrouch, Victor M. van Santen, J. Henkel|0.7142857142857143|2
1239|Whole Program Generation of Massively Parallel Shallow Water Equation Solvers|The study of ocean currents has been an active area of research for decades. As a model close to the water surface, the shallow water equations (SWE) can be used. For realistic simulations, efficient numerical solvers are necessary that exhibit a good node-level performance while still maintaining scalability. When comparing the discretized model and the actual implementation, one often finds that they differ vastly. This gap makes it hard for domain experts to implement their models and high performance computing (HPC) experts are required to ensure an optimal implementation. Using domain-specific languages (DSLs) and code generation techniques can be a useful tool to bridge this gap. In recent years, ExaStencils and its DSL ExaSlang have proven to provide a suitable platform for this. We present an extension from up to now elliptic to hyperbolic partial differential equations (PDEs) in this work, namely the SWE. After setting up a suitable discretization, we demonstrate how it can be mapped to ExaSlang code. This code is still quite similar to the original, mathematically motivated specification and can be easily written by domain experts. Still, solvers generated from this abstract representation can be run on large-scale clusters. We demonstrate this by giving performance and scalability results on the state-of-the-art GPU cluster Piz Daint where we solve for close to a trillion unknowns on 2048 GPUs. From there, we discuss the performance impact of different optimizations such as overlapping computation and communication, or switching to a hybrid CPU-GPU parallelization scheme.|IEEE International Conference on Cluster Computing|2018|10.1109/CLUSTER.2018.00020|S. Kuckuk, H. Köstler|0.7142857142857143|2
1243|A Boundary‐Integral Approach for the Poisson–Boltzmann Equation with Polarizable Force Fields|Implicit‐solvent models are widely used to study the electrostatics in dissolved biomolecules, which are parameterized using force fields. Standard force fields treat the charge distribution with point charges; however, other force fields have emerged which offer a more realistic description by considering polarizability. In this work, we present the implementation of the polarizable and multipolar force field atomic multipole optimized energetics for biomolecular applications (AMOEBA), in the boundary integral Poisson–Boltzmann solver PyGBe. Previous work from other researchers coupled AMOEBA with the finite‐difference solver APBS, and found difficulties to effectively transfer the multipolar charge description to the mesh. A boundary integral formulation treats the charge distribution analytically, overlooking such limitations. This becomes particularly important in simulations that need high accuracy, for example, when the quantity of interest is the difference between solvation energies obtained from separate calculations, like happens for binding energy. We present verification and validation results of our software, compare it with the implementation on APBS, and assess the efficiency of AMOEBA and classical point‐charge force fields in a Poisson–Boltzmann solver. We found that a boundary integral approach performs similarly to a volumetric method on CPU. Also, we present a GPU implementation of our solver. Moreover, with a boundary element method, the mesh density to correctly resolve the electrostatic potential is the same for standard point‐charge and multipolar force fields. Finally, we saw that for binding energy calculations, a boundary integral approach presents more consistent results than a finite difference approximation for multipolar force fields. © 2019 Wiley Periodicals, Inc.|Journal of Computational Chemistry|2018|10.1002/jcc.25820|C. D. Cooper|0.7142857142857143|2
1250|Applying the swept rule for explicit partial differential equation solutions on heterogeneous computing systems|Applications that exploit the architectural details of high performance computing (HPC) systems have become increasingly invaluable in academia and industry over the past two decades. The most important hardware development of the last decade in HPC has been the General Purpose Graphics Processing Unit (GPGPU), a class of massively parallel devices that now contributes the majority of computational power in the top 500 supercomputers. As these systems grow small costs such as latency---the fixed cost of memory accesses---accumulate over the numerous iterations in a large simulation and become a significant barrier to performance. The swept time-space decomposition rule is a communication-avoiding technique for time-stepping stencil update formulas that attempts to sidestep a portion of the latency costs. This work extends the swept rule by targeting heterogeneous, CPU/GPU architectures representative of current and future HPC systems. We compare our approach to a naive decomposition scheme with two test equations using an MPI+CUDA pattern on 40 processes over two nodes containing one GPU. We show that the swept rule produces a 4--18x speedup with the heat equation and a 1.5-3x speedup with the Euler equations using the same processors and work distribution. These results demonstrate the potential effectiveness of the swept rule for different equations and numerical schemes on massively parallel compute systems that incur substantial latency costs.|arXiv.org|2018|10.1016/j.jcp.2017.12.028|Kyle E. Niemeyer, Daniel J. Magee|0.7142857142857143|2
1388|Online Scheduling of Sequential Task Graphs on Hybrid Platforms|Modern computing platforms commonly include accelerators. We target the problem of scheduling applications modeled as task graphs on hybrid platforms made of two types of resources, such as CPUs and GPUs. We consider that task graphs are uncovered dynamically, and that the scheduler has information only on the available tasks, i.e., tasks whose predecessors have all been completed. Each task can be processed by either a CPU or a GPU, and the corresponding processing times are known. Our study extends a previous $4\sqrt{m/k}-competitive$ online algorithm [3], where m is the number of CPUs and k the number of GPUs (m≥k). We prove that no online algorithm can have a competitive ratio smaller than $\sqrt{m/k}$. We also study how adding flexibility on task processing, such as task migration or spoliation, or increasing the knowledge of the scheduler by providing it with information on the task graph, influences the lower bound. We provide a $(2\sqrt{m/k}+1)$-competitive algorithm as well as a tunable combination of a system-oriented heuristic and a competitive algorithm; this combination performs well in practice and has a competitive ratio in $Θ(\sqrt{m/k})$. We extend our results to more types of processors. Finally, simulations on different sets of task graphs illustrate how the instance properties impact the performance of the studied algorithms and show that our proposed tunable algorithm performs the best among the online algorithms in almost all cases and has even performance close to an offline algorithm.||2018|10.1007/978-3-319-96983-1_14|L. Marchal, Louis-Claude Canon, F. Vivien, B. Simon|0.7142857142857143|2
1428|Swept time-space domain decomposition on GPUs and heterogeneous computing systems|Friday, June 8, 2018 9:00 am, Rog 226 Modern scientific and engineering problems often require simulations with a level of resolution difficult to achieve in reasonable amounts of time—even in effectively parallelized programs. Therefore, applications that exploit high performance computing (HPC) systems have become invaluable in academia and industry over the past two decades. Addressing the questions that arise from continual scientific advancement requires solutions from hardware and software are required to supply the necessary throughput for demand across scientific disciplines. The most important development on the hardware side has been the General Purpose Graphics Processing Unit (GPGPU), a class of massively parallel device that now composes a substantial portion of the computational power of the top 500 supercomputers. As these systems grow, barriers to increased performance arise from small costs accumulated over innumerable iterations such as latency, the fixed cost of memory accesses, which becomes significantly larger when access requires communication between two distant CPU processes. This thesis implements and analyzes swept time-space domain decomposition, a communication avoiding scheme for time-stepping stencil codes, for GPGPU and heterogeneous (CPU/GPU) architectures. The GPGPU program significantly improves the execution time of finite-difference solvers for relatively simple one-dimensional time-stepping partial differential equations (PDEs). The swept decomposition code showed speedups of 2-9x compared with simple GPU domain decompositions and 7-300x compared with parallel CPU versions over a range of problem sizes: 103 – 106 spatial points. However, for a more sophisticated one-dimensional system of equations discretized with a second-order finite-volume scheme, the swept rule performs 1.2-1.9x than a standard implementation for all problem sizes. The program targeting heterogeneous systems with distributed memory patterns performs significantly better on both simple problems, speedup 4-18x, and more complex equation systems, speedup 1.5-3x, over the range of problem sizes: 105-107 spatial points. This demonstrates the benefit of GPU architecture and the contingent effectiveness of swept timespace decomposition for accelerating explicit PDE solvers on current computational architectures.||2018|10.1016/j.jcp.2017.12.028|Daniel J. Magee|0.7142857142857143|2
1500|Accelerating unstructured large eddy simulation solver with GPU|\nPurpose\nAdopting large eddy simulation (LES) to simulate the complex flow in turbomachinery is appropriate to overcome the limitation of current Reynolds-Averaged Navier–Stokes modelling and it provides a deeper understanding of the complicated transitional and turbulent flow mechanism; however, the large computational cost limits its application in high Reynolds number flow. This study aims to develop a three-dimensional GPU-enabled parallel-unstructured solver to speed up the high-fidelity LES simulation.\n\n\nDesign/methodology/approach\nCompared to the central processing units (CPUs), graphics processing units (GPUs) can provide higher computational speed. This work aims to develop a three-dimensional GPU-enabled parallel-unstructured solver to speed up the high-fidelity LES simulation. A set of low-dissipation schemes designed for unstructured mesh is implemented with compute unified device architecture programming model. Several key parameters affecting the performance of the GPU code are discussed and further speed-up can be obtained by analysing the underlying finite volume-based numerical scheme.\n\n\nFindings\nThe results show that an acceleration ratio of approximately 84 (on a single GPU) for double precision algorithm can be achieved with this unstructured GPU code. The transitional flow inside a compressor is simulated and the computational efficiency has been improved greatly. The transition process is discussed and the role of K-H instability playing in the transition mechanism is verified.\n\n\nPractical/implications\nThe speed-up gained from GPU-enabled solver reaches 84 compared to original code running on CPU and the vast speed-up enables the fast-turnaround high-fidelity LES simulation.\n\n\nOriginality/value\nThe GPU-enabled flow solver is implemented and optimized according to the feature of finite volume scheme. The solving time is reduced remarkably and the detail structures including vortices are captured.\n|Engineering computations|2018|10.1108/EC-01-2018-0043|Xin Yuan, Hongbin Liu, Xinrong Su|0.7142857142857143|2
1507|Modeling the Kerr-Nonlinearity in Mode-Division Multiplexing Fiber Transmission Systems on GPUs|We discuss the GPU-acceleration of MDM transmission system simulations and how the required memory can be drastically reduced to simulate a high number of modes. Furthermore, we show how to reduce the runtime.||2018|10.1364/BGPPM.2018.JTU5A.27|P. Krummrich, Dominik Göddeke, M. Schirwon, Marius Brehler|0.7142857142857143|2
1528|Quantum computing in geophysics: Algorithms, computational costs, and future applications|Accurate modeling of seismic wave propagation in the subsurface of the earth is essential for understanding earthquake dynamics, characterizing seismic hazards on global scales and hydrocarbon reservoir exploration and monitoring on local scales. These are among the most challenging computational problems in geoscience. Despite algorithmic advances and the increasingly powerful computational resources currently available, including fast CPUs, GPUs and large volumes of computer memory, there are still daunting computational challenges in simulating 3D seismic wave propagation in complex earth environments. Recent advances in quantum computing are suggestive that geoscience may soon begin to benefit from this promising field. For example, Finite Difference (FD) modeling is the most widely used method to simulate seismic wave propagation. In the frequency domain, FD methods reduce solutions of the wave equation into systems of linear equations; such systems are just the type that quantum algorithms may be capable of solving with exponential speedup, in comparison with classical algorithms. For the computational geophysicist, to prepare to take advantage of these speed-ups, which could arrive in as few as 5-10 years, the tasks at hand are (1) to become familiar with the logic and concepts associated with quantum computing, and (2) to map our key computational algorithms (e.g., frequency domain FD) to this domain.|SEG technical program expanded abstracts|2018|10.1190/SEGAM2018-2998507.1|D. Trad, S. Moradi, K. Innanen|0.7142857142857143|2
205|Algorithms for GPU‐based molecular dynamics simulations of complex fluids: Applications to water, mixtures, and liquid crystals|A custom code for molecular dynamics simulations has been designed to run on CUDA‐enabled NVIDIA graphics processing units (GPUs). The double‐precision code simulates multicomponent fluids, with intramolecular and intermolecular forces, coarse‐grained and atomistic models, holonomic constraints, Nosé–Hoover thermostats, and the generation of distribution functions. Algorithms to compute Lennard‐Jones and Gay‐Berne interactions, and the electrostatic force using Ewald summations, are discussed. A neighbor list is introduced to improve scaling with respect to system size. Three test systems are examined: SPC/E water; an n‐hexane/2‐propanol mixture; and a liquid crystal mesogen, 2‐(4‐butyloxyphenyl)‐5‐octyloxypyrimidine. Code performance is analyzed for each system. With one GPU, a 33–119 fold increase in performance is achieved compared with the serial code while the use of two GPUs leads to a 69–287 fold improvement and three GPUs yield a 101–377 fold speedup. © 2015 Wiley Periodicals, Inc.|Journal of Computational Chemistry|2015|10.1002/jcc.24000|N. M. Cann, M. Giovinazzo, S. Kazachenko, K. Hall|0.7|2
267|Visual Simulation of Soil-Microbial System Using GPGPU Technology|General Purpose (use of) Graphics Processing Units (GPGPU) is a promising technology for simulation upscaling; in particular for bottom–up modelling approaches seeking to translate micro-scale system processes to macro-scale properties. Many existing simulations of soil ecosystems do not recover the emergent system scale properties and this may be a consequence of “missing” information at finer scales. Interpretation of model output can be challenging and we advocate the “built-in” visual simulation afforded by GPGPU implementations. We apply this GPGPU approach to a reaction–diffusion soil ecosystem model with the intent of linking micro (micron) and core (cm) spatial scales to investigate how microbes respond to changing environments and the consequences on soil respiration. The performance is evaluated in terms of computational speed up, spatial upscaling and visual feedback. We conclude that a GPGPU approach can significantly improve computational efficiency and offers the potential added benefit of visual immediacy. For massive spatial domains distribution over GPU devices may still be required.|De Computis|2015|10.3390/computation3010058|A. Houston, R. Falconer|0.7|2
344|Multi-story power distribution networks for GPUs|High-performance chips require many power pins to support large currents, which increases fabrication cost, limits scalability, and degrades power efficiency. Multi-story serial power distribution networks (PDNs) are a promising approach to reducing pin counts and power losses. We study the feasibility of 2-story PDNs for graphics processing units (GPUs). These PDNs use either an auxiliary off-chip regulator or integrated on-die supercapacitors to stabilize the virtual rail voltage. Static SIMT thread scheduling (SSTS) and dynamic current compensation (DCC) can reduce transient impedance mismatch when the auxiliary regulator is omitted. Simulation results show that compared to a traditional 1-story design, our 2-story GPU architectures can reduce the required number of core power pins by up to 2X, power losses in the PDN by up to 3.6X, and/or maximum voltage swing by up to 2X without any performance degradation. Our results demonstrate the efficiency and cost advantages of multistory PDNs for GPUs without any impact on performance.|Design, Automation and Test in Europe|2016|10.3850/9783981537079_0900|Liangzhen Lai, Puneet Gupta, Mark Gottscho, Qixiang Zhang|0.6666666666666666|2
389|Real-Time Agent-Based Modeling Simulation with in-Situ Visualization of Complex Biological Systems: A Case Study on Vocal Fold Inflammation and Healing|We present an efficient and scalable scheme for implementing agent-based modeling (ABM) simulation with In Situ visualization of large complex systems on heterogeneous computing platforms. The scheme is designed to make optimal use of the resources available on a heterogeneous platform consisting of a multicore CPU and a GPU, resulting in minimal to no resource idle time. Furthermore, the scheme was implemented under a client-server paradigm that enables remote users to visualize and analyze simulation data as it is being generated at each time step of the model. Performance of a simulation case study of vocal fold inflammation and wound healing with 3.8 million agents shows 35x and 7x speedup in execution time over single-core and multi-core CPU respectively. Each iteration of the model took less than 200 ms to simulate, visualize and send the results to the client. This enables users to monitor the simulation in real-time and modify its course as needed.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2016|10.1109/IPDPSW.2016.20|J. JáJá, Nuttiiya Seekhao, C. Shung, N. Y. Li-Jessen, L. Mongeau|0.6666666666666666|2
454|Shallow Sand Equations: Real-Time Height Field Simulation of Dry Granular Flows|Granular media is the second-most-manipulated substance on Earth, second only to water. However, simulation of granular media is still challenging due to the complexity of granular materials and the large number of discrete solid particles. As we know, dry granular materials could form a hybrid state between a fluid and a solid, therefore we propose a two-layer model and divide the simulation domain into a dilute layer, where granules can move freely as a fluid, and a dense layer, where granules act more like a solid. Motivated by the shallow water equations, we derive a set of shallow sand equations for modeling dry granular flows by depth-integrating three-dimensional governing equations along its vertical direction. Unlike previous methods for simulating a 2D granular media, our model does not restrict the depth of the granular media to be shallow anymore. To allow efficient fluid-solid interactions, we also present a ray casting algorithm for one-way solid-fluid coupling. Finally, we introduce a particle-tracking method to improve the visual representation. Our method can be efficiently implemented based on a height field and is fully compatible with modern GPUs, therefore allows us to simulate large-scale dry granular flows in real time.|IEEE Transactions on Visualization and Computer Graphics|2019|10.1109/TVCG.2019.2944172|Sheng Li, Xiaowei He, Hongan Wang, K. Zhu, Guoping Wang|0.6666666666666666|2
466|The Castro AMR Simulation Code: Current and Future Developments|We describe recent developments to the Castro astrophysics simulation code, focusing on new features that enable our simulations of X-ray bursts. Two highlights of Castro’s ongoing development are the new integration technique to couple hydrodynamics and reactions to high order and GPU offloading. We discuss how these features will help offset some of the computational expense in X-ray burst models.|Journal of Physics: Conference Series|2019|10.1088/1742-6596/1623/1/012021|A. Nonaka, W. Zhang, J. Bell, M. Katz, M. Zingale, K. Eiden, A. Almgren, M. B. Sazo, D. Willcox, A. Harpole|0.6666666666666666|2
494|GPU-Based Rendering of Arbitrarily Complex Cutting Surfaces for Black Oil Reservoir Models|Numerical simulation of black oil reservoir models is extensively used by the oil industry to predict and plan field exploration. Such simulations produce a large amount of volume data that need to be inspected. One popular visualization technique to inspect volume data is the rendering of cutting surfaces, shaded by mapping properties associated with model elements. In this work, an efficient GPU-based algorithm for rendering arbitrarily complex cutting surfaces for reservoir models is presented. The rendering strategy is based on an efficient point location algorithm. The proposal includes a compact representation of reservoir models in the GPU memory, the use of a compact regular grid as the acceleration technique, and an accurate point location algorithm for handling hexahedral elements with non-planar faces. Computational experiments have demonstrated the effectiveness and efficiency of the proposed approach, even when applied to large models. A set of applications is discussed in the context of inspecting reservoir simulation results.|SIBGRAPI Conference on Graphics, Patterns and Images|2019|10.1109/SIBGRAPI.2019.00026|Waldemar Celes Filho, F. Abraham, Luiz Felipe Netto, Bernardo Franceschin|0.6666666666666666|2
500|Position‐Based Simulation of Elastic Models on the GPU with Energy Aware Gauss‐Seidel Algorithm|In this paper, we provide a smooth extension of the energy aware Gauss‐Seidel iteration to the Position‐Based Dynamics (PBD) method. This extension is inspired by the kinetic and potential energy changes equalization and uses the foundations of the recent extended version of PBD algorithm (XPBD). The proposed method is not meant to conserve the total energy of the system and modifies each position constraint based on the equality of the kinetic and potential energy changes within the Gauss‐Seidel process of the XPBD algorithm. Our extension provides an implicit solution for relatively better stiffness during the simulation of elastic objects. We apply our solution directly within each Gauss‐Seidel iteration and it is independent of both simulation step‐size and integration methods. To demonstrate the benefits of our proposed extension with higher frame rates, we develop an efficient and practical mesh coloring algorithm for the XPBD method which provides parallel processing on a GPU. During the initialization phase, all mesh primitives are grouped according to their connectivity. Afterwards, all these groups are computed simultaneously on a GPU during the simulation phase. We demonstrate the benefits of our method with many spring potential and strain‐based continuous material constraints. Our proposed algorithm is easy to implement and seamlessly fits into the existing position‐based frameworks.|Computer graphics forum (Print)|2019|10.1111/cgf.13759|O. Cetinaslan|0.6666666666666666|2
811|GPU-Accelerated Online Short-Circuit Interrupting Capacity Scan Based on Unified Modeling|With the increasing of transmission density and short-circuit current level, interruption capacity scanning has become an important approach for the online assessment of the circuit breaker. The short-circuit calculation scale and the expected failures significantly increase under the demand of unified analysis of the whole grid, which cause the considerable challenges for online assessment. In this study, we propose a GPU-accelerated short-circuit interruption capacity scanning algorithm for two different unified models, that is, GPU-based 2D and 3D expansion models. For both models, the redundant configuration of the fault network matrices is designed to reduce the cost of GPU computation process. Then, the batch node voltage analysis and the batch branch current calculation are derived using the considered model. In addition, the differences between two models are discussed in detail. Simulations show that the proposed algorithm with GPU-based 3D-expand model has a 3.45× speedup compared with the traditional algorithm on a 9241-bus system with 8-core CPU. Therefore, the proposed algorithms have the potential to handle the full short-circuit current scan under the unified analysis scenario.|IEEE Transactions on Power Systems|2022|10.1109/tpwrs.2021.3102617|Meng Fu, Jiahao Zhao, Yanjun Feng, Long Jin, Gan Zhou|0.6666666666666666|2
859|Scalable Microbial Strain Inference in Metagenomic Data Using StrainFacts|While genome databases are nearing a complete catalog of species commonly inhabiting the human gut, their representation of intraspecific diversity is lacking for all but the most abundant and frequently studied taxa. Statistical deconvolution of allele frequencies from shotgun metagenomic data into strain genotypes and relative abundances is a promising approach, but existing methods are limited by computational scalability. Here we introduce StrainFacts, a method for strain deconvolution that enables inference across tens of thousands of metagenomes. We harness a “fuzzy” genotype approximation that makes the underlying graphical model fully differentiable, unlike existing methods. This allows parameter estimates to be optimized with gradient-based methods, speeding up model fitting by two orders of magnitude. A GPU implementation provides additional scalability. Extensive simulations show that StrainFacts can perform strain inference on thousands of metagenomes and has comparable accuracy to more computationally intensive tools. We further validate our strain inferences using single-cell genomic sequencing from a human stool sample. Applying StrainFacts to a collection of more than 10,000 publicly available human stool metagenomes, we quantify patterns of strain diversity, biogeography, and linkage-disequilibrium that agree with and expand on what is known based on existing reference genomes. StrainFacts paves the way for large-scale biogeography and population genetic studies of microbiomes using metagenomic data.|bioRxiv|2022|10.1101/2022.02.01.478746|Byron J. Smith, Zhou Jason Shi, Xiangpeng Li, A. Abate, K. Pollard|0.6666666666666666|2
867|GPU Accelerated Parallel Processing for Large-Scale Monte Carlo Analysis: COVID-19 Parameter Estimation and New Case Forecasting|Markov Chain Monte Carlo methods have emerged as one of the premier approaches to estimating posterior distributions for use in Bayesian computations. Unfortunately, these methods often suffer from slow run times when the data become large or when the parameter values come from complex distributions. This speed issue has prevented MCMC analysis from being used to solve some of the most interesting problems for which its technique is a good fit. We used the Multiple-Try Metropolis variant of the basic Metropolis Hastings algorithm, which trades off running more parallel likelihood calculations in favor of a higher acceptance rate and faster convergence compared to traditional MCMC. We optimized our algorithm to parallelize it and to take advantage of GPU processing. We applied our approach to parameter estimation for a Susceptible-Exposed-Infectious-Removed (SEIR) model and forecasting new cases of COVID-19. In comparison to a fully parallelized CPU implementation, using a single GPU to execute the simulations resulted in more than a 13x speedup in wall clock time, running on multiple GPUs resulted in a 36.3x speedup in wall clock time, and using a cloud-based server consisting of 8 GPUs resulted in a 56.5x speedup in wall clock time. Our approach shows that MCMC methods can be utilized to tackle problems that were previously thought to be too computationally intensive and slow.|Frontiers in Applied Mathematics and Statistics|2022|10.3389/fams.2022.818016|S. Stage, H. Gurung, B. Suchoski, P. Baccam|0.6666666666666666|2
872|Uncertainty quantification in cerebral circulation simulations focusing on the collateral flow: Surrogate model approach with machine learning|Collateral circulation in the circle of Willis (CoW), closely associated with disease mechanisms and treatment outcomes, can be effectively investigated using one-dimensional–zero-dimensional hemodynamic simulations. As the entire cardiovascular system is considered in the simulation, it captures the systemic effects of local arterial changes, thus reproducing collateral circulation that reflects biological phenomena. The simulation facilitates rapid assessment of clinically relevant hemodynamic quantities under patient-specific conditions by incorporating clinical data. During patient-specific simulations, the impact of clinical data uncertainty on the simulated quantities should be quantified to obtain reliable results. However, as uncertainty quantification (UQ) is time-consuming and computationally expensive, its implementation in time-sensitive clinical applications is considered impractical. Therefore, we constructed a surrogate model based on machine learning using simulation data. The model accurately predicts the flow rate and pressure in the CoW in a few milliseconds. This reduced computation time enables the UQ execution with 100 000 predictions in a few minutes on a single CPU core and in less than a minute on a GPU. We performed UQ to predict the risk of cerebral hyperperfusion (CH), a life-threatening condition that can occur after carotid artery stenosis surgery if collateral circulation fails to function appropriately. We predicted the statistics of the postoperative flow rate increase in the CoW, which is a measure of CH, considering the uncertainties of arterial diameters, stenosis parameters, and flow rates measured using the patients’ clinical data. A sensitivity analysis was performed to clarify the impact of each uncertain parameter on the flow rate increase. Results indicated that CH occurred when two conditions were satisfied simultaneously: severe stenosis and when arteries of small diameter serve as the collateral pathway to the cerebral artery on the stenosis side. These findings elucidate the biological aspects of cerebral circulation in terms of the relationship between collateral flow and CH. Author summary Cerebral arteries generate a ring-like network that provides alternative routes for blood supply in the case of carotid artery stenosis. This collateral circulation is closely associated with the potential risk of stroke and treatment outcomes in patients with stenosis. In this study, we propose a method to elucidate the cerebral circulation of individual patients using a blood flow simulation that incorporates the patient’s clinical data. A key feature of our approach is its capability to obtain the probability of the different outputs using simulation, considering the uncertainty of patient conditions. Although this capability is essential for obtaining reliable results, the process is time-consuming and requires numerous computer resources. We solved this problem by combining the blood flow simulation with machine learning to perform predictions 43 000 times faster than conventional simulations. We applied the proposed method to predict cerebral circulation following surgery in three patients with stenosis and verified that the method can assess the surgical risk almost in real-time, even on a desktop computer. Additionally, extensive prediction results (100 000 cases for each patient) obtained using this method clarify the relationship between collateral circulation and life-threatening surgical outcomes.|bioRxiv|2022|10.1101/2022.03.10.483573|M. Oshima, Yan Chen, Shigeki Yamada, Changyoung Yuhn, M. Hayakawa|0.6666666666666666|2
884|Predicting the Soft Error Vulnerability of GPGPU Applications|As Graphics Processing Units (GPUs) have evolved to deliver performance increases for general-purpose computations as well as graphics and multimedia applications, soft error reliability becomes an important concern. The soft error vulnerability of the applications is evaluated via fault injection experiments. Since performing fault injection takes impractical times to cover the fault locations in complex GPU hardware structures, prediction-based techniques have been proposed to evaluate the soft error vulnerability of General-Purpose GPU (GPGPU) programs based on the hardware performance characteristics.In this work, we propose ML-based prediction models for the soft error vulnerability evaluation of GPGPU programs. We consider both program characteristics and hardware performance metrics collected from either the simulation or the profiling tools. While we utilize regression models for the prediction of the masked fault rates, we build classification models to specify the vulnerability level of the programs based on their silent data corruption (SDC) and crash rates. Our prediction models achieve maximum prediction accuracy rates of 96.6%, 82.6%, and 87% for masked fault rates, SDCs, and crashes, respectively.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2022|10.1109/pdp55904.2022.00025|Burak Topçu, Işıl Öz|0.6666666666666666|2
887|Performance analysis of matrix-free conjugate gradient kernels using SYCL|We examine the performance of matrix-free SYCL implementations of the conjugate gradient method for solving sparse linear systems of equations. Performance is tested on an NVIDIA A100-80GB device and a dual socket Intel Ice Lake CPU node using different SYCL implementations, and compared to CUDA BLAS (cuBLAS) implementations on the A100 GPU and MKL implementations on the CPU node. All considered kernels in the matrix-free implementation are memory bandwidth limited, and a simple performance model is applied to estimate the asymptotic memory bandwidth and the latency. Our experiments show that in most cases the considered SYCL implementations match the asymptotic performance of the reference implementations. However, for smaller but practically relevant problem sizes latency is observed to have a significant impact on performance. For some cases the SYCL latency is reasonably close to the reference (cuBLAS/MKL) implementation latency, but in other cases it is more than one order of magnitude greater. In particular, SYCL built-in reductions on the GPU and all operations for one of the SYCL implementations on the CPU exhibit high latency, and this latency limits performance at problem sizes that can in cases be representative of full application simulations, and can degrade strong scaling performance.|International Workshop on OpenCL|2022|10.1145/3529538.3529993|I. Baratta, G. N. Wells, C. Richardson|0.6666666666666666|2
899|Scaling and Benchmarking an Evolutionary Algorithm for Constructing Biophysical Neuronal Models|Single neuron models are fundamental for computational modeling of the brain's neuronal networks, and understanding how ion channel dynamics mediate neural function. A challenge in defining such models is determining biophysically realistic channel distributions. Here, we present an efficient, highly parallel evolutionary algorithm for developing such models, named NeuroGPU-EA. NeuroGPU-EA uses CPUs and GPUs concurrently to simulate and evaluate neuron membrane potentials with respect to multiple stimuli. We demonstrate a logarithmic cost for scaling the stimuli used in the fitting procedure. NeuroGPU-EA outperforms the typically used CPU based evolutionary algorithm by a factor of 10 on a series of scaling benchmarks. We report observed performance bottlenecks and propose mitigation strategies. Finally, we also discuss the potential of this method for efficient simulation and evaluation of electrophysiological waveforms.|Frontiers in Neuroinformatics|2022|10.3389/fninf.2022.882552|J. Balewski, Kyung Geun Kim, Roy Ben-Shalom, Alexander Ladd, K. Bouchard|0.6666666666666666|2
906|Scaling up Deep Reinforcement Learning for Intelligent Video Game Agents|We introduce the concept of Deep Reinforcement Learning (DRL) and describe the current state-of-the-art in subareas relevant to the author's research. We present previous and ongoing work done by the author in context of game engines, video game development and Machine Learning performance. We discuss our measurements showing the performance discrepancy between training DRL agents on game engines and end-to-end GPU-based physics simulators. We propose the use of external GPU-based physics simulators and transfer learning to accelerate the training of DRL models for game engines. As future work, we discuss the use of model decomposition in complex environments to further accelerate learning efficiency of DRL in addition to increased hardware utilization.|International Conference on Smart Computing|2022|10.1109/smartcomp55677.2022.00050|Anton Debner|0.6666666666666666|2
907|Fast and High-Quality Influence Maximization on Multiple GPUs|Influence Maximization (IM) is a popular problem focusing on finding a seed vertex set in a graph that maximizes the expected number of vertices affected via diffusion under a given, usually probabilistic model. For most diffusion models used in practice, finding an optimal seed set of a given size is NP-Hard. Hence, approximation algorithms and heuristics are often proposed and used. The Greedy approach is one of the most frequently applied approximation approach employed for IM. Indeed, this Monte-Carlo-based approach performs remarkably well in terms of seed set quality, i.e., the number of affected vertices. However, it is impractical for real-life networks containing tens of millions of vertices due to its expensive simulation costs. Recently, parallel IM kernels running on CPUs and GPUs have been proposed in the literature. In this work, we propose SUPERFUSER, a blazing-fast, sketch-based Influence Maximization algorithm developed for multiple GPUs. SUPERFUSER uses hash-based fused sampling to process multiple simulations at the same time with minimal overhead. In addition, we propose a Sampling-Aware Sample-Space Split approach to partition the edges to multiple GPUs efficiently by exploiting the unique characteristics of the sampling process. Based on our experiments, SUPERFUSER is up to 6.31× faster than its nearest competitor on a single GPU. Furthermore, we achieve 6.8× speed-up on average using 8 GPUs over a single GPU performance, and thanks to our novel partitioning scheme, we can process extremely large-scale graphs in practice without sacrificing quality too much. As an example, SUPERFUSER can generate a high-quality seed set with 50 vertices for a graph having 1.8B edges in less than 15 seconds on 2 GPUs.|IEEE International Parallel and Distributed Processing Symposium|2022|10.1109/ipdps53621.2022.00093|Gökhan Göktürk, K. Kaya|0.6666666666666666|2
911|An efficient sparse stiffness matrix vector multiplication using compressed sparse row storage format on AMD GPU|The performance of sparse stiffness matrix‐vector multiplication is essential for large‐scale structural mechanics numerical simulation. Compressed sparse row (CSR) is the most common format for storing sparse stiffness matrices. However, the high sparsity of the sparse stiffness matrix makes the number of nonzero elements per row very small. Therefore, the CSR‐scalar algorithm, light algorithm, and HOLA algorithm in the calculation will cause some threads in the GPU to be in idle state, which will not only affect the computing performance but also waste computing resources. In this article, a new algorithm, CSR‐vector row, is proposed for fine‐grained computing optimization based on the AMD GPU architecture on heterogeneous supercomputers. This algorithm can set a vector to calculate a row based on the number of nonzero elements of the stiffness matrix. CSR‐vector row has efficient reduce operations, deep memory access optimization, better memory access, and calculation overlapping kernel function configuration scheme. The access bandwidth of the algorithm on AMD GPU is more than 700 GB/s. Compared with CSR‐scalar algorithm, the parallel efficiency of CSR‐vector row is improved by 7.2 times. And floating‐point computing performance is 41%–95% higher than that of light algorithm and HOLA algorithm. In addition, CSR‐vector row is used to calculate the examples from CFD, electromagnetics, quantum chemistry, power network, and semiconductor process, the memory access bandwidth and double floating‐point performance are also improved compared with rocSPARSE‐CSR‐vector.|Concurrency and Computation|2022|10.1002/cpe.7186|Nan Xiao, Lingyu Dong, Genshen Chu, Longyue Xing, Zhezhao Ding, Zhaoshun Wang|0.6666666666666666|2
921|Computational models of direct and indirect x-ray breast imaging detectors for in silico trials.|BACKGROUND\nTo facilitate in silico studies that investigate digital mammography (DM) and breast tomosynthesis (DBT), models replicating the variety in imaging performance of the DM and DBT systems, observed across manufacturers are needed.\n\n\nPURPOSE\nThe main purpose of this work is to develop generic physics models for direct and indirect detector technology used in commercially available systems, with the goal of making them available open source to manufacturers to further tweak and develop the exact in silico replicas of their systems.\n\n\nMETHODS\nWe recently reported on an in silico version of the SIEMENS Mammomat Inspiration DM/DBT system using an open-source GPU-accelerated Monte Carlo x-ray imaging simulation code, MC-GPU. We build on the previous version of the MC-GPU codes to mimic the imaging performances of two other FDA-approved DM/ DBT systems, such as Hologic Selenia Dimensions (HSD) and the General Electric Senographe Pristina (GSP) systems. In this work, we developed a hybrid technique to model the optical spread and signal crosstalk observed in the GSP and HSD systems. MC simulations are used to track each x-ray photon till its first interaction within the x-ray detector. On the other hand, the signal spread in the x-ray detectors is modeled using previously developed analytical equations. This approach allows us to preserve the modeling accuracy offered by MC methods in the patient body, while speeding up secondary carrier transport (either electron-hole pairs or optical photons) using analytical equations in the detector. The analytical optical spread model for the indirect detector includes the depth-dependent spread and collection of optical photons and relies on a pre-computed set of point response functions that describe the optical spread as a function of depth. To understand the capabilities of the computational x-ray detector models, we compared image quality metrics like modulation transfer function (MTF), normalized noise power spectrum (NNPS), and detective quantum efficiency (DQE), simulated with our models against measured data. Please note that the purpose of these comparisons with measured data would be to gauge if the model developed as part of this work could replicate commercially used direct and indirect technology in general and not to achieve perfect fits with measured data.\n\n\nRESULTS\nWe found that the simulated image quality metrics such as MTF, NNPS and DQE were in reasonable agreement with experimental data. To demonstrate the imaging performance of the three DM/DBT systems, we integrated the detector models with the VICTRE pipeline and simulated DM images of a fatty breast model containing a spiculated mass and a calcium oxalate cluster. In general, we found that the images generated using the indirect model appeared more blurred with a different noise texture and contrast as compared to the systems with direct detectors.\n\n\nCONCLUSIONS\nWe have presented computational models of three commercially available FDA-approved DM/DBT systems, which implement both direct and indirect detector technology. The updated versions of the MC-GPU codes that can be used to replicate three systems are available in open source format through GitHub. This article is protected by copyright. All rights reserved.|Medical Physics (Lancaster)|2022|10.1002/mp.15935|A. Badano, Aunnasha Sengupta, A. Makeev, A. Badal|0.6666666666666666|2
933|Compact Update Algorithm for Numerical Schemes with Cross Stencil for Data Access Locality|Accurate fluid simulations require high computing cost. 3D modelling of fluid dynamic field evolution on a discrete mesh takes large amount of data storage, and data access becomes performance bottleneck. Our work is concerned with the task of mitigating the limitations that are caused by finite memory throughput in the parallel simulations. We use LRnLA algorithms for this issue, where localized tasks combine updates on several time layers. In this paper, the compact update for DiamondTorre LRnLA algorithm is constructed. It further improves localization of DiamondTorre algorithm, which improves arithmetic intensity for cross-stencil schemes. The ratio of loaded data to fully updated data approaches 1. The compact update is implemented with CUDA C++ for a numerical scheme for the advection-diffusion equation. 50 GLU/sec (billion lattice updates per second) performance is obtained on Nvidia RTX3090, and the maximal performance of almost 300 GLU/sec is obtained on an 8 GPU workstation. Note that the main data storage is in CPU RAM memory, but the host-device data exchange is concealed by temporal blocking: with appropriate the data transfers are concealed by the computing operations and do not affect the performance.|High Performance Computing and Cluster Technologies Conference|2022|10.1145/3560442.3560450|A. Perepelkina, B. Korneev, A. Zakirov|0.6666666666666666|2
936|Enhanced adaptive optics control with image to image translation|We aim to signiﬁcantly enhance the science return of astronomical observatories, and in particular giant terrestrial optical telescopes. Observatories employ Adaptive Optics (AO) systems in order to acquire high sensitivity diffraction limited images of the sky. The incumbent “workhorse” for control of AO systems employs a linear real-time controller in a closed loop, with sensing of state performed via a ( Shack-Hartmann ) wavefront sensor (WFS). The actuators of a deformable mirror (DM) are driven, with the action performed in each iteration having a continuous representation as an array of DC voltages. The typical control regime is practical and scalable, nonetheless, there remains a residual uncompensated turbulence that leads to optical aberrations limiting the class of scientiﬁc assets that can be acquired. We have developed and trained a translational GAN model that accurately estimates residual perturbations from WFS images. Model inference occurs in 0.34 milliseconds using off-the-shelf GPU hardware, and is applicable for use in AO control where the control loop might be running at 500Hz. We develop an AO control regime with a second controller stage actuating a second DM controlled in an open loop according to the estimated residual turbulence. Using the open-source COMPASS tool for simulation, we are able to signiﬁcantly improve the performance using our new regime.|Conference on Uncertainty in Artificial Intelligence|2022|10.1117/12.2629638|Jeffrey Smith, Jesse Cranney, Charles Gretton, D. Gratadour|0.6666666666666666|2
949|FPGA Acceleration of 3GPP Channel Model Emulator for 5G New Radio|The channel model is by far the most computing intensive part of the link level simulations of multiple-input and multiple-output (MIMO) fifth-generation new radio (5GNR) communication systems. Simulation effort further increases when using more realistic geometry-based channel models, such as the three-dimensional spatial channel model (3DSCM). Channel emulation is used for functional and performance verification of such models in the network planning phase. These models use multiple finite impulse response (FIR) filters and have a very high degree of parallelism which can be exploited for accelerated execution on Field Programmable Gate Array (FPGA) and Graphics Processing Unit (GPU) platforms. This paper proposes an efficient re-configurable implementation of the 3rd generation partnership project (3GPP) 3DSCM on FPGAs using a design flow based on high-level synthesis (HLS). It studies the effect of various HLS optimization techniques on the total latency and hardware resource utilization on Xilinx Alveo U280 and Intel Arria 10GX 1150 high-performance FPGAs, using in both cases the commercial HLS tools of the producer. The channel model accuracy is preserved using double precision floating point arithmetic. This work analyzes in detail the effort to target the FPGA platforms using HLS tools, both in terms of common parallelization effort (shared by both FPGAs), and in terms of platform-specific effort, different for Xilinx and Intel FPGAs. Compared to the baseline general-purpose central processing unit (CPU) implementation, the achieved speedups are 65X and 95X using the Xilinx UltraScale+ and Intel Arria FPGA platform respectively, when using a Double Data Rate (DDR) memory interface. The FPGA-based designs also achieved ~3X better performance compared to a similar technology node NVIDIA GeForce GTX 1070 GPU, while consuming ~4X less energy. The FPGA implementation speedup improves up to 173X over the CPU baseline when using the Xilinx UltraRAM (URAM) and High-Bandwidth Memory (HBM) resources, also achieving 6X lower latency and 12X lower energy consumption than the GPU implementation.|IEEE Access|2022|10.1109/ACCESS.2022.3221124|Nasir Ali Shah, S. Scarpina, M. Lazarescu, R. Quasso, L. Lavagno|0.6666666666666666|2
951|Bi-Level Optimization Augmented with Conditional Variational Autoencoder for Autonomous Driving in Dense Traffic|Autonomous driving has a natural bi-level structure. The goal of the upper behavioural layer is to provide appropriate lane change, speeding up, and braking decisions to optimize a given driving task. However, this layer can only indirectly influence the driving efficiency through the lower-level trajectory planner, which takes in the behavioural inputs to produce motion commands. Existing sampling-based approaches do not fully exploit the strong coupling between the behavioural and planning layer. On the other hand, end-to-end Reinforcement Learning (RL) can learn a behavioural layer while incorporating feedback from the lower-level planner. However, purely data-driven approaches often fail in safety metrics in unseen environments. This paper presents a novel alternative; a parameterized bi-level optimization that jointly computes the optimal behavioural decisions and the resulting downstream trajectory. Our approach runs in real-time using a custom GPU-accelerated batch optimizer and a Conditional Variational Autoencoder (CVAE) learnt warm-start strategy. Extensive simulations show that our approach outperforms state-of-the-art Model Predictive Control (MPC) and RL approaches in terms of collision rate while being competitive in driving efficiency.|2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)|2022|10.1109/CASE56687.2023.10260485|Jatan Shrestha, Nicola Albarella, Ashutosh Kumar Singh|0.6666666666666666|2
952|Onboard Real-Time Multi-Sensor Pose Estimation for Indoor Quadrotor Navigation with Intermittent Communication|We propose a multisensor fusion framework for onboard real-time navigation of a quadrotor in an indoor environment, by integrating sensor readings from an Inertial Measurement Unit (IMU), a camera-based object detection algorithm, and an Ultra-WideBand (UWB) localization system. The sensor readings from the camera-based object detection algorithm and the UWB localization system arrive intermittently, since the measurements are not readily available. We design a Kalman filter that manages intermittent observations in order to handle and fuse the readings and estimate the pose of the quadrotor for tracking a predefined trajectory. The system is implemented via a Hardware-in-the-loop (HIL) simulation technique, in which the dynamic model of the quadrotor is simulated in an open-source 3D robotics simulator tool, and the whole navigation system is implemented on Artificial Intelligence (AI) enabled edge GPU. The simulation results show that our proposed framework offers low positioning and trajectory errors, while handling intermittent sensor measurements.|2022 IEEE Globecom Workshops (GC Wkshps)|2022|10.1109/GCWkshps56602.2022.10008590|Evagoras Makridis, K. M. Deliparaschos, Themistoklis Charalambous, Loizos Hadjiloizou|0.6666666666666666|2
954|Accurate and Fast Deep Learning Dose Prediction for a Preclinical Microbeam Radiation Therapy Study Using Low-Statistics Monte Carlo Simulations|Simple Summary This work describes the development of a fast and accurate machine learning (ML) 3D U-Net dose engine, trained with Monte Carlo (MC) radiation transport simulations, to calculate the dose in rat patients treated in Microbeam Radiation Therapy (MRT) preclinical studies at the Imaging and Medical Beamline at the Australian Synchrotron. Digital phantoms are created based on CT scans of sixteen rats and are augmented to obtain enough anatomical data. Augmented variations of the digital phantoms are then used to simulate with Geant4 the energy depositions of an MRT beam inside the phantoms with 15% (high-noise) and 2% (low-noise) statistical uncertainty. The high-noise MC simulations are used for ML model training and validation, while the low-noise ones for testing. The results show that the ML dose engine provides a satisfactory dose description in the tumor target and generates the dose maps in less than one second. Abstract Microbeam radiation therapy (MRT) utilizes coplanar synchrotron radiation beamlets and is a proposed treatment approach for several tumor diagnoses that currently have poor clinical treatment outcomes, such as gliosarcomas. Monte Carlo (MC) simulations are one of the most used methods at the Imaging and Medical Beamline, Australian Synchrotron to calculate the dose in MRT preclinical studies. The steep dose gradients associated with the 50μm-wide coplanar beamlets present a significant challenge for precise MC simulation of the dose deposition of an MRT irradiation treatment field in a short time frame. The long computation times inhibit the ability to perform dose optimization in treatment planning or apply online image-adaptive radiotherapy techniques to MRT. Much research has been conducted on fast dose estimation methods for clinically available treatments. However, such methods, including GPU Monte Carlo implementations and machine learning (ML) models, are unavailable for novel and emerging cancer radiotherapy options such as MRT. In this work, the successful application of a fast and accurate ML dose prediction model for a preclinical MRT rodent study is presented for the first time. The ML model predicts the peak doses in the path of the microbeams and the valley doses between them, delivered to the tumor target in rat patients. A CT imaging dataset is used to generate digital phantoms for each patient. Augmented variations of the digital phantoms are used to simulate with Geant4 the energy depositions of an MRT beam inside the phantoms with 15% (high-noise) and 2% (low-noise) statistical uncertainty. The high-noise MC simulation data are used to train the ML model to predict the energy depositions in the digital phantoms. The low-noise MC simulations data are used to test the predictive power of the ML model. The predictions of the ML model show an agreement within 3% with low-noise MC simulations for at least 77.6% of all predicted voxels (at least 95.9% of voxels containing tumor) in the case of the valley dose prediction and for at least 93.9% of all predicted voxels (100.0% of voxels containing tumor) in the case of the peak dose prediction. The successful use of high-noise MC simulations for the training, which are much faster to produce, accelerates the production of the training data of the ML model and encourages transfer of the ML model to different treatment modalities for other future applications in novel radiation cancer therapies.|Cancers|2022|10.3390/cancers15072137|S. Guatelli, M. Hagenbuchner, Matthew Cameron, A. C. Tsoi, S. Corde, A. Rosenfeld, J. Weingarten, Michael Lerch, M. Barnes, E. Engels, O. Nackenhorst, K. Kröninger, Sarah Vogel, J. Paino, M. Tehei, F. Mentzel|0.6666666666666666|2
968|GPU-Accelerated Simulation Ensembles of Stochastic Reaction Networks|Stochastic Simulation Algorithms are widely used for simulating reaction networks in cellular biology. Due to the stochastic nature of models and the large parameter spaces involved, many simulation runs are frequently needed. We approach the computational challenge by expanding the hardware used for execution by massively parallel graphical processing units (GPUs) to execute these ensembles of runs concurrently in a form of coarse-grained parallelization. Such computing infrastructure in the form of GPUs is readily available in desktop workstations and clusters but is not commonly exploited as part of stochastic simulation studies. Building on the existing literature in the field, we employ state-of-the-art algorithms to study the degree to which GPUs can augment the computation resources available for ensemble studies. Furthermore, the challenge of efficient work assignment given the GPU's synchronous mode of execution is explored. There are several algorithmic tradeoffs to consider for models with different execution characteristics, which we investigate in a performance study across four different models. To explore the limitations of the GPU-based simulators, the performance characteristics when executing large models are compared to those of highly optimized CPU simulators. Our results indicate that for some models adding a typical desktop GPU has a similar effect on performance as up to 40 added CPU cores.|Online World Conference on Soft Computing in Industrial Applications|2022|10.1109/WSC57314.2022.10015448|L. Herrmann, Philipp Andelfinger, A. Uhrmacher, Till Köster|0.6666666666666666|2
969|Task Mapping for Hardware-Accelerated Robotics Applications using ReconROS|Modern software architectures for robotics map tasks to heterogeneous computing platforms comprising multi-core CPUs, GPUs, and FPGAs. FPGAs promise huge potential for energy efficient and fast computation, but their use in robotics requires profound knowledge of hardware design and is thus challenging. ReconROS, a combination of the reconfigurable operating system ReconOS and the robot operating system (ROS) aims to overcome this challenge with a consistent programming model across the hardware/software boundary and support of event-driven programming. In this paper, we summarize different approaches for mapping tasks to computational resources in ReconROS. These approaches include static and dynamic mappings, and the exploitation of data parallelism for single ROS nodes. Further, for dynamic mapping we propose and analyse different replacement strategies for hardware nodes to minimize reconfiguration overhead. We evaluate the presented techniques and illustrate ReconROS’ capabilites through an autonomous vehicle example in a hardware-in-the-loop simulation.|International Conference on Robotic Computing|2022|10.1109/IRC55401.2022.00033|Christian Lienen, M. Platzner|0.6666666666666666|2
1001|An Energy-Efficient Task Scheduling Method for CPU-GPU Heterogeneous Cloud|Given the problems of high energy consumption and unreasonable resource utilization in current heterogeneous cloud computing platforms, this paper proposes an energy-efficient task scheduling strategy. First, by analyzing the relationship between the energy consumption of physical machines and the utilization rate of CPU-GPU resources, we can establish the task scheduling model and energy consumption model. Second, a task scheduling method HCGTS (Heterogeneous CPU-GPU Task Scheduling) for energy consumption optimization of CPU-GPU heterogeneous cloud platform is proposed. Different from traditional subtasks, we divide the multi-task data flows into CPU tasks, GPU tasks and pending tasks, which are scheduled and executed by different processors to implement multi-task parallel scheduling. In addition, the execution order of subtasks is dynamically adjusted to realize the load balance of CPU and GPU, thereby reducing the task completion time and overall energy consumption. Finally, the simulation experiment is completed on the cloud simulation software. Through comparing with the other three algorithms, the HCGTS algorithm has better performance in terms of execution time, resource utilization and energy consumption optimization.|2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)|2022|10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00199|H Zhao, Shangshu Li, Guobin Zhang, Jianhua Li, Jing Wang|0.6666666666666666|2
1448|A multi-GPU accelerated virtual-reality interaction simulation framework|In this paper, we put forward a real-time multiple GPUs (multi-GPU) accelerated virtual-reality interaction simulation framework where the reconstructed objects from camera images interact with virtual deformable objects. Firstly, based on an extended voxel-based visual hull (VbVH) algorithm, we design an image-based 3D reconstruction platform for real objects. Then, an improved hybrid deformation model, which couples the geometry constrained fast lattice shape matching method (FLSM) and total Lagrangian explicit dynamics (TLED) algorithm, is proposed to achieve efficient and stable simulation of the virtual objects’ elastic deformations. Finally, one-way virtual-reality interactions including soft tissues’ virtual cutting with bleeding effects are successfully simulated. Moreover, with the purpose of significantly improving the computational efficiency of each time step, we propose an entire multi-GPU implementation method of the framework using compute unified device architecture (CUDA). The experiment results demonstrate that our multi-GPU accelerated virtual-reality interaction framework achieves real-time performance under the moderate calculation scale, which is a new effective 3D interaction technique for virtual reality applications.|PLoS ONE|2019|10.1371/journal.pone.0214852|Fengquan Zhang, Weifeng Xu, Lina Lin, X. Shao|0.6666666666666666|2
1453|Infrared image simulation of ground maneuver target and scene|Infrared scene simulation has extensive applications in military and civil fields. Based on a certain experimental environment,object-oriented graphics rendering engine( OGRE) is utilized to simulate a real three-dimensional infrared complex scene. First,the target radiation of each part is calculated based on our experimental data. Then through the analysis of the radiation characteristics of targets and related material,an infrared texture library is established and the 3ds Max software is applied to establish an infrared radiation model.Finally,a real complex infrared scene is created by using the OGRE engine image rendering technology and graphic processing unit( GPU) programmable pipeline technology. The results show that the simulation images are very similar to real images and are good supplements to real data.||2016|10.3788/irla201645.0104004|穆成坡, 彭明松, 董清先, 高翔, 张睿恒|0.6666666666666666|2
1522|Brian 2: an intuitive and efficient neural simulator|To be maximally useful for neuroscience research, neural simulators must make it possible to define original models. This is especially important because a computational experiment might not only need descriptions of neurons and synapses, but also models of interactions with the environment (e.g. muscles), or the environment itself. To preserve high performance when defining new models, current simulators offer two options: low-level programming, or mark-up languages (and other domain specific languages). The first option requires time and expertise, is prone to errors, and contributes to problems with reproducibility and replicability. The second option has limited scope, since it can only describe the range of neural models covered by the ontology. Other aspects of a computational experiment, such as the stimulation protocol, cannot be expressed within this framework. “Brian” 2 is a complete rewrite of Brian that addresses this issue by using runtime code generation with a procedural equation-oriented approach. Brian 2 enables scientists to write code that is particularly simple and concise, closely matching the way they conceptualise their models, while the technique of runtime code generation automatically transforms high level descriptions of models into efficient low level code tailored to different hardware (e.g. CPU or GPU). We illustrate it with several challenging examples: a plastic model of the pyloric network of crustaceans, a closed-loop sensorimotor model, programmatic exploration of a neuron model, and an auditory model with real-time input from a microphone.|bioRxiv|2019|10.1101/595710|Dan F. M. Goodman, R. Brette, M. Stimberg|0.6666666666666666|2
1523|Parallel computing solutions for Markov chain spatial sequential simulation of categorical fields|ABSTRACT The Markov chain random field (MCRF) model is a spatial statistical approach for modeling categorical spatial variables in multiple dimensions. However, this approach tends to be computationally costly when dealing with large data sets because of its sequential simulation processes. Therefore, improving its computational efficiency is necessary in order to run this model on larger sizes of spatial data. In this study, we suggested four parallel computing solutions by using both central processing unit (CPU) and graphics processing unit (GPU) for executing the sequential simulation algorithm of the MCRF model, and compared them with the nonparallel computing solution on computation time spent for a land cover post-classification. The four parallel computing solutions are: (1) multicore processor parallel computing (MP), (2) parallel computing by GPU-accelerated nearest neighbor searching (GNNS), (3) MP with GPU-accelerated nearest neighbor searching (MP-GNNS), and (4) parallel computing by GPU-accelerated approximation and GPU-accelerated nearest neighbor searching (GA-GNNS). Experimental results indicated that all of the four parallel computing solutions are at least 1.8× faster than the nonparallel solution. Particularly, the GA-GNNS solution with 512 threads per block is around 83× faster than the nonparallel solution when conducting a land cover post-classification with a remotely sensed image of 1000 × 1000 pixels.|International Journal of Digital Earth|2019|10.1080/17538947.2018.1464073|Weixing Zhang, Tian Zhao, Weidong Li, Chuanrong Zhang|0.6666666666666666|2
1590|Acceleration Method for Software Signal Simulators of BDS Navigation Signals and RDSS Signals Based on GPGPU|General purpose graphics processing unit (GPGPU) has a great advantage in parallel computation, which is appropriate in the development of signal simulators for global navigation satellite system (GNSS) signals. Real-time software signal simulators for BeiDou navigation satellite system (BDS) signals, including navigation signals and radio determination satellite service (RDSS) signals, are developed in this paper. The characteristics of the continuous signal simulation and the burst signal simulation are considered in the development of GPU algorithms. The GPU algorithm optimization considers memory usage, signal combination method, and GPU block design under the goal of the least time consumption. To get the best GPU block design, a generalized block design model is built in this paper. The optimized GPU block parameters are got for the BDS B1 signal simulation and the RDSS signal simulation, separately. For the BDS B1 simulation, when the parameters are not optimized, the time consumption is more than three times of the optimized result. For the RDSS signal simulation, this value is 4.5 times. The correctness of the signals is verified in many aspects, including the power spectral density (PSD) and the pseudo range precision.|IEEE Access|2019|10.1109/ACCESS.2019.2926323|Lei Wang, Feixue Wang, Jingyuan Li, Xiaomei Tang, Baiyu Li|0.6666666666666666|2
380|High Performant Simulations of Cerebellar Golgi Cells Activity|The use of High Performance Computing (HPC) technologies is gaining interest in the field of neuronal activity simulations. In fact, scientists' main goal is to understand and reproduce cells behavior in a realistic way. This will allow undertaking in silico experiments, instead of in vivo ones, to test new medicines, to study cerebral pathologies and to discover innovative therapies. To this aim, two main requirements are necessary: neurons have to be described by realistic models and their simulation hopefully have to satisfy the real-time constraint. This last property is very hard to accommodate because models used in these works are very heavy from the computational point of view. For this reason, authors decide to exploit Graphic Processing Unit (GPU) technology to simulate the cellular activity of Golgi cells, which constitute the cerebellar cortex. This paper describes an efficient Golgi cell activity simulation performed using NVIDIA GPUs. Results show that simulation times are reduced from 41 hours to about 2 hours when simulating 400'000 different cells.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2017|10.1109/PDP.2017.91|E. Torti, G. Danese, F. Leporati, Giordana Florimbi|0.625|2
1275|Characterization of photomultiplier tubes with a realistic model through GPU-boosted simulation|The accurate characterization of a photomultiplier tube (PMT) is crucial in a wide-variety of applications. However, current methods do not give fully accurate representations of the response of a PMT, especially at very low light levels. In this work, we present a new and more realistic model of the response of a PMT, called the cascade model, and use it to characterize two different PMTs at various voltages and light levels. The cascade model is shown to outperform the more common Gaussian model in almost all circumstances and to agree well with a newly introduced model independent approach. The technical and computational challenges of this model are also presented along with the employed solution of developing a robust GPU-based analysis framework for this and other non-analytical models.||2017|10.1088/1748-0221/13/02/T02011|R. Saldanha, L. Grandi, M. Anthony, Q. Lin, E. Aprile|0.625|2
181|Detection of Epistatic and Gene-Environment Interactions Underlying Three Quality Traits in Rice Using High-Throughput Genome-Wide Data|With development of sequencing technology, dense single nucleotide polymorphisms (SNPs) have been available, enabling uncovering genetic architecture of complex traits by genome-wide association study (GWAS). However, the current GWAS strategy usually ignores epistatic and gene-environment interactions due to absence of appropriate methodology and heavy computational burden. This study proposed a new GWAS strategy by combining the graphics processing unit- (GPU-) based generalized multifactor dimensionality reduction (GMDR) algorithm with mixed linear model approach. The reliability and efficiency of the analytical methods were verified through Monte Carlo simulations, suggesting that a population size of nearly 150 recombinant inbred lines (RILs) had a reasonable resolution for the scenarios considered. Further, a GWAS was conducted with the above two-step strategy to investigate the additive, epistatic, and gene-environment associations between 701,867 SNPs and three important quality traits, gelatinization temperature, amylose content, and gel consistency, in a RIL population with 138 individuals derived from super-hybrid rice Xieyou9308 in two environments. Four significant SNPs were identified with additive, epistatic, and gene-environment interaction effects. Our study showed that the mixed linear model approach combining with the GPU-based GMDR algorithm is a feasible strategy for implementing GWAS to uncover genetic architecture of crop complex traits.|BioMed Research International|2015|10.1155/2015/135782|Ying-xin Zhang, Xihong Shen, Shihua Cheng, X. Lou, Haiming Xu, Yujie Cao, Liyong Cao, Beibei Jiang, X. Zhan|0.6|2
221|Massive Parallelization of the WRF GCE Model Toward a GPU-Based End-to-End Satellite Data Simulator Unit|Modern weather satellites provide more detailed observations of cloud and precipitation processes. To harness these observations for better satellite data assimilations, a cloud-resolving model, known as the Goddard Cumulus Ensemble (GCE) model, was developed and used by the Goddard Satellite Data Simulator Unit (G-SDSU). The GCE model has also been incorporated as part of the widely used weather research and forecasting (WRF) model. The computation of the cloud-resolving GCE model is time-consuming. This paper details our massively parallel design of GPU-based WRF GCE scheme. With one NVIDIA Tesla K40 GPU, the GPU-based GCE scheme achieves a speedup of 361× as compared to its original Fortran counterpart running on one CPU core, whereas the speedup for one CPU socket (four cores) with respect to one CPU core is only 3.9×.|IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing|2015|10.1109/JSTARS.2015.2422302|A. Mehta, Hung-Lung Huang, Melin Huang, Bormin Huang, X. Li, M. Goldberg|0.6|2
266|Plasmon-based Virus Detection on Heterogeneous Embedded Systems|Embedded systems, e.g. in computer vision applications, are expected to provide significant amounts of computing power to process large data volumes. Many of these systems, such as used in medical diagnosis, are mobile devices and face significant challenges to provide sufficient performance while operating on a constrained energy budget. Modern embedded MPSoC platforms use heterogeneous CPU and GPU cores providing a large number of optimization parameters. This allows to find useful trade-offs between energy consumption and performance for a given application. In this paper, we describe how the complex data processing required for PAMONO, a novel type of biosensor for the detection of biological viruses, can efficiently be implemented on a state-of-the-art heterogeneous MPSoC platform. An additional optimization dimension explored is the achieved quality of service. Reducing the virus detection accuracy enables additional optimizations not achievable by modifying hardware or software parameters alone. Instead of relying on often inaccurate simulation models, our design space exploration employs a hardware-in-the-loop approach to evaluate the performance and energy consumption on the embedded target platform. Trade-offs between performance, energy and accuracy are controlled by a genetic algorithm running on a PC control system which deploys the evaluation tasks to a number of connected embedded boards. Using our optimization approach, we are able to achieve frame rates meeting the requirements without losing accuracy. Further, our approach is able to reduce the energy consumption by 93% with a still reasonable detection quality.|Software and Compilers for Embedded Systems|2015|10.1145/2764967.2764976|M. Engel, Pascal Libuschewski, H. Müller, P. Marwedel, Olaf Neugebauer|0.6|2
281|Accelerating complex brain-model simulations on GPU platforms|The Inferior Olive (IO) in the brain, in conjunction with the cerebellum, is responsible for crucial sensorimotor-integration functions in humans. In this paper, we simulate a computationally challenging IO neuron model consisting of three compartments per neuron in a network arrangement on GPU platforms. Several GPU platforms of the two latest NVIDIA GPU architectures (Fermi, Kepler) have been used to simulate large-scale IO-neuron networks. These networks have been ported on 4 diverse GPU platforms and implementation has been optimized, scoring 3x speedups compared to its unoptimized version. The effect of GPU L1-cache and thread block size as well as the impact of numerical precision of the application on performance have been evaluated and best configurations have been chosen. In effect, a maximum speedup of 160x has been achieved with respect to a reference CPU platform.|Design, Automation and Test in Europe|2015|10.7873/DATE.2015.0071|Z. Al-Ars, Hoang Anh Du Nguyen, C. Strydis, Georgios Smaragdos|0.6|2
299|A directive based hybrid met office NERC cloud model|Large Eddy Simulation is a critical modelling tool for the investigation of atmospheric flows, turbulence and cloud microphysics. The models used by the UK atmospheric research community are homogeneous and the latest model, MONC, is designed to run on substantial HPC systems with very high CPU core counts. In order to future proof these codes it is worth investigating other technologies and architectures which might support the communities running their codes at the exa-scale.\n In this paper we present a hybrid version of MONC, where the most computationally intensive aspect is offloaded to the GPU while the rest of the functionality runs concurrently on the CPU. Developed using the directive driven OpenACC, we consider the suitability and maturity of this technology to modern Fortran scientific codes as well general software engineering techniques which aid this type of porting work. The performance of our hybrid model at scale is compared against the CPU version before considering other tuning options and making a comparison between the energy usage of the homo- and hetero-geneous versions. The result of this work is a promising hybrid model that shows performance benefits of our approach when the GPU has a significant computational workload which can not only be applied to the MONC model but also other weather and climate simulations in use by the community.|Workshop on Accelerator Programming using Directives|2015|10.1145/2832105.2832115|M. Weiland, N. Brown, B. Shipway, C. Maynard, Angus Lepper, A. Hill|0.6|2
317|GPU-based parallel computing of energy consumption in wireless sensor networks|The lifetime of a wireless sensor network is the most important design parameter to take into account. Given the autonomous nature of the sensor nodes, this period is mainly related to their energy consumption. Hence, the high interest to evaluate through accurate and rapid simulations the energy consumption for this kind of networks. However, in the case of a network with several thousand nodes, the simulation time can be very slow and even impossible in some cases. In this paper, we present a new model for a parallel computing of energy consumption in wireless sensor networks. This model is combined with a discrete event simulation in a multi-agent environment and implemented on GPU architecture. The results show that the proposed model provides simulation times significantly faster than those obtained by the sequential model for large networks and for long simulations. This improvement is more significant if the processing on each node is very time consuming. Finally, the proposed model has been fully integrated and validated into the CupCarbon simulator.|European Conference on Networks and Communications|2015|10.1109/EuCNC.2015.7194086|B. Pottier, A. Bounceur, Arezki Laga, M. Lounis|0.6|2
385|Order statistics inference for describing topological coupling and mechanical symmetry breaking in multidomain proteins.|"Cooperativity is a hallmark of proteins, many of which show a modular architecture comprising discrete structural domains. Detecting and describing dynamic couplings between structural regions is difficult in view of the many-body nature of protein-protein interactions. By utilizing the GPU-based computational acceleration, we carried out simulations of the protein forced unfolding for the dimer WW - WW of the all-β-sheet WW domains used as a model multidomain protein. We found that while the physically non-interacting identical protein domains (WW) show nearly symmetric mechanical properties at low tension, reflected, e.g., in the similarity of their distributions of unfolding times, these properties become distinctly different when tension is increased. Moreover, the uncorrelated unfolding transitions at a low pulling force become increasingly more correlated (dependent) at higher forces. Hence, the applied force not only breaks ""the mechanical symmetry"" but also couples the physically non-interacting protein domains forming a multi-domain protein. We call this effect ""the topological coupling."" We developed a new theory, inspired by order statistics, to characterize protein-protein interactions in multi-domain proteins. The method utilizes the squared-Gaussian model, but it can also be used in conjunction with other parametric models for the distribution of unfolding times. The formalism can be taken to the single-molecule experimental lab to probe mechanical cooperativity and domain communication in multi-domain proteins."|Journal of Chemical Physics|2015|10.1063/1.4816104|V. Barsegov, Lee K Jones, O. Kononova|0.6|2
532|GPGPU performance estimation for frequency scaling using cross-benchmarking|Dynamic Voltage and Frequency Scaling (D VFS) on General-Purpose Graphics Processing Units (GPGPUs) is now becoming one of the most significant techniques to balance computational performance and energy consumption. However, there are still few fast and accurate models for predicting GPU kernel execution time under different core and memory frequency settings, which is important to determine the best frequency configuration for energy saving. Accordingly, a novel GPGPU performance estimation model with both core and memory frequency scaling is herein proposed. We design a cross-benchmarking suite, which simulates kernels with a wide range of instruction distributions. The synthetic kernels generated by this suite can be used for model pre-training or as supplementary training samples. Then we apply two different machine learning algorithms, Support Vector Regression (SVR) and Gradient Boosting Decision Tree (GBDT), to study the correlation between kernel performance counters and kernel performance. The models trained only with our cross-benchmarking suite achieve satisfying accuracy (16%~22% mean absolute error) on 24 unseen real application kernels. Validated on three modern GPUs with a wide frequency scaling range, by using a collection of 24 real application kernels, the proposed model is able to achieve accurate results (5.1%, 2.8%, 6.5% mean absolute error) for the target GPUs (GTX 980, Titan X Pascal and Tesla P100).|GPGPU@PPoPP|2020|10.1145/3366428.3380767|X. Chu, Qiang Wang, Chengjian Liu|0.6|2
537|Special issue on advanced simulation in engineering|Computer-based simulation has been playing an indispensable role in the engineering fields, such as materials, thermal fluids, energy systems, mechanics, and aerodynamics. Simulation describes, analyzes, and predicts complex systems in engineering which are too expensive, timeconsuming, dangerous, or even impossible to derive through experiments. Advances in simulation are essential to keep up with the continuously growing need from engineering fields. Supported by the continuously grown highperformance computing resources, simulation nowadays leverages high-order accuracy numerical methods, accurate models, and mesh technologies. The advances enable simulation to provide insights for engineering systems in a more high-fidelity and effective way ever than before. The special issue reports some studies on the advanced simulation in engineering. Hybrid density-functional calculation is one of the most commonly adopted electronic structure theory used in computational chemistry and materials science because of its balance between accuracy and computational cost. In “The Static Parallel Distribution Algorithms for Hybrid DensityFunctional Calculations in HONPAS Package,” Xinming Qin et al. developed a novel scheme called NAO2GTO to achieve linear scaling (Order-N) calculations for hybrid density-functionals. In their scheme, the most timeconsuming step is the calculation of the electron repulsion integrals (ERIs) part. So how to create an even distribution of these ERIs in parallel implementation is an issue of particular importance. Two static scalable distributed algorithms for the ERIs computation are presented. Firstly, the ERIs are distributed over ERIs shell pairs. Secondly, the ERIs is distributed over ERIs shell quartets. In both algorithms, the calculation of ERIs is independent of each other, so the communication time is minimized. The paper shows speedup results to demonstrate the performance of these static parallel distributed algorithms in the Hefei Order-N packages for ab initio simulations (HONPAS). The relevant parallel algorithms of mesh techniques are important research field in parallel computational fluid dynamics numerical simulation. Chimera grid method has been widely used in many aspects of engineering simulation. In “A Parallel Algorithm for Chimera Grid with Implicit Hole Cutting Method,” Xiaodong Hu et al. introduce the concept of a Cartesian Auxiliary Grid to parallelize and enhance the efficiency of the implicit hole cutting method in Chimera computational fluid dynamics (CFD) calculations. And the factors which restrict the performance and efficiency are analyzed. Some test cases are presented to demonstrate the effectiveness of this algorithm, and the calculation and data communication are reduced on the premise of maintaining accuracy. The work of this paper presents a new vision to handle parallel simulation of complex configurations. In “Accelerating and Tuning Small Matrix Multiplications on Sunway TaihuLight: A Case Study of Spectral Element CFD Code Nek5000,” Xianmeng Wang et al. present optimization of a spectral element method (SEM)-based CFD software Nek5000 on the Sunway TaihuLight supercomputer. The SEM has advantages of high-order of accuracy at low cost and excellent scalability for large-scale problems. Computational fluid dynamics software based on high-order SEM is regarded to have the potential to overcome the limitations of CFD solvers based on standard finite volume method. The SEM-based CFD solver Nek5000 has been ported to heterogeneous systems with accelerators (such as GPU) in former studies. The paper selected in this special issue presents acceleration method using computing processing elements of the ShenWei processors. The optimization work will benefit large-scale thermal-hydraulics simulation in the nuclear engineering field.|The international journal of high performance computing applications|2020|10.1177/1094342020905932|Jue Wang, Xinfu He|0.6|2
543|Blockwise Weighted Least Square Active Noise Control for CPU-GPU Architecture|Active noise control (ANC) is a technology which lowers the noise level by using the principle of destructive interference of sound wave. Even though recent developments in digital signal processing (DSP) made it possible to implement ANC algorithms in real-time, insufficient computational power is still one of the challenges to solve. In the previous research, as a way of overcoming the lack of computational power, CPU-GPU architecture was proposed so that ANC algorithms utilize the massive computing power of GPU without suffering from the block data transfer between CPU and GPU memories. However, for the feasibility test of the proposed CPU-GPU architecture in the previous research, a conventional block ANC algorithm was used, and ANC algorithm which can fully utilize the massive computing power of GPU has not been developed. In this article, ANC algorithm, which directly derives blockwise least square solution through GPU computation while generating control signals through CPU computation, is proposed. Based on the observation about speaker saturation and increase of noise level after applying the conventional blockwise least square solution, a new cost function for preventing such problems is also proposed. Therefore, blockwise weighted least square ANC (BWLS-ANC) algorithm, which derives blockwise least square solution minimizing the proposed cost function through GPU computation while generating control signals through CPU computation, is proposed throughout this research. Problems of conventional blockwise least square solution upon ANC applications are observed through simulations and experiments. The feasibility of the proposed BWLS-ANC algorithm is verified through experiments.|IEEE/ACM Transactions on Audio Speech and Language Processing|2020|10.1109/TASLP.2020.2971828|Youngjin Park, Yeongseok Kim|0.6|2
611|Flower: A Friendly Federated Learning Research Framework|Federated Learning (FL) has emerged as a promising technique for edge devices to collaboratively learn a shared prediction model, while keeping their training data on the device, thereby decoupling the ability to do machine learning from the need to store the data in the cloud. However, FL is difficult to implement realistically, both in terms of scale and systems heterogeneity. Although there are a number of research frameworks available to simulate FL algorithms, they do not support the study of scalable FL workloads on heterogeneous edge devices. In this paper, we present Flower -- a comprehensive FL framework that distinguishes itself from existing platforms by offering new facilities to execute large-scale FL experiments and consider richly heterogeneous FL device scenarios. Our experiments show Flower can perform FL experiments up to 15M in client size using only a pair of high-end GPUs. Researchers can then seamlessly migrate experiments to real devices to examine other parts of the design space. We believe Flower provides the community with a critical new tool for FL study and development.||2020|10.1109/wcsp49889.2020.9299730|Xinchi Qiu, Titouan Parcollet, Taner Topal, Akhil Mathur, N. Lane, Daniel J. Beutel|0.6|2
613|GPU Accelerated Drug Application on Signaling Pathways Containing Multiple Faults Using Boolean Networks|Cell growth is governed by the flow of information from growth factors to transcription factors. This flow involves protein-protein interactions known as a signaling pathway, which triggers the cell division. The biological network in the presence of malfunctions leads to a rapid cell division without any necessary input conditions. The effect of these malfunctions or faults can be observed if it is simulated explicitly in the Boolean derivative of the biological networks. The consequences thus produced can be nullified to a large extent, with the application of a reduced combination of drugs. This paper provides an insight into the behavior of the signaling pathway in the presence of multiple concurrent malfunctions. First, we simulate the behavior of malfunctions in the Boolean networks. Next, we apply the drug therapy to reduce the effects of malfunctions. In our approach, we introduce a parameter called probabilistic_score, which identifies the reduced drug combinations without prior knowledge of the malfunctions, and it is more beneficial in realistic cancerous conditions. The combinations of different custom drug inhibition points are chosen to produce more efficient results than known drugs. Our approach is significantly faster as GPU acceleration has been carried out during modeling the multiple faults/malfunctions in the Boolean networks.|IEEE/ACM Transactions on Computational Biology & Bioinformatics|2020|10.1109/tcbb.2020.3014172|Argha Nandan, Tapan Chowdhury, Susanta Chakraborty|0.6|2
618|Multipopulation Genetic Algorithm Based on GPU for Solving TSP Problem|A GPU-based Multigroup Genetic Algorithm was proposed, which parallelized the traditional genetic algorithm with a coarse-grained architecture island model. The original population is divided into several subpopulations to simulate different living environments, thus increasing species richness. For each subpopulation, different mutation rates were adopted, and the crossover results were optimized by combining the crossover method based on distance. The adaptive mutation strategy based on the number of generations was adopted to prevent the algorithm from falling into the local optimal solution. An elite strategy was adopted for outstanding individuals to retain their superior genes. The algorithm was implemented with CUDA/C, combined with the powerful parallel computing capabilities of GPUs, which greatly improved the computing efficiency. It provided a new solution to the TSP problem.||2020|10.1155/2020/1398595|X. Ye, Meng Zhang, T. Ergesh, Wanqiong Wang, Boqun Wang, Jia Li, Jun Nie, Hailong Zhang, Jie Wang|0.6|2
634|Large-scale binding affinity calculations on commodity compute clouds|In recent years, it has become possible to calculate binding affinities of compounds bound to proteins via rapid, accurate, precise and reproducible free energy calculations. This is imperative in drug discovery as well as personalized medicine. This approach is based on molecular dynamics (MD) simulations and draws on sequence and structural information of the protein and compound concerned. Free energies are determined by ensemble averages of many MD replicas, each of which requires hundreds of cores and/or GPU accelerators, which are now available on commodity cloud computing platforms; there are also requirements for initial model building and subsequent data analysis stages. To automate the process, we have developed a workflow known as the binding affinity calculator. In this paper, we focus on the software infrastructure and interfaces that we have developed to automate the overall workflow and execute it on commodity cloud platforms, in order to reliably predict their binding affinities on time scales relevant to the domains of application, and illustrate its application to two free energy methods.|Interface Focus|2020|10.1098/rsfs.2019.0133|D. Wright, P. Coveney, S. Zasada|0.6|2
641|Optimizing high-resolution Community Earth System Model on a heterogeneous many-core supercomputing platform|Abstract. With semiconductor technology gradually approaching its physical and thermal limits, recent supercomputers have adopted major\narchitectural changes to continue increasing the performance through more\npower-efficient heterogeneous many-core systems. Examples include Sunway\nTaihuLight that has four management processing elements (MPEs) and 256\ncomputing processing elements (CPEs) inside one processor and Summit that has\ntwo central processing units (CPUs) and six graphics processing units (GPUs)\ninside one node. Meanwhile, current high-resolution Earth system models that\ndesperately require more computing power generally consist of millions of\nlines of legacy code developed for traditional homogeneous multicore\nprocessors and cannot automatically benefit from the advancement of\nsupercomputer hardware. As a result, refactoring and optimizing the legacy\nmodels for new architectures become key challenges along the road of taking\nadvantage of greener and faster supercomputers, providing better support for\nthe global climate research community and contributing to the long-lasting\nsocietal task of addressing long-term climate change. This article reports\nthe efforts of a large group in the International Laboratory for\nHigh-Resolution Earth System Prediction (iHESP) that was established by the\ncooperation of Qingdao Pilot National Laboratory for Marine Science and\nTechnology (QNLM), Texas A&M University (TAMU), and the National Center for\nAtmospheric Research (NCAR), with the goal of enabling highly efficient\nsimulations of the high-resolution (25 km atmosphere and 10 km ocean)\nCommunity Earth System Model (CESM-HR) on Sunway TaihuLight. The refactoring\nand optimizing efforts have improved the simulation speed of CESM-HR from 1 SYPD (simulation years per day) to 3.4 SYPD (with output disabled) and\nsupported several hundred years of pre-industrial control simulations. With\nfurther strategies on deeper refactoring and optimizing for remaining\ncomputing hotspots, as well as redesigning architecture-oriented\nalgorithms, we expect an equivalent or even better efficiency to be gained on the\nnew platform than traditional homogeneous CPU platforms. The refactoring and\noptimizing processes detailed in this paper on the Sunway system should have\nimplications for similar efforts on other heterogeneous many-core systems\nsuch as GPU-based high-performance computing (HPC) systems.\n|Geoscientific Model Development|2020|10.5194/gmd-2020-18|Jim Edwards, Guo Qiang, L. Gan, Yunhui Zeng, S. Yeager, Zhiqiang Wei, Kai Xu, N. Rosenbloom, Jingshan Pan, W. Xue, Shupeng Shi, Sihai Wu, Man Yuan, Xiaohui Duan, G. Danabasoglu, P. Chang, Ping Xu, Guangwen Yang, Yuhu Chen, H. Fu, Yang Tu, Mingkui Li, W. Wan, Qiuying Zhang, Weiguo Liu, Hongsong Meng, Yuxuan Li, Jianlin Yong, A. Baker, Zedong Liu, Guanghui Zhu, Dongning Jia, Li Wang, Lixin Wu, Yangyang Yu, Haining Yu, Zhao Liu, Lanning Wang, J. Zhang, Y. Zhuang, Shaoqing Zhang, Yingyu Guo, Hong Wang, Shiming Xu|0.6|2
642|Large-Scale Field Development Optimization Using a Two-Stage Strategy|Summary The optimization of the locations of a large number of wells represents a challenging computational problem. This is because the number of optimization variables scales with the maximum number of wells considered, and some of these variables may be categorical if the determination of the number and types of wells is part of the optimization problem. In this work, we develop and test a two-stage strategy for large-scale field development optimization problems. In the first stage, wells are constrained to lie in repeated patterns, and the optimization variables define the pattern type and geometry (e.g., well spacing, orientation). This component of the optimization follows a previous procedure ( Onwunalu and Durlofsky, 2011 ), though several important modifications, including optimization of the drilling sequence, are introduced. The solution obtained in the first stage is used as an initial guess for the second stage. In this stage we apply comprehensive field development optimization, where the well location, type, drill/do not drill decision, completion interval (for 3D models), and drilling time variables are determined for each well. Pattern geometry is no longer enforced in this stage. Specialized treatments (consistent with actual drilling practice) are introduced for cases where multiple geomodels, used to capture geological uncertainty, are considered. The two-stage procedure is applied to 2D and 3D models corresponding to different geological scenarios. Both deterministic and geologically uncertain settings are considered. All optimizations are performed using a derivative-free particle swarm optimization – mesh adaptive direct search hybrid algorithm. Our most challenging example involves optimization over multiple realizations of the Olympus model, which we simulate using a GPU- based commercial flow simulator. In all cases, results using the two-stage procedure are compared to those from a standard single-stage approach. We achieve consistently better optimizer performance using the two-stage approach. For example, in one case, the optimum achieved after 17,500 flow simulations using the standard approach is found after only 4400 flow simulations using the two-stage approach. In another case, for the same computational effort, the NPV achieved using the two-stage approach exceeds that of the standard approach by 4.7%. These results suggest that this optimization strategy may indeed lead to improved results in practical problems.||2020|10.3997/2214-4609.202035030|L. Durlofsky, Y. Nasir, O. Volkov|0.6|2
657|Effective reconstruction of bioluminescence tomography based on GPU-accelerated inverse Monte Carlo method|Diffusion equations (DEs) or simplified spherical harmonic equations are commonly used forward models in bioluminescence tomography (BLT), which are usually numerically calculated by the finite element method to construct the system matrix for reconstruction. However, the numerical solver is not accurate enough. The Monte Carlo (MC) method is regarded as the golden standard for modeling light propagation in biological tissue. In this paper, we proposed a GPU-accelerated inverse MC method for BLT reconstruction. The main feature is that the system matrix for BLT reconstruction is calculated by the MC method instead of the model-based numerical approximation. We evaluated the performance of the proposed method with both phantom-based simulation and animal-based in vivo experiment. The results show that, compared with the DE-based method, the proposed GPU-accelerated inverse MC method is more accurate and effective in BLT reconstruction.||2020|10.1063/5.0027207|Shenghan Ren, Qi Zeng, Duofang Chen, Jimin Liang, Xueli Chen, Lin Wang|0.6|2
662|A Bending Model for Nodal Discretizations of Yarn‐Level Cloth|To deploy yarn‐level cloth simulations in production environments, it is paramount to design very efficient implementations, which mitigate the cost of the extremely high resolution. To this end, nodal discretizations aligned with the regularity of the fabric structure provide an optimal setting for efficient GPU implementations. However, nodal discretizations complicate the design of robust and controllable bending. In this paper, we address this challenge, and propose a model of bending that is both robust and controllable, and employs only nodal degrees of freedom. We extract information of yarn and fabric orientation implicitly from the nodal degrees of freedom, with no need to augment the model explicitly. But most importantly, and unlike previous formulations that use implicit orientations, the computation of bending forces bears no overhead with respect to other nodal forces such as stretch. This is possible by tracking optimal orientations efficiently. We demonstrate the impact of our bending model in examples with controllable anisotropy, as well as ironing, wrinkling, and plasticity.|Computer graphics forum (Print)|2020|10.1111/cgf.14112|Alejandro Rodríguez, M. Otaduy, G. Cirio, José M. Pizana|0.6|2
663|A Novel Plastic Phase‐Field Method for Ductile Fracture with GPU Optimization|In this paper, we articulate a novel plastic phase‐field (PPF) method that can tightly couple the phase‐field with plastic treatment to efficiently simulate ductile fracture with GPU optimization. At the theoretical level of physically‐based modeling and simulation, our PPF approach assumes the fracture sensitivity of the material increases with the plastic strain accumulation. As a result, we first develop a hardening‐related fracture toughness function towards phase‐field evolution. Second, we follow the associative flow rule and adopt a novel degraded von Mises yield criterion. In this way, we establish the tight coupling of the phase‐field and plastic treatment, with which our PPF method can present distinct elastoplasticity, necking, and fracture characteristics during ductile fracture simulation. At the numerical level towards GPU optimization, we further devise an advanced parallel framework, which takes the full advantages of hierarchical architecture. Our strategy dramatically enhances the computational efficiency of preprocessing and phase‐field evolution for our PPF with the material point method (MPM). Based on our extensive experiments on a variety of benchmarks, our novel method's performance gain can reach 1.56× speedup of the primary GPU MPM. Finally, our comprehensive simulation results have confirmed that this new PPF method can efficiently and realistically simulate complex ductile fracture phenomena in 3D interactive graphics and animation.|Computer graphics forum (Print)|2020|10.1111/cgf.14130|Hong Qin, Chen Li, Zipeng Zhao, Kemeng Huang, Changbo Wang|0.6|2
671|Vostok: 3D scanner simulation for industrial robot environments|Computer vision will drive the next wave of robot applications. Latest three-dimensional scanners provide increasingly realistic object reconstructions. We introduce an innovative simulator that allows interacting with those scanners within the operating environment, thus creating a powerful tool for developers, researchers and students. In particular, we present a novel approach for simulating structured-light and timeof-flight sensors. Qualitative results demonstrate the efficiency and reliability in industrial environments. By using the programmability of modern GPUs, it is now possible to make greater use of parallelized simulative approaches. Apart from the easy modification of sensor parameters, the main advantage in simulation is the opportunity of carrying out experiments under reproducible conditions, especially for dynamic scene setups. Moreover, thanks to a great computational power, it is possible to generate huge amounts of synthetic data which can be used as test datasets for training machine learning models.||2020|10.5565/rev/elcvia.1244|R. Carli, A. Rossi, Marco Barbiero|0.6|2
691|Predictive learn and apply: MAVIS application - learn|The Learn and Apply reconstruction scheme uses the knowledge of atmospheric turbulence to generate a tomographic reconstructor, and its performance is enhanced by the real-time identification of the atmosphere and the wind profile. In this paper we propose a turbulence profiling method that is driven by the atmospheric model. The vertical intensity distribution of turbulence, wind speed and wind direction can be simultaneously estimated from the Laser Guide Star measurements. We introduce the implementation of such a method on a GPU accelerated non-linear least-squares solver, which significantly increases the computation efficiency. Finally, we present simulation results to demonstrate the convergence quality from numerically generated telemetry, the end-to-end Adaptive Optics simulation results, and a time-to-solution analysis, all based on the MAVIS system design.|Astronomical Telescopes + Instrumentation|2020|10.1117/12.2561913|N. Doucet, F. Rigaut, Yuxi Hong, H. Ltaief, Jesse Cranney, Hao Zhang, D. Gratadour, D. Keyes|0.6|2
722|Multilayer-HySEA model validation for landslide generated tsunamis. Part I Rigid slides|Abstract. The present work is devoted to the benchmarking of the Multilayer-HySEA model using laboratory experiment data for landslide generated tsunamis. This first part of the work deals with rigid slides and the second part, in a companion paper, with granular slides. The US National Tsunami Hazard and Mitigation Program (NTHMP) has proposed the experimental data used and established for the NTHMP Landslide Benchmark Workshop, held in January 2017 at Galveston. The first three benchmark problems proposed in this workshop dealt with rigid slides, simulated as a moving bottom topography, that must be imposed as a prescribed boundary condition. These three benchmarks are used here to validate the Multilayer-HySEA model. This new model of the HySEA family consists of an efficient hybrid finite volume/finite difference implementation on GPU architectures of a non-hydrostatic multilayer model. A brief description of model equations, its dispersive properties, and the numerical scheme is included. The benchmarks are described and the numerical results compared against the lab measured data for each of them. The specific aim of the present work is to validate this new code for tsunamis generated by rigid slides. Nevertheless, the overall objective of the current benchmarking effort is to produce a ready-to-use numerical tool for real world landslide generated tsunami hazard assessment. This tool has already been used to reproduce the Port Valdez Alaska 1969 event.\n||2020|10.5194/nhess-2020-171|C. Escalante, J. Macías, M. Castro|0.6|2
1385|Domain Decomposition for real time Simulation of needle insertion|Our goal is to develop robotized needle insertion for drug delivery in small animals. We control the robot with a real-time Finite Element simulation that provides accurate models of the deformable environment. To predict the deformations we need to solve a contact problem which is known to be time consuming. To reduce the computational time we use the domain decomposition method: the FE mesh is split in several domains in order to extract paral-lelism for GPU computing and to concentrate the computation time around the needle.||2015|10.1785/0120140145|L. Goffin, S. Bordas, H. Courtecuisse, Yinoussa Adagolodjo, Raffaella Trivisonne, M. Mathelin|0.6|2
1414|Improving embedded linear elasticity in deformable models using stiffness matrix|Physical animation deformation is an important part of computer animation. Most geometric models commonly used in graphics have hundreds of thousands of vertices. Embedding is also a good approach because of its simplicity and ability to preserve geometric features but a standard embedding technique does not correctly model geometry with complex branching. Complex models may have a lot of parts with different properties of different materials. In such cases, it is more likely that a coarse element will contain a mix of materials, soft and hard, and not just one material. Therefore, it is difficult to select an appropriate material in the element, whether stiff or soft, that will deform in the same manner. Thus, many GPU-based collision detection algorithms have been limited to examining the circumstances of the collision in discrete time. In this research, embedding of a linear elastic deformable model is presented. This research has resulted in a significant improvement in efficient animation based on physical objects that are very detailed. To perform embedding, topology information should be taken into account. This means that parts of disconnected elements that fall into the same coarse element can be animated freely. Thus, the properties of different materials are accounted for by calculating the interpolation function together with appropriate stiffness for the coarse elements that are similar to the embedded material. Finally, coarse embedding space is also included to provide a better animation of the border. The result is a simple approach to a complex deformation simulation model with ease and speed associated with coarse regular embedding, with quality and detail that can be made at a finer resolution. Finally, in order to obtain better GPU processing time compared to the computer, an anisotropic visco-hyperelastic constitutive formulation is presented for implementation in a graphical processor unit (GPU).||2015|10.1016/j.procs.2015.06.039|Ali, N. Azura|0.6|2
1516|Research on the Technology of Multi-dimensional Dynamic Visualization of Sub-marine Oil Spill Based on OSG|"Based on Microsoft Foundation Classes(MFC)user interface class and OpenSceneGraph(OSG)3Drendering engine,key techniques of integration and multithread interactivity between MFC and OSG are researched,and a three-dimensional scene of marine environment is modeled and rendered with techniques such as LOD terrain model,seawater simulation and scene clip.Technology of real-time and dynamic simulation of large-scale submarine oil spill particle system is investigated and implemented by means of GPU high lever shader language GLSL and frame buffer object(FBO).The results show that this method is much more efficient than those(e.g.the OpenGL ""glBegin-glEnd"" mode)commonly used for the simulation of particle system.Based on the key technology researches a visualization system is developed for the submarine oil spill,which provides a convenient,friendly and intuitive visual interface for the efficient and real-time simulation and reappearance of the three-dimensional marine environment and the large-scale submarine oil spills."||2015|10.1016/j.aqpro.2015.02.225|Li Xiao-lon|0.6|2
1183|Review of selected new features in FEKO 2018|This paper describes new features to the electromagnetic simulation software FEKO as introduced in the FEKO 2018 version. These include the GPU support for the RL-GO (ray launching geometrical optics) solver, a direct solver for ACA (adaptive cross approximation), and various cable modeling extensions.|International Conference on Advances in Cybersecurity|2018|10.23919/ROPACES.2018.8364179|M. Schoeman, A. Aguilar, E. Attardo, U. Jakobus, K. Longtin, J. V. van Tonder|0.5714285714285714|2
1229|Parallelisation of equation-based simulation programs on heterogeneous computing systems|Numerical solutions of equation-based simulations require computationally intensive tasks such as evaluation of model equations, linear algebra operations and solution of systems of linear equations. The focus in this work is on parallel evaluation of model equations on shared memory systems such as general purpose processors (multi-core CPUs and manycore devices), streaming processors (Graphics Processing Units and Field Programmable Gate Arrays) and heterogeneous systems. The current approaches for evaluation of model equations are reviewed and their capabilities and shortcomings analysed. Since stream computing differs from traditional computing in that the system processes a sequential stream of elements, equations must be transformed into a data structure suitable for both types. The postfix notation expression stacks are recognised as a platform and programming language independent method to describe, store in computer memory and evaluate general systems of differential and algebraic equations of any size. Each mathematical operation and its operands are described by a specially designed data structure, and every equation is transformed into an array of these structures (a Compute Stack). Compute Stacks are evaluated by a stack machine using a Last In First Out queue. The stack machine is implemented in the DAE Tools modelling software in the C99 language using two Application Programming Interface (APIs)/frameworks for parallelism. The Open Multi-Processing (OpenMP) API is used for parallelisation on general purpose processors, and the Open Computing Language (OpenCL) framework is used for parallelisation on streaming processors and heterogeneous systems. The performance of the sequential Compute Stack approach is compared to the direct C++ implementation and to the previous approach that uses evaluation trees. The new approach is 45% slower than the C++ implementation and more than five times faster than the previous one. The OpenMP and OpenCL implementations are tested on three medium-scale models using a multi-core CPU, a discrete GPU, an integrated GPU and heterogeneous computing setups. Execution times are compared and analysed and the advantages of the OpenCL implementation running on a discrete GPU and heterogeneous systems are discussed. It is found that the evaluation of model equations using the parallel OpenCL implementation running on a discrete GPU is up to twelve times faster than the sequential version while the overall simulation speed-up gained is more than three times.|PeerJ Computer Science|2018|10.7717/peerj-cs.160|Dragan D. Nikolic|0.5714285714285714|2
1257|Nonlinear Model Predictive Motion Control of Differential Wheeled Robots|Real-time motion control of a nonholonomic mobile robot in a dynamic environment, especially in the case of multiobjective control, is a challenging problem. Model predictive control (MPC) as an optimization based control algorithm has the ability to deal with complex systems, like multiple-input and multiple-output (MIMO) system, in a dynamic environment. However, due to the complexity of optimization algorithms, the implementation of MPC in real-time applications, especially for the systems with fast transient behaviors is very challenging. With the advent of processors with the ability of parallel computing like FPGAs and GPUs, the application of MPC has become reachable. In this study, the algorithm of optimization problem as the core part of the MPC for motion control of a two-wheel differential robot was developed. Considering the final objective of coding the optimization algorithm on FPGA, the sequential quadratic programming (SQP) method was selected as the optimization algorithm. The specific algorithm equations and matrices were derived based on a simplified nonlinear model. The algorithms were then be coded in MATLAB and used to control a two-wheel robot in the simulation. This paper present the MPC design process and simulation results for the cases of path tracking and point tracking.|National Aerospace and Electronics Conference|2018|10.1109/NAECON.2018.8556691|Seyed Ata Raziei, Zhenhua Jiang|0.5714285714285714|2
1525|Preliminary validation of lava benchmark tests on the GPUSPH particle engine|Lava flow modeling is important in many practical applications, such as the simulation of potential hazard scenarios and the planning of risk mitigation measures, as well as in scientific research to improve our understanding of the physical processes governing the dynamics of lava flow emplacement. Existing predictive models of lava flow behavior include various methods and solvers, each with its advantages and disadvantages. Codes differ in their physical implementations, numerical accuracy, and computational efficiency. In order to validate their efficiency and accuracy, several benchmark test cases for computational lava flow modeling have been established. Despite the popularity that the Smoothed Particle Hydrodynamics (SPH) method has gained in Computational Fluid Dynamics (CFD), very few validations against lava flows have been successfully conducted. At the Tecnolab of INGV- Catania we designed GPUSPH, an implementation of the weakly-compressible SPH method running fully on Graphics Processing Units (GPUs). GPUSPH is a particle engine capable of modeling both Newtonian and non-Newtonian fluids, solving the three-dimensional Navier– Stokes equations, using either a fully explicit integration scheme, or a semi-implicit scheme in the case of highly viscous fluids. Thanks to the full coupling with the thermal equation, and its support for radiation, convection and phase transition, GPUSPH can be used to faithfully simulate lava flows. Here we present the preliminary results obtained with GPUSPH for some of the benchmarks introduced by Cordonnier et al. [2016], including analytical, semi- analytical and experimental problems. The results are reported in terms of correctness and performance, highlighting the benefits and the drawbacks deriving from the use of SPH to simulate lava flows.|Annals of Geophysics|2018|10.4401/AG-7870|A. Cappello, A. Hérault, R. Dalrymple, L. Fortuna, C. Del Negro, G. Bilotta, V. Zago, G. Ganci|0.5714285714285714|2
492|Accelerating Wright–Fisher Forward Simulations on the Graphics Processing Unit|Forward Wright–Fisher simulations are powerful in their ability to model complex demography and selection scenarios, but suffer from slow execution on the Central Processor Unit (CPU), thus limiting their usefulness. However, the single-locus Wright–Fisher forward algorithm is exceedingly parallelizable, with many steps that are so-called “embarrassingly parallel,” consisting of a vast number of individual computations that are all independent of each other and thus capable of being performed concurrently. The rise of modern Graphics Processing Units (GPUs) and programming languages designed to leverage the inherent parallel nature of these processors have allowed researchers to dramatically speed up many programs that have such high arithmetic intensity and intrinsic concurrency. The presented GPU Optimized Wright–Fisher simulation, or “GO Fish” for short, can be used to simulate arbitrary selection and demographic scenarios while running over 250-fold faster than its serial counterpart on the CPU. Even modest GPU hardware can achieve an impressive speedup of over two orders of magnitude. With simulations so accelerated, one can not only do quick parametric bootstrapping of previously estimated parameters, but also use simulated results to calculate the likelihoods and summary statistics of demographic and selection models against real polymorphism data, all without restricting the demographic and selection scenarios that can be modeled or requiring approximations to the single-locus forward algorithm for efficiency. Further, as many of the parallel programming techniques used in this simulation can be applied to other computationally intensive algorithms important in population genetics, GO Fish serves as an exciting template for future research into accelerating computation in evolution. GO Fish is part of the Parallel PopGen Package available at: http://dl42.github.io/ParallelPopGen/.|G3: Genes, Genomes, Genetics|2016|10.1534/g3.117.300103|David S. Lawrie|0.5555555555555556|2
1162|Systematic feasibility analysis of a quantitative elasticity estimation for breast anatomy using supine/prone patient postures.|PURPOSE\nBreast elastography is a critical tool for improving the targeted radiotherapy treatment of breast tumors. Current breast radiotherapy imaging protocols only involve prone and supine CT scans. There is a lack of knowledge on the quantitative accuracy with which breast elasticity can be systematically measured using only prone and supine CT datasets. The purpose of this paper is to describe a quantitative elasticity estimation technique for breast anatomy using only these supine/prone patient postures. Using biomechanical, high-resolution breast geometry obtained from CT scans, a systematic assessment was performed in order to determine the feasibility of this methodology for clinically relevant elasticity distributions.\n\n\nMETHODS\nA model-guided inverse analysis approach is presented in this paper. A graphics processing unit (GPU)-based linear elastic biomechanical model was employed as a forward model for the inverse analysis with the breast geometry in a prone position. The elasticity estimation was performed using a gradient-based iterative optimization scheme and a fast-simulated annealing (FSA) algorithm. Numerical studies were conducted to systematically analyze the feasibility of elasticity estimation. For simulating gravity-induced breast deformation, the breast geometry was anchored at its base, resembling the chest-wall/breast tissue interface. Ground-truth elasticity distributions were assigned to the model, representing tumor presence within breast tissue. Model geometry resolution was varied to estimate its influence on convergence of the system. A priori information was approximated and utilized to record the effect on time and accuracy of convergence. The role of the FSA process was also recorded. A novel error metric that combined elasticity and displacement error was used to quantify the systematic feasibility study. For the authors' purposes, convergence was set to be obtained when each voxel of tissue was within 1 mm of ground-truth deformation.\n\n\nRESULTS\nThe authors' analyses showed that a ∼97% model convergence was systematically observed with no-a priori information. Varying the model geometry resolution showed no significant accuracy improvements. The GPU-based forward model enabled the inverse analysis to be completed within 10-70 min. Using a priori information about the underlying anatomy, the computation time decreased by as much as 50%, while accuracy improved from 96.81% to 98.26%. The use of FSA was observed to allow the iterative estimation methodology to converge more precisely.\n\n\nCONCLUSIONS\nBy utilizing a forward iterative approach to solve the inverse elasticity problem, this work indicates the feasibility and potential of the fast reconstruction of breast tissue elasticity using supine/prone patient postures.|Medical Physics (Lancaster)|2016|10.1118/1.4941745|K. Sheng, K. Hasse, A. Santhanam, J. Neylon|0.5555555555555556|2
1339|Optical dosimetry tools and Monte Carlo based methods for applications in image guided optical therapy in the brain|Prabhu Verleker, Akshay N. Ph.D., Purdue University, August 2016. A Therapeutic Protocol for Treatment of Brain Metastasis through Optically Stimulated Drug Release. Major Professor: Keith Stantz. Purpose: The long-term goal of this research is to determine the feasibility of using near infra-red light to stimulate drug release in metastatic lesions within the brain. In this work, we focused on developing the tools needed to quantify and verify photon fluence distribution in biological tissue. To accomplish this task, an optical dosimetry probe and Monte Carlo based simulation code were fabricated, calibrated and developed to predict light transport in heterogeneous tissue phantoms of the skull and brain. Empirical model (EM) of photon transport using CT images as input were devised to provide real-time calculations capable of being translated to preclinical and clinical applications. Methods and Materials: A GPU based 3D Monte Carlo code was customized to simulate the photon transport within head phantoms consisting of skull bone, white and gray matter with differing laser beam properties, including flat, Gaussian, and superGaussian profiles that are converging, parallel, or diverging. From these simulations, the local photon fluence and tissue dosimetric distribution was simulated and validated||2016|10.1364/ao.55.009875|Prabhu Verleker, N. Akshay|0.5555555555555556|2
184|Agent-Based High-Performance Simulation of Biological Systems on the GPU|Simulation of biological systems are computationally demanding due to the large scale reaction networks of bacterial cells. This scalability issue escalates, in particular, when bacterial colonies, formed by many individual cells, are simulated. Agent-based modelling environments on parallel architectures, such as the FLAME (Flexible Large-scale Modelling Environment) framework, are good candidates to simulate such systems, but due to the complex nature of cellular systems more advance technology is needed. In this paper, we utilise FLAME GPU, extending FLAME with a high performance graphics processing unit, to simulate a pulse generator, a typical multicellular synthetic biology system. This system is specified using a membrane computing model. We also illustrate the performance improvement of FLAME GPU over FLAME.|2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems|2015|10.1109/HPCC-CSS-ICESS.2015.253|M. Burkitt, M. Gheorghe, Savas Konur, M. Kiran, F. Ipate|0.5|2
189|Model Coupling between the Weather Research and Forecasting Model and the DPRI Large Eddy Simulator for Urban Flows on GPU-accelerated Multicore Systems|In this report we present a novel approach to model coupling for shared-memory multicore systems hosting OpenCL-compliant accelerators, which we call The Glasgow Model Coupling Framework (GMCF). We discuss the implementation of a prototype of GMCF and its application to coupling the Weather Research and Forecasting Model and an OpenCL-accelerated version of the Large Eddy Simulator for Urban Flows (LES) developed at DPRI. \nThe first stage of this work concerned the OpenCL port of the LES. The methodology used for the OpenCL port is a combination of automated analysis and code generation and rule-based manual parallelization. For the evaluation, the non-OpenCL LES code was compiled using gfortran, fort and pgfortran}, in each case with auto-parallelization and auto-vectorization. The OpenCL-accelerated version of the LES achieves a 7 times speed-up on a NVIDIA GeForce GTX 480 GPGPU, compared to the fastest possible compilation of the original code running on a 12-core Intel Xeon E5-2640. \nIn the second stage of this work, we built the Glasgow Model Coupling Framework and successfully used it to couple an OpenMP-parallelized WRF instance with an OpenCL LES instance which runs the LES code on the GPGPI. The system requires only very minimal changes to the original code. The report discusses the rationale, aims, approach and implementation details of this work.|arXiv.org|2015|10.1002/cpe.3522|W. Vanderbauwhede|0.5|2
257|Research on Tool Path Planning Method of NURBS Surface Based on CPU-GPU Parallel Computing|In order to deal with the inefficiency of trational serial tool path algorithms and incompatibility issues on the heterogeneous hardware platforms, this paper suggests a tool path planning method based on CPU-GPU(Central Processing Unit-Graphic Processing Unit) heterogeneous parallel computing. The method contra poses NURBS(Non-Uniform Rational B-Splines) surface which is abstracted as a matrix multiplication on the principle of isoparametric line tool path planning method. Then a parallel algorithm in accordance with Open CL(Open Computing Language) specification is proposed. Adopting data parallel programming model, the method executes multiple work-items of the GPU on the core under control of the CPU logic, and reconstructs the isoparametric line method as parallel execution instead of traditional serial execution. Simulation results show that this algorithm takes less time to generate tool paths on the CPU-GPU heterogeneous platforms, reduced by 1.5 to 15.9 times compared with traditional serial algorithm and it is of great significance to the tool path planning's real-time or quasi real-time generation.|2017 International Conference on Computer Network, Electronic and Automation (ICCNEA)|2017|10.1109/ICCNEA.2017.52|Yangqiang Bi, Wujia Yu, Zhendong Li|0.5|2
286|Parallel circuit simulation using the direct method on a heterogeneous cloud|This paper discusses the development of a parallel SPICE circuit simulator using the direct method on a cloud-based heterogeneous cluster, which includes multiple HPC compute nodes with multi-sockets, multicores, and GPUs. A simple model is derived to optimally partition the circuit between the compute nodes. The parallel simulator is divided into four major kernels: Partition Device Model Evaluation (PME), Partition Matrix Factorization (PMF), Interconnection Matrix Evaluation (IME), and Interconnection Matrix Factorization (IMF). Another model is derived to assign each of the kernels to the most suitable execution platform of the Amazon EC2 heterogeneous cloud. The partitioning approach using heterogeneous resources has achieved an order-of-magnitude speedup over optimized multithreaded implementations of SPICE using state of the art KLU and NICSLU packages for matrix solution.|Design Automation Conference|2015|10.1145/2744769.2744888|A. Bayoumi, Y. Hanafy, A. Helal|0.5|2
305|Parallel Multivariate Spatio-Temporal Clustering of Large Ecological Datasets on Hybrid Supercomputers|A proliferation of data from vast networks of remote sensing platforms (satellites, unmanned aircraft systems (UAS), airborne etc.), observational facilities (meteorological, eddy covariance etc.), state-of-the-art sensors, and simulation models offer unprecedented opportunities for scientific discovery. Unsupervised classification is a widely applied data mining approach to derive insights from such data. However, classification of very large data sets is a complex computational problem that requires efficient numerical algorithms and implementations on high performance computing (HPC) platforms. Additionally, increasing power, space, cooling and efficiency requirements has led to the deployment of hybrid supercomputing platforms with complex architectures and memory hierarchies like the Titan system at Oak Ridge National Laboratory. The advent of such accelerated computing architectures offers new challenges and opportunities for big data analytics in general and specifically, large scale cluster analysis in our case. Although there is an existing body of work on parallel cluster analysis, those approaches do not fully meet the needs imposed by the nature and size of our large data sets. Moreover, they had scaling limitations and were mostly limited to traditional distributed memory computing platforms. We present a parallel Multivariate Spatio-Temporal Clustering (MSTC) technique based on k-means cluster analysis that can target hybrid supercomputers like Titan. We developed a hybrid MPI, CUDA and OpenACC implementation that can utilize both CPU and GPU resources on computational nodes. We describe performance results on Titan that demonstrate the scalability and efficacy of our approach in processing large ecological data sets.|IEEE International Conference on Cluster Computing|2017|10.1109/CLUSTER.2017.88|R. Mills, W. Hargrove, Vamsi Sripathi, F. Hoffman, J. Kumar, S. Sreepathi|0.5|2
326|Performance Evaluation of Tsunami Simulation Using OpenCL on GPU and FPGA|Prediction of the arrival time of tsunami is critical for evacuating people from coastal area. Solving many related to tsunami problems is important in order to decrease negative effects of this serious disaster. Numerical modeling of tsunami wave propagation is a computational intensive problem that requiresacceleration of calculations by means of parallel processing. The Method of Splitting Tsunami (MOST) is one of the well-known numerical solvers for modeling tsunami waves in the ocean. This paper focuses on design and evaluation of tsunami simulation code using OpenCL. We have developed a tsunami propagation code based on MOST, and implemented its different parallel optimizations for GPU and FPGA.|International Symposium on Embedded Multicore/Many-core Systems-on-Chip|2017|10.1109/MCSoC.2017.15|A. Vazhenin, N. Nakasato, Fumiya Kono, S. Sedukhin, K. Hayashi|0.5|2
336|gPGA: GPU Accelerated Population Genetics Analyses|Background The isolation with migration (IM) model is important for studies in population genetics and phylogeography. IM program applies the IM model to genetic data drawn from a pair of closely related populations or species based on Markov chain Monte Carlo (MCMC) simulations of gene genealogies. But computational burden of IM program has placed limits on its application. Methodology With strong computational power, Graphics Processing Unit (GPU) has been widely used in many fields. In this article, we present an effective implementation of IM program on one GPU based on Compute Unified Device Architecture (CUDA), which we call gPGA. Conclusions Compared with IM program, gPGA can achieve up to 52.30X speedup on one GPU. The evaluation results demonstrate that it allows datasets to be analyzed effectively and rapidly for research on divergence population genetics. The software is freely available with source code at https://github.com/chunbaozhou/gPGA.|PLoS ONE|2015|10.1371/journal.pone.0135028|Chaodong Zhu, Xianyu Lang, Chunbao Zhou, Yangang Wang|0.5|2
383|On some block-preconditioners for saddle point systems and their CPU–GPU performance|In this work we emphasize some aspects of the numerical and computational performance of block preconditioners for systems with matrices of saddle point form. We discuss the quality of a sparse approximation of the related Schur complement for constructing an efficient preconditioner and the achieved numerical efficiency in terms of number of iterations. We also present a performance study of the computational efficiency of the corresponding preconditioned iterative solution methods, implemented using publicly available numerical linear algebra software packages, both on multicore CPU and GPU devices. We show that the presently available GPU accelerators can be very efficiently used in computer simulations involving inner-outer solution methods and hierarchical data structures. The benchmark problem originates from a geophysical application, namely, the elastic Glacial Isostatic Adjustment model, discretized using the finite element method.||2015|10.1016/j.parco.2015.06.003|A. Dorostkar, M. Neytcheva, B. Lund|0.5|2
405|GPU-Enabled Particle Based Optimization for Robotic-Hand Pose Estimation and Self-Calibration|Humanoid robots have complex kinematic chains that are difficult to model with the precision required to reach and/or grasp objects properly. In this paper we propose a GPU-enabled vision based 3D hand pose estimation method that runs during robotic reaching tasks to calibrate in real time the kinematic chain of the robot arm. This is achieved by combining: i) proprioceptive and visual sensing, and ii) a kinematic and computer graphics model of the system. We use proprioceptive input to create visual hypotheses about the hand appearance in the image using a 3D CAD model inside the game engine from Unity Technologies. These hypotheses are compared with the actual visual input using particle filter techniques. The outcome of this processing is the best hypothesis for the hand pose and a set of joint offsets to calibrate the arm. We tested our approach in a simulation environment and verified that the angular error is reduced 3 times and the position error about 12 times comparing with the non-calibrated case (proprioception only). The used GPU implementation techniques ensures a performance 2.5 times faster than performing the computations on the CPU.|IEEE International Conference on Autonomous Robot Systems and Competitions|2015|10.1109/ICARSC.2015.25|R. Ferreira, L. Jamone, Alexandre Bernardino, Pedro Vicente|0.5|2
467|Accelerating Streamline Tracking in Groundwater Flow Modeling on GPUs|Streamline simulation in groundwater flow modeling is a time‐consuming process when a large number of streamlines are analyzed. We develop a parallelization method on graphics processing units (GPUs) for the semi‐analytical particle tracking algorithm developed by Pollock (1988). Compute Unified Device Architecture was used to implement the parallel method. Forward and backward tracking of a streamline is handled by an individual thread. A GPU includes a grid of blocks where a block handles 32 threads. We use multi‐GPUs to accelerate streamline tracking in a flow model with millions of particles. The method was examined to simulate streamlines for identifying three‐dimensional (3D) flow systems in a Tóthian basin. The speedup exceeds 1000 when 8 NVIDIA GPUs are used to simulate 5 million or more streamlines.|Ground Water|2019|10.1111/gwat.12959|Xiaohui Ji, Mulan Luo, Xu-sheng Wang|0.5|2
546|Parallel numerical simulation for a super large-scale compositional reservoir|A compositional reservoir simulation model with ten-million grids is successfully computed using parallel processing techniques. The load balance optimization principle for parallel calculation is developed, which improves the calculation speed and accuracy, and provides a reliable basis for the design of reservoir development plan. Taking M reservoir as an example, the parallel numerical simulation study of compositional model with ten million grids is carried out. When the number of computational nodes increases, message passing processes and data exchange take much time, the proportion time of solving equation is reduced. When the CPU number increases, the creation of Jacobian matrix process has the higher acceleration ratio, and the acceleration ratio of I/O process become lower. Therefore, the I/O process is the key to improve the acceleration ratio. Finally, we study the use of GPU and CPU parallel acceleration technology to increase the calculation speed. The results show that the technology is 2.4 ∼ 5.4 times faster than CPU parallel technology. The more grids there are, the better GPU acceleration effect it has. The technology of parallel numerical simulation for compositional model with ten-million grids presented in this paper has provided the foundation for fine simulation of complex reservoirs. Cited as : Lian, P., Ji, B., Duan, T., Zhao, H., Shang, X. Parallel numerical simulation for a super large-scale compositional reservoir. Advances in Geo-Energy Research, 2019, 3(4): 381-386, doi: 10.26804/ager.2019.04.05||2019|10.26804/ager.2019.04.05|Peiqing Lian, Huawei Zhao, Bingyu Ji, T. Duan, X. Shang|0.5|2
557|Photon Propagation using GPUs by the IceCube Neutrino Observatory|IceCube Neutrino Observatory is a cubic kilometer neutrino detector located at the South Pole designed to detect high-energy astrophysical neutrinos. To thoroughly understand the detected neutrinos and their properties, the detector response to simulated signal and background has to be modeled using Monte Carlo techniques. An integral part of these studies are the optical properties of the ice the observatory is built into. The propagation of individual photons from particles produced by neutrino interactions in the ice can be greatly accelerated using graphics processing units (GPUs). In this paper, we will describe how we perform the photon propagation and create a global pool of GPU resources for both production and individual users.|eScience|2019|10.1109/eScience.2019.00050|B. Riedel, A. Olivas, D. Schultz, J. C. Díaz-Vélez, C. Kopper, M. Rongen, D. Chirkin, J. Santen|0.5|2
688|GPU Optimizations for Atmospheric Chemical Kinetics|We present a series of optimizations to alleviate stack memory overflow issues and improve overall performance of GPU computational kernels in atmospheric chemical kinetics model simulations. We use heap memory in numerical solvers for stiff ODEs, move chemical reaction constants and tracer concentration arrays from stack to global memory, use direct pointer indexing for array memory access, and use CUDA streams to overlap computation with memory transfer to the device. Overall, an order of magnitude reduction in GPU memory requirements is achieved, allowing for simultaneous offloading from multiple MPI processes per node and/or increasing the chemical mechanism complexity.|International Conference on High Performance Computing in Asia-Pacific Region|2021|10.1145/3432261.3439863|E. Raffin, Georges-Emmanuel Moulard, A. Kerkweg, G. Oord, D. Taraborrelli, Theodoros Christoudias, B. V. Werkhoven, Victor Azizi, Timo Kirfel|0.5|2
690|FiCoS: A fine-grained and coarse-grained GPU-powered deterministic simulator for biochemical networks|Mathematical models of biochemical networks can largely facilitate the comprehension of the mechanisms at the basis of cellular processes, as well as the formulation of hypotheses that can be tested by means of targeted laboratory experiments. However, two issues might hamper the achievement of fruitful outcomes. On the one hand, detailed mechanistic models can involve hundreds or thousands of molecular species and their intermediate complexes, as well as hundreds or thousands of chemical reactions, a situation generally occurring in rule-based modeling. On the other hand, the computational analysis of a model typically requires the execution of a large number of simulations for its calibration or to test the effect of perturbations. As a consequence, the computational capabilities of modern Central Processing Units can be easily overtaken, possibly making the modeling of biochemical networks a worthless or ineffective effort. To the aim of overcoming the limitations of the current state-of-the-art simulation approaches, we present in this paper FiCoS, a novel “black-box” deterministic simulator that effectively realizes both a fine-grained and a coarse-grained parallelization on Graphics Processing Units. In particular, FiCoS exploits two different integration methods, namely, the Dormand–Prince and the Radau IIA, to efficiently solve both non-stiff and stiff systems of coupled Ordinary Differential Equations. We tested the performance of FiCoS against different deterministic simulators, by considering models of increasing size and by running analyses with increasing computational demands. FiCoS was able to dramatically speedup the computations up to 855×, showing to be a promising solution for the simulation and analysis of large-scale models of complex biological processes. Author summary Systems Biology is an interdisciplinary research area focusing on the integration of biological data with mathematical and computational methods in order to unravel and predict the emergent behavior of complex biological systems. The ultimate goal is the understanding of the complex mechanisms at the basis of biological processes, together with the formulation of novel hypotheses that can be then tested by means of laboratory experiments. In such a context, mechanistic models can be used to describe and investigate biochemical reaction networks by taking into account all the details related to their stoichiometry and kinetics. Unfortunately, these models can be characterized by hundreds or thousands of molecular species and biochemical reactions, making their simulation unfeasible with classic simulators running on Central Processing Units (CPUs). In addition, a large number of simulations might be required to calibrate the models and/or to test the effect of perturbations. In order to overcome the limitations imposed by CPUs, Graphics Processing Units (GPUs) can be effectively used to accelerate the simulations of these models. We thus designed and developed a novel GPU-based tool, called FiCoS, to speed-up the computational analyses typically required in Systems Biology.|bioRxiv|2021|10.1101/2021.01.15.426855|D. Besozzi, G. Capitoli, A. Tangherloni, S. Spolaor, Marco S. Nobile, G. Mauri, L. Rundo, P. Cazzaniga|0.5|2
709|Experiences With Adding SYCL Support to GROMACS|GROMACS is an open-source, high-performance molecular dynamics (MD) package primarily used for biomolecular simulations, accounting for 5% of HPC utilization worldwide. Due to the extreme computing needs of MD, significant efforts are invested in improving the performance and scalability of simulations. Target hardware ranges from supercomputers to laptops of individual researchers and volunteers of distributed computing projects such as Folding@Home. The code has been designed both for portability and performance by explicitly adapting algorithms to SIMD and data-parallel processors. A SIMD intrinsic abstraction layer provides high CPU performance. Explicit GPU acceleration has long used CUDA to target NVIDIA devices and OpenCL for AMD/Intel devices. In this talk, we discuss the experiences and challenges of adding support for the SYCL platform into the established GROMACS codebase and share experiences and considerations in porting and optimization. While OpenCL offers the benefits of using the same code to target different hardware, it suffers from several drawbacks that add significant development friction. Its separate-source model leads to code duplication and makes changes complicated. The need to use C99 for kernels, while the rest of the codebase uses C++17, exacerbates these issues. Another problem is that OpenCL, while supported by most GPU vendors, is never the main framework and thus is not getting the primary support or tuning efforts. SYCL alleviates many of these issues, employing a single-source model based on the modern C++ standard. In addition to being the primary platform for Intel GPUs, the possibility to target AMD and NVIDIA GPUs through other implementations (e.g., hipSYCL) might make it possible to reduce the number of separate GPU ports that have to be maintained. Some design differences from OpenCL, such as flow directed acyclic graphs (DAGs) instead of in-order queues, made it necessary to reconsider the GROMACS’s task scheduling approach and architectural choices in the GPU backend. Additionally, supporting multiple GPU platforms presents a challenge of balancing performance (low-level and hardware-specific code) and maintainability (more generalization and code-reuse). We will discuss the limitations of the existing codebase and interoperability layers with regards to adding the new platform; the compute performance and latency comparisons; code quality considerations; and the issues we encountered with SYCL implementations tested. Finally, we will discuss our goals for the next release cycle for the SYCL backend and the overall architecture of GPU acceleration code in GROMACS.|International Workshop on OpenCL|2021|10.1145/3456669.3456690|E. Lindahl, Szilárd Páll, Andrey Alekseenko|0.5|2
724|Parameter Identification of a Friction Model in Metal Cutting Simulations with GPU-Accelerated Meshfree Methods|A modular computational framework is presented for the identification of friction parameters in metal machining applications. Numerical simulation of such processes using mesh-based techniques (usually) necessitates the cumbersome re-meshing procedures and is hard to parallelize. Therefore, the present framework synthesizes the advantages of mesh-free methods with GPU parallel computing, offering an efficient tool for the optimization procedures in thermo-mechanical modeling of cutting problems. The proposed approach employs an inverse method to determine the unknown coefficients of a temperature-dependent friction model in high-speed metal cutting. Good agreement between our numerical results and experimental data is found, providing both quantitative and qualitative assessments.||2021|10.23967/WCCM-ECCOMAS.2020.273|H. Klippel, K. Wegener, M. Afrasiabi, M. Roethlin|0.5|2
741|Francis 99 CFD through RapidCFD accelerated GPU code|Francis turbines research and development (R&D) requires performance assessment through hydraulic laboratory model testing which can be assisted by auxiliary tools like computational fluid dynamics (CFD), widely used recently. CFD has a history of seeking and requiring ever higher computational performance (HPC) because of the parallelism where Graphics Processor Units (GPUs) have emerged as a major paradigm for solving complex computational problems. However, their implementation to CFD solvers is still a challenge and the tremendous computational power of the GPUs has been wasted. This work presents how the open source RapidCFD code, based on OpenFOAM and ported to Nvidia CUDA, enabled GPUs to be able of running almost entire simulations in thousands of parallel stream cores packed in small form factor hardware in order to solve the incompressible Reynolds-Average-Navier-Stokes (RANS) equations. The simulations were based on a full 3D Francis turbine case which consisted of a grid domain of 23 million cells, including spiral case with stay vanes, distributor, runner and draft tube. CFD results of shaft torque, static pressure and velocity components in steady state deploying a multiple reference frame (MRF) motion approach were compared with available experimental data for main operation conditions at different distributor opening angles: best efficiency operation point (BEP), part load operation point (PL) and full load operation point (HL). The obtained data showed that by transferring directly all the computations to the GPUs, it is possible to make CFD simulations faster compared with central processing units (CPUs). Thus, it is expected to obtain an affordable low computational cost in optimization processes or full range performance evaluations.|IOP Conference Series: Earth and Environment|2021|10.1088/1755-1315/774/1/012016|G. Solorio, F. Dominguez, D. Molinero, S. Galván, L. Ibarra|0.5|2
750|Efficient State-space Exploration in Massively Parallel Simulation Based Inference|Simulation-based Inference (SBI) is a widely used set of algorithms to learn the parameters of complex scientific simulation models. While primarily run on CPUs in HPC clusters, these algorithms have been shown to scale in performance when developed to be run on massively parallel architectures such as GPUs. While parallelizing existing SBI algorithms provides us with performance gains, this might not be the most efficient way to utilize the achieved parallelism. This work proposes a new algorithm, that builds on an existing SBI method - Approximate Bayesian Computation with Sequential Monte Carlo(ABC-SMC). This new algorithm is designed to utilize the parallelism not only for performance gain, but also toward qualitative benefits in the learnt parameters. The key idea is to replace the notion of a single 'step-size' hyperparameter, which governs how the state space of parameters is explored during learning, with step-sizes sampled from a tuned Beta distribution. This allows this new ABC-SMC algorithm to more efficiently explore the state-space of the parameters being learnt. We test the effectiveness of the proposed algorithm to learn parameters for an epidemiology model running on a Tesla T4 GPU. Compared to the parallelized state-of-the-art SBI algorithm, we get similar quality results in $\sim 100$x fewer simulations and observe ~80x lower run-to-run variance across 10 independent trials.|arXiv.org|2021|10.1007/978-981-15-9003-0|S. Kulkarni, C. A. Moritz|0.5|2
771|The NICAM 3.5km-1024 ensemble simulation: Performance optimization and scalability of NICAM-LETKF on supercomputer Fugaku|In parallel with the new Japanese flagship supercomputer, Fugaku, we have continued improving a nonhydrostatic icosahedral atmospheric model (NICAM). Here, we introduce the results of our system-application co-design since 2014. Fugaku's CPU (A64FX) is based on the Arm instruction-set architecture. This 48-core many-core CPU is equipped with 32GB of HBM2 memory, showing data transfer performance comparable to GPUs. We have implemented kernel-level optimizations to take advantage of Fugaku's high memory performance. Among them, we recognized trade-offs related to ensuring memory locality and parallelism, and register allocation. We improved the application's average arithmetic intensity through detailed loop-by-loop performance measurements and reduced memory pressure by actively using single-precision operations. We also redesigned the data layout and the file I/O component of the ensemble data assimilation (DA) system and achieved good scalability in the atmospheric simulation and DA. We performed a global 3.5km mesh, 1024-member ensemble simulation, and DA using 82% of the Fugaku system (131,072 nodes, 6,291,456 cores). In this world's most massive ensemble DA benchmark experiment, the simulation and the DA achieved 29 PFLOPS and 79 PFLOPS of effective performance.||2021|10.5194/EGUSPHERE-EGU21-4771|C. Kodama, H. Tomita, M. Satoh, K. Terasaki, Toshiyuki Imamura, Shuhei Kudo, Yuta Kawai, T. Miyoshi, M. Nakano, H. Yashiro, K. Minami|0.5|2
778|Coupling Friction with Visual Appearance|We present a novel meso-scale model for computing anisotropic and asymmetric friction for contacts in rigid body simulations that is based on surface facet orientations. The main idea behind our approach is to compute a direction dependent friction coefficient that is determined by an object's roughness. Specifically, where the friction is dependent on asperity interlocking, but at a scale where surface roughness is also a visual characteristic of the surface. A GPU rendering pipeline is employed to rasterize surfaces using a shallow depth orthographic projection at each contact point in order to sample facet normal information from both surfaces, which we then combine to produce direction dependent friction coefficients that can be directly used in typical LCP contact solvers, such as the projected Gauss-Seidel method. We demonstrate our approach with a variety of rough textures, where the roughness is both visible in the rendering and in the motion produced by the physical simulation.|Proceedings of the ACM on Computer Graphics and Interactive Techniques|2021|10.1145/3480138|S. Andrews, Kenny Erleben, Loic Nassif, P. Kry|0.5|2
785|Parallel Swarm Intelligent Motion Planning with Energy-Balanced for Multirobot in Obstacle Environment|Multirobot motion planning is always one of the critical techniques in edge intelligent systems, which involve a variety of algorithms, such as map modeling, path search, and trajectory optimization and smoothing. To overcome the slow running speed and imbalance of energy consumption, a swarm intelligence solution based on parallel computing is proposed to plan motion paths for multirobot with many task nodes in a complex scene that have multiple irregularly-shaped obstacles, which objective is to find a smooth trajectory under the constraints of the shortest total distance and the energy-balanced consumption for all robots to travel between nodes. In a practical scenario, the imbalance of task allocation will inevitably lead to some robots stopping on the way. Thus, we firstly model a gridded scene as a weighted MTSP (multitraveling salesman problem) in which the weights are the energies of obstacle constraints and path length. Then, a hybridization of particle swarm and ant colony optimization (GPSO-AC) based on a platform of Compute Unified Device Architecture (CUDA) is presented to find the optimal path for the weighted MTSPs. Next, we improve the A\n \n ∗\n \n algorithm to generate a weighted obstacle avoidance path on the gridded map, but there are still many sharp turns on it. Therefore, an improved smooth grid path algorithm is proposed by integrating the dynamic constraints in this paper to optimize the trajectory smoothly, to be more in line with the law of robot motion, which can more realistically simulate the multirobot in a real scene. Finally, experimental comparisons with other methods on the designed platform of GPUs demonstrate the applicability of the proposed algorithm in different scenarios, and our method strikes a good balance between energy consumption and optimality, with significantly faster and better performance than other considered approaches, and the effects of the adjustment coefficient \n \n q\n \n on the performance of the algorithm are also discussed in the experiments.|Wireless Communications and Mobile Computing|2021|10.1155/2021/8902328|Shoubao Su, Chishe Wang, Wei Zhao|0.5|2
787|Interactive 3D Human Heart Simulations on Segmented Human MRI Hearts|Understanding cardiac arrhythmic mechanisms and developing new strategies to control and terminate them using computer simulations requires realistic physiological cell models with anatomically accurate heart structures. Furthermore, numerical simulations must be fast enough to study and validate model and structure parameters. Here, we present an interactive parallel approach for solving detailed cell dynamics in high-resolution human heart structures with a local PC's GPU. In vitro human heart MRI scans were manually segmented to produce 3D structures with anatomically realistic electrophysiology. The Abubu.js library was used to create an interactive code to solve the OVVR human ventricular cell model and the FDA extension of the model in the human MRI heart structures, allowing the simulation of reentrant waves and investigation of their dynamics in real time. Interactive simulations of a physiological cell model in a detailed anatomical human heart reveals propagation of waves through the fine structures of the trabeculae and pectinate muscle that can perpetuate arrhythmias, thereby giving new insights into effects that may need to be considered when planning ablation and other defibrillation methods.|2021 Computing in Cardiology (CinC)|2021|10.23919/cinc53138.2021.9662948|J. Glimm, J. Berman, E. Cherry, T. Iles, Hyun-Kyung Lim, Ilija Uzelac, F. Fenton, Shahriar Iravanian, S. Smolka, P. Iaizzo, A. Kaboudian|0.5|2
788|The HighPerMeshes framework for numerical algorithms on unstructured grids|Solving partial differential equations (PDEs) on unstructured grids is a cornerstone of engineering and scientific computing. Heterogeneous parallel platforms, including CPUs, GPUs, and FPGAs, enable energy‐efficient and computationally demanding simulations. In this article, we introduce the HighPerMeshes C++‐embedded domain‐specific language (DSL) that bridges the abstraction gap between the mathematical formulation of mesh‐based algorithms for PDE problems on the one hand and an increasing number of heterogeneous platforms with their different programming models on the other hand. Thus, the HighPerMeshes DSL aims at higher productivity in the code development process for multiple target platforms. We introduce the concepts as well as the basic structure of the HighPerMeshes DSL, and demonstrate its usage with three examples. The mapping of the abstract algorithmic description onto parallel hardware, including distributed memory compute clusters, is presented. A code generator and a matching back end allow the acceleration of HighPerMeshes code with GPUs. Finally, the achievable performance and scalability are demonstrated for different example problems.|Concurrency and Computation|2021|10.1002/cpe.6616|Tobias Kenter, D. Grünewald, Samer Alhaddad, F. Pfreundt, Stefan Groth, Y. Grynko, Frank Hannig, M. Schotte, Christian Plessl, J. Förstner, M. Weiser, T. Steinke, Florian Wende, J&#252;rgen Teich|0.5|2
823|Differential Evolution Based Nonlinear Model Predictive Speed Control of PMSM Implemented on GPU|In this paper, the novel approach to the nonlinear model predictive speed control of a permanent magnet synchronous motor and its implementation is introduced. The implementation is performed using general-purpose computing on graphics processing unit. The introduced algorithm uses the optimization method based on the differential evolution to get the optimal increment of stator voltage. The proposed algorithm is tested in the processor in the loop simulation with the Simscape model for the simulation of PMSM and the Jetson Xavier embedded device for the algorithm execution. The results show the ability of the algorithm to ensure the reference tracking and to keep the requested variables within their limits.|International Symposium on Industrial Electronics|2021|10.1109/ISIE45552.2021.9576359|M. Kozubik, Dominik Friml|0.5|2
840|A GPU-accelerated framework for individualized estimation of organ dose in digital tomosynthesis.|PURPOSE\nEstimation of organ dose in digital tomosynthesis (DT) is challenging due to the lack of existing tools to accurately and flexibly model protocol- and view-specific collimations and the motion trajectories of the source and detector for a variety of exam protocols and the computational inefficiencies of conducting MC simulations. The purpose of this study was to overcome these limitations by developing and benchmarking a GPU-accelerated MC simulation framework compatible with patient-specific computational phantoms for individualized estimation of organ dose in DT.\n\n\nMATERIALS AND METHODS\nThe framework for individualized estimation of dose in DT was developed as a two-step workflow: (1) a custom MATLAB code that accepts a patient-specific computational phantom and exam description (organ markers for defining the extremities of the anatomical region of interest, tube voltage, source-to-image distance, angular sweep range, number of projection views, and the distance of the pivot point from the detector about which the source translates - PPID) to compute the field-of-views (FOVs) for a clinical DT system, and (2) a MC tool (developed using MC-GPU) modeling the geometry of a clinical DT system to estimate organ doses based on the computed FOVs. Using this framework, we estimated organ doses for 28 radiosensitive organs in an adult reference patient model (M; 30 yrs) imaged using a commercial DT system (VolumeRad, GE Healthcare, Waukesha, WI). The estimates were benchmarked against values from a comparable organ dose estimation framework (reference dataset developed by the Advanced Laboratory for Radiation Dosimetry Studies at University of Florida) for a posterior-anterior chest (PAC) exam. The resulting differences were quantified as percent relative errors and analyzed to identify any potential sources of bias and uncertainties. The timing performance (run duration in s) of the framework for the same simulation was also quantified to gauge the feasibility of the workflow for time-constrained clinical applications.\n\n\nRESULTS\nThe organ dose estimates from the developed framework showed a close agreement with the reference dataset, with percent relative errors ranging from -6.9% to 5.0% and a mean absolute percent difference of 1.7% over all radiosensitive organs, with the exception of testes and eye lens, for which the percent relative errors were higher at -18.9% and -27.6%, respectively, due to their relative positioning outside the primary irradiation field, leading to fewer photons depositing energy and consequently higher errors in estimated organ doses. The run duration for the same simulation was 916.3 s, representing a substantial improvement in performance over existing non-parallelized MC tools.\n\n\nCONCLUSIONS\nThis study successfully developed and benchmarked a GPU-accelerated framework compatible with patient-specific anthropomorphic computational phantoms for accurate individualized estimation of organ doses in DT. By enabling patient-specific estimation of organ doses, this framework can aid clinicians and researchers by providing them with tools essential for tracking the radiation burden to patients for dose monitoring purposes and identifying the trends and relationships in organ doses for a patient population to optimize existing and develop new exam protocols. This article is protected by copyright. All rights reserved.|Medical Physics (Lancaster)|2021|10.1002/mp.15400|W. Bolch, Shobhit Sharma, A. Kapadia, E. Samei, W. Paul Segars, J. Brown|0.5|2
857|The Optimization of Model Parallelization Strategies for Multi-GPU Training|Data parallelism (DP) is most widely used among all the parallel methods in largescale network training but the speedup from leveraging DP begins to scale poorly as the number of devices in data parallel training grows. Therefore, model parallelism (MP) is needed for further accelerating. In this paper, an integer linear programming based tool, NetPlacer, is proposed to find optimal operation-to-device placement. Unlike existing methods using time estimation, NetPlacer discards estimating simulated execution time and uses strict memory and computing balancing constraints on the network to find a scheme with a balance between memory and computation on different devices. As far as we know, we are the first to use equipment equalization as a target for model parallel strategy optimization. A speedup of at least 20% is achieved through NetPlacer for Inception-V3 on a 2-GPU machine compared to what one single GPU alone can achieve.|Global Communications Conference|2021|10.1109/GLOBECOM46510.2021.9685964|Zechao Zhang, Bing Hu, Jianfeng Chen|0.5|2
877|A Bayesian method to cluster single-cell RNA sequencing data using Copy Number Alterations|Motivation Cancers are composed by several heterogeneous subpopulations, each one harbouring different genetic and epigenetic somatic alterations that contribute to disease onset and therapy response. In recent years, copy number alterations leading to tumour aneuploidy have been identified as potential key drivers of such populations, but the definition of the precise makeup of cancer subclones from sequencing assays remains challenging. In the end, little is known about the mapping between complex copy number alterations and their effect on cancer phenotypes. Results We introduce CONGAS, a Bayesian probabilistic method to phase bulk DNA and single-cell RNA measurements from independent assays. CONGAS jointly identifies clusters of single cells with subclonal copy number alterations, and differences in RNA expression. The model builds statistical priors leveraging bulk DNA sequencing data, does not require a normal reference and scales fast thanks to a GPU backend and variational inference. We test CONGAS on both simulated and real data, and find that it can determine the tumour subclonal composition at the single-cell level together with clone-specific RNA phenotypes in tumour data generated from both 10x and Smart-Seq assays. Availability CONGAS is available as 2 packages: CONGAS (https://github.com/caravagnalab/congas), which implements the model in Python, and RCONGAS (https://caravagnalab.github.io/rcongas/), which provides R functions to process inputs, outputs, and run CONGAS fits. The analysis of real data and scripts to generate figures of this paper are available via RCONGAS; code associated to simulations is available at https://github.com/caravagnalab/rcongas_test. Contact gcaravagna@units.it Supplementary information Supplementary data are available at Bioinformatics online.|bioRxiv|2021|10.1101/2021.02.02.429335|R. Bergamin, Lucrezia Patruno, N. Calonaci, Salvatore Milite, G. Caravagna|0.5|2
895|A Word-length Optimized Parallel Framework for Accelerating Option Pricing Model|We present an efficient hardware structure for accelerating option pricing, using the Black-Schole model. Within the structure, a Multiplier Array is devised to make the pricing process fully-pipelined, and an efficient WELL19937 structure is adopted as the basic uniform random number generator, which is proven to have excellent equidistribution, very long period, and is also easy to be parallelized. Moreover, to maximize the performance/complexity efficiency, a word-length optimization method is used to determine the optimal integer and fractional word-length for signals in the design. We also develop a framework to parallelize the option pricing process, based on the Fast Jump Ahead technique. Experimental results show that our parallel framework has good scalability and is capable of achieving performance speedup roughly linearly with the number of parallel simulation cores, while retain exactly the same calculation results. Meanwhile, our framework is superior to other existing architectures reported in the literatures in terms of throughput rate, resource usage as well as the period and quality of the basic uniform random number generator. Furthermore, we implement the framework with 17 parallel simulation core targeting Xilinx xc5vlx330t FPGA device. Compared with implementations on CPU and GPU with similar chip manufacturing process, the throughput is about 365 and 2.6 times faster, while the throughput-power efficiency achieves 3293 and 59.4 time speedup, respectively.|2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)|2021|10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00109|Yi Dai, Lei Zhang, Yuan Li, Y. Sun|0.5|2
967|Improving Effectiveness of Simulation-Based Inference in the Massively Parallel Regime|"Simulation-based Inference (SBI) is a widely used set of algorithms to learn the parameters of complex scientific simulation models. While primarily run on CPUs in High-Performance Compute clusters, these algorithms have been shown to scale in performance when developed to be run on massively parallel architectures such as GPUs. While parallelizing existing SBI algorithms provides us with performance gains, this might not be the most efficient way to utilize the achieved parallelism. This work proposes a new parallelism-aware adaptation of an existing SBI method, namely approximate Bayesian computation with Sequential Monte Carlo(ABC-SMC). This new adaptation is designed to utilize the parallelism not only for performance gain, but also toward qualitative benefits in the learnt parameters. The key idea is to replace the notion of a single ‘step-size’ hyperparameter, which governs how the state space of parameters is explored during learning, with step-sizes sampled from a tuned Beta distribution. This allows this new ABC-SMC algorithm to more efficiently explore the state-space of the parameters being learned. We test the effectiveness of the proposed algorithm to learn parameters for an epidemiology model running on a Tesla T4 GPU. Compared to the parallelized state-of-the-art SBI algorithm, we get similar quality results in <inline-formula><tex-math notation=""LaTeX"">$\sim 100$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>∼</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=""kulkarni-ieq1-3238045.gif""/></alternatives></inline-formula>x fewer simulations and observe <inline-formula><tex-math notation=""LaTeX"">$\sim 80$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>∼</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=""kulkarni-ieq2-3238045.gif""/></alternatives></inline-formula>x lower run-to-run variance across 10 independent trials."|IEEE Transactions on Parallel and Distributed Systems|2023|10.1109/TPDS.2023.3238045|S. Kulkarni, C. A. Moritz|0.5|2
978|Comprehensive Automated Routine Implementation, Validation, and Benchmark of the Anisotropic Force Field (AUA4) Using Python and GROMACS.|Molecular simulation users are sometimes discouraged from using specific molecular models because of the inconvenience of finding the force field parameters and preparing and validating the topology files. To facilitate this process and make the accurate anisotropic force field AUA4 available to molecular dynamics users, we have created and validated an automated topology and coordinate file creation routine for the GROMACS molecular simulation software. In the present work, we describe the AUA4, explain its particularities and how it was implemented, thoroughly validating the implementation, and for the first time, perform a molecular dynamics benchmark for this transferable force field. Several properties were computed, namely, liquid density, vapor pressure, and vaporization enthalpy by conducting explicit vapor-liquid interface simulations. The results evidence the correct implementation showing slight deviations from the parametrization studies. The benchmark shows the superior predictive capability of the AUA4 in recreating liquid density (RMSD equal to 17.0 kg/m3) and vaporization enthalpy (RMSD equal to 1.3 kJ/mol) compared to other transferable force fields. In addition, its superior computational time performance doubles or even triples compared to an all-atom force field such as the OPLS, depending on whether the workstation counts with GPU.|Journal of Physical Chemistry A|2023|10.1021/acs.jpca.2c08335|Luis Eduardo Castro Anaya, G. Orozco, Sergio Y Gómez|0.5|2
981|Multitask Scheduling of Computer Vision Workload on Edge Graphical Processing Units|The increasing urbanization in developing countries calls for more efficient and safer transportation systems. A key technique used to enhance such efficiency and/or safety is to utilize running of computer vision algorithms to identify obstructions that may come up, and notify vehicles in real-time. Such real-time detection and notification requires sufficient computation resources located logically and physically close to the cameras. While utilization of edge compute devices has been proposed in the literature, it is unclear how such devices with heterogeneous processing units can handle real-time detection while multi-tasking. In this work, we profile the performance of a few devices with embedded and desktop-quality GPUs, and show that the performance while multi-tasking can be modeled as a submodular function. We utilize this observation to model load-balancing of camera videos as an instance of a submodular welfare problem, and solve it using a greedy algorithm. Our extensive trace-driven simulations show that our technique outperforms the baseline by over 40%.|International Conference on Communication Systems and Networks|2023|10.1109/COMSNETS56262.2023.10041358|A. Banerjee, Saumya Jaipuria, N. Narendra, D. Garg, Paritosh Shukla, A. Bhattacharya|0.5|2
982|MOD2IR: High-Performance Code Generation for a Biophysically Detailed Neuronal Simulation DSL|Advances in computational capabilities and large volumes of experimental data have established computer simulations of brain tissue models as an important pillar in modern neuroscience. Alongside, a variety of domain specific languages (DSLs) have been developed to succinctly express properties of these models, ensure their portability to different platforms, and provide an abstraction that allows scientists to work in their comfort zone of mathematical equations, delegating concerns about performance optimizations to downstream compilers. One of the popular DSLs in modern neuroscience is the NEURON MODeling Language (NMODL). Until now, its compilation process has been split into first transpiling NMODL to C++ and then using a C++ toolchain to emit the efficient machine code. This approach has several drawbacks including the reliance on different programming models to target heterogeneous hardware, maintainability of multiple compiler back-ends and the lack of flexibility to use the domain information for C++ code optimization. To overcome these limitations, we present MOD2IR, a new open-source code generation pipeline for NMODL. MOD2IR leverages the LLVM toolchain to target multiple CPU and GPU hardware platforms. Generating LLVM IR allows the vector extensions of modern CPU architectures to be targeted directly, producing optimized SIMD code. Additionally, this gives MOD2IR significant potential for further optimizations based on the domain information available when LLVM IR code is generated. We present experiments showing that MOD2IR is able to produce on-par execution performance using a single compiler back-end implementation compared to code generated via state-of-the-art C++ compilers, and can even surpass them by up to 1.26×. Moreover, MOD2IR supports JIT-execution of NMODL, yielding both efficient code and an on-the-fly execution workflow.|International Conference on Compiler Construction|2023|10.1145/3578360.3580268|F. Schürmann, P. Kumbhar, Ioannis Magkanaris, G. Mitenkov, A. Donaldson, O. Awile|0.5|2
985|A Quantitative Logarithmic Transformation-Based Intrusion Detection System|Intrusion detection systems (IDS) play a vital role in protecting networks from malicious attacks. Modern IDS use machine-learning or deep-learning models to deal with the diversity of attacks that malicious users may employ. However, effective machine-learning methods incur a considerable cost in both the pretraining stage and the online detection process itself. Accordingly, this study proposes a quantitative logarithmic transformation-based intrusion detection system (QLT-IDS) that uses a straightforward statistical approach to analyze network behavior. Compared with machine-learning or deep-learning-based IDS methods, the proposed system requires neither a time-consuming and expensive data collection and training process, nor a GPU-included device to achieve a real-time detection performance. Furthermore, the system can deal not only with North-South attacks, but also East-West attacks, which pose a significant risk in real-world operations. The effectiveness of the proposed system is evaluated for both real-world campus network traffic and simulated traffic. The results confirm that QLT-IDS is able to detect a wide range of malicious attacks with a high precision, even under high down-sampling rate of the NetFlow records.|IEEE Access|2023|10.1109/ACCESS.2023.3248261|Blue Lan, C. Shieh, Ta-Chun Lo, Rico Wei, Heng-Yu Tang|0.5|2
994|Accelerating cluster dynamics simulation of fission gas behavior in nuclear fuel on deep computing unit–based heterogeneous architecture supercomputer|High fidelity simulation of fission gas behavior is able to help us understand and predict the performance of nuclear fuel under different irradiation conditions. Cluster dynamics (CD) is a mesoscale simulation method which is rapidly developed in nuclear fuel research area in recent years, and it can effectively describe the microdynamic behavior of fission gas in nuclear fuel; however, due to the huge cost of computation needed for CD model solution, the application scenario of CD has been limited. Thus, how to design the acceleration algorithm for the given computing resources to improve the computing efficiency and simulation scale has become a key problem of CD simulation. In this work, we present an accelerating cluster dynamics model based on the spatially dependent cluster dynamics model, combined with multi optimization methods on a DCU (deep computing unit)-based heterogeneous architecture supercomputer. The correctness of the model is verified by comparing with experimental data and Xolotl—a software of SciDAC program from the U.S. Department of Energy’s Office of Science. Furthermore, our model implementation has a better computing performance than Xolotl’s GPU version. Our code has gained great strong/weak scaling performance with more than 72.75%/84.07% parallel efficiency on 1024 compute nodes. This work developed a new efficient model for CD simulation of fission gas in nuclear fuel.|The international journal of high performance computing applications|2023|10.1177/10943420231162831|Dandan Chen, Changjun Hu, Shuai Ren, He Bai, Genshen Chu, Yuhan Zhu|0.5|2
995|HybriD-GM: A Framework for Quantum Computing Simulation Targeted to Hybrid Parallel Architectures|"This paper presents the HybriD-GM model conception, from modeling to consolidation. The D-GM environment is also extended, providing efficient parallel executions for quantum computing simulations, targeted to hybrid architectures considering the CPU and GPU integration. By managing projection operators over quantum structures, and exploring coalescing memory access patterns, the HybriD-GM model enables granularity control, optimizing hardware resources in distributed computations organized as tree data structures. In the HybriD-GM evaluation, simulations of Shor’s and Grover’s algorithms achieve significant performance improvements in comparison to the previous D-GM version, and also with other related works, for example, LIQUi|⟩ and ProjectQ simulators."|Entropy|2023|10.3390/e25030503|Bruno M. P. Moura, A. Avila, A. Cruz, S. X. D. Souza, R. Reiser, Giancarlo Lucca, A. Yamin, H. Santos|0.5|2
1017|A Novel GPU Acceleration Algorithm Based on CUDA and MPI for Ray Tracing Wireless Channel Modeling|With the improvement of hardware performance, graphics processing unit (GPU) has been applied to the acceleration of ray tracing (RT) wireless channel modeling. In this paper, a new RT acceleration algorithm based on message passing interface (MPI) and GPU called MPI fused with GPU tree algorithm (MGTree) is proposed. Moreover, the indoor conference room is modeled, and the forward algorithm shooting and bouncing ray (SBR) is used for RT. Considering the conventional reflection and diffraction, the three-dimensional (3D) RT acceleration algorithm based on the unified computing device architecture (CUDA) is used to optimize the conventional algorithm, which combines GPU with MPI to form multi-core distributed computing. Then field information is extracted at the receiving side and the wireless channel simulations are conducted. Through experiments, the new algorithm can effectively improve the upper limit of the acceleration ratio, support more rays, and alleviate the pressure of a single GPU kernel.|IEEE Wireless Communications and Networking Conference|2023|10.1109/WCNC55385.2023.10118847|Yinghua Wang, Chenghai Wang, Jie Huang, Jinxuan Chen|0.5|2
1018|Real-Time Sand Dune Simulation|We present a novel real-time method for simulating aeolian sand transport and dune propagation. Our method is a GPU-based extension of the Desertscapes Simulation sand propagation model to additionally capture echo dunes and obstacle interaction. We validate our method by comparing it against an existing study of echo dune evolution in a wind tunnel environment. Additionally, we demonstrate the significantly improved performance of our method via comparison to the existing, CPU-based method. Lastly, we validate our method by comparing it to a published study exploring the evolution of dunes in a bidirectional wind environment driven by an offline, cellular autonoma based method. We conclude that the presented method is a simple and helpful tool for users in multiple domains who wish to capture physically plausible desertscape evolution in real time.|Proceedings of the ACM on Computer Graphics and Interactive Techniques|2023|10.1145/3585510|J. Keyser, B. Taylor|0.5|2
1023|Immersed boundary method of two-phase flow based on DCU parallel acceleration|DCU (Deep Computing Unit) acceleration device is a kind of GPU-like device, which is developed by GPU technology authorized by AMD, so it is suitable to solve the parallel problems of many data elements in the same program. In this paper, a parallel immersed boundary method based on DCU acceleration device was studied to realize efficient two-phase flow simulation. The Jacobi multigrid method was used to solve the pressure Poisson equation in the simulations. Based on our serial code and combined with the hardware characteristics of DCU and HIP (Heterogeneous Interface for Portability) heterogeneous programming model, the calculation of pressure Poisson equation used in immersed boundary method was transplanted to DCU, and the parallel immersed boundary method based on multigrid method was realized. Three DCU parallel simulations were carried out: (1) solutions of Poisson equation; (2) two-phase flows of droplet deformation in shear flow and (3) bubble rising process in the static fluid. We tested them on Song Shan supercomputing platform. Compared with CPU version, the DCU parallel immersed boundary method has achieved the highest speed-up ratios of 16.888, 5.223 and 9.604 for resoling Poisson equation, drop deformation in shear flow and rising drop simulations, respectively.|Other Conferences|2023|10.1117/12.2681641|Ludi Sun, Yi Zhang, Haobo Hua, Qianqian Jin, Pu Han, Lin Han|0.5|2
1025|2D Simplified Wildfire Spreading Model in Python: From NumPy to CuPy|Wildfires are a problem of great interest in the world because of the damage they cause every year to forest, wild fauna and flora, an also threatening human lives, among others. There are some computational models for numerical simulations to address this phenomena, and a few of them have an open-source implementation. The goal of this work is to extend a CPU Python’s implementation of wildfire open-source framework developed in NumPy, to a GPU improved version using CuPy. The algorithm used is based on a numerical discretization of a wildfire spreading mathematical model described by a system of partial differential equations. Computational and mathematical components, numerical simulations and applications are described in details in the document. In addition, this work includes includes a brief performance comparison between both implementations, pointing out that we can achieve good execution times using the CuPy GPU implementation without spending enough programming effort.|CLEI Electronic Journal|2023|10.19153/cleiej.26.1.5|Claudio Torres, Daniel San Martín|0.5|2
1030|MCR toolkit: A GPU-based toolkit for multi-channel reconstruction of preclinical and clinical x-ray CT data.|BACKGROUND\nThe advancement of x-ray CT into the domains of photon counting spectral imaging and dynamic cardiac and perfusion imaging has created many new challenges and opportunities for clinicians and researchers. To address challenges such as dose constraints and scanning times while capitalizing on opportunities such as multi-contrast imaging and low-dose coronary angiography, these multi-channel imaging applications require a new generation of CT reconstruction tools. These new tools should exploit the relationships between imaging channels during reconstruction to set new image quality standards while serving as a platform for direct translation between the preclinical and clinical domains.\n\n\nPURPOSE\nWe outline and demonstrate a new Multi-Channel Reconstruction (MCR) Toolkit for GPU-based analytical and iterative reconstruction of preclinical and clinical multi-energy and dynamic x-ray CT data. To promote open science, open-source distribution of the Toolkit will coincide with the release of this publication (GPL v3; gitlab.oit.duke.edu/dpc18/mcr-toolkit-public).\n\n\nMETHODS\nThe MCR Toolkit source code is implemented in C/C++ and NVIDIA's CUDA GPU programming interface, with scripting support from MATLAB and Python. The Toolkit implements matched, separable footprint CT reconstruction operators for projection and backprojection in two geometries: planar, cone-beam CT (CBCT) and 3rd generation, cylindrical multi-detector row CT (MDCT). Analytical reconstruction is performed using filtered backprojection (FBP) for circular CBCT, weighted FBP (WFBP) for helical CBCT, and cone-parallel projection rebinning followed by WFBP for MDCT. Arbitrary combinations of energy and temporal channels are iteratively reconstructed under a generalized multi-channel signal model for joint reconstruction. We solve this generalized model algebraically using the split Bregman optimization method and the BiCGSTAB(l) linear solver interchangeably for both CBCT and MDCT data. Rank-sparse kernel regression (RSKR) and patch-based singular value thresholding (pSVT) are used to regularize the energy and time dimensions, respectively. Under a Gaussian noise model, regularization parameters are estimated automatically from the input data, dramatically reducing algorithm complexity for end users. Multi-GPU parallelization of the reconstruction operators is supported to manage reconstruction times.\n\n\nRESULTS\nDenoising with RSKR and pSVT and post-reconstruction material decomposition are illustrated with preclinical and clinical cardiac photon-counting (PC)CT data. A digital MOBY mouse phantom with cardiac motion is used to illustrate single energy (SE), multi-energy (ME), time resolved (TR), and combined multi-energy and time-resolved (METR) helical, CBCT reconstruction. A fixed set of projection data is used across all reconstruction cases to demonstrate the Toolkit's robustness to increasing data dimensionality. Identical reconstruction code is applied to in vivo cardiac PCCT data acquired in a mouse model of atherosclerosis (METR). Clinical cardiac CT reconstruction is illustrated using the XCAT phantom and the DukeSim CT simulator, while dual-source, dual-energy CT reconstruction is illustrated for data acquired with a Siemens Flash scanner. Benchmarking results with NVIDIA RTX 8000 GPU hardware demonstrate 61%-99% efficiency in scaling computation from one to four GPUs for these reconstruction problems.\n\n\nCONCLUSIONS\nThe MCR Toolkit provides a robust solution for temporal and spectral x-ray CT reconstruction problems and was built from the ground up to facilitate translation of CT research and development between preclinical and clinical applications.|Medical Physics (Lancaster)|2023|10.1002/mp.16532|C. Badea, D. Clark|0.5|2
1032|Accelerated Geophysical Inversion for Airborne Transient Electromagnetic Data Using GPU|In airborne geophysical exploration, the survey area can be extended to several hundred square miles resulting in massive recorded data. This makes the inversion of large-scale airborne transient electromagnetic (TEM) data computationally expensive by conventional CPU computing. Geophysical inversion commonly involves repeated regularized least squares, partial derivative and matrices inverses calculations. This paper proposes to use GPU for fast inversion computation. Schemes for partitioning the TEM data into blocks that tailor well with the GPU architecture are described. Simulation results show that the proposed GPU schemes are 3 to 4 magnitudes faster than the inversion software on the CPU.|International Conference on Industrial Technology|2023|10.1109/ICIT58465.2023.10143157|H. Tai, Nengyi Fu|0.5|2
1036|cudaMMC: GPU-enhanced multiscale Monte Carlo chromatin 3D modelling|Motivation Investigating the 3D structure of chromatin provides new insights into transcriptional regulation. With the advancements in 3C new-sequencing techniques such as ChiA-PET and Hi-C, there has been a substantial increase in the volume of collected data, necessitating faster algorithms for chromatin spatial modelling. This study presents the cudaMMC method, which utilises the Simulated Annealing Monte Carlo approach extended by GPU-accelerated computing to generate ensembles of chromatin 3D structures efficiently. Results The cudaMMC calculations demonstrate significantly faster performance and lower (better) model scores compared to our previous method on the same workstation. cudaMMC substantially reduces the computation time required for generating ensembles of large chromatin models, making it an invaluable tool for studying chromatin spatial conformation. Availability Open-source software and manual and sample data are freely available on https://github.com/SFGLab/cudaMMC Contact Dariusz.Plewczynski@pw.edu.pl|bioRxiv|2023|10.1093/bioinformatics/btad588|Damian Roszczyk, Krzysztof Kaczmarski, Michał Wlasnowolski, Dariusz M Plewczynski, Paweł Grabowski|0.5|2
1041|Hybrid programming and multiple GPUs implementation for Particle-In-Cell|Numerical modelling in fusion physics is crucial for studying fusion devices, space, and astrophysical systems. The plasma simulations of fusion devices demand a kinetic approach to handle extreme nonlinearities methods. One of the most used plasma kinetic simulation codes is the Particle-In-Cell (PIC). The HPC systems worldwide are getting more powerful with the combination of CPU, GPU, and other accelerators (e.g., FPGAs and Quantum Processors). Moreover, we can already notice that several exascale machines are operational worldwide; one typical example is the Frontier (Oak Ridge National Laboratory) exascale machine. In parallel, the same effort is being made for scientific algorithms to use robust HPC systems efficiently. Many programming frameworks (e.g., OpenACC, OpenMP offloading, and SYCL) mainly offer excellent support portability to the existing scientific codes to use the exascale HPC systems. This work demonstrates hybrid and multiple GPUs capabilities (or portability) for Simple Particle-In-Cell (SIMPIC) based on the PIC algorithm. First, we have implemented the hybrid (MPI+OpenMP) portability and multiple GPUs (multiple node GPU with the help of MPI) offloading portability. The first implementation gains a speed up to 40% compared to the plain MPI version, and the second implementation achieves up to 40% speedups compared to the hybrid (MPI+OpenMP) implementation.|International Conference on Communication, Computing & Security|2023|10.1109/ICCCS57501.2023.10150523|L. Kos, P. Bouvry, I. Vasileska, E. Krishnasamy|0.5|2
1049|HeROfake: Heterogeneous Resources Orchestration in a Serverless Cloud – An Application to Deepfake Detection|Serverless is a trending service model for cloud computing. It shifts a lot of the complexity from customers to service providers. However, current serverless platforms mostly consider the provider's infrastructure as homogeneous, as well as the users' requests. This limits possibilities for the provider to leverage heterogeneity in their infrastructure to improve function response time and reduce energy consumption. We propose a heterogeneity-aware serverless orchestrator for private clouds that consists of two components: the autoscaler allocates heterogeneous hardware resources (CPUs, GPUs, FPGAs) for function replicas, while the scheduler maps function executions to these replicas. Our objective is to guarantee function response time, while enabling the provider to reduce resource usage and energy consumption. This work considers a case study for a deepfake detection application relying on CNN inference. We devised a simulation environment that implements our model and a baseline Knative orchestrator, and evaluated both policies with regard to consolidation of tasks, energy consumption and SLA penalties. Experimental results show that our platform yields substantial gains for all those metrics, with an average of 35% less energy consumed for function executions while consolidating tasks on less than 40% of the infrastructure's nodes, and more than 60% less SLA violations.|IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing|2023|10.1109/CCGrid57682.2023.00024|Jalil Boukhobza, Laurent d'Orazio, Laurent Beaulieu, Olivier Weppe, Amine Kacete, Esther Bernard, Olivier Barais, S. Paquelet, Vincent Lannurien|0.5|2
1052|Research on the Construction of Financial Computing Model Based on BSDE Algorithm|Economic and social development has made financial engineering an increasingly important research area, and more and more financial problems cannot be solved directly by analytical formulas. In view of this, algorithms that apply computer technology to financial engineering have emerged. In this study, the Backward Stochastic Differential Equation (BSDE) algorithm is used to investigate and analyse the problem of option pricing calculation in finance. In the research process, GBSDE-Theta parallel algorithm composed of BSDE-Theta algorithm and GPU algorithm uses the new algorithm to establish a computing model in the financial engineering field, which applies to the calculation of enterprise option pricing. The research results show that compared with the basic algorithm, the actual option values of the option pricing data obtained by using the GBSDE-Theta parallel algorithm are more closely matched. The computational model can achieve a speedup ratio of about 230 times of the serial version with the number of time steps [Formula: see text] and the number of simulated paths 80,000. About the relative error of the GBSDE-Theta algorithm, there are 80 points within 3% and only 16 points over 3.00%, which is a relatively small error. The above results show that the financial computing system obtained in this study is highly feasible and effective, and can provide a new research idea for the progress and development of other computations in the financial field.|Journal of Information & Knowledge Management|2023|10.1142/s0219649223500296|Y. Cai|0.5|2
1055|A Practical Wave Optics Reflection Model for Hair and Fur|Traditional fiber scattering models, based on ray optics, are missing some important visual aspects of fiber appearance. Previous work [Xia et al. 2020] on wave scattering from ideal extrusions demonstrated that diffraction produces strong forward scattering and colorful effects that are missing from ray-based models. However, that work was unable to include some important surface characteristics such as surface roughness and tilted cuticle scales, which are known to be important for fiber appearance. In this work, we take an important step to study wave effects from rough fibers with arbitrary 3D microgeometry. While the full-wave simulation of realistic 3D fibers remains intractable, we developed a 3D wave optics simulator based on a physical optics approximation, using a GPU-based hierarchical algorithm to greatly accelerate the calculation. It simulates surface reflection and diffractive scattering, which are present in all fibers and typically dominate for darkly pigmented fibers. The simulation provides a detailed picture of first order scattering, but it is not practical to use for production rendering as this would require tabulation per fiber geometry. To practically handle geometry variations in the scene, we propose a model based on wavelet noise, capturing the important statistical features in the simulation results that are relevant for rendering. Both our simulation and practical model show similar granular patterns to those observed in optical measurement. Our compact noise model can be easily combined with existing scattering models to render hair and fur of various colors, introducing visually important colorful glints that were missing from all previous models.|ACM Transactions on Graphics|2023|10.1145/3592446|E. Michielssen, O. Maury, Mengqi Xia, Steve Marschner, C. Hery, B. Walter|0.5|2
1066|PyPO: a Python package for Physical Optics|PyPO is a Python interface for end-to-end design, simulation, and analysis of (quasi-)optical reflector systems. It can model the forward and backward propagation of electromagnetic field distributions between multiple planar and (off-axis) quadric surfaces, as well as far-field propagation. Simulations are performed using either geometrical optics (GO) or the equivalent surface current approach, belonging to the field of physical optics (PO) (Balanis, 1989). The GO and PO calculations are performed using libraries written in C++ and CUDA, allowing for multi-threading and GPU acceleration. Common figures of merit, such as aperture efficiency and half-power beamwidth, can be calculated and used for quantitative analysis of the designed system. Input beam patterns can be selected from a range of models, such as Gaussian beams, point sources, and uniform current distributions. Custom beam patterns can also be imported to, for example, model the propagation of measured beam patterns through simulated optical systems. PyPO can be used through either a scripting-based approach, where simulations are defined in Python scripts, or through the graphical user interface (GUI). It only carries core dependencies on NumPy (Harris et al.,|Journal of Open Source Software|2023|10.21105/joss.05478|K. Karatsu, A. Endo, Arend Moerman, Maikel H. Gafaji|0.5|2
1069|PPChain: A Blockchain for Pandemic Prevention and Control Assisted by Federated Learning|Taking COVID-19 as an example, we know that a pandemic can have a huge impact on normal human life and the economy. Meanwhile, the population flow between countries and regions is the main factor affecting the changes in a pandemic, which is determined by the airline network. Therefore, realizing the overall control of airports is an effective way to control a pandemic. However, this is restricted by the differences in prevention and control policies in different areas and privacy issues, such as how a patient’s personal data from a medical center cannot be effectively combined with their passenger personal data. This prevents more precise airport control decisions from being made. To address this, this paper designed a novel data-sharing framework (i.e., PPChain) based on blockchain and federated learning. The experiment uses a CPU i7-12800HX and uses Docker to simulate multiple virtual nodes. The model is deployed to run on an NVIDIA GeForce GTX 3090Ti GPU. The experiment shows that the relationship between a pandemic and aircraft transport can be effectively explored by PPChain without sharing raw data. This approach does not require centralized trust and improves the security of the sharing process. The scheme can help formulate more scientific and rational prevention and control policies for the control of airports. Additionally, it can use aerial data to predict pandemics more accurately.|Bioengineering|2023|10.3390/bioengineering10080965|Yong Pan, Tianruo Cao, Honghui Chen, T. Hu, Jianming Zheng|0.5|2
1071|Libra: Contention-Aware GPU Thread Allocation for Data Parallel Training in High Speed Networks|Overlapping gradient communication with backward computation is a popular technique to reduce communication cost in the widely adopted data parallel S-SGD training. However, the resource contention between computation and All-Reduce communication in GPU-based training reduces the benefits of overlap. With GPU cluster network evolving from low bandwidth TCP to high speed networks, more GPU resources are required to efficiently utilize the bandwidth, making the contention more noticeable. Existing communication libraries fail to account for such contention when allocating GPU threads and have suboptimal performance. In this paper, we propose to mitigate the contention by balancing the overlapped computation and communication time. We formulate an optimization problem that decides the communication thread allocation to reduce overall backward time. We develop a dynamic programming based near-optimal solution and extend it to co-optimize thread allocation with tensor fusion. We conduct simulated study and real-world experiment using an 8-node GPU cluster with 50Gb RDMA network training four representative DNN models. Results show that our method reduces backward time by 10%-20% compared with Horovod-NCCL, by 6%-13% compared with tensor-fusion-optimization-only methods. Simulation shows that our method achieves the best scalability with a training speedup of 1.2x over the best-performing baseline as we scale up cluster size.|IEEE Conference on Computer Communications|2023|10.1109/INFOCOM53939.2023.10228922|Bo Jiang, Yunzhuo Liu, Tao Lin, Xinbing Wang, Cheng Zhou, Shi-Ming Zhao|0.5|2
1075|TREFU: An Online Error Detecting and Correcting Fault Tolerant GPGPU Architecture|General Purpose Graphics Processing Units (GPGPUs) are extensively used in high-performance applications/systems, whose execution times may vary from a few days to months. Many times, these systems are expected to provide high reliability and availability. On the other hand, the high-throughput GPGPUs are fabricated with the latest cutting-edge technology. The shrinking transistor feature size and aggressive voltage scaling resulted in increased susceptibility to soft errors. Hence, GPGPU execution results cannot be trusted. This necessitates the employment of error detection and correction methods for reliable results. To mitigate soft error effects in the GPGPU execution pipeline, we propose a fault-tolerant microarchitecture called Triple modular Redundant Execution with idle Functional Units (TREFU) to detect and correct errors online. The proposed method is transparent to the application software. A new microarchitecture structure, replay buffer, is introduced to store temporary operands and results and used as a checkpoint. On error detection, the data in the duplicate copy of the replay buffers are used for Triple Modular Redundant (TMR) execution and error correction. The effectiveness of TREFU is demonstrated through the ISPASS 2009 and RODINIA benchmarks. TREFU's performance and power overheads are evaluated for an error-free run and at various error rates of executed instructions ranging from 1 to 50K. The simulation results show that complete error detection and correction across all threads can be achieved with a mean performance overhead of 4%, an average power overhead of 4%, and a peak power overhead of 5%.|IEEE International Symposium on On-Line Testing and Robust System Design|2023|10.1109/IOLTS59296.2023.10224865|Varaprasad B.K.S.V.L., Virendra Singh, M. Reorda, Raghunandana K K|0.5|2
1081|ReRAM-based graph attention network with node-centric edge searching and hamming similarity|The graph attention network (GAT) has demonstrated its advantages via local attention mechanism but suffered from low energy and latency efficiency when implemented on conventional von-Neumann hardware. This work proposes and experimentally demonstrates an algorithm-hardware co-designed GAT that runs efficiently and reliably in ReRAM-based hardware. The neighborhood information is retrieved from trained node embeddings stored on crossbars in a single time step, and attention is implemented by efficient hashing and hamming similarity for higher robustness. Our scaled simulation based on the experimentally-validated model shows only 0.9% accuracy loss with over 35,500x energy improvement on the Cora dataset compared with GPU, and 1.1% accuracy improvement with 2× energy improvement compared with state-of-the-art ReRAM-based GNN accelerator.|Design Automation Conference|2023|10.1109/DAC56929.2023.10247735|Catherine Graves, Xia Sheng, Cong Xu, Can Li, Ruibin Mao|0.5|2
1109|Performance Evaluation of Heterogeneous GPU Programming Frameworks for Hemodynamic Simulations|Preparing for the deployment of large scientific and engineering codes on upcoming exascale systems with GPU-dense nodes is made challenging by the unprecedented diversity of device architectures and heterogeneous programming models. In this work, we evaluate the process of porting a massively parallel, fluid dynamics code written in CUDA to SYCL, HIP, and Kokkos with a range of backends, using a combination of automated tools and manual tuning. We use a proxy application along with a custom performance model to inform the results and identify additional optimization strategies. At scale performance of the programming model implementations are evaluated on pre-production GPU node architectures for Frontier and Aurora, as well as on current NVIDIA device-based systems Summit and Polaris. Real-world workloads representing 3D blood flow calculations in complex vasculature are assessed. Our analysis highlights critical trade-offs between code performance, portability, and development time.|SC Workshops|2023|10.1145/3624062.3624188|J. Insley, William Ladd, A. Randles, S. Rizzi, Aristotle X. Martin, Seyong Lee, Jeffrey S. Vetter, Geng Liu, John P. Gounley, V. Mateevitsi, Saumil Patel|0.5|2
1140|Towards Practical Cell-Free 6G Network Deployments: An Open-Source End-to-End Ray Tracing Simulator|The advent of 6G wireless communication marks a transformative era in technological connectivity, bringing forth challenges and opportunities alike. This paper unveils an innovative, open-source simulator, meticulously crafted for cell-free 6G wireless networks. This simulator is not just a tool but a gateway to the future, blending cutting-edge channel models with the simulation of both physical propagation effects and intricate system-level protocols. It stands at the forefront of technological advancement by integrating LIS and MIMO technologies, harnessing the power of the Unity game engine for efficient ray-tracing and GPU-accelerated computations. The unparalleled flexibility in scenario configuration, coupled with its unique ability to dynamically simulate interactions across network layers, establishes this simulator as an indispensable asset in pioneering&G systems' research and development.||2023|10.1109/ieeeconf59524.2023.10476998|Aleksei Fedorov, Emma Fitzgerald, L. V. D. Perre, William Tarneberg, Gilles Callebaut|0.5|2
1224|Controlling Complexity of Cerebral Cortex Simulations—I: CxSystem, a Flexible Cortical Simulation Framework|Simulation of the cerebral cortex requires a combination of extensive domain-specific knowledge and efficient software. However, when the complexity of the biological system is combined with that of the software, the likelihood of coding errors increases, which slows model adjustments. Moreover, few life scientists are familiar with software engineering and would benefit from simplicity in form of a high-level abstraction of the biological model. Our primary aim was to build a scalable cortical simulation framework for personal computers. We isolated an adjustable part of the domain-specific knowledge from the software. Next, we designed a framework that reads the model parameters from comma-separated value files and creates the necessary code for Brian2 model simulation. This separation allows rapid exploration of complex cortical circuits while decreasing the likelihood of coding errors and automatically using efficient hardware devices. Next, we tested the system on a simplified version of the neocortical microcircuit proposed by Markram and colleagues (2015). Our results indicate that the framework can efficiently perform simulations using Python, C++, and GPU devices. The most efficient device varied with computer hardware and the duration and scale of the simulated system. The speed of Brian2 was retained despite an overlying layer of software. However, the Python and C++ devices inherited the single core limitation of Brian2. The CxSystem framework supports exploration of complex models on personal computers and thus has the potential to facilitate research on cortical networks and systems.|Neural Computation|2019|10.1162/neco_a_01120|Henri Hokkanen, S. Vanni, Vafa Andalibi|0.5|2
1292|Accelerating Large-Scale Interconnection Network Simulation by Cellular Automata Concept|State-of-the-art parallel systems employ a huge number of computing nodes that are connected by an interconnection network. An interconnection network (ICN) plays an important role in a parallel system, since it is responsible to communication capability. In general, an ICN shows non-linear phenomena in its communication performance, most of them are caused by congestion. Thus, designing a large-scale parallel system requires sufficient discussions through repetitive simulation runs. This causes another problem in simulating large-scale systems within a reasonable cost. This paper shows a promising solution by introducing the cellular automata concept, which is originated in our prior work. Assuming 2D-torus topologies for simplification of discussion, this paper discusses fundamental design of router functions in terms of cellular automata, data structure of packets, alternative modeling of a router function, and miscellaneous optimization. The proposed models have a good affinity to GPGPU technology and, as representative speed-up results, the GPU-based simulator accelerates simulation upto about 1264 times from sequential execution on a single CPU. Furthermore, since the proposed models are applicable in the shared memory model, multithread implementation of the proposed methods achieve about 162 times speed-ups at the maximum. key words: interconnection networks, simulation, GPGPU, multithreading|IEICE Trans. Inf. Syst.|2019|10.1587/TRANSINF.2018EDP7131|T. Yokota, Takeshi Ohkawa, K. Ootsu|0.5|2
1294|Acceleration of unstructured implicit low-order finite-element earthquake simulation using OpenACC on Pascal GPUs|We accelerate CPU-based unstructured implicit low-order finite-element simulations by porting to a GPU-CPU heterogeneous compute environment using OpenACC. We modified performance-sensitive parts of the code, such as sparse matrix-vector multiplication and MPI communication, so that computations would be suitable for GPUs. Other parts of the earthquake simulation code are ported by directly inserting OpenACC directives into the CPU code. This porting approach enables high performance with relatively low development costs. When comparing eight K computer nodes and eight NVIDIA Pascal P100 GPUs, we achieve 20.8 times speedup for the 3 × 3 block Jacobi preconditioned conjugate gradient finite-element solver. We show the effectiveness of the proposed method through many-case crust-deformation simulations and a large-scale computation using finite element model with 109 degrees-of-freedom on a GPU cluster.|International Journal of High Performance Computing and Networking|2019|10.1504/IJHPCN.2019.10018083|T. Ichimura, M. Hori, Lalith Maddegedara, K. Fujita, Takuma Yamaguchi|0.5|2
1341|Novel many-core architecture design for real-time image processing|Based on the data-flow model and hardware reconfigurable technology,a polymorphic reconfigurable many-core processor architecture is presented for image processing.It is a scalable hierarchically organized parallel architecture,which is capable of supporting a dynamic mixture of multiple parallel computing models,and overcomes the inefficiency of traditional data-flow implementation by using distributed shared memory and neighbor interconnect architecture with hardware handshaking.From the beginning of the architecture design,based on the VC+ +,the integrated simulation platform(ISE)is developed for verifying the architecture and the performance of the instruction set.In addition,we also implement the proposed architecture on the FPGA.Experimental results show that the architecture can be used in many image processing applications,and achieve the throughput close to that of the ASIC and the performance better than that of the GPU.||2015|10.1109/prime.2015.7251376|Liu Zhen-ta|0.5|2
1376|Computational Hydrodynamics: How Portable and Scalable Are Heterogeneous Programming Paradigms?|New many-core era applications at the interface of math-ematics and computer science adopt modern parallel programming paradigms and expose parallelism through proper algorithms. We present new performance results for a novel massively parallel free surface wave model suitable for advanced simulations in arbitrary size Numerical Wave Tanks. The application has already been studied in a series of works (see References) and is demonstrated to ex-hibit excellent performance portability and scalability using hybrid MPI-OpenCL/CUDA. Furthermore, it can be executed on arbitrary heterogeneous multi-device sys-tem sizes from desktops to large HPC systems such as superclusters and in the cloud utilizing heterogeneous devices like multi-core CPUs, GPUs, and Xeon Phi co-processors. The numerical efﬁciency is evaluated on heterogeneous devices like multi-core CPUs, GPUs and Xeon Phi co-processors to test the performance with respect to both portability and scalability. This study contributes to investigating the potential of code acceleration for reducing turn-around times of in-dustrial CFD applications on heterogeneous hardware. large-scale of ocean|IEEE International Conference on Computational Science and Engineering|2015|10.1007/978-3-319-16549-3_45|W. Pawlak, S. L. Glimberg, A. Engsig-Karup|0.5|2
1419|Parallel Implementation of a Sequential Markov Chain in Monte Carlo Simulations of Physical Systems with Pairwise Interactions.|In molecular simulations performed by Markov Chain Monte Carlo (typically employing the Metropolis criterion), each state of a system is obtained by a small random modification of the previous state. Therefore, the process consists of an immense number of small, quick to calculate steps, which are inherently sequential and hence considered to be very hard to parallelise. Here, we present a novel protocol for efficient calculation of multiple sequential steps in parallel. To this end, we first precompute in parallel energy components of all states achievable in a sequence of steps. Then we select a single path through all achievable states, which is identical with the path obtained with the sequential algorithm. As an example, we carried out simulations of the TIP5P water model with the new protocol and compared results with those obtained using the standard Metropolis Monte Carlo scheme. The implementation on the Titan X (Pascal) graphic processor (GPU) architectures allows for a 30-fold speedup in comparison with a simulation on a single core of a multicore CPU. The protocol is general and not limited to the GPU; it can also be used on multicore CPU when the longest possible length of the single simulation is required.|Journal of Chemical Theory and Computation|2019|10.1021/acs.jctc.8b01168|M. Marchwiany, K. Dutka, P. Gumienny, D. Gront, W. Rudnicki, Szymon Migacz|0.5|2
1421|Full-System Simulation of Mobile CPU/GPU Platforms|Graphics Processing Units (GPUs) critically rely on a complex system software stack comprising kernel- and user-space drivers and Just-in-time (JIT) compilers. Yet, existing GPU simulators typically abstract away details of the software stack and GPU instruction set. Partly, this is because GPU vendors rarely release sufficient information about their latest GPU products. However, this is also due to the lack of an integrated CPU/GPU simulation framework, which is complete and powerful enough to drive the complex GPU software environment. This has led to a situation where research on GPU architectures and compilers is largely based on outdated or greatly simplified architectures and software stacks, undermining the validity of the generated results. In this paper we develop a full-system system simulation environment for a mobile platform, which enables users to run a complete and unmodified software stack for a state-of-the-art mobile Arm CPU and Mali-G71 GPU powered device. We validate our simulator against a hardware implementation and Arm's stand-alone GPU simulator, achieving 100% architectural accuracy across all available toolchains. We demonstrate the capability of our GPU simulation framework by optimizing an advanced Computer Vision application using simulated statistics unavailable with other simulation approaches or physical GPU implementations. We demonstrate that performance optimizations for desktop GPUs trigger bottlenecks on mobile GPUs, and show the importance of efficient memory use.|IEEE International Symposium on Performance Analysis of Systems and Software|2019|10.1109/ISPASS.2019.00015|T. Spink, M. O’Boyle, Björn Franke, Bruno Bodin, Harry Wagstaff, Kuba Kaszyk, Henrik Uhrenholt|0.5|2
1446|High Resolution Numerical Modelling of In-Vehicle Mobile Calls|The scenario of making a mobile phone call inside a vehicle is numerically modeled using a high resolution FDTD gridding; solved by a 3-D full-wave electromagnetic simulation software on a high performance workstation machine with Tesla GPU accelerator. The simulated mobile call inside a vehicle involved; a high resolution human head with shoulder phantom (VH_Head_Model 9), high resolution realistic hand model (posable right-hand), realistic mobile phone handset CAD-models with different antenna types and a vehicle CAD-model (big-sedan Camaro-brand). The impact of the in-vehicle mobile calls on the electromagnetic interaction of the handset antenna and human is anticipated at different operating frequencies, i.e., GSM-900 and GSM-1800, and different handset positions with respect to head, i.e., cheek and tilt-position. A significant degradation in the handset antenna performance was observed due to the presence of the vehicle body while making a mobile call inside. Nevertheless, no change in the peak spatial-average specific absorption rate in head tissues was detected while making a mobile call inside vehicle, as compared with the specific absorption rate due to the mobile call in free-space.||2015|10.5923/j.ijea.20150501.05|S. Yahya, Yazen A. Khalil|0.5|2
1449|New impressive capabilities of SE-workbench for EO/IR real-time rendering of animated scenarios including flares|To provide technical assessments of EO/IR flares and self-protection systems for aircraft, DGA Information superiority resorts to synthetic image generation to model the operational battlefield of an aircraft, as viewed by EO/IR threats. For this purpose, it completed the SE-Workbench suite from OKTAL-SE with functionalities to predict a realistic aircraft IR signature and is yet integrating the real-time EO/IR rendering engine of SE-Workbench called SE-FAST-IR. This engine is a set of physics-based software and libraries that allows preparing and visualizing a 3D scene for the EO/IR domain. It takes advantage of recent advances in GPU computing techniques. The recent past evolutions that have been performed concern mainly the realistic and physical rendering of reflections, the rendering of both radiative and thermal shadows, the use of procedural techniques for the managing and the rendering of very large terrains, the implementation of Image- Based Rendering for dynamic interpolation of plume static signatures and lastly for aircraft the dynamic interpolation of thermal states. The next step is the representation of the spectral, directional, spatial and temporal signature of flares by Lacroix Defense using OKTAL-SE technology. This representation is prepared from experimental data acquired during windblast tests and high speed track tests. It is based on particle system mechanisms to model the different components of a flare. The validation of a flare model will comprise a simulation of real trials and a comparison of simulation outputs to experimental results concerning the flare signature and above all the behavior of the stimulated threat.|SPIE Security + Defence|2015|10.1117/12.2195092|Alain Le Goff, T. Cathala, J. Latger|0.5|2
1457|New GPU computing algorithm for wind load uncertainty analysis on high-rise systems|In recent years, the Graphics Processing Unit (GPU) has become a competitive computing technology in comparison with the standard Central Processing Unit (CPU) technology due to reduced unit cost, energy and computing time. This paper describes the derivation and implementation of GPU-based algorithms for the analysis of wind loading uncertainty on high-rise systems, in line with the research field of probability-based wind engineering. The study begins by presenting an application of the GPU technology to basic linear algebra problems to demonstrate advantages and limitations. Subsequently, Monte-Carlo integration and synthetic generation of wind turbulence are examined. Finally, the GPU architecture is used for the dynamic analysis of three high-rise structural systems under uncertain wind loads. In the first example the fragility analysis of a single degree-of-freedom structure is illustrated. Since fragility analysis employs sampling-based Monte Carlo simulation, it is feasible to distribute the evaluation of different random parameters among different GPU threads and to compute the results in parallel. In the second case the fragility analysis is carried out on a continuum structure, i.e., a tall building, in which double integration is required to evaluate the generalized turbulent wind load and the dynamic response in the frequency domain. The third example examines the computation of the generalized coupled wind load and response on a tall building in both along-wind and cross-wind directions. It is concluded that the GPU can perform computational tasks on average 10 times faster than the CPU.||2015|10.12989/WAS.2015.21.5.461|Cui Wei, Caracoglia Luca|0.5|2
1473|The many roads to the simulation of reaction systems|Reaction systems are a computational model inspired by the bio-chemical reactions that happen inside biological cells. They have been and currently are studied for their many nice theoretical properties. They are also a useful modeling tool for biochemical systems, but in order to be able to employ them effectively in the field the presence of efficient and widely available simulators is essential. Here we explore three different algorithms and implementations of the simulation, comparing them to the current state of the art. We also show that we can obtain performances comparable to GPU-based simulations on real-world systems by using a carefully tuned CPU-based simulator.|Fundamenta Informaticae|2019|10.3233/FI-2020-1878|A. Leporati, L. Manzoni, A. Porreca, C. Ferretti|0.5|2
1475|Performance analysis of parallel gravitational N-body codes on large GPU clusters|We compare the performance of two very different parallel gravitational N-body codes for astrophysical simulations on large Graphics Processing Unit (GPU) clusters, both of which are pioneers in their own fields as well as on certain mutual scales - NBODY6++ and Bonsai. We carry out benchmarks of the two codes by analyzing their performance, accuracy and efficiency through the modeling of structure decomposition and timing measurements. We find that both codes are heavily optimized to leverage the computational potential of GPUs as their performance has approached half of the maximum single precision performance of the underlying GPU cards. With such performance we predict that a speed-up of 200 – 300 can be achieved when up to 1k processors and GPUs are employed simultaneously. We discuss the quantitative information about comparisons of the two codes, finding that in the same cases Bonsai adopts larger time steps as well as larger relative energy errors than NBODY6++, typically ranging from 10 – 50 times larger, depending on the chosen parameters of the codes. Although the two codes are built for different astrophysical applications, in specified conditions they may overlap in performance at certain physical scales, thus allowing the user to choose either one by fine-tuning parameters accordingly.||2015|10.1088/1674-4527/16/1/011|P. Berczik, R. Spurzem, Siyi Huang|0.5|2
1554|Locality properties of 3D data orderings with application to parallel molecular dynamics simulations|Application performance on graphical processing units (GPUs), in terms of execution speed and memory usage, depends on the efficient use of hierarchical memory. It is expected that enhancing data locality in molecular dynamic simulations will lower the cost of data movement across the GPU memory hierarchy. The work presented in this article analyses the spatial data locality and data reuse characteristics for row-major, Hilbert and Morton orderings and the impact these have on the performance of molecular dynamics simulations. A simple cache model is presented, and this is found to give results that are consistent with the timing results for the particle force computation obtained on NVidia GeForce GTX960 and Tesla P100 GPUs. Further analysis of the observed memory use, in terms of cache hits and the number of memory transactions, provides a more detailed explanation of execution behaviour for the different orderings. To the best of our knowledge, this is the first study to investigate memory analysis and data locality issues for molecular dynamics simulations of Lennard-Jones fluids on NVidia’s Maxwell and Tesla architectures.|The international journal of high performance computing applications|2019|10.1177/1094342019846282|D. Walker, Ibrahim Al-Kharusi|0.5|2
1589|Database for Research Projects to Solve the Inverse Heat Conduction Problem|To achieve the optimal performance of an object to be heat treated, it is necessary to know the value of the Heat Transfer Coefficient (HTC) describing the amount of heat exchange between the work piece and the cooling medium. The prediction of the HTC is a typical Inverse Heat Transfer Problem (IHCP), which cannot be solved by direct numerical methods. Numerous techniques are used to solve the IHCP based on heuristic search algorithms having very high computational demand. As another approach, it would be possible to use machine-learning methods for the same purpose, which are capable of giving prompt estimations about the main characteristics of the HTC function. As known, a key requirement for all successful machine-learning projects is the availability of high quality training data. In this case, the amount of real-world measurements is far from satisfactory because of the high cost of these tests. As an alternative, it is possible to generate the necessary databases using simulations. This paper presents a novel model for random HTC function generation based on control points and additional parameters defining the shape of curve segments. As an additional step, a GPU accelerated finite-element method was used to simulate the cooling process resulting in the required temporary data records. These datasets make it possible for researchers to develop and test their IHCP solver algorithms.|International Conference on Data Technologies and Applications|2019|10.3390/DATA4030090|I. Felde, S. Szénási|0.5|2
1617|Modeling, Prediction and Optimization of Energy Consumption of MPI Applications using SimGrid. (Modélisation, prédiction et optimisation de la consommation énergétique d'applications MPI à l'aide de SimGrid)|"The High-Performance Computing (HPC) community is currently undergoingdisruptive technology changes in almost all fields, including a switch towardsmassive parallelism with several thousand compute cores on a single GPU oraccelerator and new, complex networks. Powering a massively parallel machinebecomesThe energy consumption of these machines will continue to grow in the future,making energy one of the principal cost factors of machine ownership. This explainswhy even the classic metric ""flop/s"", generally used to evaluate HPC applicationsand machines, is widely regarded as to be replaced by an energy-centric metric""flop/watt"".One approach to predict energy consumption is through simulation, however, a pre-cise performance prediction is crucial to estimate the energy faithfully. In this thesis,we contribute to the performance and energy prediction of HPC architectures. Wepropose an energy model which we have implemented in the open source SimGridsimulator. We validate this model by carefully and systematically comparing itwith real experiments. We leverage this contribution to both evaluate existingand propose new DVFS governors that are part*icularly designed to suit the HPCcontext."||2019|10.1109/cluster.2019.8891011|F. Heinrich|0.5|2
1618|CUDA offloading for energy‐efficient and high‐frame‐rate simulations using tablets|The multiple sensors and touch capabilities of mobile devices are defining new methods of computer interaction. However, the computing power of such devices is not currently sufficient for new applications that require compute‐intensive applications. Using graphics processing units (GPUs) for general‐purpose computing with GPU programming models such as Compute Unified Device Architecture (CUDA) has been proved to accelerate simulations in supercomputers. Although, CUDA‐capable chips such as the Tegra K1 have been released on tablets can accelerate computer simulations, their absolute computing power and performance per watt are not comparable with ordinary GPUs. In this paper, we analyze a heterogeneous system composed of both of a tablet (client) and notebook with a low‐power GPU (server). Intensive computations on a tablet device are offloaded to a notebook GPU using the rCUDA middleware. Molecular dynamics (MD) simulations are performed using our test system, and the computing speed and performance per watt are reported. Implementing dynamic parallelism (DP) reduced the latency, doubling the total frames per second in some cases. Our system achieves better computational performance, and higher performance per watt than a tablet powered by a CUDA‐capable GPU. We achieved 21.7 Gflops/W by combining multiple client tablets and server, compared with 21.3 Gflops/W from the server itself.|Concurrency and Computation|2019|10.1002/cpe.5488|Edgar Josafat Martinez-Noriega, S. Yazaki, T. Narumi|0.5|2
253|Fast simulated annealing and adaptive Monte Carlo sampling based parameter optimization for dense optical-flow deformable image registration of 4DCT lung anatomy|Deformable image registration (DIR) is an important step in radiotherapy treatment planning. An optimal input registration parameter set is critical to achieve the best registration performance with the specific algorithm. Methods In this paper, we investigated a parameter optimization strategy for Optical-flow based DIR of the 4DCT lung anatomy. A novel fast simulated annealing with adaptive Monte Carlo sampling algorithm (FSA-AMC) was investigated for solving the complex non-convex parameter optimization problem. The metric for registration error for a given parameter set was computed using landmark-based mean target registration error (mTRE) between a given volumetric image pair. To reduce the computational time in the parameter optimization process, a GPU based 3D dense optical-flow algorithm was employed for registering the lung volumes. Numerical analyses on the parameter optimization for the DIR were performed using 4DCT datasets generated with breathing motion models and open-source 4DCT datasets. Results showed that the proposed method efficiently estimated the optimum parameters for optical-flow and closely matched the best registration parameters obtained using an exhaustive parameter search method.|SPIE Medical Imaging|2016|10.1117/12.2217194|Y. Min, P. Kupelian, David H. Thomas, A. Santhanam, J. Neylon, T. Dou|0.4444444444444444|2
274|Modeling energy consumption of parallel applications|The paper presents modeling and simulation of energy consumption of two types of parallel applications: geometric Single Program Multiple Data (SPMD) and divide-and-conquer (DAC). Simulation is performed in a new MERPSYS (Modeling Efficiency, Reliability and Power consumption of multilevel parallel HPC SYStems using CPUs and GPUs) environment. Model of an application uses the Java language with extensions representing message exchange between processes working in parallel. Simulation is performed by running threads representing distinct process codes of an application, with consideration of process counts. Instead of running time consuming calculations, their times are simulated using functions representing computational time dependent on input data sizes. The simulator considers performance and power consumption values for compute devices stored in its database. We performed verification of running the two applications on up to 512 and 1024 processes respectively on a large cluster from Academic Computer Center in Gdansk demonstrating a high degree of accuracy between simulated and measured results.|Conference on Computer Science and Information Systems|2016|10.15439/2016F308|P. Rosciszewski, P. Czarnul, J. Proficz, Jaroslaw Kuchta|0.4444444444444444|2
419|Highly parallel implementation of forest fire propagation models on the GPU|Forest fire simulation is a challenging, complex problem which requires large amounts of data to be processed for an accurate simulation. This paper implements Rothermel's fire spread equations on three different spread methodologies. Both a sequential implementation of each spread methodology along with a parallel implementation are presented in this paper. The parallel version is implemented on the Graphics Processing Unit (GPU) using NVIDIA's CUDA programming language. The parallel spread methods achieved runtimes in those ranges realistic for fighting a real-time forest fire. The GPU implementation achieved faster running times in each of the spread methodologies, ranging from 64x to 229x faster than the sequential implementation.|International Symposium on High Performance Computing Systems and Applications|2016|10.1109/HPCSim.2016.7568432|Jessica Smith, F. Harris, Lee Barfed, S. Dascalu|0.4444444444444444|2
420|Designing and Enabling Simulation of Real-World GPU Network Applications with ns-3 and DCE|The ability to execute the original source code for network protocols and applications within a network simulation environment frees the simulation modeler from the time consuming task of having to create, test and debug models representing these applications. This work extends the functionality of the Direct Code Execution (DCE) framework of ns-3 by incorporating the ability to call NVIDIA CUDA kernels from within simulated ns-3 nodes. This new functionality allows researchers to simulate large scale GPU applications in the realm of new and more flexible paradigms such as software-defined networking. Along with presenting this new functionality, this paper examines the different options available within the framework for communicating between the simulated nodes and the GPU. Each implementation is tested with multiple example CUDA kernels to demonstrate how they perform.|IEEE/ACM International Symposium on Modeling, Analysis, and Simulation On Computer and Telecommunication Systems|2016|10.1109/MASCOTS.2016.12|G. Riley, M. Loper, Jared S. Ivey, B. Swenson|0.4444444444444444|2
436|Controlling swarms of medical nanorobots using CPPSO on a GPU|Nanotechnology has the potential to revolutionize our lives and to provide technological solutions to our problems in energy, the environment and medicine. This paper describes a swarm intelligence-based control mechanism for medical nanorobots that operates as artificial platelets to search for wounds within the human body. We present a coloured perceptive particle swarm (CPPSO) algorithm to control the movement of nanorobots in self-assembly. To predict emergent nanorobot behaviors, we designed a parallel simulator that models how nanorobots interact with each other and the environment. We will show that due to their implicitly parallel structure, swarm intelligence algorithms can benefit from GPU-based implementations. The algorithm is implemented with CUDA. With the GPU-based implementation adopted here, we find that CPPSO is faster than a PPSO implementation.|International Symposium on High Performance Computing Systems and Applications|2016|10.1109/HPCSim.2016.7568316|G. Spezzano, Davide Ceraso|0.4444444444444444|2
226|Nuclear Reactor Simulation on OpenCL FPGA: a Case Study of RSBench|Field-programmable gate arrays (FPGAs) are becoming a promising choice as a heterogeneous computing component for scientific computing when floating-point optimized architectures are added to the current FPGAs. The emerging high-level synthesis tools such as the Intel OpenCL SDK for FPGA highlight a streamlined design flow to facilitate the use of FPGAs in scientific computing. Investigating the characteristics of supercomputing applications, such as nuclear reactor simulation, with the emerging HLS development flow is important for researchers to evaluate and adopt FPGA-based heterogeneous programming models in research facilities and laboratories. In this paper, we evaluate the OpenCL-based FPGA design of a nuclear reactor simulation application RSBench. We describe the OpenCL implementations and optimization methods on an Intel Arria10-based FPGA platform. Compared with the naïve OpenCL kernel, the optimizations of the kernel increase the performance by a factor of 295 on the FPGA. Compared with an Intel Xeon 16-core CPU and an Nvidia K80 GPU, the performance per watt on the FPGA is 3.59 X better than the CPU and 5.8X lower than the GPU.|International Workshop on OpenCL|2018|10.1145/3204919.3204921|Zheming Jin, H. Finkel|0.42857142857142855|2
306|Multi-level timing simulation on GPUs|Timing-accurate simulation of circuits is an important task in design validation of modern nano-scale CMOS circuits. With shrinking technology nodes, detailed simulation models down to transistor level have to be considered. While conventional simulation at logic level lacks the ability to accurately model timing behavior for complex cells, more accurate simulation at lower abstraction levels becomes computationally expensive for larger designs. This work presents the first parallel multi-level waveform-accurate timing simulation approach on graphics processing units (GPUs). The simulation uses logic and switch level abstraction concurrently, thus allowing to combine their advantages by trading off speed and accuracy. The abstraction can be lowered in arbitrary regions of interest to locally increase the accuracy. Waveform transformations allow for transparent switching between the abstraction levels. With the utilization of GPUs and thoughtful unification of algorithms and data structures, a fast and versatile high-throughput multi-level simulation is obtained that is scalable for millions of cells while achieving runtime savings of up to 89% compared to full simulation at switch level.|Asia and South Pacific Design Automation Conference|2018|10.1109/ASPDAC.2018.8297368|M. Kochte, H. Wunderlich, E. Schneider|0.42857142857142855|2
333|CWLP: coordinated warp scheduling and locality-protected cache allocation on GPUs|As we approach the exascale era in supercomputing, designing a balanced computer system with a powerful computing ability and low power requirements has becoming increasingly important. The graphics processing unit (GPU) is an accelerator used widely in most of recent supercomputers. It adopts a large number of threads to hide a long latency with a high energy efficiency. In contrast to their powerful computing ability, GPUs have only a few megabytes of fast on-chip memory storage per streaming multiprocessor (SM). The GPU cache is inefficient due to a mismatch between the throughput-oriented execution model and cache hierarchy design. At the same time, current GPUs fail to handle burst-mode long-access latency due to GPU’s poor warp scheduling method. Thus, benefits of GPU’s high computing ability are reduced dramatically by the poor cache management and warp scheduling methods, which limit the system performance and energy efficiency. In this paper, we put forward a coordinated warp scheduling and locality-protected (CWLP) cache allocation scheme to make full use of data locality and hide latency. We first present a locality-protected cache allocation method based on the instruction program counter (LPC) to promote cache performance. Specifically, we use a PC-based locality detector to collect the reuse information of each cache line and employ a prioritised cache allocation unit (PCAU) which coordinates the data reuse information with the time-stamp information to evict the lines with the least reuse possibility. Moreover, the locality information is used by the warp scheduler to create an intelligent warp reordering scheme to capture locality and hide latency. Simulation results show that CWLP provides a speedup up to 19.8% and an average improvement of 8.8% over the baseline methods.|Frontiers of Information Technology & Electronic Engineering|2018|10.1631/FITEE.1700059|Chuan Tang, Cang Liu, Yang Zhang, Zuocheng Xing|0.42857142857142855|2
1202|Architecture for Modular Microsimulation of Real Estate Markets and Transportation|Integrating land use, travel demand, and traffic models represents a gold standard for regional planning, but is rarely achieved in a meaningful way, especially at the scale of disaggregate data. In this paper, we present a new architecture for modular microsimulation of urban land use, travel demand, and traffic assignment. UrbanSim is an open-source microsimulation platform used by metropolitan planning organizations worldwide for modeling the growth and development of cities over long (~30 year) time horizons. ActivitySim is an agent-based modeling platform that produces synthetic origin-destination travel demand data, developed from the UrbanSim model and software framework. For traffic assignment, we have integrated two approaches. The first is a static user equilibrium approach that is used as a benchmark. The second is a traffic microsimulation approach that we have highly parallelized to run on a GPU in order to enable full-model microsimulation of agents through the entire modeling workflow. This paper introduces this research agenda, describes this project's achievements so far in developing this modular platform, and outlines further research.|arXiv.org|2018|10.31235/osf.io/74zaw|G. Boeing, Samuel M. Maurer, Daniel G. Aliaga, I. Garcia-Dorado, Emily Porter, P. Waddell, Max Gardner|0.42857142857142855|2
1216|Interactive Two-Way Shape Design of Elastic Bodies|We present a novel system for interactive elastic shape design in both forward and inverse fashions. Using this system, the user can choose to edit the rest shape or the quasistatic shape of an elastic solid, and obtain the other shape that matches under the quasistatic equilibrium condition at the same time. The development of this system is based on the discovery that inverse quasistatic simulation can be immediately solved by Newton's method with a direct solver. To implement our simulator, we propose a Jacobian matrix evaluation scheme for the inverse elastic problem and we present step length and matrix evaluation techniques that improve the simulation performance. While our simulator is efficient, it is still not fast enough for the system to generate the result in real time. Our solution is a shape initialization method using the recent projective dynamics technique. Shape initialization not only works as a fast preview function during the user editing process, but also speeds up the convergence of quasistatic or inverse quasistatic simulation afterwards. The use of a heterogeneous algorithm structure allows the system to further reduce its preview cost, by utilizing the power of both the CPU and the GPU. Our experiment demonstrates that the whole system is fast, robust, and convenient for the designer to use in both forward and inverse elastic shape design. It can handle a variety of nonlinear elastic material models, and its runtime performance has space for more improvement.|PACMCGIT|2018|10.1145/3203196|Longhua Wu, Huamin Wang, Rajaditya Mukherjee|0.42857142857142855|2
1280|On the implementation of multilevel Monte Carlo simulation of the stochastic volatility and interest rate model using multi-GPU clusters|Abstract We explore different methods of solving systems of stochastic differential equations by first implementing the Euler–Maruyama and Milstein methods with a Monte Carlo simulation on a CPU. The performance of the methods is significantly improved through the recently developed antithetic multilevel Monte Carlo estimator, which yields a computation complexity of 𝒪 ⁢ ( ϵ - 2 ) {\mathcal{O}(\epsilon^{-2})} root-mean-square error and does so without the approximation of Lévy areas. Further improvements in performance are gained by moving the algorithms to a GPU - first on a single device and then on a multi-GPU cluster. Our GPU implementation of the antithetic multilevel Monte Carlo displays a major speedup in computation when compared with many commonly used approaches in the literature. While our work is focused on the simulation of the stochastic volatility and interest rate model, it is easily extendable to other stochastic systems, and it is of particular interest to those with non-diagonal, non-commutative noise.|Monte Carlo Methods Appl.|2018|10.1515/mcma-2018-2025|Zane Colgin, Harold A. Lay, A. Khaliq, Viktor Reshniak|0.42857142857142855|2
1403|Performance and Power Prediction of Compute Accelerators Using Machine Learning|"Author(s): O'Neal, Kenneth | Advisor(s): Brisk, Philip | Abstract: CPUs and dedicated accelerators (namely GPUs and FPGAs) continue to grow increasingly large and complex to support todays demanding power and performance requirements. Designers are tasked with evaluating the performance and power of increasingly large design spaces during pre-silicon design. Validating and evaluating the performance during the pre-silicon stage catches performance and device issues in advance of fabrication. This reduces time-to-market by reducing the bugs that must be found and fixed after manufacturing, or after synthesizing an FPGA accelerator using high-level synthesis tools.Cycle-accurate simulators are integral to architectural design and are often the only tool at computer architects’ disposal to evaluate workloads on architectural design points in advance of manufacturing. To enable optimization of the architecture without the exorbitant cost of repeatedly fabricating designs, a high-level of simulator precision is required. As such, the simulators are compute intensive, limiting the number of simulations, and thus workloads or design points that can be evaluated before manufacturing. Historically the computational cost of simulation is absorbed, which slows down time to market and increases the cost of development.Machine Learning and statistical prediction models have emerged as viable tools to avoid repeated cycle accurate simulations by accurately predicting the metrics generated by cycle-accurate simulators. After one-time model training, the predictive models can then be used in lieu of simulation. Here I will present my research into the development of machine learning frameworks for GPU and FPGA architectures, which leverage cross-architecture predictive statistical modeling to significantly reduce the time required to evaluate workloads and architectural design points."||2018|10.3758/s13428-018-1127-3|Kenneth O'Neal|0.42857142857142855|2
1538|FRFB: Integrate Receptive Field Block Into Feature Fusion Net for Single Shot Multibox Detector|SSD (Single Shot Multibox Detector) is one of the best object detection algorithms with both high accuracy and fast speed. FSSD (Feature Fusion Single Shot Multibox Detector) proposed feature fusion module which can improve the performance significantly. RFB Net(Receptive Field Block Net for Accurate and Fast Object Detection) proposed RFB module to simulate Receptive Fields (RFs) in human visual systems and gain higher accuracy. In this paper, we proposed FRFB Net (Integrate Receptive Field Block Feature into Fusion Net for Single Shot Multibox Detector), an enhanced FSSD with a RFB module,which not only fully utilize the pyramidal features, but also change the RFs of the fused feature map. To make the model more robust,we use Gaussian Blur to process training images,in addition to use the data augmentation in SSD.On the Pascal VOC 2007 test, our network can achieve 79.6 mAP with the input size $300\times 300$ using a single Nvidia 1080 GPU with any bells and whistles. In addition, our result on COCO is also better than FSSD, achieves 2.7mAP improvement compared to FSSD. Our FRFBNet outperforms a lot of state-of-the-art object detection algorithms in accuracy and speed.|International Conference on Semantics, Knowledge and Grid|2018|10.1109/SKG.2018.00032|Jiong Mu, Yu Zhu, Haibo Pu, Baiyi Shu|0.42857142857142855|2
1615|Computational methods for, and applications of, high-performance computing in flood simulation|In England alone, more than 5.2 million properties are at risk of flooding. Decisions regarding future investment in defences, and operational priorities during major incidents are made with the assistance of numerical modelling, relating observations or probabilities for rainfall intensities and durations, to the likely extent of flooding. These models typically solve the shallow water equations (SWEs), or simplifications thereof, which can reproduce flooding from pluvial, fluvial, tidal, or catastrophic failure of dams or defences, with appropriate numerical schemes. Increased availability and ubiquity of data for use in flood models is an opportunity for improved accuracy, but computational power has long limited SWE models. This thesis presents novel computational methods to perform flood simulation using SWE models with graphics processing units (GPUs) and multi-core processors, thereby significantly reducing simulation runtime. Further improvements are made by domain decomposition, with an approach allowing numerous computer systems with multiple GPU processing devices to work together for a single simulation. Simulation accuracy is not inhibited by these methods, provided 64-bit floating-point arithmetic is used. The methods are applied successfully to flooding which is fluvial in Carlisle, pluvial in Newcastle upon Tyne, defence failure in Thamesmead, and dam failure in Malpasset. Firstand second-order numerical schemes are provided, with the latter found necessary in some tests. Sensitivity of model results to spatial resolution, and parameterisations, is thoroughly evaluated. Grid refinement improves simulation accuracy, but during high-velocity events also increases sensitivity; failure to capture topographic complexity with the spatial grid risks underestimating flood extent. No discernible improvement in results was found by refinement beyond 2m. Reliable data is scarce for short-lived pluvial flood events. Social media is considered as a potential new data source, with a new framework providing fruitful results. Future application of these methods in real-time flood forecasting systems is recommended.||2018|10.1109/hpec.2018.8547535|L. Smith|0.42857142857142855|2
196|GPU implementation for real-time hyperspectral anomaly detection|Hyperspectral anomaly detection which has been widely used to find targets on a timely basis requires high computing performance. In this paper we further study the real-time processing of anomaly target detection algorithm (RT-CR-RXD) and propose a new implementation on graphics processing units (GPUs). In the implementation, a real-time process of target detection is simulated, where the pixel data can be sent into detector firstly after collected and then the detector gives result for RT-CR-RXD immediately. And we have taken advantage of graphics processing units (GPUs) in parallel to accelerate the complex calculation. We achieve RT-CT-RXD on the parallel structure and it can be easily further introduced into embedded systems. The presented developments are tested in different scenarios (synthetic and real hyperspectral data) using two different GPU architectures by NVIDIA: GeForce GTX750Ti and GTX610. The results reveal significant speedup compared to CPU implementation at same detection accuracy.|International Conference on Digital Signal Processing|2015|10.1109/ICDSP.2015.7252015|Jia Wang, C. Zhao, Wei You, Yulei Wang|0.4|2
229|Large-scale simulations of synthetic markets|High-frequency trading has been experiencing an increase of interest both for practical purposes within financial institutions and within academic research; recently, the UK Government Office for Science reviewed the state of the art and gave an outlook analysis. Therefore, models for tick-by-tick financial time series are becoming more and more important. Together with high-frequency trading comes the need for fast simulations of full synthetic markets for several purposes including scenario analyses for risk evaluation. These simulations are very suitable to be run on massively parallel architectures. Aside more traditional large-scale parallel computers, high-end personal computers equipped with several multi-core CPUs and general-purpose GPU programming are gaining importance as cheap and easily available alternatives. A further option are FPGAs. In all cases, development can be done in a unified framework with standard C or C++ code and calls to appropriate libraries like MPI (for CPUs) or CUDA for (GPGPUs). Here we present such a prototype simulation of a synthetic regulated equity market. The basic ingredients to build a synthetic share are two sequences of random variables, one for the inter-trade durations and one for the tick-by-tick logarithmic returns. Our extensive simulations are based on several distributional choices for the above random variables, including Mittag-Leffler distributed inter-trade durations and alpha-stable tick-by-tick logarithmic returns.||2015|10.1088/1742-6596/664/9/092003|E. Scalas, G. Germano, L. Gerardo-Giorda|0.4|2
262|Self-scaling Kinematic Hand Skeleton for Real-time 3D Hand-finger Pose Estimation|Since low cost RGB-D sensors have become available, gesture detection has gained more and more interest in the field of human computer and human robot interaction. It is possible to navigate through interactive menus by waving one’s hand and to confirm menu items by pointing at them. Such applications require real-time body or hand-finger pose estimation algorithms. This paper presents a kinematic approach to estimate the full pose of the hand including the angles of the finger joints. A self-scaling kinematic hand skeleton model is presented and fitted into the 3D data of the hand in real-time on standard hardware with up to 30 frames per second without using a GPU. This approach is based on the least-square minimization and an intelligent choice of the error function. The tracking accuracy is evaluated on the basis of a recorded dataset as well as simulated data. Qualitative results are presented to emphasize the tracking ability under hard conditions like full hand turning and self-occlusion.|International Conference on Computer Vision Theory and Applications|2015|10.5220/0005257501850196|Jan Helge Klüssendorff, Kristian Ehlers|0.4|2
369|Accelerating fDOT image reconstruction based on path-history fluorescence Monte Carlo model by using three-level parallel architecture.|The excessive time required by fluorescence diffuse optical tomography (fDOT) image reconstruction based on path-history fluorescence Monte Carlo model is its primary limiting factor. Herein, we present a method that accelerates fDOT image reconstruction. We employ three-level parallel architecture including multiple nodes in cluster, multiple cores in central processing unit (CPU), and multiple streaming multiprocessors in graphics processing unit (GPU). Different GPU memories are selectively used, the data-writing time is effectively eliminated, and the data transport per iteration is minimized. Simulation experiments demonstrated that this method can utilize general-purpose computing platforms to efficiently implement and accelerate fDOT image reconstruction, thus providing a practical means of using path-history-based fluorescence Monte Carlo model for fDOT imaging.|Optics Express|2015|10.1364/OE.23.025996|Xu Jiang, Qingming Luo, Yong Deng, Zhaoyang Luo|0.4|2
378|Lattice Quantum Chromodynamics with Overlap Fermions on GPUs|Lattice quantum chromodynamics (QCD) calculations were one of the first applications to demonstrate the potential of GPUs in the area of high-performance computing; the nature of lattice QCD calculations matches well with the GPU's computational model. This article discusses ways to effectively use GPUs for lattice calculations using the overlap operator, a discretization that preserves chiral symmetry even at nonzero lattice spacing and makes possible lattice QCD simulations in the parameter region relevant to nuclear physics. The author shows that the large memory footprint of these codes requires the use of multiple GPUs in parallel and discusses methods for implementing this operator efficiently: hybrid CPU/GPU memory use for eigensolvers and MPI/OpenMP/CUDA parallelization strategies required to take full advantage of both GPU and CPU resources. He then compares the performance of codes on a GPU cluster and a CPU cluster with similar interconnects, discussing the strong scaling for problem sizes relevant to current lattice QCD simulations.|Computing in science & engineering (Print)|2015|10.1109/MCSE.2014.114|A. Alexandru|0.4|2
404|A real time haptic simulator of spine surgeries|Spine surgeries are high risk operations which require the surgeons to have ample experiences. For young surgeons, effective and extensive training is critical. This paper presents a real time haptic spine surgical simulator that will be used to train residents, fellows and spine surgeons in a hospital training program. It provides a realistic environment for the trainees to practice spine surgeries and has the advantages of being interactive, low-cost, representative, and repeatable over conventional training approaches. Haptic Phantom offers the users force feedback, differentiating our system from other screen-based training systems. Computational efficiency was achieved by developing advanced graphical rendering methods. The volumetric data was classified into surface voxel cloud and inner voxel cloud by the adjacency graph which stored the relationship among voxels. To speed up the collision detection and real time rendering between the virtual surgical tools and the lumbar model, Octree-based algorithms and GPU technique were applied. To enhance the physical realism, three dimensional lumbar vertebrae models were reconstructed from CT images and associated with non-homogeneous bone density such that the rendered model best represents the spine anatomy and mechanics. We demonstrate system performance by conducting pedicle screw insertion.|Virtual Reality Software and Technology|2015|10.1145/2821592.2821613|A. Moshirfar, Qi Xing, Jim X. Chen, Qi Wei, M. Theiss, Jihui Li|0.4|2
442|Interactive Fusion and Tracking For Multi‐Modal Spatial Data Visualization|Scientific data acquired through sensors which monitor natural phenomena, as well as simulation data that imitate time‐identified events, have fueled the need for interactive techniques to successfully analyze and understand trends and patterns across space and time. We present a novel interactive visualization technique that fuses ground truth measurements with simulation results in real‐time to support the continuous tracking and analysis of spatiotemporal patterns. We start by constructing a reference model which densely represents the expected temporal behavior, and then use GPU parallelism to advect measurements on the model and track their location at any given point in time. Our results show that users can interactively fill the spatio‐temporal gaps in real world observations, and generate animations that accurately describe physical phenomena.|Computer graphics forum (Print)|2015|10.1111/cgf.12637|K. Matkovič, M. Gad, Hicham G. Elmongui, M. El-Shehaly, D. Gračanin|0.4|2
498|An alternative approach for collaborative simulation execution on a CPU+GPU hybrid system|In the past few years, the graphics processing unit (GPU) has been widely used to accelerate time-consuming models in simulations. Since both model computation and simulation management are main factors that affect the performance of large-scale simulations, only accelerating model computation will limit the potential speedup. Moreover, models that can be well accelerated by a GPU could be insufficient, especially for simulations with many lightweight models. Traditionally, the parallel discrete event simulation (PDES) method is used to solve this class of simulation, but most PDES simulators only utilize the central processing unit (CPU) even though the GPU is commonly available now. Hence, we propose an alternative approach for collaborative simulation execution on a CPU+GPU hybrid system. The GPU supports both simulation management and model computation as CPUs. A concurrency-oriented scheduling algorithm was proposed to enable cooperation between the CPU and the GPU, so that multiple computation and communication resources can be efficiently utilized. In addition, GPU functions have also been carefully designed to adapt the algorithm. The combination of those efforts allows the proposed approach to achieve significant speedup compared to the traditional PDES on a CPU.|International Conference on Advances in System Simulation|2020|10.1177/0037549719885178|Yiping Yao, Wentong Cai, Xiao Song, Wenjie Tang, Feng Zhu|0.4|2
507|Tiling-Based Programming Model for Structured Grids on GPU Clusters|Currently, more than 25% of supercomputers employ GPUs due to their massively parallel and power-efficient architectures. However, programming GPUs efficiently in a large scale system is a demanding task not only for computational scientists but also for programming experts as multi-GPU programming requires managing distinct address spaces, generating GPU-specific code and handling inter-device communication. To ease the programming effort, we propose a tiling-based high-level GPU programming model for structured grid problems. The model abstracts data decomposition, memory management and generation of GPU specific code, and hides all types of data transfer overheads. We demonstrate the effectiveness of the programming model on a heat simulation and a real-life cardiac modeling on a single GPU, on a single node with multiple-GPUs and multiple-nodes with multiple-GPUs. We also present performance comparisons under different hardware and software configurations. The results show that the programming model successfully overlaps communication and provides good speedup on 192 GPUs.|International Conference on High Performance Computing in Asia-Pacific Region|2020|10.1145/3368474.3368485|D. Unat, Burak Bastem|0.4|2
533|Actual and experiential shadow origin tagging: A 2.5D algorithm for efficient precinct-scale modelling|This article describes a novel algorithm for built environment 2.5D digital model shadow generation that allows identities of shadowing sources to be efficiently precalculated. For any point on the ground, all sources of shadowing can be identified and are classified as actual or experiential obstructions to sunlight. The article justifies a 2.5D raster approach in the context of modelling of architectural and urban environments that has in recent times shifted from 2D to 3D, and describes in detail the algorithm which builds on precedents for 2.5D raster calculation of shadows. The algorithm is efficient and is applicable at even precinct scale in low-end computing environments. The simplicity of this new technique, and its independence of GPU coding, facilitates its easy use in research, prototyping and civic engagement contexts. Two research software applications are presented with technical details to demonstrate the algorithm’s use for participatory built environment simulation and generative modelling applications. The algorithm and its shadow origin tagging can be applied to many digital workflows in architectural and urban design, including those using big data, artificial intelligence or community participative processes.||2020|10.1177/1478077119895218|G. Kimm|0.4|2
541|Accelerated Simulations of Chemical Reaction Systems using the Stochastic Simulation Algorithm on GPUs|Stochasticity due to fluctuations in chemical reactions can play important roles in cellular network-driven processes. Although the Stochastic Simulation Algorithm (SSA, aka Gillespie Algorithm) has long been accepted as a suitable method to solve the time-dependent chemical master equation, its computational cost is prohibitive for large scale complex networks such as those found in cellular processes. Here we present GPU-SSA, an implementation of the SSA formalism utilizing Graphics Processing Units for use in Python using the PySB modeling framework. We show that the GPU implementation of SSA can achieve significant speedup compared to parallel CPU or single-core CPU implementations. We further include supplementary didactic material to demonstrate how to incorporate GPU-SSA workflows for interested readers.|bioRxiv|2020|10.1101/2020.02.14.948612|James C. Pino, Alexander L. R. Lubbock, Carlos F. Lopez, Martina Prugger, L. A. Harris|0.4|2
549|Simulation of three-dimensional eutectic growth multi-phase field based on OpenCL parallel|Based on the muti-component eutectic multi-phase field model of S. G. Kim, W. T. Kim, T. Suzuki et al. [J. Cryst. Growth 261, 135–158 (2004)], the high-performance computing method of hardware and software architecture of OpenCL + graphics processing unit (GPU) was studied. Taking CBr4–C2Cl6 as an example, the evolution process of large-scale three-dimensional eutectic structure growth is realized by concurrent execution of multiple processes and multiple threads on two heterogeneous platforms of AMD and NVIDIA, respectively. The effects of different initial lamellar spacing and flow on eutectic lamellar morphology were also studied. The results show that, with the increasing of eutectic lamellar spacing, the morphology of eutectic lamellar changes are as follows: the symmetrically steady-state perpendicular to the interface is based on growth, the slightly oscillating state is unstable, and the large oscillating state is unstable; Under the condition of forced convection, the symmetrically alternating growth pattern of the original eutectic lamellar was broken, the melt flow led to the change of eutectic growth morphology, and the eutectic lamellar grew in the opposite direction of the flow. At the same computing scale, compared with the serial algorithm on the central processing unit platform, the acceleration ratio on the single GPU on the heterogeneous platform reaches 24.3 times and 21.6 times respectively, which improves the computing efficiency. At the same time, with its strong floating-point computing power to obtain more accurate simulation results and achieve the dual needs of computational efficiency and portability, it has also proven to solve the problems of a large amount of calculation, low efficiency and limited to qualitative research existing in the traditional phase-field models.Based on the muti-component eutectic multi-phase field model of S. G. Kim, W. T. Kim, T. Suzuki et al. [J. Cryst. Growth 261, 135–158 (2004)], the high-performance computing method of hardware and software architecture of OpenCL + graphics processing unit (GPU) was studied. Taking CBr4–C2Cl6 as an example, the evolution process of large-scale three-dimensional eutectic structure growth is realized by concurrent execution of multiple processes and multiple threads on two heterogeneous platforms of AMD and NVIDIA, respectively. The effects of different initial lamellar spacing and flow on eutectic lamellar morphology were also studied. The results show that, with the increasing of eutectic lamellar spacing, the morphology of eutectic lamellar changes are as follows: the symmetrically steady-state perpendicular to the interface is based on growth, the slightly oscillating state is unstable, and the large oscillating state is unstable; Under the condition of forced convection, the symmetrically alternating gr...||2020|10.1063/1.5129806|Changsheng Zhu, Xiuting Guo, X.J. Jin, Li Feng|0.4|2
562|Making Better Use of Processing-in-Memory Through Potential-Based Task Offloading|There is an increasing demand for a novel computing structure for data-intensive applications such as artificial intelligence and virtual reality. The processing-in-memory (PIM) is a promising alternative to reduce the overhead caused by data movement. Many studies have been conducted on the utilization of the PIM taking advantage of the bandwidth increased by the through silicon via (TSV). One approach is to design an optimized PIM architecture for a specific application, the other is to find the tasks that will be more advantageous when offloading to PIM. The goal of this paper is to make the PIM, a newly introduced technology, be easily applied to various applications. The programmable GPU-based PIM is the target system. The essential but simple task offloading conditions are proposed to secure as many candidate tasks as possible when there is any potential benefit from the PIM. The PIM design options then are explored reflecting the characteristics of the candidate tasks actively. When determining offloading conditions, it is difficult to simultaneously consider three time-energy-power objectives. Thus, the problem is divided into two sub-problems. The first offloading condition is designed based on time-energy constraints, whereas the second offloading condition is modeled to satisfy time-power constraints. During the whole processes, the offloading conditions and the PIM design options are carefully configured in a complementary manner to reduce the tasks that are excluded from the offloading. In the simulation results, the suitability of the modeled two offloading conditions and the proposed PIM design are verified using various benchmarks and then, they are compared with previous works in terms of processing speed and energy.|IEEE Access|2020|10.1109/ACCESS.2020.2983432|B. Kim, Chae-Eun Rhee|0.4|2
574|Highly parallelized rendering of the retinal image through a computer-simulated human eye for the design of virtual reality head-mounted displays|For long, the VR optics designer has been taking advantage of the fact that the eye is not optically perfect and striving towards creating “perfectly imperfect” VR optics. These cheaper and lighter lens systems could have high off-axis aberration, but with negligible impact on visual quality. A good way to test and validate these designs could be simulating the rendered image of the pixel array on the retina. Earlier attempts used the principle of virtual ray tracing, which is inherently a Monte Carlo method and therefore subjects to noise at sharp transition edges. Furthermore, this approach cannot render the Point Spread Function (PSF). We took the reverse approach of physical ray tracing. Rays are generated from each illuminating point and traced forward to the retina. After the tracing process, each light source point generates a number of ray-retina intersections. We propose a novel rasterization algorithm to render the radiometric image on the virtual retina. For the radiometric-correct determination of pixel value, we used the Intersection-over-Area metric. This metric is an indicator for how much the back-projected area of each retinal cell overlaps the triangle formed by the cone of light flux falling on the retina. The geometry of the overlapping was found with the Sutherland-Hodgman algorithm. All images of source points were then superposed to find the resulting image. With the proposed software, it is possible to simulate different VR-optics in combination with eye models. The most significant challenge of the physical ray tracing was the huge calculation workload. Our software overcame this by utilizing parallel GPU computing capability offered by the CUDA platform. For evaluation, we used the software with different schematic models of the eye, including those from Gullstrand, Le Grand, Koojiman and Navarro. We found that the former two, so-called paraxial, eye models predict the eye has higher sagittal resolution than tangential. On the other hand, the latter two (so-called finite eye models) predict the opposite. With this simulation software, optical designers can validate their VR-optics-design and gain more insights about the image formation process. This tool can be used as a virtual optical test bench for the simulation of the imaging result before building a costly prototype.||2020|10.1117/12.2555872|C. Vu, W. Stork, Simon Stock, L. Fan|0.4|2
614|cuProCell: GPU-Accelerated Analysis of Cell Proliferation With Flow Cytometry Data|The investigation of cell proliferation can provide useful insights for the comprehension of cancer progression, resistance to chemotherapy and relapse. To this aim, computational methods and experimental measurements based on in vivo label-retaining assays can be coupled to explore the dynamic behavior of tumoral cells. ProCell is a software that exploits flow cytometry data to model and simulate the kinetics of fluorescence loss that is due to stochastic events of cell division. Since the rate of cell division is not known, ProCell embeds a calibration process that might require thousands of stochastic simulations to properly infer the parameterization of cell proliferation models. To mitigate the high computational costs, in this paper we introduce a parallel implementation of ProCell's simulation algorithm, named cuProCell, which leverages Graphics Processing Units (GPUs). Dynamic Parallelism was used to efficiently manage the cell duplication events, in a radically different way with respect to common computing architectures. We present the advantages of cuProCell for the analysis of different models of cell proliferation in Acute Myeloid Leukemia (AML), using data collected from the spleen of human xenografts in mice. We show that, by exploiting GPUs, our method is able to not only automatically infer the models’ parameterization, but it is also $237\times$ faster than the sequential implementation. This study highlights the presence of a relevant percentage of quiescent and potentially chemoresistant cells in AML in vivo, and suggests that maintaining a dynamic equilibrium among the different proliferating cell populations might play an important role in disease progression.|IEEE journal of biomedical and health informatics|2020|10.1109/JBHI.2020.3005423|T. Vlachou, P. Cazzaniga, Eric Nisoli, P. Pelicci, D. Besozzi, S. Spolaor, Marco S. Nobile, G. Mauri|0.4|2
635|Continuous-Time Algorithms for Solving Maxwell's Equations using Analog Circuits|In this paper, we propose solutions to Maxwell's equations that can be computed using analog computers. Spatially-discrete time-continuous (SDTC) algorithms running on analog computers can be potentially faster and more energy-efficient than fully-discrete numerical solvers. The implementations of fully-discrete partial differential equation (PDE) solvers on high speed digital processors, such as graphics processing units (GPUs), take many clock cycles to compute a single temporal frame of the update equation and thus have relatively low equivalent bandwidths. Our approach is to directly implement temporal recursions in continuous-time by using analog circuits. Such circuits can have bandwidths that greatly exceed the equivalent bandwidths of GPUs. In particular, we propose two analog computing methods that compute the SDTC solutions to Maxwell's equations. In addition to Maxwell's equations, such platforms can be used to accelerate other hard computational problems that involve PDEs derived from continuous-time systems. In continuous-time in Laplace domain (CTLD) method (first approach), the spatial domain partial derivatives in the governing PDE are approximated using discrete finite differences, while applying the Laplace transformation along the time dimension. The resulting spatially-discrete time-continuous update equation is utilized to design an analog circuit that can compute the continuous-time solution. The all-pass delay approximate (APDA) method (second approach) replaces the discrete-time difference operators in the standard finite difference time domain (FDTD) cell (Yee cell) using continuous-time delay operators, which can be realized using analog all-pass filters. Both methods have been simulated using ideal analog circuits in Cadence Spectre for the Dirichlet, Neumann, and radiation boundary conditions. The performance of the proposed methods have been quantified using i) mean squared differences between the results and fully-discrete FDTD simulations, and ii) the noise to signal energy ratio. The CTLD and APDA methods are able to compute the solutions to Maxwell's equations with a noise energy to signal energy ratio γ better than −26 dB and −19 dB, respectively. Both methods have been extended to design analog circuits that compute the continuous-time solution of the 1-D and 2-D wave equations. The CTLD-based 1-D and 2-D analog wave equation solvers are able to compute the solutions with γ better than −72 dB and −60 dB, respectively. The APDA-based 1-D wave equation solver is simulated with a dominant-pole model (which better approximates the non-ideal circuit behavior) along with a propagation delay compensation technique. The non-ideal analog models compute the solution with a difference smaller than −13 dB (in terms of γ). Experimental results from a simplified board-level low-frequency implementation are also presented. The key challenges toward CMOS implementations of the proposed solvers are identified and briefly discussed with possible solutions.|International Symposium on Circuits and Systems|2020|10.1109/ISCAS45731.2020.9181014|L. Belostotski, S. I. Hariharan, A. Madanayake, L. Bruton, N. Udayanga, S. Mandal|0.4|2
653|Research on Real-time Simulation Method of Vascular Interventional Surgery Based on Model Order Reduction|In the vascular interventional surgery virtual simulation training system, there is a high demand for the real-time nature of the simulation. However, the amount of storage and calculation required by the current system is very large. We need some methods that can increase the calculation rate and speed up the system simulation rate. In this study, for the existing simulation training system, we improved the two parts of the blood vessel local simulation and the entire system simulation. First, the blood vessel is simulated and modeled using a model-based order reduction method. After a set of offline simulations based on the entire model, eigen-orthogonal decomposition is applied to greatly reduce the number of states of the robot model, and super-reduction is used to perform integration on the simplified domain. Then, a multi-threaded parallel operation method is adopted. The movement between the blood vessel and the guide wire and between the guide wire and the tactile force feedback device are calculated in parallel to increase the calculation rate. Finally, CUDA is used for GPU computing to accelerate the simulation modeling speed of the system. The experimental results show that the method proposed in this paper can speed up the system simulation rate and improve the real-time performance of the system simulation.|2020 IEEE International Conference on Mechatronics and Automation (ICMA)|2020|10.1109/ICMA49215.2020.9233617|Baofeng Gao, Lamei Shang|0.4|2
672|Prediction-Based Error Correction for GPU Reliability with Low Overhead|Scientific and simulation applications are continuously gaining importance in many fields of research and industries. These applications require massive amounts of memory and substantial arithmetic computation. Therefore, general-purpose computing on graphics processing units (GPGPU), which combines the computing power of graphics processing units (GPUs) and general CPUs, have been used for computationally intensive scientific and big data processing applications. Because current GPU architectures lack hardware support for error detection in computation logic, GPGPU has low reliability. Unlike graphics applications, errors in GPGPU can lead to serious problems in general-purpose computing applications. These applications are often intertwined with human life, meaning that errors can be life threatening. Therefore, this paper proposes a novel prediction-based error correction method called Prediction-based Error Correction (PRECOR) for GPU reliability, which detects and corrects errors in GPGPU platforms with a focus on errors in computational elements. The implementation of the proposed architecture needs a small number of checkpoint buffers in order to fix errors in computational logic. The PRECOR architecture has prediction buffers and controller units for predicting erroneous outputs before performing rollback. Following a rollback, the architecture confirms the accuracy of its predictions. The proposed method effectively reduces the hardware and time overheads required to correct errors. Experimental results confirm that PRECOR efficiently fixes errors with low hardware and time overheads.|Electronics|2020|10.3390/electronics9111849|Tae Hyun Kim, Sungho Kang, Hyunyul Lim|0.4|2
684|Model construction of photon propagation based on the geometrical symmetries and GPU technology for the quad-head PET system|Compact quad-head PET systems have been developed by many researchers. This compact configuration leads to depth of interaction (DOI) blurring, which greatly reduces the quality of the reconstructed image. How to accurately model the photon propagation is the key to relieving DOI blurring. In this study, photon propagation is modelled by using the multi-ray method. As we all know, the precision of the multi-ray model is proportional to its computational complexity. Focusing on this problem, we have developed the symmetries of the quad-head PET system according to the different structural features of the opposite and adjacent detector heads. Given all the symmetrical properties, the construction of the photon propagation model becomes a trade-off between the calculation precision and computational complexity of the multi-ray method. In addition, when photon propagation is stored in the system response matrix (SRM), the storage cost is also reduced largely. In the proposed quad-head PET system, the SRM is compressed about 559 times, and the matrix scale is approximately 1.7 GB. The calculation of the SRM can be finished in 60 s when the graphics processing unit (GPU) acceleration technology is adopted. In order to verify the imaging performances, several simulated experiments are carried out. The metrics FWHM, location accuracy, Contrast Recovery (CR), and Coefficient of Variation (COV) are adopted to quantify the imaging results. We find that the mean location difference can reach 1.29% based on a simulated multi-spheres experiment. Through this experiment, the FWHMs can reach to about 1.0 mm in the center or close to the edge of the field of view (FOV). In the image quality experiment, the CR can up to 0.95 in the hot region, and the COV reaches 5.82% in the background region.|Journal of Instrumentation|2020|10.1088/1748-0221/15/12/P12015|J. Cheng, F. Meng, W. Qin, S. Zhu, X. Lyu, G. Zhang, Y. Shi, C. Li|0.4|2
710|Supporting Irregularity in Throughput-Oriented Computing by SIMT-SIMD Integration|The last two decades have seen continued exponential performance increases in HPC systems, well after the predicted end of Moore's Law for CPUs, largely due to the widespread adoption of throughput-oriented compute accelerators such as GPUs. When faced with irregular yet throughput-oriented applications, their simple, grid-based computing model turns into a serious limitation. Instead of repeatedly tackling the issues of irregularity on the application layer, we argue that a generalization of the CUDA model to irregular grids can be supported through minor modifications to already established throughput-oriented architectures. To that end, we propose a unifying approach that combines techniques from both SIMD and MIMD approaches, but adheres to the SIMT principles - based on an unlikely ally: a wide-SIMD vector architecture. We extend CUDA's familiar programming model and implement SIMT-inspired strategies for dealing with data and control flow irregularities. Our approach requires only minimal hardware changes and an additional compiler phase. Using a model-based software simulation, we demonstrate that the proposed system can be a first step towards native support for irregularity on throughput-oriented processors while greatly simplifying the development of irregular applications.|Workshop on Irregular Applications: Architectures and Algorithms|2020|10.1109/IA351965.2020.00010|D. Thuerck|0.4|2
1372|The Evolution of the Computing Time in the simulation of Mimbot-Biped Robot using Parallel Algorithms|This work presents a biped walking robot with a reduced number of degrees of freedom. Based on classical mechanisms, MIMBOT robot is able to emulate advanced human skills. Their topologies are described as well as mathematical models are proposed in order to solve the kinematics. Resulting explicit equations from this model are implemented in several computing platforms in two ways, the classical one using the CPU to execute the routines and another one taking advance of the parallel capabilities of GPUs. Several simulations are carried out over all platforms with the aim of analyzing the behaviour of the hardware and software used. Finally, results are presented showing that not always parallelizing the simulation is the best option.||2015|10.1007/978-3-319-19264-2_5|A. Bustos, Jesús Meneses, H. Rubio, J. C. G. Prada|0.4|2
1380|Two-phase flow simulation in the large digital rock by using high performance cluster|A numerical implementation based on a Graphics Processing Unit (GPU) is proposed for the acceleration of the two-phase simulation using Lattice Boltzmann Method (LBM). The LBM yields regular, data-parallel computations; therefore, it is espe-cially well ﬁtted to GPU calculations. This study focuses on the application of the LBM for ﬂuid displacement computations in real rock sample. For this purpose, the digital rock model is reconstructed from the micro-CT scanned images of reservoir sample with a resolution of 2.0 um. In order to obtain reliable and accurate results from the developed numerical model, the computational domain must be large enough to cover the representative element size (REV) of sample rock. As a result, pore scale LBM simulation of multiphase porous medium systems with sufﬁcient resolution and large grid-number are very compu-tationally challenging. To achieve this extremely large-scale simulation, multi-GPU parallel scheme by using CUDA and MPI is developed. Careful optimizations include sparse storage scheme, efﬁcient domain decomposition and non-blocking commu-nication are desired for algorithm implementation. Finally, we succeeded to perform a two-phase simulation with 10 billion (1000 x1000x1000) mesh sizes using a small-scale GPU cluster. The developed large-scale simulation method enables the direct upscaling from pore scale to core scale which is a very powerful tool for many engineering applications such as enhanced oil recovery (EOR) and Carbon Capture and Storage (CCS).||2015|10.2961/jlmn.2015.03.0017|J. Fei, Tsuji Takeshi|0.4|2
1451|Enhancing Cloud Radiative Processes and Radiation Efficiency in the Advanced Research Weather Research and Forecasting (WRF) Model|The objective of this research has been to evaluate and implement enhancements to the computational performance of the RRTMG radiative transfer option in the Advanced Research version of the Weather Research and Forecasting (WRF) model. Efficiency is as essential as accuracy for effective numerical weather prediction, and radiative transfer is a relatively time-consuming component of dynamical models, taking up to 30-50 percent of the total model simulation time. To address this concern, this research has implemented and tested a version of RRTMG that utilizes graphics processing unit (GPU) technology (hereinafter RRTMGPU) to greatly improve its computational performance; thereby permitting either more frequent simulation of radiative effects or other model enhancements. During the early stages of this project the development of RRTMGPU was completed at AER under separate NASA funding to accelerate the code for use in the Goddard Space Flight Center (GSFC) Goddard Earth Observing System GEOS-5 global model. It should be noted that this final report describes results related to the funded portion of the originally proposed work concerning the acceleration of RRTMG with GPUs in WRF. As a k-distribution model, RRTMG is especially well suited to this modification due to its relatively large internal pseudo-spectral (g-point) dimension that,more » when combined with the horizontal grid vector in the dynamical model, can take great advantage of the GPU capability. Thorough testing under several model configurations has been performed to ensure that RRTMGPU improves WRF model run time while having no significant impact on calculated radiative fluxes and heating rates or on dynamical model fields relative to the RRTMG radiation. The RRTMGPU codes have been provided to NCAR for possible application to the next public release of the WRF forecast model.« less||2015|10.2172/1172166|M. Iacono|0.4|2
271|Performance portability for room acoustics simulations|Numerical modelling of the 3-D wave equation can result in very accurate virtual auralisation, at the expense of computational cost. Implementations targeting modern highly-parallel processors such as NVIDIA GPUs (Graphics Processing Units) are known to be very effective, but are tied to the specific hardware for which they are developed. In this paper, we investigate extending the portability of these models to a wider range of architectures without the loss of performance. We show that, through development of portable frameworks, we can achieve acoustic simulation software that can target other devices in addition to NVIDIA GPUs, such as AMD GPUs, Intel Xeon Phi many-core CPUs and traditional Intel multi-core CPUs. The memory bandwidth offered by each architecture is key to achievable performance, and as such we observe high performance on AMD as well as NVIDIA GPUs (where high performance is achievable even on consumer-class variants despite their lower floating point capability), whilst retaining portability to the other less-performant architectures.||2017|10.1109/waspaa.2017.8170007|A. Gray, Christophe Dubach, S. Bilbao, Larisa Stoltzfus|0.375|2
382|A GPU Implemented 3F Cellular Automata-Based Model for a 2D Evacuation Simulation Pattern|This study presents the principles of a Cellular Automata (CA) based model that incorporates an enhanced version of the floor-field model targeting the fire spreading representation, thus called fire-floor-field (3F). The aim of the model is to simulate evacuation processes fast and reliably, in order to act as the core module of a near real-time effective anticipation system. To this direction, the model takes advantage of massive parallelism, an inherent feature of CA, by employing the efficient response of the floor field model and the accurate computational reproduction of fire-front evolution. Furthermore, a Graphic Processing Unit (GPU) based implementation of the proposed model is presented. Such a realisation aims at further speeding up the response of the model and it reinforces the fundamental goal of rapid activation. The model is validated quantitatively and qualitatively by being applied in the case of the two-dimensional (2D) simulated evacuation of the Building B, of the Department of Electrical and Computer Engineering, Democritus University of Thrace, under fire spreading conditions.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2017|10.1109/PDP.2017.93|G. Trunfio, Jarosław Wąs, I. Georgoudas, Isaac Koumis, G. Sirakoulis|0.375|2
390|Fault simulation acceleration for TRAX dictionary construction using GPUs|To ensure robustness of integrated systems, the TRAnsition-X (TRAX) fault model has been used with on-chip test and diagnosis hardware, utilizing fault dictionaries for diagnosis. Generating a fault dictionary requires fault simulation with no fault dropping, requiring extensive computational resources. This paper presents the design and implementation of an efficient fault simulator for the TRAX fault model, designed to run on a graphics processing unit (GPU). The inherent parallelism of the fault simulation problem maps well to the large number of concurrent threads supported by a modern GPU, and a GPU can be used to accelerate the construction of a fault dictionary. Our approach employs both pattern-parallel and fault-parallel algorithms in the GPU kernel implementations. Experiments involving various circuits, including the OpenSPARC T2 processor, demonstrate a mean speed-up of nearly 8x.|International Test Conference|2017|10.1109/TEST.2017.8242078|R. D. Blanton, Matthew Beckler|0.375|2
410|Large-Scale Interconnection Network Simulation Methods Based on Cellular Automata|Interconnection network (ICN) has been playing an important role in parallel systems, since it greatly affects the system-level organization as well as its inherent communication capability. State-of-the-art parallel systems employ a huge number of computing nodes that are connected by an interconnection network. In general, an interconnection network shows nonlinear phenomena in its communication performance, most of them are caused by congestion. Thus, designing a large-scale parallel system requires sufficient discussions through repetitive simulation runs. This causes another problem in simulating a large-scale system within a reasonable cost. We have introduced the cellular automata principle in the ICN simulation and proposed an effective simulation model as a promising solution. However, the proposed method is insufficient in packet length and memory usage. This paper extends our prior simulation model to solve the problems. We firstly extend the existing cellular automata based model to match multiple-length packets. Furthermore, we discuss reduction methods of memory resources and propose a compressed buffer model and composite packet information. The proposed models have a good affinity to GPGPU technology and the GPU-based simulator accelerates simulation upto about 364 times from sequential execution on a single CPU. Furthermore, since the proposed models are applicable in the shared memory model, multithread implementation of the proposed methods achieve about 78 times speed-ups at the maximum.|International Symposium on Computing and Networking - Across Practical Development and Theoretical Research|2017|10.1109/CANDAR.2017.52|T. Yokota, Takeshi Ohkawa, K. Ootsu|0.375|2
1177|Enhancing regional ocean modeling simulation performance with the Xeon Phi architecture|Ocean studies are crucial to many scientific disciplines. Due to the difficulty in probing the deep layers of the ocean and the scarcity of data in some of the oceans, the scientific community relies heavily on ocean simulation models. Ocean modeling is complex and computationally intensive, and improving the performance of these models will greatly advance and improve the work of ocean scientists. This paper presents a detailed exploration of the acceleration of the Regional Ocean Model System (ROMS) software with the latest Intel Xeon Phi x200 architectures. Both shared-memory and distributed-memory parallel computing models are evaluated. Results show run time improvements of nearly a factor of 16 compared to a serial implementation. Further experiments and optimizations, including the use of a GPU acceleration model, are discussed and results are presented.|Oceans|2017|10.1109/OCEANSE.2017.8084988|M. Pantoja, C. Lupo, P. Choboter|0.375|2
1212|FULL GPU Implementation of Lattice-Boltzmann Methods with Immersed Boundary Conditions for Fast Fluid Simulations|Lattice Boltzmann Method (LBM) has shown great potential in fluid simulations, but performance issues and difficulties to manage complex boundary conditions have hindered a wider application. The upcoming of Graphic Processing Units (GPU) Computing offered a possible solution for the performance issue, and methods like the Immersed Boundary (IB) algorithm proved to be a flexible solution to boundaries. Unfortunately, the implicit IB algorithm makes the LBM implementation in GPU a non-trivial task. This work presents a fully parallel GPU implementation of LBM in combination with IB. The fluid-boundary interaction is implemented via GPU kernels, using execution configurations and data structures specifically designed to accelerate each code execution. Simulations were validated against experimental and analytical data showing good agreement and improving the computational time. Substantial reductions of calculation rates were achieved, lowering down the required time to execute the same model in a CPU to about two magnitude orders.||2017|10.21152/1750-9548.11.1.1|J. Dottori, G. Boroni, P. Rinaldi|0.375|2
1387|Modeling of Radiotherapy Linac Source Terms Using ARCHER Monte Carlo Code: Performance Comparison for GPU and MIC Parallel Computing Devices|Monte Carlo (MC) simulation is well recognized as the most accurate method for radiation dose calculations. For radiotherapy applications, accurate modelling of the source term, i.e. the clinical linear accelerator is critical to the simulation. The purpose of this paper is to perform source modelling and examine the accuracy and performance of the models on Intel Many Integrated Core coprocessors (aka Xeon Phi) and Nvidia GPU using ARCHER and explore the potential optimization methods. Phase Space-based source modelling for has been implemented. Good agreements were found in a tomotherapy prostate patient case and a TrueBeam breast case. From the aspect of performance, the whole simulation for prostate plan and breast plan cost about 173s and 73s with 1% statistical error.||2017|10.1051/EPJCONF/201715304010|B. Bednarz, Hui Lin, L. Su, P. Caracappa, Tianyu Liu, X. Xu|0.375|2
188|Generalized GPU Acceleration for Applications Employing Finite-Volume Methods|Scientific HPC applications are increasingly ported to GPUs to benefit from both the high throughput and the powerful computing capacity. Many of these applications, such as atmospheric modeling and hydraulic erosion simulation, are adopting the finite volume method (FVM) as the solver algorithm. However, the communication components inside these applications generally lead to a low flop-to-byte ratio and an inefficient utilization of GPU resources. This paper aims at optimizing FVM solver based on the structured mesh. Besides a high-level overview of the finite-volume method as well as its basic optimizations on modern GPU platforms, we further present two generalized tuning techniques including an explicit cache mechanism as well as an inner-thread rescheduling method that tries to achieve a suitable mapping between the algorithm feature and the platform architecture. To the end, we demonstrate the impact of our generalized optimization methods in two typical atmospheric dynamic kernels (Euler and SWE) based on four mainstream GPU platforms. According to the experimental results of Tesla K80, speedups of 24.4x for SWE and 31.5x for Euler could be achieved over a 12-core Intel E5-2697 CPU, which is a great promotion compared with its original speedup (18x and 15.47x) without applying these two methods.|IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing|2016|10.1109/CCGrid.2016.30|Jingheng Xu, L. Gan, Shizhen Xu, Guangwen Yang, H. Fu, Xinliang Wang, Chao Yang, Wei Xue, Wenlai Zhao, Bingwei Chen|0.3333333333333333|2
219|Development and application of real-time and interactive software for complex system|Soft materials have attracted considerable interest in recent years for predicting the characteristics of phase separation and self-assembly in nanoscale structures. A popular method for demonstrating and simulating the dynamic behaviour of particles (e.g. particle tracking) and to consider effects of simulation parameters is cell dynamic simulation (CDS). This is a cellular computerisation technique that can be used to investigate different aspects of morphological topographies of soft material systems. The acquisition of quantitative data from particles is a critical requirement in order to obtain a better understanding and of characterising their dynamic behaviour. To achieve this objective particle tracking methods considering quantitative data and focusing on different properties and components of particles is essential. Despite the availability of various types of particle tracking used in experimental work, there is no method available to consider uniform computational data. In order to achieve accurate and efficient computational results for cell dynamic simulation method and particle tracking, two factors are essential: computing/calculating time-scale and simulation system size. Consequently, finding available computing algorithms and resources such as sequential algorithm for implementing a complex technique and achieving precise results is critical and rather expensive. Therefore, it is highly desirable to consider a parallel algorithm and programming model to solve time-consuming and massive computational processing issues. Hence, the gaps between the experimental and computational works and solving time consuming for expensive computational calculations need to be filled in order to investigate a uniform computational technique for particle tracking and significant enhancements in speed and execution times. \nThe work presented in this thesis details a new particle tracking method for integrating diblock copolymers in the form of spheres with a shear flow and a novel designed GPU-based parallel acceleration approach to cell dynamic simulation (CDS). In addition, the evaluation of parallel models and architectures (CPUs and GPUs) utilising the mixtures of application program interface, OpenMP and programming model, CUDA were developed. Finally, this study presents the performance enhancements achieved with GPU-CUDA of approximately ~2 times faster than multi-threading implementation and 13~14 times quicker than optimised sequential processing for the CDS computations/workloads respectively.||2016|10.1109/ths.2016.7568889|H. Soltani|0.3333333333333333|2
238|High performance space VPX payload computing architecture study|This paper describes a functional reference design for a high-performance payload processor that captures images and spectra from multiple high-resolution instruments, processes and integrates multiple real-time data streams to perform feature recognition and spatial transformations providing autonomous navigation and rendezvous capability for future spacecraft and is equally applicable to Unmanned Aerial Systems (UAS). The proposed design uses two new standards: VITA 78 (SpaceVPX) for multi-processor architecture, and RapidIO (RIO) as the interconnect fabric. The SpaceVPX standard specifies physical form factor, logical, and physical interconnect technologies and architectures that can lead to high-performance fault tolerant computing for high-performance payloads. An overview of SpaceVPX and its relationship to OpenVPX is provided as a guide to practical implementations. The proposed design features a general-purpose host processor with GPU and FPGA-based image processing hardware. RIO is used for the instrument and processor interconnects, providing multiple gigabits per second of data communication capability. An overview of RIO features and operation is presented to complement the SpaceVPX architecture. A notional Reference Architecture is proposed for analysis using multiple methods for estimating avionics performance. The study objectives are to characterize throughput, latency and sub-system utilization using conventional system analysis, hardware prototype measurements and modeling and simulation software. We conducted first-order performance studies to identify bottlenecks in memory speed, I/O capacity and processing power. Initial performance analysis was performed on memory throughput rates, producing first-order values used as a performance baseline. A model of the Reference Architecture using VisualSim Architect was created and simulations run, producing insight into the complex interactions occurring between subsystems. Furthermore, the results of a prototype hardware implementation focusing on RIO throughput are presented as additional metrics. The study predicts RIO throughput between key elements of the Reference Architecture and identify major bottlenecks, and improvements needed for meeting mission requirements. The objective of this paper is to provide guidance to avionics designers regarding the adoption of SpaceVPX today and its anticipated evolution in the next few years.|IEEE Aerospace Conference|2016|10.1109/AERO.2016.7500650|Soumik Sinharoy, R. Alena, Mohammad Ahkter, P. Collier, D. Shankar|0.3333333333333333|2
279|CVC Verilog Compiler - Fast Complex Language Compilers Can be Simple|This paper explains how to develop Verilog hardware description language (HDL) optimized flow graph compiled simulators. It is claimed that the methods and algorithms described here can be applied in the development of flow graph compilers for other complex computer languages. The method uses the von Neumann computer architecture (MRAM model) as the best abstract model of computation and uses comparison and selection of alternative machine code sequences to utilize modern processor low level parallelism. By using the anti formalist method described here, the fastest available full IEEE 1364 2005 Verilog HDL standard simulators has been developed. The compiler only required 95,000 lines of C code and two developers. This paper explains how such a compiled simulator validates the anti-formalism computer science methodology best expressed by Peter Naur's datalogy and provides specific guidelines for applying the method. Development history from a slow interpreter into a fast flow graph based machine code compiled simulator is described. The failure of initial efforts that tried to convert a full 1364 compliant interpreter into interpreted execution of possibly auto generated virtual machines is discussed. The argument that fast Verilog simulation requires detail removing abstraction is shown to be incorrect. Reasons parallel GPU Verilog simulation has not succeeded are given.|arXiv.org|2016|10.1007/978-94-6300-229-5_10|S. Meyer|0.3333333333333333|2
302|Configurable XOR Hash Functions for Banked Scratchpad Memories in GPUs|"Scratchpad memories in GPU architectures are employed as software-controlled caches to increase the effective GPU memory bandwidth. Through the use of well-known optimization techniques, such as privatization and tiling, they are properly exploited. Typically, they are banked memories which are addressed with a <inline-formula> <tex-math notation=""LaTeX"">$\text{mod}(2^N)$</tex-math><alternatives> <inline-graphic xlink:type=""simple"" xlink:href=""vandenbraak-ieq1-2479595.gif""/></alternatives></inline-formula> bank indexing scheme. Although their bandwidth is fully exploited for linear memory accesses, their performance is burdened when non-unit strides appear in memory access patterns because they provoke bank conflicts. This paper explores the use of configurable <italic>bit-vector</italic> and <italic>bitwise</italic> XOR-based hash functions to evenly distribute memory addresses of the access patterns over the memory banks, reducing the number of bank conflicts. An exhaustive, but lightweight, search is used to configure bit-vector hash functions. Bitwise hash functions are configured with heuristics. Hardware and software implementations are carried out. For the hardware approach, the experimental results show 24 percent performance speed-up for 22 benchmarks on GPGPU-Sim, a Fermi-like simulator. Bank conflicts are reduced by 96 percent with bit-vector hash functions, and 97 percent with bitwise hash functions using our proposed Minimum Imbalance Heuristic. The software approach, using bit-vector hash functions, demonstrates 23 percent speed-up and 96 percent bank conflict reduction on a Fermi GPU, and 33 percent speed-up and 99 percent bank conflict reduction on a Kepler GPU."|IEEE transactions on computers|2016|10.1109/TC.2015.2479595|Juan Gómez-Luna, H. Corporaal, Gert-Jan van den Braak, Nicolás Guil Mata, José María González-Linares|0.3333333333333333|2
375|A Domain Specific Language for accelerated Multilevel Monte Carlo simulations|Monte Carlo simulations are used to tackle a wide range of exciting and complex problems, such as option pricing and biophotonic modelling. Since Monte Carlo simulations are both computationally expensive and highly parallelizable, they are ideally suited for acceleration through GPUs and FPGAs. Alongside these accelerators, Multilevel Monte Carlo techniques can be harnessed to further hasten simulations. However, researchers and application developers must invest a great deal of effort to design, optimise and test such Monte Carlo simulations. Furthermore, these models often have to be rewritten from scratch to target new hardware accelerators. This paper presents Neb, a Domain Specific Language for describing and generating Multilevel Monte Carlo simulations for a variety of hardware architectures. Neb compiles equations written in LATEX to C++, OpenCL or Maxeler's MaxJ language, allowing acceleration through GPUs or FPGAs. Neb can be used to solve stochastic equations or to generate paths for analysis with other tools. To evaluate the performance of Neb, a variety of financial models are executed on CPUs, GPUs and FPGAs, demonstrating peak acceleration of 3.7 times with FPGAs in 40nm transistor technology, and 14.4 times with GPUs in 28nm transistor technology. Furthermore, the energy efficiency of these accelerators is compared, revealing FPGAs to be 8.73 times and GPUs 2.52 times more efficient than CPUs.|IEEE International Conference on Application-Specific Systems, Architectures, and Processors|2016|10.1109/ASAP.2016.7760778|Ben Lindsey, W. Luk, M. Leslie|0.3333333333333333|2
422|ONAC: Optimal number of active cores detector for energy efficient GPU computing|Graphics Processing Units (GPUs) have become a prevalent platform for high throughput general purpose computing. The peak computational throughput of GPUs has been steadily increasing with each technology node by scaling the number of cores on the chip. Although this vastly improves the performance of several compute-intensive applications, our experiments show that some applications can achieve peak performance without utilizing all cores on the chip. We refer to the number of cores at which performance of an application saturates as the optimal number of active cores (Nopt). We propose executing the application on Nopt cores, and power-gating the unused cores to reduce static power consumption. Towards this target, we present ONAC (Optimal Number of Active Cores detector), a runtime technique to detect Nopt. ONAC uses a novel estimation model, which significantly reduces the number of hardware samples taken to detect the optimal core count, compared to a sequential detection technique (Seq-Det). We implement ONAC and Seq-Det in a cycle-level GPU performance simulator and analyze their effect on performance, power and energy. Our evaluation shows that ONAC and Seq-Det can reduce energy consumption by 20% and 10% on average for memory-intensive applications, without sacrificing more than 2% performance. The higher energy savings for ONAC comes from reducing the detection time by 45% as compared to Seq-Det.|ICCD|2016|10.1109/ICCD.2016.7753335|Mihir Awatramani, D. Rover, Xian Zhu, Joseph Zambreno|0.3333333333333333|2
470|Performance optimisation strategies for automatically generated FPGA accelerators for biomedical models|Biomedical modelling that is mathematically described by ordinary differential equations (ODEs) is often one of the most computationally intensive parts of simulations. With high inherent parallelism, hardware acceleration based on field programmable gate array has great potential to increase the computational performance of the ODE model integration while being very power efficient. ODE‐based Domain‐specific Synthesis Tool is a tool we proposed previously to automatically generate the complete hardware/software co‐design framework for computing biomedical models based on CellML. Although it provides remarkable performance improvement and high energy efficiency compared with CPUs and GPUs, there is still a great potential for optimisation. In this paper, we investigate a set of optimisation strategies including compiler optimisation, resource fitting and balancing, and multiple pipelines. They all have in common that they can be performed automatically and hence can be integrated in our domain‐specific high level synthesis tool. We evaluate the optimised hardware accelerator modules generated by ODE‐based Domain‐specific Synthesis Tool on real hardware based on their resource usage, processing speed and power consumption. The results are compared with single threaded and multi‐core CPUs with/without Streaming SIMD Extension (SSE) optimisation and a graphics card. The results show that the proposed optimisation strategies provide significant performance improvement and result in even more energy‐efficient hardware accelerator modules. Furthermore, the resources of the target field programmable gate array device can be more efficiently utilised in order to fit larger biomedical models than before. Copyright © 2015 John Wiley & Sons, Ltd.|Concurrency and Computation|2016|10.1002/cpe.3699|Ting-Rong Yu, O. Sinnen, J. Oppermann, C. Bradley|0.3333333333333333|2
478|ACCEPTOR: a model and a protocol for real-time multi-mode applications on reconfigurable heterogeneous platforms|In this work, we consider hard real-time applications scheduled upon heterogeneous multiprocessor platforms. The originality of this study is to consider multi-mode real-time applications (software aspects) and reconfigurable-heterogeneous hardware platforms (composed of CPUs, GPUs, FPGAs...). Our approach is based on a multi-mode protocol, for mode-dependent tasks upon reconfigurable hardware. The goal is to handle predictable switches between different task sets and different hardware settings. The novelty here is the dynamic hardware and software reconfigurability. First, we propose a formal model of the applications and reconfigurable hardware platforms. We then propose and prove correct a mode change protocol. We propose in particular a validity test for the verification of the timing constraints of the application --- including the time allowed to complete a mode change. We also perform a complete evaluation. We study the theoretical complexity of the protocol, use a simulation to evaluate the efficiency of our solution, and finally propose a competitive analysis of our protocol to prove that it is 2-competitive.|International Conference on Real-Time and Network Systems|2019|10.1145/3356401.3356420|Xavier Poczekajlo, Paul Rodríguez, Antonio Paolillo, J. Goossens|0.3333333333333333|2
482|An FPGA-Based Architecture to Simulate Cellular Automata with Large Neighborhoods in Real Time|In this paper we present a reconfigurable logic-based parallel architecture for the computation of 29X29 large-neighborhood cellular automata at 60 frames-per-second (FPS) real time update rate, using a small FPGA. The computation for each one of the n^2 elements of a two-dimensional input is O(k^2), where k is the size of the neighborhood in each dimension. All buffering and computation is performed internally in the FPGA. In terms of performance results, our architecture outperforms a general-purpose CPU running highly optimized software programmed in C by up to 51X; in neighborhoods up to 11X11 in which there are published results from GPUs our architecture has similar performance to GPUs at one-tenth the energy requirements, however, our architecture has the same performance for 29X29 neighborhoods whereas GPU performance drops as neighborhood grows. We expect this work to provide enabling new tools for the use of cellular automata models in the physical sciences.|International Conference on Field-Programmable Logic and Applications|2019|10.1109/FPL.2019.00024|A. Dollas, Nikolaos Kyparissas|0.3333333333333333|2
495|Cluster-Based Spectral-Spatial Segmentation of Hyperspectral Imagery|In this paper, we present a multi-step method for performing unsupervised segmentation of hyperspectral imagery using modified clustering algorithms which incorporate both the spatial and spectral information present within the scene. The algorithm does not require apriori knowledge of the number of targets, spectral signatures, or any training data. Instead, the method divides the image into spatially connected regions based solely on an intrinsic notion of spectral similarity. As such, the method is expected to generalize well to many real-world analysis scenarios. The overall runtime complexity of the method is $O(n^{3})$, but the method is amenable to GPU-based parallel computation and algorithmic optimizations which allow for considerable speedup in practice. Using a simplified, synthetic data generation model, we perform Monte Carlo simulations on various images to provide performance metrics in idealized scenarios. Finally, we provide anecdotal evidence of the viability of the method by segmenting several common hyperspectral images found in the literature and providing the graphical result.|Workshop on Hyperspectral Image and Signal Processing|2019|10.1109/WHISPERS.2019.8921232|James W. Scrofani, William Williamson, Sean M. Kennedy|0.3333333333333333|2
509|Accelerated X-Ray Diffraction (Tensor) Tomography Simulation Using OptiX GPU Ray-Tracing Engine|X-ray diffraction tomography (XDT) is used to probe the material composition of objects providing improved contrast between materials compared with conventional transmission-based computed tomography (CT). Current challenges presented with XDT include long image acquisition and simulation time. To accelerate the simulation speed, our approach is to adopt NVIDIA’s OptiX ray-tracing engine, a parallelized pipeline for graphics processing units (GPUs), to perform XDT simulations on objects by making use of the innovative transformation from conventional 3-D physical space into a 2-D quasi-reciprocal space. The advantage is that ray tracing in this domain requires only 3-D mesh objects, yielding calculations without the need for voxels. The simulated XDT projections demonstrate high consistency with voxel models, with a normalized mean square difference less than 0.66%, and the ray-tracing time is two orders of magnitude less than the previously reported voxel-based GPU ray-tracing results. Due to an accelerated simulation time, XDT projections of objects with three spatial dimensions (4-D tensor) have also been reported, demonstrating the feasibility for large-scale high-dimensional tensor tomography simulations.|IEEE Transactions on Nuclear Science|2019|10.1109/TNS.2019.2948796|Joseph Ulseth, Zheyuan Zhu, Shuo Pang, Yangyang Sun|0.3333333333333333|2
518|Three-Dimensional Numerical Method for Simulating Large-Scale Free Water Surface by Massive Parallel Computing on a GPU|Water wave dynamics and its engineering application have always been a key issue in the field of hydraulics, and effective and efficient numerical methods need to be proposed to perform three-dimensional (3-D) simulation of large-scale water fluctuation in engineering practice. A single-phase free-surface lattice Boltzmann method (SPFS-LB method) is coupled with a large-eddy simulation approach for simulating large-scale free water surface flows, and the simulation is accelerated on a GPU (graphic processing unit). The coupling model is used to simulate the evolution process of dam-break wave after complete and partial dam-break. The formation mechanism of horizontal and vertical vortices in water after partial dam-break and the advance and evolution process of dam-break flow on non-contour riverbed are analyzed. The method has been verified to be reasonable and can obtain a more accurate time curve of water level fluctuation. Applying this method to practical arch dams, discharge coefficients consistent with empirical formulas can be obtained by comparison and analysis, and the surface flow phenomena (such as tongue diffusion, surface fragmentation, and surface fusion) can be well simulated by this method. In addition, based on the key technology of parallel computing on a GPU, the implementation of the SPFS-LB model on a GPU unit achieves tens of millions of lattice updates per second, which is over fifty times higher than that on a single CPU chip. It is proved that the proposed method for large-scale water fluctuations can be used to study practical engineering problems. The mathematical model method realizes the efficient and accurate simulation of practical physical problems.|Water|2019|10.3390/w11102121|Wei Diao, Shu‐Qing Yang, Chunze Zhang, Yongqing Peng, Xujin Zhang|0.3333333333333333|2
535|GPU-accelerated Simulator for Optical Tomography applied to Two-Phase Flows|Optical tomography (OT) is a modality of tomographic imaging that can provide cross-sectional imaging of phase distributions in two-phase pipe flows, thus having potential application in process monitoring. Due to the strong effect of refraction and reflection on phase boundaries (e.g. gas-liquid interfaces), OT measurements of two-phase flows cannot be modeled using hard-field assumptions. This fact renders the use of traditional tomographic reconstruction techniques unsuitable for this problem. In this paper, we present a GPU-accelerated system capable of providing simulation of light transport in an OT imaging system. Sinograms of real OT measurements taken of a phantom are visually compared with simulations, yielding positive results. Quantitative validation of the simulator is left for future works.|International Symposium on Telecommunications|2019|10.1109/IST48021.2019.9010472|R. Morales, E. N. Santos, M. J. Silva, R. S. Bernardelli, D. Pipa|0.3333333333333333|2
547|Multi-Phase Gearbox Modelling Using GPU-Accelerated Smoothed Particle Hydrodynamics Method|\n Developments in automotive design such as electrification of engines and a growing need to improve driveline efficiency requires adaption of old techniques. The ability to make fast and accurate Computational Fluid Dynamics (CFD) assessment is of high importance to the development of novel powertrains. Consequently, innovative numerical techniques and continuous improvements to existing CFD codes is relevant to ensure reliability. This work extends the capabilities of a Smoothed Particle Hydrodynamics (SPH) code to include multiphase modeling, studied using a gearbox model.\n A vast majority of CFD codes use grid-based approaches following the Eulerian spatial discretization, which is quite established in engineering applications. Lagrangian based approaches where the moving fluid particles are discretized over time and space present a promising alternative. One of the most common methods of this kind is the Smoothed Particle Hydrodynamics (SPH) method, a fully Lagrangian, particle-based approach for fluid-flow simulations. The main advantage is the absence of numerical grid for computations, which eliminates complexities of interface handling. Nowadays, the SPH approach is more commonly used for hydro-engineering applications involving free-surface flows. New techniques to perform numerical simulations on Graphics Processing Units (GPU) virtually eliminates some of the disadvantages of the method. In this work, we present our multi-GPU solution designed for both GPU-equipped desktops and multi-GPU supercomputers.\n Fluid dynamic simulations on a single gearbox model is used to validate the multiphase model, by comparing the results with earlier simulations that use a single-phase model omitting air-lubricant interface in the gearbox. The base case in the study is a single bevel gear placed inside a cuboid case with a lubricant depth equivalent to 25% gear diameter. Simulations are performed at various rotational speeds, and corresponding lubricant distribution and churning losses are obtained. The current study targets a comparison of the single-phase and multiphase models in approximating the lubricant distribution and churning loss values at nominal rotational speeds. This serves to standardize the numerical procedure, which will help in improving the accuracy of churning loss calculations through validations against experimental results in the future.|Volume 3A: Fluid Applications and Systems|2019|10.1115/ajkfluids2019-5592|Vishal Maurya, Muraleekrishnan Menon, K. Szewc|0.3333333333333333|2
622|Repeatability, Reproducibility, Computer Science and High Performance Computing : Stochastic simulations can be reproducible too…|Many of us know the important work of Karl Popper, as philosopher of Science, specialized in modeling and simulation. Karl Popper is generally regarded as one of the greatest philosophers of Science of the 20th century. Among the criteria of the scientific method, which are intangible principles, we find refutability, non-contradiction and reproducibility. The latter is one of the conditions on which Popper distinguishes between the scientific or pseudo-scientific character of a study. Indeed, Science works by drawing “laws” or “principles” from reproducible observations whose main property is to be true as long as no observation has proved otherwise. Scientific conclusions can only be drawn from a well observed and described “result/event”, which has appeared several times, and has been observed by different people and/or studies. This criterion eliminates what seriously distorts the results as well as errors in judgment or even manipulations by scientists.Traditionally we have 2 main branches of the scientific method: the deductive branch with Mathematics and formal logic proofs and the empirical branch with statistical analysis of controlled experiments. We now see claims or hopes that Large Scale Simulation could be a 3rd branch and data intensive or data driven computer Science would be a4 th branch. The problem is that `our’ Computer Science is a young Science that do not currently meet the standards of branches 1 and 2.Computer Science rely on programs, that humans have to debug. Therefore, what is first required is even more stringent than reproducibility, it is repeatability. This means that with exactly the same input and execution environment, we expect exactly the same results. This is essential at least for debugging. Repeatability with single-core CPUs was relatively straightforward. With high performance computing and the hardware developments of last decades it is becoming more tough, if not impossible to obtain repeatability, often named “bitwise reproducibility” by computer Scientists. In recent computer Science papers we often see the term reproducibility but in fact it means the possibility to repeat the experiment and obtain the same result when the scientific computation is run several times with the same input data and the same execution context [2]. Replicability avoids changes and is required for debugging, reproducibility requires changes in the general sense. Reproducing computer experiments can be achieved by different studies, with different methods and hardware, but they will produce the same scientific conclusion.The problem with High Performance Computing (HPC) is that we are losing repeatability. It started with parallel systems where we realized that we could not obtain run to run repeatability, even on the same machine [1], and it became harder and harder, if not impossible to reach this repeatability goal with the numerical experiment run on different machines, or with different numbers of processing units, different types of processors/accelerators with different architectures, execution environments, computational loads, etc. Even our processor manufacturers started to introduce the loss of repeatability with non-determinism at the level of CPU cores (with out-of-order execution – see Intel documentation on this point1) This optimization sometimes impacts the numerical results because floating point operations are non-associative. Most computer scientists are aware of that (not all), but the majority of HPC end-users do not realize this and they expect repeatability in sensitive area. Hybrid computing combining multicore CPUs and compute accelerators like GPUs, many-core processors like Intel Xeon Phi, or even dedicated FPGAs is amplifying this lack of repeatability. The main problem of this fact is that it is becoming particularly tough to setup debugged parallel programs, even at the level of a single computing node which already embed a lot of parallelism. When we have to track and understand a specific event, like particle tracking, the lack of run to run repeatability can be a problem. The quest for repeatability is not limited to debugging.1Run-to-Run Reproducibility of Floating-Point Calculations for Applications on Intel® Xeon Phi™ Coprocessors (and Intel® Xeon® Processors) – by Martin Cordel, https://software.intel.com/en-us/articles/run-to-run-reproducibility-of-floating-point-calculations-for-applications-on-intel-xeonThe trend in numerical simulation is to build more complex models, sometimes with strict accuracy requirements. This often implies the use of high performance computing platforms tuned to obtain the results in an acceptable time (precise climate simulation is an example). For extreme cases in optimization problems, stochastic methods are used to produce interesting approximate solutions much faster than what deterministic algorithms would produce. In the case of complex numerical integration on high dimensional domains, the deterministic methods are often slow and thus the use of Monte Carlo methods is required. In this context, parallel stochastic simulations are too often presented as “non-reproducible” since they have built-in randomness. But in fact, Monte Carlo methods rely on pseudo-random numbers on purpose. When we use pseudo-random numbers for stochastic models, we are running deterministic experiments since pseudo-random number generators have been carefully crafted to be repeatable. This is essential for debugging. In addition, a careful and thorough parallelization of the pseudo-random number generators has to be seriously considered to avoid introducing strange biases in the statistics of your stochastic simulation [3].However, anyone wishing to produce a scientific work of quality must pay attention to the numerical reproducibility of simulation results [4]. In this talk we will see that significant differences are observed in the results of parallel stochastic simulations if the practitioner fails to apply best practices. Remembering that pseudo-random number generators are deterministic, it is possible to reproduce the same numerical results for parallel stochastic simulations by implementing a rigorous method. A test application running with a billion threads will show that the method enables checking the parallel results with their sequential counterpart before scaling. This possible comparison, at small scale, increases confidence in the proposed models. This talk will present this method in the current context of numerical reproducibility which include soft errors impacting all top 500 machines, including the future Exascale machines that are coming.|International Symposium on High Performance Computing Systems and Applications|2019|10.1109/HPCS48598.2019.9188157|D. Hill|0.3333333333333333|2
841|Morphling: A Reconfigurable Architecture for Tensor Computation|Tensor algebra plays a major role in various applications, including data analysis, machine learning, and hydrodynamics simulation. Different tensor algebra inherently varies in dimension, size, and computation, leading to different execution preference, including parallelization, data arrangement, and accumulation. Another critical aspect for tensor algebra is the involved tensors can be with varying mixes of dense and sparse representation. Such diversified applications are notoriously difficult to accelerate. Prior ASIC architectures do not meet the needs due to fixed dataflow and prior fine-grained fabrics (e.g., FPGAs) solutions offer limited performance and power improvement due to bit-level reconfigurable structure. In this article, we propose Morphling, a reconfigurable architecture that can flexibly handle both dense and sparse tensor computation. We first generalize a flexible execution model that decomposes tensor operations into three steps, including tensor vectorization, vector computation, and output reduction. The dense and sparse tensor computation share the same execution model, but differ in the vector computation step where the multiplications are conducted. Depending on the number of inputs and outputs that are linked together in the computation step, we define three parallel patterns, including many-to-one, one-to-many, and one-to-one, which correspond to different implementations for dense and sparse computation. Furthermore, to efficiently support sparse tensor, we design a tiled-BCSR format that enables high parallelism and balanced workload. At the architecture level, we propose a reconfigurable design to support the execution model. The hardware units can be reconfigured to support different datapath and enable different types of data reuse. We evaluate Morphling using various tensor operations and compare it with CPU, GPU, FPGA, and state-of-the-art ASIC designs. Overall, Morphling achieves 13.4X, 677.7X, 44.7X energy efficiency over Xilinx ZC706 FPGA, Intel i7-9700K CPU, and NVIDIA TitanX GPU.|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|2022|10.1109/TCAD.2021.3135322|Liqiang Lu, Yun Liang|0.3333333333333333|2
851|${\tt simwave}$ -- A Finite Difference Simulator for Acoustic Waves Propagation|${\tt simwave}$ is an open-source Python package to perform wave simulations in 2D or 3D domains. It solves the constant and variable density acoustic wave equation with the finite difference method and has support for domain truncation techniques, several boundary conditions, and the modeling of sources and receivers given a user-defined acquisition geometry. The architecture of ${\tt simwave}$ is designed for applications with geophysical exploration in mind. Its Python front-end enables straightforward integration with many existing Python scientific libraries for the composition of more complex workflows and applications (e.g., migration and inversion problems). The back-end is implemented in C enabling performance portability across a range of computing hardware and compilers including both CPUs and GPUs.||2022|10.3997/2214-4609.2022.80009|E. Gomi, J. B. D. Moreira, K. Roberts, E. C. Silva, H. Senger, J. Souza, Roussian R. A. Gaioso|0.3333333333333333|2
862|Accelerated Airborne Virus Spread Simulation: Coupling Agent-based Modeling with GPU-accelerated Computational Fluid Dynamics|The Coronavirus Disease 2019 (COVID-19) has shown us the necessity to understand its transmission mechanisms in detail in order to establish practice in controlling such infectious diseases. An important instrument in doing so are mathematical models. However, they do not account for the spatiotemporal heterogeneity introduced by the movement and interaction of individuals with their surroundings. Computational fluid dynamics (CFD) simulations can be used to analyze transmission on micro- and mesostructure level, however become infeasible in larger scale scenarios. Agent-based modeling (ABM) on the other hand is missing means to simulate airborne virus transmission on a micro- and mesostructure level. Therefore, we present a system that combines CFD simulations with the dynamics given by trajectories from an ABM to form a basis for producing deeper insights. The proposed system is still work in progress;thus we focus on the system architecture and show preliminary results.|VISIGRAPP|2022|10.5220/0010904500003124|Lin Shao, Christoph Schinko, Marius Erdt, Kan Chen, D. Weber, Xingzi Zhang, V. Settgast, E. Eggeling, J. Mueller-Roemer, Alexander Steinhardt, Eugene Lee, Bastian Sander|0.3333333333333333|2
870|Novel Perlin-based phantoms using 3D models of compressed breast shapes and fractal noise|Virtual clinical trials (VCTs) have been used widely to evaluate digital breast tomosynthesis (DBT) systems. VCTs require realistic simulations of the breast anatomy (phantoms) to characterize lesions and to estimate risk of masking cancers. This study introduces the use of Perlin-based phantoms to optimize the acquisition geometry of a novel DBT prototype. These phantoms were developed using a GPU implementation of a novel library called Perlin-CuPy. The breast anatomy is simulated using 3D models under mammography cranio-caudal compression. In total, 240 phantoms were created using compressed breast thickness, chest-wall to nipple distance, and skin thickness that varied in a {[35, 75], [59, 130), [1.0, 2.0]} mm interval, respectively. DBT projections and reconstructions of the phantoms were simulated using two acquisition geometries of our DBT prototype. The performance of both acquisition geometries was compared using breast volume segmentations of the Perlin phantoms. Results show that breast volume estimates are improved with the introduction of posterior-anterior motion of the x-ray source in DBT acquisitions. The breast volume is overestimated in DBT, varying substantially with the acquisition geometry; segmentation errors are more evident for thicker and larger breasts. These results provide additional evidence and suggest that custom acquisition geometries can improve the performance and accuracy in DBT. Perlin phantoms help to identify limitations in acquisition geometries and to optimize the performance of the DBT prototypes.|Medical Imaging|2022|10.1117/12.2612565|P. Bakic, R. Acciavatti, Joao P. V. Teixeira, T. Vent, Andrew D. A. Maidment, S. Krishnamoorthy, M. Dustler, Yuri Malheiros, B. Barufaldi, Telmo M. Silva Filho, S. Surti, T. G. do Rêgo|0.3333333333333333|2
883|Parallel OpenMP and OpenACC Mixing Layer Simulation|It is estimated that up to 25% of the grain crop ends up being lost in the post-harvest. The correct drying of the beans is one of the measures to contain this loss. As the grain mass is a set of solid and empty spaces, its drying could be considered a problem of the coupled open-porous medium. In this paper, a mathematical and computer simulation model was proposed, which describes the convection in a free flow with a porous obstacle applied to the drying of the grain. A computational fluid dynamics scheme was implemented in FORTRAN using Finite Volume to simulate and compute the numerical solutions. The code is parallel implemented using OpenMP and OpenACC programming interfaces. As a result, there was a significant reduction in processing time in both cases. The total simulation time was eight times less for a multicore architecture (16 physical cores) and 17.3 times using a single GPU (Quadro M5000).|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2022|10.1109/pdp55904.2022.00036|C. Schepke, H. Silva, C. Cristaldo, Natiele Lucca, Dalmo Paim de Oliveira|0.3333333333333333|2
897|GPU-Accelerated All-atom Particle-Mesh Ewald Continuous Constant pH Molecular Dynamics in Amber|Constant pH molecular dynamics (MD) simulations sample protonation states on the fly according to the conformational environment and user specified pH condition; however, the accuracy of most current applications is limited due to the use of implicit-solvent models or hybrid solvent scheme. Here we report the first GPU-accelerated implementation, parameterization, and validation of the all-atom continuous constant pH MD (CpHMD) method with particle-mesh Ewald (PME) electrostatics in the Am-ber22 pmemd.cuda engine. The titration parameters for Asp, Glu, His, Cys, and Lys were derived for the CHARMM c22 and Amber ff14sb and ff19sb force fields. We then evaluated the PME-CpHMD method using the asynchronous pH replicaexchange titration simulations with the c22 force field for six benchmark proteins, including BBL, hen egg white lysozyme (HEWL), staphylococcal nuclease (SNase), thioredoxin, ribonuclease A (RNaseA), and human muscle creatine kinase (HMCK). The root-mean-square deviation from the experimental pKa’s of Asp, Glu, His, and Cys is 0.76 pH units, and the Pearson’s correlation coefficient for the pKa shifts with respect to model values is 0.80. We demonstrated that a finite-size correction or much enlarged simulation box size can remove a systematic error of the calculated pKa’s and improve agreement with experiment. Importantly, the simulations captured the relevant biology in several challenging cases, e.g., the titration order of the catalytic dyad Glu35/Asp52 in HEWL and the coupled residues Asp19/Asp21 in SNase, the large pKa upshift of the deeply buried catalytic Asp26 in thioredoxin, and the large pKa downshift of the deeply buried catalytic Cys283 in HMCK. We anticipate that PME-CpHMD will offer proper pH control to improve the accuracies of MD simulations and enable mechanistic studies of proton-coupled dynamical processes that are ubiquitous in biology but remain poorly understood due to the lack of experimental tools and limitation of current MD simulations.|bioRxiv|2022|10.1101/2022.06.04.494833|Julie A. Harris, Jana K. Shen, Vinícius Martins de Oliveira, Jack A. Henderson, Ruibin Liu, Erik Vaquez Montelongo|0.3333333333333333|2
902|Protea: client profiling within federated systems using flower|Federated Learning (FL) has emerged as a prospective solution that facilitates the training of a high-performing centralised model without compromising the privacy of users. While successful, FL research is currently limited by the difficulties of establishing a realistic large-scale FL system at the early stages of experimentation. Simulation can help accelerate this process. To facilitate efficient scalable FL simulation of heterogeneous clients, we design and implement Protea, a flexible and lightweight client profiling component within federated systems using the FL framework Flower. It allows automatically collecting system-level statistics and estimating the resources needed for each client, thus running the simulation in a resource-aware fashion. The results show that our design successfully increases parallelism for 1.66 X faster wall-clock time and 2.6X better GPU utilisation, which enables large-scale experiments on heterogeneous clients.|arXiv.org|2022|10.1145/3556557.3557950|Wanru Zhao, J. Fernández-Marqués, P. P. B. D. Gusmão, Xinchi Qiu, N. Lane|0.3333333333333333|2
919|A fast GPU‐accelerated Monte Carlo engine for calculation of MLC‐collimated electron fields|Abstract Background Although intensity‐modulated radiation therapy and volumetric arc therapy have revolutionized photon external beam therapies, the technological advances associated with electron beam therapy have fallen behind. Modern linear accelerators contain technologies that would allow for more advanced forms of electron treatments, such as beam collimation, using the conventional photon multi‐leaf collimator (MLC); however, no commercial solutions exist that calculate dose from such beam delivery modes. Additionally, for clinical adoption to occur, dose calculation times would need to be on par with that of modern dose calculation algorithms. Purpose This work developed a graphics processing unit (GPU)‐accelerated Monte Carlo (MC) engine incorporating the Varian TrueBeam linac head geometry for a rapid calculation of electron beams collimated using the conventional photon MLC. Methods A compute unified device architecture framework was created for the following: (1) transport of electrons and photons through the linac head geometry, considering multiple scattering, Bremsstrahlung, Møller, Compton, and pair production interactions; (2) electron and photon propagation through the CT geometry, considering all interactions plus the photoelectric effect; and (3) secondary particle cascades through the linac head and within the CT geometry. The linac head collimating geometry was modeled according to the specifications provided by the vendor, who also provided phase‐space files. The MC was benchmarked against EGSnrc/DOSXYZnrc/GEANT by simulating individual interactions with simple geometries, pencil, and square beam dose calculations in various phantoms. MC‐calculated dose distributions for MLC and jaw‐collimated electron fields were compared to measurements in a water phantom and with radiochromic film. Results Pencil and square beam dose distributions are in good agreement with DOSXYZnrc. Angular and spatial distributions for multiple scattering and secondary particle production in thin slab geometries are in good agreement with EGSnrc and GEANT. Dose profiles for MLC and jaw‐collimated 6–20‐MeV electron beams showed an average absolute difference of 1.1 and 1.9 mm for the FWHM and 80%–20% penumbra from measured profiles. Percent depth doses showed differences of <5% for as compared to measurement. The computation time on an NVIDIA Tesla V100 card was 2.5 min to achieve a dose uncertainty of <1%, which is ∼300 times faster than published results in a similar geometry using a single‐CPU core. Conclusions The GPU‐based MC can quickly calculate dose for electron fields collimated using the conventional photon MLC. The fast calculation times will allow for a rapid calculation of electron fields for mixed photon and electron particle therapy.|Medical Physics (Lancaster)|2022|10.1002/mp.15938|J. Antolak, Eric E. Brost, H. S. Wan Chan Tseung|0.3333333333333333|2
929|CompF2: Theoretical Calculations and Simulation Topical Group Report|This report summarizes the work of the Computational Frontier topical group on theoretical calculations and simulation for Snowmass 2021. We discuss the challenges, potential solutions, and needs facing six diverse but related topical areas that span the subject of theoretical calculations and simulation in high energy physics (HEP): cosmic calculations, particle accelerator modeling, detector simulation, event generators, perturbative calculations, and lattice QCD (quantum chromodynamics). The challenges arise from the next generations of HEP experiments, which will include more complex instruments, provide larger data volumes, and perform more precise measurements. Calculations and simulations will need to keep up with these increased requirements. The other aspect of the challenge is the evolution of computing landscape away from general-purpose computing on CPUs and toward special-purpose accelerators and coprocessors such as GPUs and FPGAs. These newer devices can provide substantial improvements for certain categories of algorithms, at the expense of more specialized programming and memory and data access patterns.||2022|10.2172/1895409|K. Pedro, Ji Qiang, P. Boyle|0.3333333333333333|2
935|Towards the Simulation of a Realistic Large-Scale Spiking Network on a Desktop Multi-GPU System|The reproduction of the brain ’sactivity and its functionality is the main goal of modern neuroscience. To this aim, several models have been proposed to describe the activity of single neurons at different levels of detail. Then, single neurons are linked together to build a network, in order to reproduce complex behaviors. In the literature, different network-building rules and models have been described, targeting realistic distributions and connections of the neurons. In particular, the Granular layEr Simulator (GES) performs the granular layer network reconstruction considering biologically realistic rules to connect the neurons. Moreover, it simulates the network considering the Hodgkin–Huxley model. The work proposed in this paper adopts the network reconstruction model of GES and proposes a simulation module based on Leaky Integrate and Fire (LIF) model. This simulator targets the reproduction of the activity of large scale networks, exploiting the GPU technology to reduce the processing times. Experimental results show that a multi-GPU system reduces the simulation of a network with more than 1.8 million neurons from approximately 54 to 13 h.|Bioengineering|2022|10.3390/bioengineering9100543|E. Torti, G. Danese, Arianna Dorici, F. Leporati, Giordana Florimbi|0.3333333333333333|2
958|Error Model (EM)―A New Way of Doing Fault Simulation|This paper introduces the concept of error model (EM) that replaces a traditional fault-model-based simulation. EM is a temporal simulation of fault symptoms in an application processor. This paper shows that the application resiliency metrics, such as fault coverage, derived through EM are more comprehensive and accurate than those derived through empirical models like single stuck-at faults. In addition, EM can be used on high-level simulation models (behavioral RTL, emulation or in some cases in-silicon). EM approach gives greater than three orders of performance improvement over gate netlist models using stuck-at fault simulation. This paper shows that the coverage metrics, for a billion-logic-gate GPU design, obtained through in-silicon EM closely match the corresponding coverage metrics estimated from a low-level netlist with single stuck-at fault simulation.|International Test Conference|2022|10.1109/ITC50671.2022.00040|N. Saxena, A. Lotfi|0.3333333333333333|2
970|GPU Adaptive In-situ Parallel Analytics (GAP)|Despite the popularity of in-situ analytics in scientific computing, there is only limited work to date on in-situ analytics for simulations running on GPUs. Notably, two unaddressed challenges are 1) performing memory-efficient in-situ analysis on accelerators and 2)automatically choosing the processing resources and suitable data representation for a given query and platform. This paper addresses both problems. First, GAP makes several new contributions toward making bitmap indices suitable, effective, and efficient as a compressed data summary structure for the GPUs - this includes introducing a layout structure, a method for generating multi-attribute bitmaps, and novel techniques for bitmap-based processing of major operators that comprise complex data analytics. Second, this paper presents a performance modeling methodology, aiming to predict the placement (i.e., CPU or GPU) and the data representation choice (summarization or original) that yield the best performance on a given configuration. Our extensive evaluation of complex in-situ queries and real-world simulations shows that with our methods, analytics on GPU using bitmaps almost always outperforms other options, and the GAP performance model predicts the optimal placement and data representation for most scenarios.|International Conference on Parallel Architectures and Compilation Techniques|2022|10.1145/3559009.3569661|R. Ramnath, G. Agrawal, Haoyuan Xing|0.3333333333333333|2
972|The Design and Utilisation of PanSim, a Portable Pandemic Simulator|The COVID-19 pandemic has presented a clear and present need for urgent decision making. Set in an environment of uncertain and unreliable data, and a diverse range of possible interventions, there is an obvious need for integrating HPC into workflows that include model calibration, and the exploration of the decision space. In this paper, we present the design of PanSim, a portable, performant, and productive agent-based simulator, which has been extensively used to model and forecast the pandemic in Hungary. We show its performance and scalability on CPUs and GPUs, then we discuss the workflows PanSim integrates into. We describe the heterogeneous, resource-constrained HPC environment available to us, and formulate a scheduling optimisation problem, as well as heuristics to solve them, to either minimise the execution time of a given number of simulations or to maximise the number of simulations executed in a given time frame.|2022 First Combined International Workshop on Interactive Urgent Supercomputing (CIW-IUS)|2022|10.1109/CIW-IUS56691.2022.00006|Péter Polcz, G. Szederkényi, G. Horváth, Attila Csikász-Nagy, János Juhász, B. Keömley-Horváth, Kálmán Tornai, I. Reguly, György Cserey, Bálint Siklósi|0.3333333333333333|2
974|Time-series ML-regression on Graphcore IPU-M2000 and Nvidia A100|We compare the ML-training performance of a Graphcore IPU-M2000-based system with Nvidia A100 GPU-based system on the Perlmutter HPC machine at NERSC/LBL. The multivariate regression of time series data from a simulated biological neuron was the scientific benchmark problem. The ML-model consisted of several convolutional, batch normalization, and fully connected layers. The training data were distributed in CPUs memory to eliminate the system dependent IO cost. The data-parallel training runs resulted in the same samples throughput on both GC200 IPUs and A100 GPUs for any choice of the number of accelerators between 1 and 256. The achieved best MSE validation loss on IPUs was only 10% to 20% larger. The aggregated energy use per 1 training epoch was between 2.5 to 3 times smaller for the Graphcore system in comparison to the Nvidia system. This paper also discusses aspects of software-hardware co-design to achieve highest efficiency on the IPU using PopTorch.|International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems|2022|10.1109/PMBS56514.2022.00019|A. Tsyplikhin, Jan Balewski, Kristofer E Bouchard, Z. Liu, Manuel Lopez Roland|0.3333333333333333|2
979|ApproxTorch: An Approximate Multiplier Evaluation Environment for CNNs based on Pytorch|Recently, approximate multipliers for CNNs have been studied hard, but evaluation of CNNs with approximate multipliers is always slow and requires many coding efforts. To solve this problem, we present ApproxTorch, an evaluation environment to simulate CNN models with 8-bit approximate multipliers. ApproxTorch provides Python classes for approximate convolution layers and fully-connected layers just like Pytorch classes which makes model transformation much easier. The behavior of an approximate multiplier is represented as a look-up-table and implemented as memory access. By exploiting the powerful Pytorch library for GPU, ApproxTorch can run CNNs with approximate multipliers much faster than traditional methods on CPU.|International SoC Design Conference|2022|10.1109/ISOCC56007.2022.10031519|Ke Ma, S. Kimura|0.3333333333333333|2
1149|Evolving MPI+X Toward Exascale|The recent trend in highperformance computing (HPC) to adopt accelerators such as GPUs, eld-programmable gate arrays, and coprocessors has led to signi cant heterogeneity in computation and memory subsystems. Application developers typically employ a hierarchical message passing interface (MPI) programming model across the cluster’s compute nodes, and an intranode model such as OpenMP or an accelerator-speci c library such as compute uni ed device architecture (CUDA) or open computing language (OpenCL) for the CPUs and accelerator devices within each compute node. To achieve acceptable performance levels, application programmers must have in-depth knowledge of machine topology, compute capability, memory hierarchy, compute-memory synchronization semantics, and other system characteristics. However, explicit management of computation and memory resources along with a disjointed programming model mean that programmers must make tradeo s between performance and productivity. In “MPI-ACC: Accelerator-Aware MPI for Scienti c Applications” (IEEE Trans. Parallel and Distributed Systems, vol. 27, no. 5, 2016, pp. 1401–1414), Ashwin Aji and his colleagues from Virginia Tech, Argonne National Laboratory, North Carolina State University, and Rice University present a uni ed programming model and runtime system for HPC clusters with heterogeneous computing devices. Speci cally, they introduce MPI-ACC, an evolutionary step in the MPI+X programming model, which is the de facto standard for distributed memory clusters. By evolving an already popular programming model, the authors make it easier to modernize the code of existing MPI-based applications. Aji and his team note that when invoking a data-movement routine in MPI-ACC, programmers can simply describe additional data attributes speci c to the within-node elements— such as the GPU command queue, execution stream, or device context— without changing the MPI standard. MPI-ACC’s runtime system employs user-speci ed data attributes to not only perform end-to-end data movement across the network but also synchronize with inight GPU kernels to achieve e cient overlap of communication with computation. The authors contrast their simple descriptive approach with the complex prescriptive approach of existing GPU-aware MPI implementations. They argue that although other approaches provide end-to-end data movement support between GPUs, they don’t have a mechanism to express the data’s execution attributes, which puts the burden of overlapping communication with computation on end users. The investigators also performed in-depth analysis of how MPI-ACC can be used to scale in-production scienti c applications such as an epidemic spread simulation and a seismology simulation. They further show that the MPI-ACC’s pipelined end-to-end data movement, scalable intermediate resource-management techniques, and enhanced execution progress engine outperform baseline implementations that use MPI and CUDA separately.|Computer|2016|10.1109/MC.2016.232|David A. Bader|0.3333333333333333|2
1179|Garment Simulation and Collision Detection on a Mobile Device|This paper describes several techniques for accelerating a virtual try-on garment simulation on a mobile device smartphone or tablet using parallel computing on a multicore CPU, GPU computing or both depending on the mobile hardware. The system exploits a mass-spring cloth model with velocity modification approach to overcome the super-elasticity. The simulation starts from flat garment pattern meshes positioned around a 3D human body, then seaming forces are applied on the edges of the panels until the garment is seamed and several cloth draping steps are performed in the end. The cloth-body collision detection and response algorithm is based on image-space interference tests and the cloth-cloth collision detection uses entirely GPU based approach on the newer hardware or recursive parallel algorithm on the CPU. As the results section shows the average time of dressing a virtual body with a garment on a modern smart phone supporting OpenGL ES2.0 is 2 seconds and on a tablet supporting OpenGL ES3.0 or 3.1 is less than one second.|Int. J. Mob. Comput. Multim. Commun.|2016|10.4018/IJMCMC.2016070101|T. Vassilev|0.3333333333333333|2
1262|A heterogeneous computing approach to maximum likelihood parameter estimation for the Heston model of stochastic volatility|Stochastic volatility models are of fundamental importance to the pricing of derivatives. One of the most commonly used models of stochastic volatility is the Heston model in which the price and volatility of an asset evolve as a pair of coupled stochastic differential equations. The computation of asset prices and volatilities involves the simulation of many sample trajectories with conditioning. The problem is treated using the method of particle filtering. While the simulation of a shower of particles is computationally expensive, each particle behaves independently making such simulations ideal for massively parallel heterogeneous computing platforms. We present a portable OpenCL implementation of the Heston model and discuss its performance and efficiency characteristics on a range of architectures including Intel CPUs, Nvidia GPUs, and Intel Many-Integrated-Core accelerators. \n \n References Y. Ait-Sahalia and R. Kimmel. Maximum likelihood estimation of stochastic volatility models. J. Financ. Econ. , 83:413–452, 2007. doi:10.1016/j.jfineco.2005.10.006 C. S. Forbes, G. M. Martin and J. Wright. Inference for a class of stochastic volatility models using option and spot prices: Application of a bivariate Kalman filter. Economet. Rev. , 26:387–418, 2007. doi:10.1080/07474930701220584 Khronos Opencl Working Group. The Opencl specification. Technical report, Khronos Group, October 2009. http://www.khronos.org/registry/cl/specs/opencl-1.0.pdf S. L. Heston. A closed-form solution for options with stochastic volatility with applications to bond and currency options. Rev. Financ. Stud. , 6:326–343, 1993. doi:10.1093/rfs/6.2.327 A. S. Hurn, K. A. Lindsay and A. J. McClelland. Estimating parameters of stochastic volatility models using option price data. J. Bus. Econ. Stat. , 33(4):579–594, 2015. doi:10.1080/07350015.2014.981634 A. S. Hurn, K. A. Lindsay and A. J. McClelland. On the efficacy of Fourier series approximations for pricing European and digital options. Appl. Math. , 5(17):2786–2807, 2015. doi:10.4236/am.2014.517267 M. S. Johannes, N. G. Polson and J. R. Stroud. Optimal filtering of jump diffusions: Extracting latent states from asset prices. Rev. Financ. Stud. , 22:2759–2799, 2009. doi:10.1093/rfs/hhn110 V. W. Lee, C. Kim, J. Chhugani, M. Deisher, D. Kim, A. D. Nguyen, N. Satish, M. Smelyanskiy, S. Chennupaty, P. Hammarlund, R. Singhal and P. Dubey. Debunking the 100x GPU vs CPU myth: an evaluation of throughput computing on CPU and GPU. In Proceedings of the 37th Annual International Symposium on Computer Architecture , pages 451–460, New York, NY, USA, 2010. ACM. doi:10.1145/1815961.1816021 J. A. Nelder and R. Mead. A simplex method for function minimization. Comput. J. , 7(4):308–313, 1965. doi:10.1093/comjnl/7.4.308|International Conference on High Performance Computing|2016|10.21914/ANZIAMJ.V57I0.10425|D. Warne, S. Hurn, K. Lindsay|0.3333333333333333|2
1431|LARGE-SCALE FREE-SURFACE FLOW SIMULATION USING LATTICE BOLTZMANN METHOD ON MULTI-GPU CLUSTERS|Turbulent free-surface flows around ship strongly affect maneuverability and safety. In order to understand the details of the turbulent flow and surface deformation, it is necessary to carry out high-order accurate and large-scale CFD simulations. We have developed a CFD code based on LBM (Lattice Boltzmann Method) with a single-phase free-surface model. Since violent flows are turbulent with high Reynolds number, a LES (Large-Eddy Simulation) model has to be introduced to solve the LBM equation. The coherent-structure Smagorinsky model is a state-of-the-art sub-grid scale model. Since this model is able to determine the model constant locally, it is suitable for a large-scale calculation containing complicated solid bodies. Our code is written in CUDA and MPI. The GPU kernel function is tuned to achieve high performance on the TSUBAME 2.5 supercomputer at Tokyo Institute of Technology. We obtained good scalability in weak scaling test. Each GPU handles a domain of 192 × 192 × 192, and 27 components are defined at a grid by the D3Q27 model. The fairly high performance of 809 MLUPS(Mega lattice update per second) is achieved by using 1000 GPUs in single precision. By executing this high-performance computation, turbulent violent flow simulation with real ship data is performed, and details of turbulent flows and freesurface deformations will be simulated with much higher accuracy than ever before.||2016|10.7712/100016.1886.8019|Naoyuki Onodera, K. Ohashi|0.3333333333333333|2
1535|A vertex model of recrystallization with stored energy implemented in GPU|Recrystallization models and simulations have been the subject of much attention in materials community in the past decades due to this process having a significant effect on many technologically important materials characteristics. Statistical analysis performed close to the steady state requires large-scale simulations, which are often prohibitively expensive from computational point of view. Graphical Processing Unit (GPU)-based realizations provide a viable approach to addressing this challenge, yet they remain relatively under-explored in this context.In the present manuscript, we develop a fully-parallelizable matrix-free GPU-based algorithm for implementing a two-dimensional vertex model of recrystallization based on the stored energy formalism. Nucleation is assumed to take place at triple junctions and obeys a Metropolis-type criterion. We include a complete mathematical analysis of the nucleation model deriving conditions under which nucleation is successful. Stability analysis of the dynamics of a triple junction under the presence of bulk energy is provided. On the computational side, we propose a novel polling system for handling topological transitions to ensure robust GPU implementation. Single grain tests are performed for benchmarking purposes. Finally, a set of numerical experiments for large scale systems is presented to explore the effect of initial distributions of stored energy on several statistical characteristics.|Materials Research Express|2019|10.1088/2053-1591/ab00f8|M. Emelianenko, D. Golovaty, Claudio E. Torres, Alejandro H. J. Sazo|0.3333333333333333|2
1537|OpenCL vs: Accelerated Finite-Difference Digital Synthesis|Digital audio synthesis has become an important component of modern music production with techniques that can produce realistic simulations of real instruments. Physical modelling sound synthesis is a category of audio synthesis that uses mathematical models to emulate the physical phenomena of acoustic musical instruments including drum membranes, air columns and strings. The synthesis of physical phenomena can be expressed as discrete variants of Newton's laws of motion, using, for example, the Finite-Difference Time-Domain method or FDTD. FDTD is notoriously computationally expensive and the real time demands of sound synthesis in a live setting has led implementers to consider offloading to GPUs. In this paper we present multiple OpenCL implementations of FDTD for real time simulation of a drum membrane. Additionally, we compare against an AVX optimized CPU implementation and an OpenGL version that utilizes a careful mapping to the GPU texture cache. We find using a discrete, laptop class, AMD GPU that for all but the smallest mesh sizes, the OpenCL implementation out performs the others. Although, to our surprise we found that optimizing for work-group local memory provided only a small performance benefit.|International Workshop on OpenCL|2019|10.1145/3318170.3318172|Tom Mitchell, Benedict R. Gaster, Harri Renney|0.3333333333333333|2
1547|An Efficient Numerical Solution Technique for VLSI Interconnect Equations on Many-Core Processors|This paper presents a technique to accelerate transient simulations of analog circuits using an explicit integration method parallelised on a many-core computer. Usual methods used by SPICE-type simulators are based on Newton-Raphson iterations, which are reliable and numerically stable, but require long CPU processing times. However, although the integration time step in explicit methods is smaller than that used in implicit methods, this technique avoids the calculation of time-consuming computations due to the Jacobian matrix inversion. The proposed method uses an explicit integration scheme based on the fourth order Adams–Bashforth formula. The algorithm has been parallelised on a NVIDIA general purpose GPU using the CUDA programming model. As a case study, the RC ladder model of a VLSI interconnect is simulated on a general purpose graphic processing unit and the achieved performance is then evaluated against that of a multiprocessor CPU. The results show that the proposed technique achieves a speedup of one order of magnitude in comparison with implicit integration techniques executed on a CPU.|International Symposium on Circuits and Systems|2019|10.1109/ISCAS.2019.8702085|T. Kazmierski, G. Doménech-Asensi|0.3333333333333333|2
1551|A High Performance Computing Framework for Multiphase, Turbulent Flows on Structured Grids|We present a high performance computing framework for multiphase, turbulent flows on structured grids. The computational methods are validated on a number of benchmark problems such as the Taylor-Green vortex that are extended by the inclusion of bubbles in the flow field. We examine the effect of bubbles on the turbulent kinetic energy dissipation rate and provide extensive data for bubble trajectories and velocities that may assist the development of engineering models. The implementation of the present solver on massively parallel, GPU enhanced architectures allows for large scale and high throughput simulations of multiphase flows.|Platform for Advanced Scientific Computing Conference|2019|10.1145/3324989.3325727|S. Litvinov, Michail Chatzimanolakis, P. Karnakov, P. Koumoutsakos, F. Wermelinger|0.3333333333333333|2
1566|RDGC: A Reuse Distance-Based Approach to GPU Cache Performance Analysis|In the present paper, we propose RDGC, a reuse distance-based performance analysis approach for GPU cache hierarchy. RDGC models the thread-level parallelism in GPUs to generate appropriate cache reference sequence. Further, reuse distance analysis is extended to model the multi-partition/multi-port parallel caches and employed by RDGC to analyze GPU cache memories. RDGC can be utilized for architectural space exploration and parallel application development through providing hit ratios and transaction counts. The results of the present study demonstrate that the proposed model has an average error of 3.72 % and 4.5 % (for L1 and L2 hit ratios, respectively). The results also indicate that the slowdown of RDGC is equal to 47 000 times compared to hardware execution, while it is 59 times faster than GPGPU-Sim simulator.|Computing and informatics|2019|10.31577/CAI_2019_2_421|Mohsen Kiani, Amir Rajabzadeh|0.3333333333333333|2
1597|A Fast Local Algorithm for Track Reconstruction on Parallel Architectures|The reconstruction of particle trajectories, tracking, is a central process in the reconstruction of particle collisions in High Energy Physics detectors. At the LHCb detector in the Large Hadron Collider, bunches of particles collide 30 million times per second. These collisions produce about 10^9 particle trajectories per second that need to be reconstructed in real time, in order to filter and store data. Upcoming improvements in the LHCb detector will deprecate the hardware filter in favour of a full software filter, posing a computing challenge that requires a renovation of current algorithms and the underlying hardware. We present Search by triplet, a local tracking algorithm optimized for parallel architectures. We design our algorithm reducing Read-After-Write dependencies as well as conditional branches, incrementing the potential for parallelization. We analyze the complexity of our algorithm and validate our results. We show the scaling of our algorithm for an increasing number of collision events. We show sustained tests for our algorithm sequence given a simulated dataflow. We develop CPU and GPU implementations of our work, and hide the transmission times between device and host by executing a multi-stream pipeline. Our results provide a reliable basis for an informed assessment on the feasibility of LHCb event reconstruction on parallel architectures, enabling us to develop cost models for upcoming technology upgrades. The created software infrastructure is extensible and permits the addition of subsequent reconstruction algorithms.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2019|10.1109/IPDPSW.2019.00118|A. Riscos-Núñez, D. C. Pérez, N. Neufeld|0.3333333333333333|2
1600|Diffuse optical tomography with polarized light: a GPU-accelerated polarization-sensitive Monte Carlo simulations for efficient sensitivity kernel computation|In an effort to address Monte Carlo (MC) light prorogation shortcomings in terms of computational burden and light polarization-sensitivity, we report an efficient GPU-based MC for modeling polarized light in scattering medium.|European Conference on Biomedical Optics|2019|10.1117/12.2526939|J. Wojak, C. Macdonald, A. Silva, Hind Oulhaj, Vadim A. Markel, U. Tricoli|0.3333333333333333|2
1605|Probabilistic-Based Transient Stability Assessment of Power Systems with Virtual Synchronous Machines|In this paper, probabilistic-based transient stability analysis is performed on a four-machine 13-bus test system with virtual synchronous machines. The newly developed synchronverter model with bounded frequency and voltage integral controllers is implemented, which replaces the conventional synchronous generators in the test system one at a time at different locations. A large number of potential power system faults were simulated with GPU parallel computing to ensure the convergence of the probabilistic approach. The results quantified the transient stability of the system with synchronverters in terms of different locations, penetration levels, and parameter selections.|International Symposium on Industrial Electronics|2019|10.1109/ISIE.2019.8781299|Zhao Liu, Ziang Zhang|0.3333333333333333|2
185|Relative debugging for a highly parallel hybrid computer system|Relative debugging traces software errors by comparing two executions of a program concurrently - one code being a reference version and the other faulty. Relative debugging is particularly effective when code is migrated from one platform to another, and this is of significant interest for hybrid computer architectures containing CPUs accelerators or coprocessors. In this paper we extend relative debugging to support porting stencil computation on a hybrid computer. We describe a generic data model that allows programmers to examine the global state across different types of applications, including MPI/OpenMP, MPI/OpenACC, and UPC programs. We present case studies using a hybrid version of the `stellarator' particle simulation DELTA5D, on Titan at ORNL, and the UPC version of Shallow Water Equations on Crystal, an internal supercomputer of Cray. These case studies used up to 5,120 GPUs and 32,768 CPU cores to illustrate that the debugger is effective and practical.|International Conference for High Performance Computing, Networking, Storage and Analysis|2015|10.1145/2807591.2807605|Bob Moench, Chao Jin, A. Vose, M. N. Dinh, D. Abramson, L. D. Rose, Andrew Gontarek|0.3|2
225|3D Workload Subsetting for GPU Architecture Pathfinding|Growth of high-end 3D gaming, expansion of gaming to new devices like tablets and phones, and evolution of multiple Graphics APIs like Direct3D 10+, and OpenGL 3.0+ have led to an explosion in the number of workloads that need to be evaluated for GPU architecture path-finding. To decide on the optimal architecture configuration, the workloads need to be simulated on a wide range of architecture designs which incurs huge cost, both in terms of time and resources. In order to reduce the simulation cost of path-finding, extracting workload subsets from 3D workloads is essential. This paper presents a methodology to find representative workload subsets from 3D workloads by combining clustering and phase detection. In the first part, this paper presents a methodology to group draw-calls based on performance similarity by clustering on their micro architecture independent characteristics. Across 717 frames encompassing 828K draw-calls, the clustering solution obtained an average performance prediction error per frame of 1.0% at an average clustering efficiency of 65.8%. The clustering quality is additionally evaluated by calculating cluster outliers, which are clusters with intra cluster prediction error greater than 20%. The clustering quality, measured using cluster outliers, is an indication of the performance similarity of the individual clusters. Across the spectrum of frames, we found that on an average only 3.0% of the clusters are outliers which indicates a high clustering quality. In order to detect repetitive behavior in 3D workloads, we propose characterization of frame intervals using shader vectors and then using shader vector equality to extract the repeating patterns. We show that phases exist in each game in the Bio shock series enabling extraction of small representative subsets from the workloads. Performance improvement of the workload subsets, which are less than one percent of parent workload, with GPU frequency scaling has high correlation (correlation coefficient=99.7%+) to the performance improvement of its parent workload.|IEEE International Symposium on Workload Characterization|2015|10.1109/IISWC.2015.24|V. George|0.3|2
247|Real-time reconstruction solution for positron emission mammography imaging-guided intervention|Positron Emission Mammography (PEM) imaging systems with the ability in detection of millimeter-sized tumors were developed in recent years. And some of them have been well used in clinical applications. In consideration of biopsy application, a double-plane PET is more practical than a ring detector for the convenience of breast immobilization. In this study, the possibility of the imaging-guided intervention under a double-plane PET was studied. The distance between the planes is changeable with different breast size. And the system matrix is computed in real-time for every plane spacing. The biopsy motivation under the double-plane PET requires a high sensitivity, great resolution performance and a quickly reconstruction speed. The sensitivity result shows the central value is 13.28% with the 6cm spacing and 200-750keV energy window. The resolution modeling method shows the resolution recovery achieved in the cross-plane. The 3D-OSEM algorithm is applied. The reconstruction is supported with GPU (GeForce GTX 660). For the GPU acceleration, both projection and back-projection procedures use the tube model. Each tube is distributed with a thread and calculated in parallel in the GPU. A double plane PET is simulated above Geant4 Application for Emission Tomography (GATE) software based on MC methods. With all the span tubes calculated, the GPU takes 19 seconds to reconstruct the image while the CPU takes 1037 seconds. The image quality was tested with different acquisition times. The series of biopsy images are showed. The point and rod source both could be imaged in the simulated biopsy process.|Nuclear Science Symposium and Medical Imaging Conference|2015|10.1109/NSSMIC.2015.7582065|L. Shang, M. Yun, Long Wei, Zhen-Rui Lu, Wei Zhou, Lin Li, Pengfei Ying, Xiaoyue Gu|0.3|2
312|Real-time surgery simulation of intracranial aneurysm clipping with patient-specific geometries and haptic feedback|Providing suitable training for aspiring neurosurgeons is becoming more and more problematic. The increasing popularity of the endovascular treatment of intracranial aneurysms leads to a lack of simple surgical situations for clipping operations, leaving mainly the complex cases, which present even experienced surgeons with a challenge. To alleviate this situation, we have developed a training simulator with haptic interaction allowing trainees to practice virtual clipping surgeries on real patient-specific vessel geometries. By using specialized finite element (FEM) algorithms (fast finite element method, matrix condensation) combined with GPU acceleration, we can achieve the necessary frame rate for smooth real-time interaction with the detailed models needed for a realistic simulation of the vessel wall deformation caused by the clamping with surgical clips. Vessel wall geometries for typical training scenarios were obtained from 3D-reconstructed medical image data, while for the instruments (clipping forceps, various types of clips, suction tubes) we use models provided by manufacturer Aesculap AG. Collisions between vessel and instruments have to be continuously detected and transformed into corresponding boundary conditions and feedback forces, calculated using a contact plane method. After a training, the achieved result can be assessed based on various criteria, including a simulation of the residual blood flow into the aneurysm. Rigid models of the surgical access and surrounding brain tissue, plus coupling a real forceps to the haptic input device further increase the realism of the simulation.|Medical Imaging|2015|10.1117/12.2082053|Wolfgang Fenz, J. Dirnberger|0.3|2
415|GPU acceleration of iterative physical optics-based electromagnetic simulations|High fidelity prediction of the link budget between a pair of transmitting and receiving antennas in dense and complex environments is computationally very intensive at high frequencies. Iterative physical optics (IPO) is a scalable solution for electromagnetic (EM) simulations with complex geometry. In this paper, an efficient and robust solution is presented to predict the link budget between antennas in a dense environment. Two Nvidia GPUs with different number of cores and device memory were targeted for benchmarking the performance of the IPO algorithm. The results indicate that the GPU implementation of the IPO algorithm is memory bound. Also, the K40c GPU only provides 2× speedup over the GTX650M for cases less than 25K triangles, although it has 7.5× more cores than the GTX650M. The Nvidia K40c GPU provides a best case speedup of 7366× for a model that consists of 25K triangles at f = 2.4GHz.|IEEE Conference on High Performance Extreme Computing|2015|10.1109/HPEC.2015.7322465|V. Venugopalan, C. Tokgoz|0.3|2
418|Fast Daylight Coefficient Calculation Using Graphics Hardware|As people increasingly work in indoor environments, the need to provide natural lighting is becoming more widely recognized. Recent modelling standards such as LM-83 require the use of climate-based metrics based on daylight coefficients, rather than illuminance-based metrics that simulate single points in time. While calculations based on daylight coefficients are fast, computation of the daylight coefficients themselves is a slow process that must be repeated whenever the scene’s geometry or materials change. Therefore, it remains impractical to obtain accurate annual daylight simulation results during early design stages when designs are fluid and quickly changing. This paper describes the development of a new, faster tool to compute daylight coefficients using graphics hardware. The tool is an adaptation of rtrace_dc, the executable used by DAYSIM to calculate daylight coefficients, written using OptiXTM, a free library for GPU-based ray tracing. The effectiveness of the new tool is demonstrated using models of a typical office space with speed and results compared to rtrace_dc. The speed of the new tool is measured both on a typical workstation graphics card and on a high-end graphics server. The results show that the new tool achieves similar accuracy to the serial version but does so in one-fifth the time.||2015|10.26868/25222708.2015.2462|Nathaniel L. Jones, C. Reinhart|0.3|2
428|Accurately modeling the GPU memory subsystem|Nowadays, research on GPU processor architecture is extraordinarily active since these architectures offer much more performance per watt than CPU architectures. This is the main reason why massive deployment of GPU multiprocessors is considered one of the most feasible solutions to attain exascale computing capabilities. In this context, ongoing GPU architecture research is required to improve GPU programmability as well as to integrate CPU and GPU cores in the same die. One of the most important research topics in current GPUs, is the GPU memory hierarchy, since its design goals are very different from those of conventional CPU memory hierarchies. To explore novel designs to better support General Purpose computing in GPUs (GPGPU computing) as well as to improve the performance of GPU and CPU/GPU systems, researchers often require advanced microarchitectural simulators with detailed models of the memory subsystem. Nevertheless, due to fast speed at which current GPU architectures evolve, simulation accuracy of existing state-of-the-art simulators suffers. This paper focuses on accurately modeling the GPU memory subsystem. We identified three main aspects that should be modeled with more accuracy: i) miss status holding registers, ii) coalescing vector memory requests, and iii) non-blocking GPU stores. In this sense, we extend the Multi2Sim heterogeneous CPU/GPU processor simulator to model these aspects with enough accuracy. Experimental results show that if these aspects are not considered in the simulation framework, performance deviations can rise in some applications up to 70%, 75%, and 60%, respectively.|International Symposium on High Performance Computing Systems and Applications|2015|10.1109/HPCSim.2015.7237038|S. Petit, F. Candel, J. Sahuquillo, J. Duato|0.3|2
437|Photoelasticity Raycasting|We present a novel physically‐based method to visualize stress tensor fields. By incorporating photoelasticity into traditional raycasting and extending it with reflection and refraction, taking into account polarization, we obtain the virtual counterpart to traditional experimental polariscopes. This allows us to provide photoelastic analysis of stress tensor fields in arbitrary domains. In our model, the optical material properties, such as stress‐optic coefficient and refractive index, can either be chosen in compliance with the subject under investigation, or, in case of stress problems that do not model optical properties or that are not transparent, be chosen according to known or even new transparent materials. This enables direct application of established polariscope methodology together with respective interpretation. Using a GPU‐based implementation, we compare our technique to experimental data, and demonstrate its utility with several simulated datasets.|Computer graphics forum (Print)|2015|10.1111/cgf.12626|T. Ertl, F. Sadlo, M. Bußler|0.3|2
1225|DISCRETE PARTICLE METHOD IS A PREDICTIVE TOOL FOR SIMULATION OF MINE BLAST – A PARAMETER STUDY OF THE PROCESS AND APPROACH|The modeling of a buried charge is a very complex engineering task since many Design Variables need to be considered. The variables in question are directly related to the method chosen to perform the analysis and the process modeled. In order to have a Predictive Tool two main objectives have to be carried out, the first is a verification of the numerical approach with experimental data, the second objective is a sensitivity study of the numerical and process parameters. The emphasis of the present study covers the second objective. To perform this task a comprehensive sensitivity study of fourteen Design Variables was completed which required 1000+ computational hours. The modeling approach that was chosen was the Discrete Particle Method (DPM) to model the Soil and HE and the Finite Element Method for the Structure. The basis for the study was a blast event applied to a model of the TARDEC Generic Vehicle Hull. The Response Parameter was chosen to be the Total Blast Impulse on the structure. The non-linear transient dynamic explicit Finite Element solver used for the analysis was the IMPETUS Afea Solver ® which has implemented the DPM for blast simulations. The study includes soil characteristics, charge related parameters, such as size, type, geometry and location. Also the DOB, number of Discrete Particles etc. were considered. The results provide guide lines and in depth understanding of modeling buried charges with a coupled FEM and DPM approach. INTRODUCTION A large percentage of casualties in recent military conflicts are due to blast from buried mines, typically coming from Improvised Explosive Devices (IED). The number of Hostile Deaths resulting from “Operation Enduring FreedomAfghanistan” between 2001 and 2014 is around 2782. Of these, 1401 deaths were caused by IED’s, which is nearly 50% [1]. The design of better vehicle protection is necessary to reduce the casualties of our soldiers. At the heart of the design process is simulation technology which allows engineers to test and refine their ideas in a virtual environment before the costly step of building a prototype for physical testing. It is of vital importance to have predictive numerical tools for this process to minimize the number of physical tests. To obtain a predictive simulation tool for a specific application area, two main objectives have to be carried out successfully as shown in Figure 1. The first objective is verification against experiments, where the software is calibrated against simple tests and this is used as a base for prediction of more complicated scenarios where no experiments are available beforehand. Tuning of numerical and process parameters against already available experimental data does not make the software predictive. The second objective that must be considered is a sensitivity study of both the process parameters and the numerical parameters. This gains knowledge about the response of the numerical model and helps in the model calibration phase as well as illustration of the stability of the software. Proceedings of the 2015 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) Discrete Particle Method is a Predictive Tool for Simulation of Mine Blast – A Parameter Study of the Process and Approach, Jensen, et al. Page 2 of 13 Figure 1: Necessary objectives to consider when evaluating if a software program is a predictive tool. In this paper the Discrete Particle Method (DPM) provided in the IMPETUS Afea Solver ® , an Explicit Nonlinear Transient Dynamic FE solver is applied for modeling the mine blast event. The DPM module is described in great detail in [2, 3, 4] where the approach was verified against mine blast experiments. The module was also successfully applied in [5]. The benefit of the DPM method is further enhanced when combined with a solver that takes full advantage of the parallelization provided by GPU Technology. IMPETUS has been proved to accurately simulate mine blast during the last eight years, both by researchers and in commercial projects. An extensive number of experimental blast tests have been simulated. This means that the IMPETUS Solver satisfied the first objective of being a predictive software tool. The benefit to using the DPM is its high degree of accuracy for modeling Soil, Air and HE. The parameters for a HE are calibrated for a particular explosive based upon a standard cylinder test [2]. Similarly the soil parameters have to be determined as well but the variation of soil type does not include a simple list as the characteristics of the soil are affected by moisture content, level of compaction and the soil make up, e.g., sand, dirt, rocks, etc. [6-11]. The procedure to calibrate the soil model requires a good understanding of how the various DPM parameters influence the resulting blast load on a structure. The best way to explain this is with a sensitivity study, which is the second objective for obtaining a predictive simulation tool and the main purpose of this study. Recently an IMPETUS Afea finite element model of the TARDEC Generic Vehicle Hull was created. The model along with the IMPETUS Afea Hybrid III 50 th Percentile Dummy model is illustrated in Figure 2. The Hull model was chosen for the parameter study omitting the dummy as it does not influence the blast loading. It is a particularly relevant structure to use for the study as it is a real structure that has been blast tested by the US Army and continues to be used by TARDEC to better understand how to protect the occupant. Figure 2: IMPETUS model of the TARDEC Generic Vehicle Hull. By utilizing GPU technology in the IMPETUS Solver the computational time for this Base Model is approximately 9 hours. In this discussion the Blast Impulse on the structure was chosen as the Response Parameter. Note, the numerical results are compared with the “Base Model” numerical result since experimental data has not yet been publicly released by TARDEC. BASE MODEL RESULTS AND DESIGN SPACE Modeling blast events with the DPM is very straight forward in IMPETUS. It is done with the *PBLAST command where domains are defined for the Soil, HE and Air (if used). By simply specifying a total number of particles the solver automatically calculates the correct ratio between the domains. The solver has built-in packing algorithms for the domains in which Lagrangian structures can be embedded easily by simply including a part ID in the part set of the structural parts that interact with the particles. Furthermore, friction can be specified for the interaction between the soil and the structure. The modeling of HE is done with rigid spheres that have elastic impact for interparticle contact. The implemented approach is described in [2]. The type of HE is easily defined, e.g., C4 or TNT, by selecting one as part of the input, but in addition a user defined HE is also available. Next, the coordinates of the detonation point within the HE domain is defined by the user. The soil is also modeled with discrete rigid particles but the inter particle contact includes both friction and damping. Proceedings of the 2015 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) Discrete Particle Method is a Predictive Tool for Simulation of Mine Blast – A Parameter Study of the Process and Approach, Jensen, et al. Page 3 of 13 The rheological model for the soil is illustrated in Figure 3, showing the springs and the damper. The normal and tangential spring constants are given the same value. Figure 3: The applied rheological model for the soil. The soil is packed using a unit cell with periodic boundaries that makes it possible to repeat the geometry to generate the Soil Bed. These unit cells are then scaled which affects the inter particle stiffness which becomes k=L/L0·k0 where L is the scaled size of the unit cell, L0 is the un-scaled size and k0 is the stiffness of the un-scaled unit cell. The details of the implementation are shown in [2, 3]. As for the HE, the soil can also be specified using built-in calibrated models, either as dry or wet but it is recommended to calibrate the soil based on a blast test of a rigid flat plate using the Soil Bed that is to be used for the more complicated structure. This will require using the “user defined soil option” which is straight forward to specify. It includes the soil density, the soil particle stiffness, the soil particle friction and damping. For dry soil, stiffness and friction is used and for wet soil stiffness and damping. A detailed description of the procedure for soil calibration can be found in [12]. The set-up in the command file only requires a few lines as illustrated in Figure 4. Figure 4: The *PBLAST command is used for defining the blast set-up [13]. Design Space It is always a challenge to define the experimental matrix or the Design Space when a sensitivity study or optimization is carried out. In fact, the Design Space often changes during the study due to unknown constraints on the design variables or physical limitations on the process parameters. Based on the experience and interest of the authors, 14 design variables were chosen to illustrate both approach but also process parameters. For each of the design variables between three to five or more variations where tested, leading to around 80 entities in the Design Space, each representing a numerical simulation. Less could of course have been selected but the knowledge obtained will be very helpful in future work in the field of mine blast simulations. The following characterization of the parameters illustrates the base for the Design Space:  Soil: Density, packing routine, inter particle stiffness, inter particle friction, inter particle damping, soil domain size and friction between structure and soil.  Charge: Charge size, geometry, HE type, orientation (angle), off center location, DOB.  General: Total number of particles. Each of the design variables and their settings are described in the numeri||2015|10.1115/detc2015-46104|Morten Rikard Jensen, Wilford Smith|0.3|2
1263|Parallel Numerical Simulations of Three-Dimensional Electromagnetic Radiation with MPI-CUDA Paradigms|Using parallel computation can enhance the performance of numerical simulation of electromagnetic radiation and get great runtime reduction. We simulate the electromagnetic radiation calculation based on the multicore CPU and GPU Parallel Architecture Clusters by using MPI-OpenMP and MPI-CUDA hybrid parallel algorithm. This is an effective solution comparing to the traditional finite-difference time-domain method which has a shortage in the calculation of the electromagnetic radiation on the problem of inadequate large data space and time. What is more, we use regional segmentation, subregional data communications, consolidation, and other methods to improve procedures nested parallelism and finally verify the correctness of the calculation results. Studying these two hybrid models of parallel algorithms run on the high-performance cluster computer, we draw the conclusion that both models are suitable for large-scale numerical calculations, and MPI-CUDA hybrid model can achieve higher speedup.||2015|10.1155/2015/823426|Longji Tang, Anping Song, Bing He, Jiang-rong Xie, Xiaowei Wang|0.3|2
1330|Optimizing Quantum Simulation for Heterogeneous Computing: a Hadamard Transformation Study|The D-GM execution environment improves distributed simulation of quantum algorithms in heterogeneous computing environments comprising both multi-core CPUs and GPUs. The main contribution of this work consists in the optimization of the environment VirD-GM, conceived in three steps: (i) the theoretical studies and implementation of the abstractions of the Mixed Partial Process defined in the qGM model, focusing on the reduction of the memory consumption regarding multidimensional QTs; (ii) the distributed/parallel implementation of such abstractions allowing its execution on clusters of GPUs; (iii) and optimizations that predict multiplications by zero-value of the quantum states/transformations, implying reduction in the number of computations. The results obtained in this work embrace the distribute/parallel simulation of Hadamard gates up to 21 qubits, showing scalability with the increase in the number of computing nodes.||2015|10.1088/1742-6596/649/1/012004|M. Pilla, A. Maron, R. Reiser, Murilo F Schumalfuss, A. B. de Ávila|0.3|2
1336|A heterogeneous computing approach to simulation of the Heston Stochastic Volatility Model|Stochastic volatility models are of fundamental importance to the pricing of derivatives. One of the most commonly used models of stochastic volatility is the Heston Model in which the price and volatility of an asset evolve as a pair of coupled stochastic differential equations. The computation of asset prices and volatilities involves the simulation of many sample trajectories with conditioning. The problem is treated using the method of particle filtering. While the simulation of a shower of particles is computationally expensive, each particle behaves independently making such simulations ideal for massively parallel heterogeneous computing platforms. In this paper, we present our portable Opencl implementation of the Heston model and discuss its performance and efficiency characteristics on a range of architectures including Intel cpus, Nvidia gpus, and Intel Many-Integrated-Core (mic) accelerators.|International Conference on High Performance Computing|2015|10.21914/anziamj.v57i0.10425|D. Warne, K. Lindsay|0.3|2
1481|Viscosity flow simulation using improved SPH method based on GPU parallel calculation|The smoothed particle hydrodynamics( SPH) method has a good adaptability for the simulation of breaking wave problems. The GPU computing platform based on many-core architecture has a strong advantage in SPH method acceleration. In view of the low efficiency and the accuracy problem of traditional SPH method,this paper puts forward a new GPU parallel computing model based on the particle pair and improved δ-SPH method for simulating viscosity flows such as lid-drive cavity flow,Poiseuille flow,Couette flow and solitary wave slamming.According to the comparison of different boundary handling methods,their rules on viscous flow simulation are got.Furthermore,two GPU parallel calculation methods which are respectively based on the particle pair and single particle are researched,and their accuracy and CPU time are compared. The results show that the GPU parallel calculation method based on particle pairs makes δ-SPH exceed 10 times of the maximum speed-up ratio.||2015|10.1049/cp.2015.1473|Jin Shanqi|0.3|2
213|Splay thread cooperation on ray tracing as a load balancing technique in speculative parallelism and GPGPU|The introduction of the speculative parallelism into any models can improve the performance and provides significant benefits and increases the Instruction level parallelism (ILP) and Thread level Parallelism (TLP). A General Purpose Graphics Processing Unit (GPGPU) is the future computing technology working with both Graphics Processing Unit (CPU) and GPU to solve many real-world problems not only the graphics problems but also the general purpose applications. As GPU uses data parallelism tasks, the dynamic memory creation and the splay trees which are self adjusting allows for the increase in throughput and load balancing. The frequently used nodes near to the root are an advantage for finding locality of threads as well as for caching and garbage collection. A technique used to render and to study complex scenes into images and to render color, intensity of pixels, distance between pixels is referred as Ray tracing. Multithreading is a promising technique which increases the performance of the computer systems by increasing the instruction level parallelism and thread level speculation. In this paper a new technique is proposed for workload balancing on the Graphics processors and CPU that can be implemented on the graphics processors along with the CPU which provides the optimal result with the speculation techniques and Lorentz Transformation, which is used to determine color and brightness of the ray which are refracted or reflected and also the relative distance between the thread spawning which results in time dilation and contraction. A Graphics Processing Unit Ocelot (GPUOCELOT) is a compilation framework, a simulator used for the execution of the programs which has resulted in the increase in the performance of the instructions which uses the amortized cost.|˜The œinternational Arab journal of information technology|2018|10.1109/iementech.2018.8465144|S. Shivaraju, Gopalan Pudur|0.2857142857142857|2
228|Locality-protected cache allocation scheme with low overhead on GPUs|Graphics processing units (GPUs) are playing more important roles in parallel computing. Using their multi-threaded execution model, GPUs can accelerate many parallel programmes and save energy. In contrast to their strong computing power, GPUs have limited on-chip memory space which is easy to be inadequate. The throughput-oriented execution model in GPU introduces thousands of hardware threads, which may access the small cache simultaneously. This will cause cache thrashing and contention problems and limit GPU performance. Motivated by these issues, the authors put forward a locality-protected method based on instruction programme counter (LPC) to make use of data locality in L1 data cache with very low hardware overhead. First, they use a simple Program Counter (PC)-based locality detector to collect reuse information of each cache line. Then, a hardware-efficient prioritised cache allocation unit is proposed to coordinate data reuse information with time-stamp information to predict the reuse possibility of each cache line, and to evict the line with the least reuse possibility. Their experiment on the simulator shows that LPC provides an up to 17.8% speedup and an average of 5.0% improvement over the baseline method with very low overhead.|IET Computers & Digital Techniques|2018|10.1049/iet-cdt.2017.0004|Chuan Tang, Cang Liu, Yang Zhang, Zuocheng Xing|0.2857142857142857|2
307|GPU-based Acceleration of Detailed Tissue-Scale Cardiac Simulations|We present a GPU based implementation for tissue-scale 3D simulations of the human cardiac ventricle using a physiologically realistic cell model. Computational challenges in such simulations arise from two factors, the first of which is the sheer amount of computation when simulating a large number of cardiac cells in a detailed model containing 104 calcium release units, 106 stochastically changing ryanodine receptors and 1.5 x 105 L-type calcium channels per cell. Additional challenges arise from the fact that the computational tasks have various levels of arithmetic intensity and control complexity, which require careful adaptation of the simulation code to the target device. By exploiting the strengths of the GPU, we obtain a performance that is far superior to that of the CPU, and also significantly higher than that of other state of the art manycore devices, thus paving the way for detailed whole-heart simulations in future generations of leadership class supercomputers.|GPGPU@PPoPP|2018|10.1145/3180270.3180274|Neringa Altanaite, J. Langguth|0.2857142857142857|2
318|MedFDTD: A Parallel and Open-Source Cross-Platform Framework for Bioelectromagnetics Field Simulation|MedFDTD, a new parallel and open-source cross-platform framework for bioelectromagnetics research, has been developed by solving Maxwell’s equations using the finite-difference time-domain method. This framework implements the complex-frequency-shifted perfectly matched layer, supports the import of antenna and bioelectromagnetic models through media with non-dispersive/dispersive properties, and can calculate the antenna output power and specific absorption rate of biological tissues, thereby making it favorable for bioelectromagnetics studies. In addition, MedFDTD can be implemented on multiple CPUs or a variety of chip-based GPUs to accelerate all parallel computing tasks.|IEEE Access|2018|10.1109/ACCESS.2017.2784838|Liu Qian, Wang Meng|0.2857142857142857|2
1215|Performance Evaluation and Enhancements of a Flood Simulator Application for Heterogeneous HPC Environments|This paper presents a practical implementation of a 2D flood simulation model using hybrid distributed-parallel technologies including MPI, OpenMP, CUDA, and evaluations of its performance under various configurations that utilize these technologies. The main objective of this research work was to improve the computational performance of the flood simulation in a hybrid architecture. Modern desktops and small cluster systems owned by domain researchers are able to perform these simulations efficiently due to multicore and GPU computing devices, but lack the expertise needed to fully utilize software libraries designed to take advantage of the latest hardware. By leveraging knowledge of our experimentation environment, we were able to incorporate MPI and multiple GPU devices to improve performance over a single-process OpenMP version up to 18x, depending on the size of the input data. We discuss some observations that have significant effects on overall performance, including process-to-device mapping, communication strategies and data partitioning, and present some experimental results. The limitations of this work are discussed, and we propose some ideas to relieve or overcome such limitations in future work.|International Journal of Networking and Computing|2018|10.15803/IJNC.8.2_387|A. Kalyanapu, Mike Rogers, T. Dullo, Ryan J. Marshall, S. Ghafoor|0.2857142857142857|2
1234|A New FEM-based Brain Tissue Model for Neurosurgical Simulation Using the Optimization Implicit Euler Method|An accurate and computationally efficient model for the deformation of brain tissue is very important in virtual neurosurgical simulation. In this paper, we introduced a new Finite Element Method (FEM) model, which is based on optimization implicit Euler method, for brain tissue deformation. Specifically, both the anisotropic and viscoelastic properties of brain tissue are incorporated into the model, providing more realistic and accurate description of the mechanical features of brain tissue. In the meantime, the model is particularly suitable for GPU-based computing, making it possible to achieve real-time performance for neurosurgical simulation. Simulation results show that the deformation model exhibits the behaviors of anisotropy and viscoelasticity. The proposed model was implemented on a neurosurgical simulator and it showed that the deformation of brain tissue can be rendered with a relatively high degree of visual realism at a refreshment rate of 23 frames per second in a normal PC.|Cybersecurity and Cyberforensics Conference|2018|10.23919/CHICC.2018.8484193|Shichao Liu, Wenguo Hou, Minhua Zheng, P. X. Liu|0.2857142857142857|2
1245|Position-Based Time-Integrator for Frictional Articulated Body Dynamics|We present a new time-integrator for modeling the frictional dynamics of articulated bodies. Our formulation represents the configuration of the articulated body using position variables and then uses those variables to model the friction forces between the articulated body and the environment. Our approach corresponds to a Newton-type optimization scheme that is guaranteed to converge so that it is stable with large timestep sizes. We evaluate the accuracy and stability of our time-integrator by comparing it with a conventional formulations based on the Newton-Euler equation and demonstrate the benefits on standard controller-optimization applications. We achieve 3–5 times speedup over a Newton-Euler-based simulator on a CPU. Our approach can be easily parallelized on a GPU and results in additional 4–15 times performance improvement.|IEEE/RJS International Conference on Intelligent RObots and Systems|2018|10.1109/IROS.2018.8593817|Dinesh Manocha, Zherong Pan|0.2857142857142857|2
1255|AC922 Data Movement for CORAL|Recent publications have considered the challenge of movement in and out of the high bandwidth memory in attempt to maximize GPU utilization and minimize overall application wall time. This paper builds on previous contributions, [5] [17], which simulate software models, advocated optimization, and suggest design considerations. This contribution characterizes the data movement innovations of the AC922 nodes IBM delivered to Oak Ridge National Labs and Lawrence Livermore National Labs as part of the 2014 Collaboration of Oak Ridge, Argonne, and Livermore (CORAL) joint procurement activity. With a single HPC system able to perform up to 200PF of processing with access to 2.5PB of memory, this architecture motivates a careful look at data movement. The AC922 POWER9 system with NVIDIA V100 GPUs have cache line granularity, more than double the bandwidth of PCIe Gen3, low latency interfaces and are interconnected by dual-rail Mellanox CAPI/EDR HCAs. As such, the bandwidth and latency assumptions from previous simulations should be revisited and compared to characterization results on product hardware. Our characterization approach attempts to leverage existing performance approaches, as applicable, to ease comparison and correlation. The results show that by refocusing our attention on the interconnect between processing elements, it is possible to design efficient logically coherent heterogeneous systems.|IEEE Conference on High Performance Extreme Computing|2018|10.1109/HPEC.2018.8547707|John Walthour, S. Roberts, P. Ramanna|0.2857142857142857|2
1308|A Preconditioned Iterative Approach for Efficient Full Chip Thermal Analysis on Massively Parallel Platforms|Efficient full-chip thermal simulation is among the most challenging problems facing the EDA industry today, especially for modern 3D integrated circuits, due to the huge linear systems resulting from thermal modeling approaches that require unreasonably long computational times. While the formulation problem, by applying a thermal equivalent circuit, is prevalent and can be easily constructed, the corresponding 3D equations network has an undesirable time-consuming numerical simulation. Direct linear solvers are not capable of handling such huge problems, and iterative methods are the only feasible approach. In this paper, we propose a computationally-efficient iterative method with a parallel preconditioned technique that exploits the resources of massively-parallel architectures such as Graphic Processor Units (GPUs). Experimental results demonstrate that the proposed method achieves a speedup of 2.2× in CPU execution and a 26.93× speedup in GPU execution over the state-of-the-art iterative method.|Technologies|2018|10.3390/TECHNOLOGIES7010001|Konstantis Daloukas, N. Evmorfopoulos, G. Floros, G. Stamoulis|0.2857142857142857|2
1397|New features of parallel implementation of N-body problems on Gpu|This paper focuses on the parallel implementation of a direct $N$-body method~(particle-particle algorithm) and the application of multiple GPUs for galactic dynamics simulations. Application of a hybrid OpenMP-CUDA technology is considered for models with a number of particles $N \sim 10^5 \div 10^7$. By means of $N$-body simulations of gravitationally unstable stellar galactic we have investigated the algorithms parallelization efficiency for various Nvidia Tesla graphics processors~(K20, K40, K80). Particular attention was paid to the parallel performance of simulations and accuracy of the numerical solution by comparing single and double floating-point precisions~(SP and DP). We showed that the double-precision simulations are slower by a factor of~$1.7$ than the single-precision runs performed on Nvidia Tesla K-Series processors. We also claim that application of the single-precision operations leads to incorrect result in the evolution of the non-axisymmetric gravitating $N$-body systems. In particular, it leads to significant quantitative and even qualitative distortions in the galactic disk evolution. For instance, after $10^4$ integration time steps for the single-precision numbers the total energy, momentum, and angular momentum of a system with $N = 2^{20}$ conserve with accuracy of $10^{-3}$, $10^{-2}$ and $10^{-3}$ respectively, in comparison to the double-precision simulations these values are $10^{-5}$, $10^{-15}$ and $10^{-13}$, respectively. Our estimations evidence in favour of usage of the second-order accuracy schemes with double-precision numbers since it is more efficient than in the fourth-order schemes with single-precision numbers.||2018|10.14529/mmp180111|S. Khrapov, A. Khoperskov, S. Khoperskov|0.2857142857142857|2
1408|A multi‐GPU finite element computation and hybrid collision handling process framework for brain deformation simulation|This paper offers a fast multi‐graphics processing unit (GPU) parallel simulation framework to the problem of real‐time and nonlinear finite element computation of brain deformation. A load balancing strategy is proposed to ensure the efficient distribution of nonlinear finite element computation on multi‐GPU. A data storage structure is designed to minimize the amount of data transfer and make full use of the overlay technique of GPU to reduce the transferring latency between multi‐GPUs. We further present a fast central processing unit (CPU)–GPU parallel continuous collision detection and response method, which not only can deal with the collision between the brain and skull but also can handle the self‐collision of the brain. Our method can make full use of CPU and GPU to implement a parallel computation about deformation and collision detection. Our experimental results show that our method is able to handle a brain geometric model with high detail gyrus composed of more than 40,000 tetrahedron elements. This can facilitate the fidelity of the current virtual brain surgery simulator. We evaluate our approach qualitatively and quantitatively and compare it with related works.|Comput. Animat. Virtual Worlds|2018|10.1002/cav.1846|Xukun Shen, Yong Hu, Ye Tian|0.2857142857142857|2
1436|Computational estimation of ms-sec atomistic folding times|Despite the development of massively parallel computing hardware including inexpensive graphics processing units (GPUs), it has remained infeasible to simulate the folding of atomistic proteins at room temperature using conventional molecular dynamics (MD) beyond the μs scale. Here we report the folding of atomistic, implicitly solvated protein systems with folding times τf ranging from ~100 μs to ~10s using the weighted ensemble (WE) strategy in combination with GPU computing. Starting from an initial structure or set of structures, WE organizes an ensemble of GPU-accelerated MD trajectory segments via intermittent pruning and replication events to generate statistically unbiased estimates of rate constants for rare events such as folding; no biasing forces are used. Although the variance among atomistic WE folding runs is significant, multiple independent runs are used to reduce and quantify statistical uncertainty. Three systems were examined: NTL9 at low solvent viscosity (yielding τf ~ 5μs), NTL9 at water-like viscosity (τf ~ 40μs), and Protein G at low viscosity (τf ~ 10s). In all cases the folding time, uncertainty, and ensemble properties could be estimated from WE simulation; for protein G, this characterization required significantly less overall computing than would be required to observe a single folding event with conventional MD simulations. Our results suggest discrepancies with experimental folding times that should enable improvement of force fields and solvent models.|bioRxiv|2018|10.1101/427393|D. Zuckerman, Andrew A. Petersen, U. Adhikari, Barmak Mostofian|0.2857142857142857|2
1533|Massively Parallel Computational Studies of Material Response at High Strain Rate Deformation|Large scale molecular dynamics (MD) simulations are now commonly utilized to study materials at extreme conditions: high pressure and/or temperatures and ultra-high strain rates of deformation. A variety of emerging architectures such as general purpose graphics processing units (GP-GPU) and many-integrated core (MIC) architecture as well as new execution models have changed the traditional approach of high-performance computing. ExMatEx, the DoE initiative for enabling exa-scale (10 flops) performance in scientific applications, has developed several proxy applications to facilitate co-design of novel algorithms and hardware by software developers and microchip vendors. We have used CoMD, a proxy application for classical MD, to investigate load balancing of shock wave simulation problems in various platforms as well as how best to improve performance of embedded-atom method (EAM) force evaluation kernels in GPUs. We also have implemented quasi-isentropic (QI) compression and expansion model in Los Alamos MD code SPaSM. QI uniaxial compression is achieved by incorporating a strain rate function in the position and velocity equations of motion. In this new formalism the change in internal energy is exactly equal to the work done during the compression or expansion. Large-Scale molecular dynamics (MD) simulations comprising 4 to 34 million atoms were performed to systematically study the poorly understood interplay between initial dislocation density and strain rate on deformation twinning in bcc metals. Using tantalum as a test case, for which a large body of experimental data exist. The atomic interactions were modeled employing an embedded-atom method (EAM) potential of Ta, we examined both compressive and tensile deformation at strain rates in the range of 10 − 1011s−1. At these high-strain rates, twin nucleation thresholds can clearly be measured. Under both expansion and compression, deformation twinning increases with strain rate for strain-rates||2018|10.1063/1.5044784|J. A. M. M. Abeywardhana|0.2857142857142857|2
1534|GPU-Accelerated Matrix Exponentiation for 5-D STEM-DCI Simulations|First order coupled partial differential equations are ubiquitous in science and engineering and form the basis for a number of mathematical models. Examples range from economic modeling, predator-prey models and supersymmetric and string theory. In the context of high energy electron microscopy, these equations describe the dynamical scattering of amplitudes between different reciprocal lattice vectors and are known as the Darwin-Howie-Whelan (DHW) equations. These equations have a well known solu-tion which involves the calculation of a matrix exponential. With continual improvements in computer hardware, it has been possible to compute these exponentials for a large number of scattered beams resulting in very accurate n−beam dynamical diffraction simulations. This has found application in various microscopy techniques such as Electron Back-Scatter Diffraction (EBSD), Electron Channeling Contrast Imaging (ECCI), phase contrast microscopy etc. However, with improvements in the microscope hardware, new modalities, such as 4−D Scanning Transmission Electron Microscopy Diffraction Contrast Imaging (STEM-DCI), have emerged as new state-of-the-art tools in the field. These techniques routinely generate enormous amounts of data and require the availability of fast and efficient forward models to interpret contrast features seen in real complex microstructures. This necessitates the need to speed up the key step in the dynamical diffraction computations, namely the matrix exponentiation. The speed-up in computation time allows simulations for addition parameters to the 4−D STEM, such as time series, load series etc. In this contribution, we discuss a GPU based matrix exponentiation algorithm, targeted mainly for the STEM-DCI modality, but easily extendable to other diffraction modalities both in the backscatter and forescatter geometries. Comparisons with existing CPU based algorithms are made showing impressive speed-ups. Finally, we will present results of STEM-DCI image calculations for a complex two phase γ − γ′ microstructure with misfit displacements at the interfaces. The n−beam DHW equations for a single phase microstructure in the presence of lattice defects are given by a set of coupled first order differential equations, where the number n denotes the number of reflections. This equation is usually written in the concise matrix notation shown in eq. 1, where the modified wave function, Φ = [φg1 φg2 · · · ] T is a column vector with the amplitudes of the electron wave functions for each reciprocal vector; the matrix A contains the geometry of the diffraction (diagonal sg terms) and coupling coefficients (off-diagonal qg terms). The true wavefunction is related to the modi-fied wavefunction through a simple Hadamard product (denoted by ◦) with a column vector phase factor, Θ = [θg1 θg2 · · · ] . These coupled differential equations have a well known solution involving the exponential of the structure matrix and the wavefunction at the crystal-vacuum boundary (Φ (0)) as shown in eq. 1.|Microscopy and Microanalysis|2018|10.1017/S1431927618001605|M. Graef, Saransh Singh|0.2857142857142857|2
1565|Enable the Flow for GPGPU-Sim Simulators with Fixed-Point Instructions|GPGPU-Sim nowadays has become an important vehicle for academic architecture research. In the aspect of machine learning, it has now been widely used in various applications, such as auto-drive, mobile device, and medication, etc. As these machine learning applications are power-consuming, which has become a critical issue in the machine learning area. This paper proposes the implementation of fixed-point instructions and enabled flow on GPGPU-Sim to replace floating-point instructions in machine learning applications which is with scalable precision. Preliminary experimental results with our revised GPGPU-Sim models show that this design saves GPU energy consumptions by 11% on average when using 16-bit fixed-point as the data type.|ICPP Workshops|2018|10.1145/3229710.3229722|Jenq-Kuen Lee, Bing-Sung Lu, Min-Yih Hsu, Chao-Lin Lee|0.2857142857142857|2
210|Improved Path Loss Simulation Incorporating Three-Dimensional Terrain Model Using Parallel Coprocessors|Current network simulators abstract out wireless propagation models due to the high computation requirements for realistic modeling. As such, there is still a large gap between the results obtained from simulators and real world scenario. In this paper, we present a framework for improved path loss simulation built on top of an existing network simulation software, NS-3. Different from the conventional disk model, the proposed simulation also considers the diffraction loss computed using Epstein and Peterson’s model through the use of actual terrain elevation data to give an accurate estimate of path loss between a transmitter and a receiver. The drawback of high computation requirements is relaxed by offloading the computationally intensive components onto an inexpensive off-the-shelf parallel coprocessor, which is a NVIDIA GPU. Experiments are performed using actual terrain elevation data provided from United States Geological Survey. As compared to the conventional CPU architecture, the experimental result shows that a speedup of 20x to 42x is achieved by exploiting the parallel processing of GPU to compute the path loss between two nodes using terrain elevation data. The result shows that the path losses between two nodes are greatly affected by the terrain profile between these two nodes. Besides this, the result also suggests that the common strategy to place the transmitter in the highest position may not always work.|Wireless Communications and Mobile Computing|2017|10.1155/2017/5492691|K. Lee, Pohkit Chong, W. Yap, Zhang Bin Loo|0.25|2
250|On the Simulation of Complex Visibilities in Imaging Radiometry by Aperture Synthesis|The basic observables of an imaging interferometer by aperture synthesis are the complex visibilities. Under some conditions, they can be simulated with the aid of the van Cittert–Zernike theorem. However, owing to underlying assumptions, some important effects that may alter them cannot be taken into account. This paper is devoted to the numerical simulation of complex visibilities without any reference to the van Cittert–Zernike theorem, in such a way that these effects can be taken into account. The emission from an extended source is modeled using a linear superposition of random waves emitted by a collection of point sources, which are all assumed to behave like black bodies at thermal equilibrium. These random waves are numerically generated with the aid of white noises filtered in such a way that their power spectral densities follow the shape of Planck distributions at the temperature of the point sources over a wide range of frequencies. The radio signal is then transported to the antennas, where the voltage patterns are taken into account as well as the filters response of the bandpass receivers. It is, therefore, sent to the correlator unit for being cross-correlated. From emission to correlation, perturbing effects can be introduced at any time. To illustrate this modeling method, numerical simulations are carried out in the L-band around 1413.5 MHz in reference to the SMOS-next project led by the French Space Agency. The results are discussed and compared with the estimates provided by the van Cittert–Zernike theorem. Owing to the amount of calculations to be performed, massive parallel architectures like that found in GPU have been required.|IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing|2017|10.1109/JSTARS.2017.2686449|A. Khazâal, F. Cabot, É. Anterrieu, Y. Kerr|0.25|2
303|PFSI.sw: A programming framework for sea ice model algorithms based on Sunway many-core processor|Sea ice model is a typical high performance computing problem. CPU and GPU based parallel method has been proposed to accelerate the simulation process, but it is still hard to meet the large-scale calculation demand due to the compute-intensive nature of the model. Sunway TaihuLight supercomputer use the SW26010 processor as its computing unit and achieves high performance for large-scale scientific computing. In this paper we present a programming framework (PFSI.sw) for sea ice model algorithms based on Sunway many-core processor. Based on this framework, programmer can exploit the parallelism of existing sea ice model algorithms and achieve good performance. Several strategies are introduced to this framework, data dividing, data transfer as well as the load balance are the main aspects we currently concerned. This framework has been implemented and tested with two sea ice model algorithms by using real world dataset on Sunway many-core processors. The experiment demonstrates comparable performance to the traditional parallel implementation on Sunway many-core processor and our framework improves the performance up to 40%.|IEEE International Conference on Application-Specific Systems, Architectures, and Processors|2017|10.1109/ASAP.2017.7995268|Bo Li, D. Qian, Binyang Li|0.25|2
339|From Python Scripting to Parallel Spatial Modeling: Cellular Automata Simulations of Land Use, Hydrology and Pest Dynamics|Many spatial phenomena are difficult, dangerous, costly or impossible to reproduce, making their simulation the only option. Wildfires, large floods or epidemics are examples in which computer models are fundamental for the scientific study of their dynamics. Today computer models enable us to perform simulations more accurately than ever before thanks to the increasing spatio-temporal resolution of data. However, more data entails more calculations, and too many calculations quickly become computationally prohibitive. While data and computational demands increase, sequential computers have stagnated and parallel computing seems the way forward. Unfortunately parallel codes are long, complex and not very reusable. Therefore, although modelers require parallel performance, they prefer not to give up the comfort of their sequential scripting languages. We address the parallel issue by leveraging compiler techniques that automatically transform sequential python scripts to parallel GPU codes. Our methodology can also process datasets larger than main memory. To make all this possible we restrict our domain to models based on local interactions, e.g. Cellular Automata. We test the automatic parallelization of three CA in land use, hydrology and pest dynamics. The first predicts future urban developments, the second models the flow of water and the third simulates an outbreak of olive fruit fly. The methodology reduces the execution times by harnessing the parallelism of the models and enables the processing of very large datasets, all of this without compromises in programming effort.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2017|10.1109/PDP.2017.18|Jesús Carabaño, J. Westerholm|0.25|2
351|Accelerating advection via approximate block exterior flow maps|Flow visualization techniques involving extreme advection workloads are becoming increasingly popular. While these techniques often produce insightful images, the execution times to carry out the corresponding computations are lengthy. With this work, we introduce an alternative to traditional advection, which improves on performance at the cost of decreased accuracy. Our approach centers around block exterior flow maps (BEFMs), which can be used to accelerate flow computations by reducing redundant calculations. Our algorithm uses Lagrangian interpolation, but falls back to Eulerian advection whenever regions of high error are encountered. In our study, we demonstrate that the BEFM-based approach can lead to significant savings in time, with limited loss in accuracy. Introduction A myriad of scientific simulations, including those modeling fluid flow, astrophysics, fusion, thermal hydraulics, and others, model phenomena where constituents move through their volume. This movement is captured by a velocity field stored at every point on the mesh. Further, other vector fields, such as force fields for electricity, magnetism, and gravity, also govern movement and interaction. A wide range of flow visualization techniques are used to understand such vector fields. The large majority of these techniques rely on placing particles in the volume and analyzing the trajectories they follow. Traditionally, the particles are displaced through the volume using an advection step, i.e., solving an ordinary differential equation using a Runge-Kutta integrator. As computational power on modern desktops has increased, flow visualization algorithms have been empowered to consider designs that include more and more particles advecting for longer and longer periods. Techniques such as Line Integral Convolution and Finite-Time Lyapunov Exponents (FTLE) seed particles densely in a volume and examine where these particles end up. For these operations, and many others, only the ending position of the particle is needed, and not the details of the path the particle took to get there. Despite seemingly abundant computational power, some techniques have excessively long running times. For example, ocean modelers often study the FTLE within an ocean with both high seeding density and very long durations for the particles (years of simulation time) [2, 3]. As another example, fusion scientists are interested in FTLE computations inside a tokamak where particles travel for hundreds of rotations [1]. In both cases, FTLE calculations, even on supercomputers, can take tens of minutes. With this work, we consider an alternative to traditional Eulerian advection. The key observation that motivates the work is that, in conditions with dense seeding and long durations, particles will tread the same (or very similar) paths over and over. Where the current paradigm carries out the same computation over and over, we consider a new paradigm where a computation can be carried out a single time, and then reused. That said, we find that, while particle trajectories do often travel quite close to each other, they typically follow their own (slightly) unique paths. Therefore, to effectively reuse computations, we consider a method where we interpolate new trajectories from existing ones, effectively trading accuracy for speed. Our method depends on Block Exterior Flow Maps, or BEFMs. The idea behind BEFMs is to pre-compute known trajectories that lie on block boundaries. It assumes data in blockdecomposed, but this assumption is common when dealing with parallel, distributed-memory computations. When a computeintensive flow visualization algorithm is then calculated, it consults with the BEFMs and does Lagrangian-style interpolation from its known trajectories. While this approach introduces error, it can be considerably faster, since it avoids Eulerian advection steps inside each block. The contributions of the paper are as follows: • Introduction of BEFMs as an operator for accelerating dense particle advection calculations; • A novel method for generating an approximate BEFM that can be used in practice; • A study that evaluates the approximate BEFM approach, including comparisons with traditional advection. Related Work McLouglin et al. recently surveyed the state of the art in flow visualization [4], and the large majority of techniques they described incorporate particle advection. Any of these techniques could possibly benefit from the BEFM approach, although the tradeoff in accuracy is only worthwhile for those that have extreme computational costs, e.g., Line Integral Convolution [5], finite-time Lyapunov exponents [6], and Poincare analysis [7]. One solution for dealing with extreme advection workloads is parallelization. A summary of strategies for parallelizing particle advection problems on CPU clusters can be found in [8]. The basic approaches are to parallelize-over-data, parallelizeover-particles, or a hybrid of the two [9]. Recent results using parallelization-over-data demonstrated streamline computation on up to 32,768 processors and eight billion cells [11]. These parallelization approaches are complementary with our own. That is, traditional parallel approaches can be used in the current way, but the phase where they advect particles through a region could be replaced by our BEFM approach. In terms of precomputation, the most notable related work comes from Nouanesengsy et al. [10]. They precomputed flow patterns within a region and used the resulting statistics to decide which regions to load. While their precomputation and ours have similar elements, we are using the results of the precomputation in different ways: Nouanesengsy et al. for load balancing and ourselves to replace multiple integrations with one interpolation. In terms of accelerating particle advection through approximation, two works stand out. Brunton et al. [18] also looked at accelerating FTLE calculation, but they considered the unsteady state problem, and used previous calculations to accelerate new ones. While this is a compelling approach, it does not help with the steady state problem we consider. Hlwatsch et al. [15] employ an approach where flow is calculated by following hierarchical lines. This approach is well-suited for their use case, where all data fits within the memory of a GPU, but it is not clear how to build and connect hierarchical lines within a distributed memory parallel setting. In contrast, our method, by focusing on flow between exteriors of blocks, is well-suited for this type of parallelism. Bhatia et al. [19] studied edge maps, and the properties of flow across edge maps. While this work clearly has some similar elements to our, their focus was more on topology and accuracy, and less on accelerating particle advection workloads. Scientific visualization algorithms are increasingly using Lagrangian calculations of flow. Jobard et al. [12] presented a Lagrangian-Eulerian advection scheme which incorporated forward advection with a backward tracing Lagrangian step to more accurately shift textures during animation. Salzbrunn et al. delivered a technique for analyzing circulation and detecting vortex cores given predicates from pre-computed sets of streamlines [14] and pathlines [13]. Agranovsky et al. [16] focused on extracting a basis of Lagrangian flows as an in situ compression operator, while Chandler at al. [17] focused on how to interpolate new pathlines from arbitrary existing sets. Of these works, none share our focus on accelerating advection. Method Our method makes use of block exterior flow maps (BEFM). We begin by defining this mapping, in Section . We then describe our method, and how it incorporates these maps, in Section . Block Exterior Flow Map Definition In scientific computing, parallel simulation codes often partition their spatial volume over their compute nodes. Restated, each compute node will operate on one spatial region, and that compute node will be considered the “owner” of that region. Such a region is frequently referred to as a block. For example, a simulation over the spatial region X: [0-1], Y: [0-1], and Z: [0-1] and having N compute nodes could have N blocks, with each block covering a volume of 1 N . Consider a point P that lies on the exterior of a block B. If the velocity field points toward the interior of B at point P, then Eulerian advection of a particle originating at P will take the particle through the interior of B until it exits. In this case, the particle will exit B at some location P′, where P′ is also located on the exterior of B. The BEFM captures this mapping. The BEFM’s domain is all spatial locations on the exterior of blocks, and its range is also spatial locations on the exteriors of blocks. Further, for any given P in the BEFM’s domain, BEFM(P,B) will produce a location that is on B’s exterior. Saying it concisely, the BEFM is the mapping from particles at exteriors of blocks to the locations where those particles will exit the block under Eulerian advection. Figure 1 illustrates an example of a BEFM.|Visualization and Data Analysis|2017|10.2352/ISSN.2470-1173.2017.1.VDA-397|L. Sugiyama, H. Childs, C. Garth, R. Bleile|0.25|2
372|Optimization and benchmarking of a perturbative Metropolis Monte Carlo quantum mechanics/molecular mechanics program.|In this work, we present an optimized perturbative quantum mechanics/molecular mechanics (QM/MM) method for use in Metropolis Monte Carlo simulations. The model adopted is particularly tailored for the simulation of molecular systems in solution but can be readily extended to other applications, such as catalysis in enzymatic environments. The electrostatic coupling between the QM and MM systems is simplified by applying perturbation theory to estimate the energy changes caused by a movement in the MM system. This approximation, together with the effective use of GPU acceleration, leads to a negligible added computational cost for the sampling of the environment. Benchmark calculations are carried out to evaluate the impact of the approximations applied and the overall computational performance.|Journal of Chemical Physics|2017|10.1063/1.5009820|P. Tomás, Sebastião Miranda, N. Roma, F. Pratas, J. Feldt, R. Mata|0.25|2
727|CBMOS: a GPU-enabled Python framework for the numerical study of center-based models|Background Cell-based models are becoming increasingly popular for applications in developmental biology. However, the impact of numerical choices on the accuracy and efficiency of the simulation of these models is rarely meticulously tested. Without concrete studies to differentiate between solid model conclusions and numerical artifacts, modelers are at risk of being misled by their experiments’ results. Most cell-based modeling frameworks offer a feature-rich environment, providing a wide range of biological components, but are less suitable for numerical studies. There is thus a need for software specifically targeted at this use case. Results We present CBMOS, a Python framework for the simulation of the center-based or cell-centered model. Contrary to other implementations, CBMOS’ focus is on facilitating numerical study of center-based models by providing access to multiple ordinary differential equation solvers and force functions through a flexible, user-friendly interface and by enabling rapid testing through graphics processing unit (GPU) acceleration. We show-case its potential by illustrating two common workflows: (1) comparison of the numerical properties of two solvers within a Jupyter notebook and (2) measuring average wall times of both solvers on a high performance computing cluster. More specifically, we confirm that although for moderate accuracy levels the backward Euler method allows for larger time step sizes than the commonly used forward Euler method, its additional computational cost due to being an implicit method prohibits its use for practical test cases. Conclusions CBMOS is a flexible, easy-to-use Python implementation of the center-based model, exposing both basic model assumptions and numerical components to the user. It is available on GitHub and PyPI under an MIT license. CBMOS allows for fast prototyping on a central processing unit for small systems through the use of NumPy. Using CuPy on a GPU, cell populations of up to 10,000 cells can be simulated within a few seconds. As such, it will substantially lower the time investment for any modeler to check the crucial assumption that model conclusions are independent of numerical issues.|bioRxiv|2021|10.1101/2021.05.06.442893|Sonja Mathias, A. Coulier, A. Hellander|0.25|2
732|Parameters Identification for Fractional-fractal Model of Filtration-Consolidation Using GPU|The paper considers some computational problems arising in the important practical field of the determination of safe operation conditions of engineering facilities that pollute soils and groundwater. In the case of complex geological and hydrological conditions, such problems are widely considered using mathematical modeling of deformation and consolidation processes in water-saturated soils, particularly, in the foundations of hydraulic structures. To simulate the dynamics of such processes, we use a fractional-fractal approach that allows considering temporal non-locality of transfer processes in media of fractal structure. The used one-dimensional differential model contains a non-local Caputo derivative with respect to the time variable and a local fractal derivative with respect to the space variable. Some of model’s parameters, namely the orders of fractional derivatives, can only be determined fitting them to the measured data related to the state of a process. We propose to use particle swarm optimization algorithm to perform an identification of fractional derivatives’ orders and present the results of its testing on noised subsets of direct problem solutions. In this context, we have determined that the order of space-fractal derivative is restored with a relative error of not more than 1% while the order of time-fractional derivative is restored with higher errors of not more than 10%. The lowest number of observation points that ensures stable restoration of the orders was equal to 25. As high computational complexity is combined with highly independent computational blocks while applying evolutionary optimization algorithms to the problems of differential models’ parameters identification, we implemented the proposed algorithm on graphical processing units (GPU) using OpenCL framework and on multi-threaded systems using OpenMP. The results of performance testing showed up to 4-times lower GPU execution time compared to the case of multi-threaded execution on 6 cores of central processing unit (CPU).|International Workshop on Computer Modeling and Intelligent Systems|2021|10.32782/cmis/2864-36|Vsevolod Bohaienko, A. Gladky|0.25|2
738|Optics Versus Computation: Influence of Illumination and Reconstruction Model Accuracy in Focal-Plane-Scanning Optical Projection Tomography|Optical Projection Tomography (OPT) imaging provides isotropic resolution for samples up to a few millimeters. High resolution OPT is achieved by deconvolving Focal-Plane-Scanning (FPS-OPT) data but it requires to accurately know the system’s Point Spread Function (PSF). While the presence of noise and inaccuracies in the PSF model or parameters affects reconstruction quality, their effect is difficult to assess quantitatively in practice and the computational cost of naive simulations is prohibitively expensive. Here, we present an efficient approach to carry out FPS-OPT simulations for a wide range of illumination geometries, including Focal-Sheet-Scanning OPT (FSS-OPT), a method using a lateral light-sheet illumination to perform FPS-OPT. We implement a simulation framework that can accomodate large size 3D data by dividing the forward model into elements that can be efficiently processed by GPUs. We compare the performance of FPS-OPT and FSS-OPT on simulated data. In the presence of Poisson noise, we show that FSS-OPT outperforms FPS-OPT with deconvolution even if all model parameters are accurately known. We then validate these results on experimentally acquired data. The availability of an efficient 3D OPT simulation framework for quantitative comparison of imaging scenarios is an essential tool for determining efficient imaging geometries to evaluate the relative benefits of computational and hardware variations.|IEEE International Symposium on Biomedical Imaging|2021|10.1109/ISBI48211.2021.9433834|M. Liebling, François Marelli|0.25|2
745|FPGA-accelerated Agent-Based Simulation for COVID-19|Agent-based models (ABMs) can provide realistic dynamics for epidemics at the individual level so that users can observe and predict the spreading pattern and the effectiveness of intervention over time and space. This paper proposes an FPGA-based accelerator for agent-based epidemic modeling for COVID-19. The optimizations enabling the effective acceleration of the simulation procedure are presented. The key idea is to partition the calculation properly to decouple the on-chip resource usage from the population size. Also, an algorithmic adaptation is proposed to reduce the latency caused by conditional branches within loops. An experimental implementation on an Intel Arria 10 GX 10AX115S2F45I1SG FPGA running at 240MHz achieves 2.2 and 1.9 times speed-up respectively over a CPU reference using 10 cores on an Intel Xeon Gold 6230 CPU and a GPU reference on an Nvidia GeForce RTX 2080 Ti GPU.|International Conference on Artificial Intelligence Circuits and Systems|2021|10.1109/AICAS51828.2021.9458570|Lei Fu, Ce Guo, W. Luk|0.25|2
772|Modelling realistic ballast shape to study the lateral pull behaviour using GPU computing|The use of the Discrete Element Method to model engineering structures implementing granular materials has proven to be an efficient method to response under various behaviour conditions. However, the computational cost of the simulations increases rapidly, as the number of particles and particle shape complexity increases. An affordable solution to render problems computationally tractable is to use graphical processing units (GPU) for computing. Modern GPUs offer up 10496 compute cores, which allows for a greater parallelisation relative to 32-cores offered by high-end Central Processing Unit (CPU) compute. This study outlines the application of BlazeDEM-GPU, using an RTX 2080Ti GPU (4352 cores), to investigate the influence of the modelling of particle shape on the lateral pull behaviour of granular ballast systems used in railway applications. The idea is to validate the model and show the benefits of simulating non-spherical shapes in future large-scale tests. The algorithm, created to generate the shape of the ballast based on real grain scans, and using polyhedral shape approximations of varying degrees of complexity is shown. The particle size is modelled to scale. A preliminary investigation of the effect of the grain shape is conducted, where a sleeper lateral pull test is carried out in a spherical grains sample, and a cubic grains sample. Preliminary results show that elementary polyhedral shape representations (cubic) recreate some of the characteristic responses in the lateral pull test, such as stick/slip phenomena and force chain distributions, which looks promising for future works on railway simulations. These responses that cannot be recreated with simple spherical grains, unless heuristics are added, which requires additional calibration and approximations. The significant reduction in time when using non-spherical grains also implies that larger granular systems can be investigated.|EPJ Web of Conferences|2021|10.1051/EPJCONF/202124906003|J. Ferellec, N. Govender, F. Nader, D. Wilke, P. Pizette|0.25|2
779|Computer architecture and high performance computing|In this special issue of Concurrency and Computation Practice and Experience, we are pleased to present eight selected papers that were previously presented at the Brazilian “XX Simpósio em Sistemas Computacionais de Alto Desempenho,” WSCAD 2019. The event was held in conjunction with the 31st International Symposium on Computer Architecture and High Performance Computing, SBAC-PAD 2019, in Campo Grande, MS, Brazil, from October 15 to 18, 2019. The WSCAD workshop has been presenting important research in the fields of computer architectures, high performance computing, and distributed systems, since the beginning of the 2000s. The scope of the current special issue is broad and representative, with different forms of contributions to our discipline: methodological papers, technology papers, application papers, and system papers. The topics covered in the papers include architecture issues, compiler optimization, performance evaluation, parallel algorithms, energy efficiency, and applications. The title of the first paper is “Structural testing for communication events into loops of message-passing parallel programs,” by Diaz et al.1 In this paper, the authors propose new structural testing criteria for message-passing parallel programs, focusing on defects from communication primitives into loops. A new test model is presented to support their criteria for structural testing of MPI-applications. The testing criteria are validated through experimental studies using a tool called ValiMPI. The results show that unknown defects from communication and synchronization events can be revealed in different loop iterations, increasing the quality of message-passing parallel programs. In the second contribution, entitled “Smart selection of optimizations in dynamic compilers,” Rosario et al.2 present an approach that uses machine learning to select sequences of optimization for dynamic compilation that considers both code quality and compilation overhead. Their approach starts by training a model, offline, with a knowledge bank of those sequences with low overhead and high-quality code generation capability using a genetic heuristic. Then, this bank is used to guide the smart selection of optimizations sequences for the compilation of code fragments during the emulation of an application. The proposed strategy is evaluated in two LLVM-based dynamic binary translators, namely, OI-DBT and HQEMU, showing that these two translators can achieve average speedups of 1.26× and 1.15× in MiBench and Spec Cpu benchmarks, respectively. In the third contribution, entitled “Memory allocation anomalies in high-performance computing applications: A study with numerical simulations,” Gomes et al.3 propose a method for identifying, locating, characterizing, and fixing allocation anomalies, and a tool for developers to apply the method. A numerical simulator that approximates the solutions to partial differential equations using a finite element method is used in the experiments. It is shown that taming allocation anomalies in the simulator reduces both its execution time and the memory footprint of its processes, irrespective of the specific heap allocator being employed with it. They conclude that the developer of HPC applications can benefit from the method and tool during the software development cycle. The fourth contribution, entitled “Investigating memory prefetcher performance over parallel applications: From real to simulated,” by Girelli et al.,4 contributes to shed light on the memory prefetcher’s role in the performance of parallel high-performance computing applications, considering the prefetcher algorithms offered by both the real hardware and the simulators. The authors performed a careful experimental investigation, executing the NAS parallel benchmark (NPB) on a real Skylake machine, and as well in a simulated environment with the ZSim and Sniper simulators, taking into account the prefetcher algorithms offered by both Skylake and the simulators. The experimental results show that: (i) prefetching from the L3 to L2 cache presents better performance gains, (ii) the memory contention in the parallel execution constrains the prefetcher’s effect, (iii) Skylake’s parallel memory contention is poorly simulated by ZSim and Sniper, and (iv) Skylake’s noninclusive L3 cache hinders the accurate simulation of NPB with the Sniper’s prefetchers. In the fifth contribution, entitled “Energy efficiency and portability of oil and gas simulations on multicore and graphics processing unit architectures,” Serpa et al.5 propose three optimizations for an oil and gas application, reverse time migration (RTM), which reduce the floating-point operations by changing the equation derivatives. They evaluate these optimizations in different multicore and GPU architectures, investigating the impact of different APIs on the performance, energy efficiency, and portability of the code. The experimental results show that the dedicated CUDA implementation running on the NVIDIA Volta architecture has the best performance and energy efficiency for RTM on GPUs, while the OpenMP version is the best for Intel Broadwell in the multicore. Also, the OpenACC version, which has a lower programming effort and executes on both architectures, has up to 20% better performance and energy efficiency than the nonportable ones.|Concurrency and Computation|2021|10.1002/cpe.6526|Fabrizio Marozzo, W. Martins, R. Camargo|0.25|2
781|Fast scalable implicit solver with convergence of equation-based modeling and data-driven learning: earthquake city simulation on low-order unstructured finite element|We developed a new approach in converging equation-based modeling and data-driven learning on high-performance computing resources to accelerate physics-based earthquake simulations. Here, data-driven learning based on data generated while conducting equation-based modeling was used to accelerate the convergence process of an implicit low-order unstructured finite-element solver. This process involved a suitable combination of data-driven learning for estimating high-frequency components and coarsened equation-based models for estimating low-frequency components of the problem. The developed solver achieved a 12.8-fold speedup over the state-of-art solver with a 96.4% size-up scalability up to 24,576 nodes (98,304 MPI processes × 12 OpenMP threads = 1,179,648 CPU cores) of Fugaku with 126,581,788,413 degrees-of-freedom, leading to solving a huge city earthquake shaking analysis in a 10.1-fold shorter time than the previous state-of-the-art solver. Furthermore, to show that the developed method attains high performance on variety of systems with small implementation costs, we ported the developed method to recent GPU systems by use of directive based methods (OpenACC). The equation based modeling and the data-driven learning are of utterly different characteristics, and hence they are rarely combined. The developed approach of combining them is effective, and remarkable results mentioned above are achieved.|Platform for Advanced Scientific Computing Conference|2021|10.1145/3468267.3470616|N. Ueda, T. Ichimura, Yuma Kikuchi, M. Hori, Lalith Maddegedara, K. Fujita, T. Nishiki, K. Minami, Miwako Tsuji, S. Nishizawa, Hikaru Inoue, Ryota Kusakabe, Kentaro Koyama|0.25|2
807|Research on Acceleration Algorithm for Raw Data Simulation of High Resolution Squint Spotlight SAR|In order to realize the raw data simulation of high resolution squint spotlight Synthetic Aperture Radar (SAR) efficiently, an effective acceleration algorithm is proposed. This algorithm combines the time-domain raw data simulation model and its signal characteristics to compensate the range cell migration (RCM) existing in the raw data simulation process of squint spotlight SAR. An adaptive data partitioning algorithm is used, and computes partitioned data respectively in GPU. Then the partitioned data are transmitted and spliced. The algorithm improves the computational efficiency of time-domain raw data simulation, and it solves the problems of huge volume of raw data, limitation of GPU memory and data transmission. The experimental results of point target and distributed target show that the speedup ratio of this algorithm reaches 209.93, which verifies the effectiveness of the proposed method.|2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS|2021|10.1109/IGARSS47720.2021.9553276|Zewen Fu, L. Bai, Zhengwei Guo, Lin Min, Ning Li|0.25|2
809|A C++ Cherenkov photons simulation in CORSIKA 8|CORSIKA is a standard software for simulations of air showers induced by cosmic rays. It has been developed mainly in Fortran 77 continuously over the last thirty years. It has become very difficult to add new physics features to CORSIKA 7. CORSIKA 8 aims to be the future of the CORSIKA project. It is a framework in C++17 which uses modern concepts in object oriented programming for an efficient modularity and flexibility. The CORSIKA 8 project aims to attain high performance by exploiting techniques such as vectorization, gpu/cpu parallelization, extended use of static polymorphism and the most precise physical models available. In this paper, we focus on the Cherenkov photon propagation module of CORSIKA, which is of particular interest for gamma-ray experiments, like the Cherenkov Telescope Array. First, we present the optimizations that we have applied to the Cherenkov module thanks to the results of detailed profiling using performance counters. Then, we report our preliminary work to develop the Cherenkov Module in the CORSIKA 8 framework. Finally, we will demonstrate the first performance comparison with the current CORSIKA software as well as physics validation.|EPJ Web of Conferences|2021|10.1051/epjconf/202125103011|G. Vasileiadis, L. Arrabito, P. Langlois, M. Carrère, David Parello, J. Bregeon|0.25|2
816|Software-Defined Vector Processing on Manycore Fabrics|We describe a tiled architecture that can fluidly transition between manycore (MIMD) and vector (SIMD) execution. The hardware provides a software-defined vector programming model that lets applications aggregate groups of manycore tiles into logical vector engines. In manycore mode, the machine behaves as a standard parallel processor. In vector mode, groups of tiles repurpose their functional units as vector execution lanes and scratchpads as vector memory banks. The key mechanism is an instruction forwarding network: a single tile fetches instructions and sends them to other trailing cores. Most cores disable their frontends and instruction caches, so vector groups amortize the intrinsic hardware costs of von Neumann control. Vector groups also use a decoupled access/execute scheme to centralize their memory requests and issue coalesced, wide loads. We augment an existing RISC-V manycore design with a minimal hardware extension to implement software-defined vectors. Cycle-level simulation results show that software-defined vectors improve performance by an average of 1.7 × over standard MIMD execution while saving 22% of the energy. Compared to a similarly configured GPU, the architecture improves performance by 1.9 ×.|Micro|2021|10.1145/3466752.3480099|Adrian Sampson, Neil Adit, Philip Bedoukian, Edwin Peguero|0.25|2
821|Porting Real-World Applications to GPU Clusters: A Celerity and Cronos Case Study|Accelerator clusters are an ongoing trend in high performance computing, continuously gaining traction and forming a ubiquitous hardware resource for domain scientists to run large-scale simulations on. However, there is often a gap between new hardware technologies and adoption by legacy code bases. Porting real-world applications to new programming models is a difficult undertaking, aggravated by the need for support for both distributed-memory and accelerator parallelism. In this work, we present a case study of porting Cronos, a real-world code from the field of magnetohydrodynamics, to Celerity, a high-level programming model for distributed-memory accelerator clusters. We discuss the numerical, algorithmic and implementation properties of the application and motivate our decisions for adapting them where necessary. Preliminary results show a parallel efficiency of up to 87% for 16 GPUs.|eScience|2021|10.1109/eScience51609.2021.00019|Philip Salzmann, R. Kissmann, Fabian Knorr, T. Fahringer, D. Huber, Peter Thoman, P. Gschwandtner|0.25|2
822|Principles towards Real-Time Simulation of Material Point Method on Modern GPUs|Physics-based simulation has been actively employed in generating offline visual effects in the film and animation industry. However, the computations required for high-quality scenarios are generally immense, deterring its adoption in real-time applications, e.g., virtual production, avatar live-streaming, and cloud gaming. We summarize the principles that can accelerate the computation pipeline on single-GPU and multi-GPU platforms through extensive investigation and comprehension of modern GPU architecture. We further demonstrate the effectiveness of these principles by applying them to the material point method to build up our framework, which achieves $1.7\times$--$8.6\times$ speedup on a single GPU and $2.5\times$--$14.8\times$ on four GPUs compared to the state-of-the-art. Our pipeline is specifically designed for real-time applications (i.e., scenarios with small to medium particles) and achieves significant multi-GPU efficiency. We demonstrate our pipeline by simulating a snow scenario with 1.33M particles and a fountain scenario with 143K particles in real-time (on average, 68.5 and 55.9 frame-per-second, respectively) on four NVIDIA Tesla V100 GPUs interconnected with NVLinks.|arXiv.org|2021|10.1145/3476576.3476674|Yuhan Huang, Yun Fei, Ming Gao|0.25|2
839|Parallelization of lattice Boltzmann software for execution on multi-GPU clusters with application to the simulation of blood flow through human arteries|It is important to consider the blood flow pattern when planning vascular interventions of atherosclerotic plaques. Large scale computer modeling can be very helpful in this case. The software presented in this paper numerically models blood flow through patient-specific blood vessels. Lattice Boltzmann method was used to simulate blood flow. The principles of GPU (Graphics Processing Unit) programming are applied during implementation and the developed software was parallelized using the CUDA (Compute Unified Device Architecture) and optimized to run on a multi-GPU cluster using the MPI approach. Using the multi-GPU infrastructure, numerical simulations can utilize larger amount of memory resources for the computation, making the level of reality of the models an order of magnitude higher. Execution of the presented software enables fast and reliable case-studies and parametric analyses useful for medical decision-making. The presented software can give medical professionals fast quantitative information about fluid flow in diseased arteries and assist them in selecting the most appropriate treatment.|International Conferences on Biological Information and Biomedical Engineering|2021|10.1109/BIBE52308.2021.9635318|T. Djukić, N. Filipovic|0.25|2
1147|Adaptive co-simulation of functional-thermal behaviour of integrated circuits|Thermal issues become serious bottlenecks for high-end and mobile ICs as well. To analyse the temperature dependent behaviour, functional-thermal co-simulation can be performed. We are developing a framework called LogiTherm which can be used to perform such simulation of digital and mixed signal circuits on different abstraction levels. The framework utilizes the so-called simulator coupling technique to connect the thermal and functional domain. Multiple thermal simulator engines are integrated into our system (e.g. 3D-ICE, HotSpot, SUNRED), but their implementation is not efficient on modern computer architectures, thus the simulation speed can be limited. In this paper, we describe a new thermal model and a simulator engine designed for multicore processors and GPUs to enable more detailed simulation. A new, adaptive co-simulation strategy is also described, which can further reduce the computational overhead of the thermal simulations.|International Workshop on Thermal Investigations of ICs and Systems|2017|10.1109/THERMINIC.2017.8233823|L. Jani, A. Poppe|0.25|2
1152|Applications of the OpenCAL scientific library in the context of CFD: Applications to debris flows|We here present computational results as performed by the new OpenCAL scientific library for seamless implementation of models referred to Computational Fluid Dynamics (CFD). In particular, OpenCAL allows for the straightforward definition of Cellular Automata- and Finite Differences-based simulation models of complex systems, by also supporting Extended Cellular Automata and all those computing paradigms based on regular computational grids. Two different parallel implementations are presented, which permit to exploit both multi-core CPUs on shared memory computers, and a wide range of heterogeneous devices like GPUs and other many-core coprocessors. Experiments, referred to a well-known cellular automata, namely the SCIDDICA S3−hex model for landslide simulation, exhibit good scalability, and numerical correctness and efficiency have been assessed on both multi-core CPUs and different many-core coprocessors. The good performances that were achieved in all benchmarks confirm the library's suitability for model development of complex systems and their execution on parallel heterogeneous computational devices.|2017 IEEE 14th International Conference on Networking, Sensing and Control (ICNSC)|2017|10.1109/ICNSC.2017.8000179|R. Rongo, D. Spataro, D. D'Ambrosio, W. Spataro, A. Rango|0.25|2
1157|Accelerator-in-switch: A framework for tightly coupled switching hub and an accelerator with FPGA|Accelerator-in-Switch (AiS) is a framework for building an accelerator logic tightly coupled with a switching hub in a single FPGA for high performance computation with heterogeneous environment with CPUs and GPUs. AiS is implemented on a partial reconfigurable region of an FPGA whose permanent region is used for a switching hub. A port of the switching hub is connected to the registers and local memory of AiS directly. AiS has a standard interface for a standard bus (Avalon MM bus, here) to exchange data between on-board DDR SDRAM and the local memory, and various types of accelerators can be implemented just by providing such an interface. The data input and output are performed with the DMA controller inside the switching hub with the shared memory model between host CPUs and GPUs. We implemented two example accelerators: a reduction calculator for a radiation transfer equations solver (RED) and LET generator for N-body simulation (LET) were implemented as the AiS on PEACH3, a switching hub for a PCIe direct interconnection network with Altera's Stratix V. The use of partial reconfiguration makes it possible to switch multiple accelerators without stopping the switching hub. As a result, we reduced the time for place&route of an accelerator by 47% compared to the case of the design combining the accelerator into the switching hub.|International Conference on Field-Programmable Logic and Applications|2017|10.23919/FPL.2017.8056846|H. Amano, Chiharu Tsuruta, Naoki Nishikawa, Takahiro Kaneda|0.25|2
1170|Fast Hardware Construction and Refitting of Quantized Bounding Volume Hierarchies|There is recent interest in GPU architectures designed to accelerate ray tracing, especially on mobile systems with limited memory bandwidth. A promising recent approach is to store and traverse Bounding Volume Hierarchies (BVHs), used to accelerate ray tracing, in low arithmetic precision. However, so far there is no research on refitting or construction of such compressed BVHs, which is necessary for any scenes with dynamic content. We find that in a hardware‐accelerated tree update, significant memory traffic and runtime savings are available from streaming, bottom‐up compression. Novel algorithmic techniques of modulo encoding and treelet‐based compression are proposed to reduce backtracking inherent in bottom‐up compression. Together, these techniques reduce backtracking to a small fraction. Compared to a separate top‐down compression pass, streaming bottom‐up compression with the proposed optimizations saves on average 42% of memory accesses for LBVH construction and 56% for refitting of compressed BVHs, over 16 test scenes. In architectural simulation, the proposed streaming compression reduces LBVH runtime by 20% compared to a single‐precision build, and 41% compared to a single‐precision build followed by top‐down compression. Since memory traffic dominates the energy cost of refitting and LBVH construction, energy consumption is expected to fall by a similar fraction.|Computer graphics forum (Print)|2017|10.1111/cgf.13233|Kalle Immonen, P. Jääskeläinen, J. Takala, T. Viitanen, M. Koskela|0.25|2
1201|A New Method for Modeling Clouds Combining Procedural and Implicit Models|Cloud models are important components in 3D open environment games and virtual reality applications. In the literature, different approaches have been proposed to solve the problem of modeling the complex geometry and texture of clouds including physical simulation, photo-based modeling and a combination of implicit modeling with procedural techniques. The latter is simple and produces satisfactory results and can be also done in realtime. Nevertheless, it is still difficult to produce the desired results by combining implicit modeling with procedural techniques. The reasons for this limitation are twofold. First, the parameters are not intuitive and intricately coupled with each other. The second reason is that it is hard to combine the overall shape defined by the implicit model with the details generated by the procedural models in a smooth and localized way. In this work, we propose a new way of modeling clouds by combining volumetric implicit modeling with procedural noise techniques. Differently from previous works, the details produced by noise are attached to each implicit primitive independently, in different scales, before they are smoothly blended. This provides a greater level of control in the process of designing and modeling the clouds. By implementing our technique in GPUs it is possible to change the parameters and the blending of primitives in realtime and observe immediately the produced results. This interactive behavior enables the modeler to more easily build and experiment different versions of his ideas. Although we applied this technique to cloud modeling we show that the same approach can be useful for modeling many natural objects as rocks, terrain and planet shapes, among others.|Brazilian Symposium on Games and Digital Entertainment|2017|10.1109/SBGames.2017.00027|B. Dembogurski, E. Clua, A. Montenegro, Icaro Baptista|0.25|2
1204|On the Performance and Energy Consumption of Molecular Dynamics Applications for Heterogeneous CPU-GPU Platforms Based on Gromacs|Abstract High Performance Computing (HPC) accelerates life science discoveries by enabling scientists to analyze large data sets, to develop detailed models of entire biological systems and to simulate complex biological processes. As computational experiments, molecular dynamics simulations are widely used in life sciences to evaluate the equilibrium nature of classical many-body systems The modelling and molecular dynamics study of surfactant, polymer solutions and the stability of proteins and nucleic acids under different conditions, as well as deoxyribonucleic acid proteins are studied. The study aims to understand the scaling behavior of Gromacs (Groningen machine for chemical simulations) on various platforms, and the maximum performance in the prospect of energy consumption that can be accomplished by tuning the hardware and software parameters. Different system sizes (48K, 64K, and 272K) from scientific investigations have been studied show that the GPU (Graphics Processing Unit) scales rather beneficial than other resources, i.e., with GPU support. We track 2-3 times speedup compared to the latest multi-core CPUs. However, the so-called “threading effect” leads to the better results.||2017|10.1515/cait-2017-0056|A. Poghosyan, W. Narsisian, H. Astsatryan, Y. Mamasakhlisov|0.25|2
1251|A novel parallel algorithm for computing the mooring line based on lumped-mass method|Dynamic modeling and simulation of the mooring system are the key technologies in anchor handling simulator (AHS). Built up the mooring line’s dynamics model based on lumped-mass method (LMM), and fourth-order Runge–Kutta method was used to solve the model; because of the huge amounts of calculation in the model’s solving, the very time-consuming process brings great impact on the real-time, fidelity and immersed feeling in the anchor handling scene simulation, seriously hindered its application in AHS. A novel parallel algorithm was proposed to speed-up the model’s solving process by taking the advantages of graphic processing units (GPU’s) massive parallel computing and float point computing capability. The model’s solving process was implemented on vertex shader based on the transform feedback (TF) mechanism in modern GPU. Experimental results show that, the new algorithm reduced the calculating time largely without losing accuracy, and can finally realize the real-time solving and simulation.|Advances in Complex Systems|2017|10.1142/S1793962317500040|M. M. Movania, Zhong Zhu, Yong Yin|0.25|2
1267|Improved simulation of the Nvidia Kepler memory hierarchy through microbenchmarking|of the Thesis Improved Simulation of the Nvidia Kepler Memory Hierarchy through Microbenchmarking by Brad J Courville Master of Science in Electrical and Computer Engineering Northeastern University, April 2017 Prof. David Kaeli, Advisor General Purpose Graphics Processing Unit (GPGPU)s are frequently used to accelerate the performance of many types of parallel scientific and engineering workloads. The advent of GPU programming frameworks, such as Nvidia’s CUDA and Khronos’s OpenCL, has made it far easier to program these devices by providing a familiar C-syntax programming environment. Along with the increased popularity of these accelerators, comes an increased demand for simulation software that is capable of emulating their performance. Such computer architecture simulation software can be immensely useful in the performance evaluation of new hardware and architectural design decisions, and can be used to influence future microarchitectures. One such heterogeneous simulator capable of emulating the Nvidia Kepler device is Multi2Sim. The utility of any simulation infrastructure is dependent upon how well it reflects the execution on actual hardware. In this thesis we utilize microbenchmarks to highlight a number of microarchitectural properties of an NVIDIA Kepler GPU. The goal is to develop an understanding of the memory access timings and cache parameters present in the Kepler memory hierarchy. This knowledge is then integrated into the Multi2sim Kepler model to improve simulation accuracy. These enhancements improve the timing accuracy of the MultiSim Kepler model by an average of 10.8% and a maximum of 26.8% with for memory intensive benchmarks when compared to physical hardware.||2017|10.3384/ecp1713247|Brad J Courville|0.25|2
1272|Comparison of Three Different Parallel Computation Methods for a Two-Dimensional Dam-Break Model|Three parallel methods (OpenMP, MPI, and OpenACC) are evaluated for the computation of a two-dimensional dam-break model using the explicit finite volume method. A dam-break event in the Pangtoupao flood storage area in China is selected as a case study to demonstrate the key technologies for implementing parallel computation. The subsequent acceleration of the methods is also evaluated. The simulation results show that the OpenMP and MPI parallel methods achieve a speedup factor of 9.8× and 5.1×, respectively, on a 32-core computer, whereas the OpenACC parallel method achieves a speedup factor of 20.7× on NVIDIA Tesla K20c graphics card. The results show that if the memory required by the dam-break simulation does not exceed the memory capacity of a single computer, the OpenMP parallel method is a good choice. Moreover, if GPU acceleration is used, the acceleration of the OpenACC parallel method is the best. Finally, the MPI parallel method is suitable for a model that requires little data exchange and large-scale calculation. This study compares the efficiency and methodology of accelerating algorithms for a dam-break model and can also be used as a reference for selecting the best acceleration method for a similar hydrodynamic model.||2017|10.1155/2017/1970628|Wenda Li, Zhu Jing, Yong Zhao, Y. Yi, Shang-hong Zhang|0.25|2
1382|Development of a solver of the Maxwell-Bloch equations with GPGPUs|Faculdade de Ciências da Universidade do Porto Integrated Master’s in Engineering Physics Development of a solver of the Maxwell-Bloch equations with GPGPUs by João Costa This thesis describes the development of a solver of the three-dimensional MaxwellBloch equations implemented using C++ and GPGPU computing technologies. The solver is also a modular software that provides a broad set of tools to study physical phenomena regarding the interaction between electromagnetic fields and atomic gases. Furthermore, this work reviews the physical models that govern the dynamics of the atomic states in the presence of electromagnetic fields using the Maxwell-Bloch equations and accesses some of the available computational methods capable of simulating them. The key concepts of GPGPU computing are presented alongside the design and implementation details of the solver. We compare the performance of the solver using multiple GPU and CPU backends and test the numerical stability of the implementation. Finally, we apply our solver to physical scenarios including two and three-level atomic gases and combine our software with a PIC code to demonstrate its modularity. UNIVERSIDADE DO PORTO||2017|10.1117/12.2272052|João C. Costa|0.25|2
1389|A Performance-Portable Implementation of the Finite Element Assembly in an Atmosphere & Land-Ice Code using the Kokkos Library.|Performance portability to new and emerging architecture machines (e.g., multi-core, manycore and GPU systems) is becoming a requirement in many applications, climate modeling in particular. As high performance computing architectures become increasingly more heterogeneous, climate modeling tools must also adapt and be more efficient in taking advantage of potential performance capabilities. Performance portability of a finite element code can be achieved using the Kokkos [1] library and programming model. Developed at Sandia National Laboratories, Kokkos is a librarybased programming model that enables computational kernels to be performance portable across many-core architectures. This is accomplished by decoupling computational kernels from device-specific data access performance requirements (e.g., NVIDIA coalesced memory access) through an intuitive multidimensional array API. Specifically, the Kokkos library provides data abstractions to adjust (at compile time) the memory layout of basic data structures like 2D and 3D arrays and allow the transparent utilization of special hardware load and store operations. These features enable Kokkos to maximize the amount of user code that can be compiled for diverse devices and obtain similar performance as a variant of the code that is written specifically for that device. This talk will detail our effort in developing a performance portable implementation of the finite element assembly in a multi-physics C++ code known as Albany [2], which houses several climate applications. We will describe the process of refactoring the finite element assembly process for two different climate simulation modules within Albany: the Aeras atmosphere solver and the FELIX land-ice dynamical core. We will then show results of strong as well as weak scalability studies on MPI+OpenMP and MPI+GPU frameworks for these two climate applications. We will conclude by discussing several methods for improving the performance of the code for future optimizations.||2017|10.1007/978-94-024-1251-2_5|Jerry Watkins, I. Tezaur, I. Demeshko|0.25|2
1391|Visualization and Heuristic Modeling for Planning of Minimally- and Non-Invasive Tissue Ablation|This PhD thesis describes methods for the support of the planning process for minimally- and non-invasive tissue ablation procedures. It focuses on the utilization of visualization and heuristic modeling to solve complex problems such as access path determination for needle-based tumor ablation therapies or sonication planning for high-intensity focused ultrasound. \nFor needle-based interventions, the manual selection of suited access paths is addressed, which typically requires the careful inspection of every candidate path as a whole to make sure that no structure at risk would be penetrated. Especially for angulated paths that are not included in one image slice, this results in repeated inspection of multiple slices. To improve this process, a visualization method for the highlighting of infeasible paths in common 2D viewers is proposed. \nAs an alternative to manual planning of percutaneous procedures, heuristic approaches to therapy plan optimization are investigated. An algorithm based on projection utilizing the GPU and image processing is proposed. Under consideration of multiple clinically relevant criteria, the method generates a list of optimal paths within a few seconds. A second, semi-automatic method is proposed, which represents a compromise between fully automatic and manual planning. It combines the fast projection based method with a numerical approach for Pareto-front approximation and allows for interactive exploration of the solution space. \nTo improve the manual planning of high-intensity focused ultrasound therapies, a real-time approximation of temperature and thermal dose is developed. It is based on numerical simulation for a range of exemplary configurations in a preprocessing step. During interactive planning, the temperature or thermal dose field for the situation at hand is interpolated and combined with an approximation of the heat sink effect resulting from vessels in proximity. The result is visualized in an interactive manner.||2017|10.1109/isse.2017.8000991|C. Schumann|0.25|2
204|Investigation of Portable Event-Based Monte Carlo Transport Using the NVIDIA Thrust Library|Power consumption considerations are driving future high performance computing platforms toward many-core computing architectures. The Trinity machine to become available at Los Alamos National Laboratory in 2016 will use both Intel Xeon Haswell processors and Intel Xeon Phi Knights Landing many integrated core (MIC) architecture coprocessors. The Sierra machine to be available at Lawrence Livermore National Laboratory beginning in 2018 will use an IBM PowerPC architecture along with Nvidia graphics processing unit (GPU) architecture accelerators. As a result of these different advanced architectures, the computing landscape for the upcoming years is complex. Traditional approaches to Monte Carlo transport do not work efficiently on these new computing platforms. MIC architectures require vectorization to operate efficiently, vectorization is difficult to achieve in Monte Carlo transport. GPU architectures require additional code to explicitly use the hardware, requiring significant code changes or hardware specific branches in the source code. A significant challenge for Monte Carlo transport projects is to simultaneously support efficient versions of simulation codes for both the current generation and the different advanced computing architectures within a single source code base. In order to address these issues, two important changes are typically made: a new algorithmic approach to solving Monte Carlo transport, and explicit use of the GPU hardware in software. In this paper, we describe initial research investigations of an event based Monte Carlo transport algorithm [1] implemented using the Nvidia Thrust library [2] on a GPU for a Monte Carlo test code. The event based algorithm targets many-core architectures by increasing SIMD (single instruction multiple data) parallelism, while Thrust provides portable performance by allowing one source code base to compile code targeted for both CPUs and GPUs||2016|10.1108/lhtn-04-2016-0017|R. Bleile, S. Dawson, H. Childs, P. Brantley, M. O’Brien|0.2222222222222222|2
239|Spectral Domain Decomposition Using Local Fourier Basis: Application to Ultrasound Simulation on a Cluster of GPUs|The simulation of ultrasound wave propagation through biological tissue has a wide range of practical applications. However, large grid sizes are generally needed to capture the phenomena of interest. Here, a novel approach to reduce the computational complexity is presented. The model uses an accelerated k -space pseudospectral method which enables more than one hundred GPUs to be exploited to solve problems with more than 3*10^9 grid points. The classic communication bottleneck of Fourier spectral methods, all-to-all global data exchange, is overcome by the application of domain decomposition using local Fourier basis. Compared to global domain decomposition, for a grid size of 1536 x 1024 x 2048, this reduces the simulation time by a factor of 7.5 and the simulation cost by a factor of 3.8.|Supercomputing Frontiers and Innovations|2016|10.14529/JSFI160305|Filip Vaverka, B. Treeby, J. Jaros|0.2222222222222222|2
429|A phenomenological scattering model for order-independent transparency|"Translucent objects such as fog, smoke, glass, ice, and liquids are pervasive in cinematic environments because they frame scenes in depth and create visually compelling shots. Unfortunately, they are hard to simulate in real-time and have thus previously been rendered poorly compared to opaque surfaces in games. This paper introduces the first model for a real-time rasterization algorithm that can simultaneously approximate the following transparency phenomena: wavelength-varying (""colored"") transmission, translucent colored shadows, caustics, partial coverage, diffusion, and refraction. All render efficiently on modern GPUs by using order-independent draw calls and low bandwidth. We include source code for the transparency and resolve shaders."|ACM Symposium on Interactive 3D Graphics and Games|2016|10.1145/2856400.2856418|M. McGuire, Michael Mara|0.2222222222222222|2
435|GPU-powered Bat Algorithm for the parameter estimation of biochemical kinetic values|The emergent behavior of biochemical systems can be investigated by means of mathematical modeling and computational analyses, which usually require the automatic inference of the unknown values of the model's parameters. This problem, known as Parameter Estimation (PE), is usually tackled with bio-inspired meta-heuristics for global optimization, most notably Particle Swarm Optimization (PSO). In this work we assess the performances of PSO and Bat Algorithm with differential operator and Lévy flights trajectories (DLBA). In particular, we compared these meta-heuristics for the PE using two biochemical models: the expression of genes in prokaryotes and the heat shock response in eukaryotes. In our tests, we also evaluated the impact on PE of different strategies for the initial positioning of individuals within the search space. Our results show that DLBA achieves comparable results with respect to PSO, but it converges to better results when a uniform initialization is employed. Since every iteration of DLBA requires three fitness evaluations for each bat, the whole methodology is built around a GPU-powered biochemical simulator (cupSODA) which is able to parallelize the process. We show that the acceleration achieved with cupSODA strongly reduces the running time, with an empirical 61× speedup that has been obtained comparing a Nvidia GeForce Titan GTX with respect to a CPU Intel Core i7-4790K. Moreover, we show that DLBA always outperforms PSO with respect to the computational time required to execute the optimization process.|IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology|2016|10.1109/CIBCB.2016.7758103|P. Cazzaniga, A. Tangherloni, Marco S. Nobile|0.2222222222222222|2
569|pySeismicFMM: Python based Travel Time Calculation in Regular 2D and 3D Grids in Cartesian and Geographic Coordinates using Fast Marching Method|"Seismic wave travel time calculation is the most common numerical operation in seismology. The most efficient is travel time calculation in 1D velocity model – for given source, receiver depths and angular distance time is calculated within fraction of a second. Unfortunately, in most cases 1D is not enough to encounter differentiating local and regional structures. Whenever possible travel time through 3D velocity model has to be calculated. It can be achieved using ray calculation or time propagation in space. While single ray path calculation is quick it is complicated to find the ray path that connects source with the receiver. Time propagation in space using Fast Marching Method seems more efficient in most cases, especially when there are multiple receivers. In this presentation a Python module pySeismicFMM is presented – simple and very efficient tool for calculating travel time from sources to receivers. Calculation requires regular 2D or 3D velocity grid either in Cartesian or geographic coordinates. On desktop class computer calculation speed is 200k grid cells per second. Calculation has to be performed once for every source location and provides travel time to all receivers. pySeismicFMM is free and open source. Development of this tool is a part of authors PhD thesis. National Science Centre Poland provided financial support for this work via NCN grant DEC-2011/02/A/ST10/00284. This work is part of PhD thesis. Full source code of pySeismicFMM will be published later this year. pySeismicFMM Example: from SeismicFMM import SeismicFMM3D import numpy myFMM = SeismicFMM3D() myFMM.SetModelSize(631, 536, 6261) myFMM.SetGridSize(numpy.deg2rad(0.01), numpy.deg2rad(0.02), 10) myFMM.ReadVelocityModel(""MODELVPF.bin"", numpy.single) myFMM.ReadLatVector(""lat.bin"", numpy.double) myFMM.ReadLonVector(""lon.bin"", numpy.double) myFMM.ReadHVector(""H.bin"", numpy.double) myFMM.CreateCalculationVariables() myFMM.SetSource(192,538,244) myFMM.Do(numpy.sum(myFMM.model_velocity > 0)) myFMM.Save(""PythonTime.bin"") Traveltime in 3D model Traveltime in 1D iasp’91 model Traveltime defference 3D vs 1D Model and efficiency pySeismicFMM is designed to work efficiently on regular 2D and 3D grids and was prepared mainly to work with 3D P-wave velocity model of Poland (Grad et. al 2015). The model covers area of Poland from topography to 60 km deep. Total grid size is 631 x 536 x 6261 (2.117.570.376) cells. In 1.390.642.955 P-wave velocity is provided. To allow correct simulation 3D model has always to be of convex shape. Single simulation provides traveltime to all grid cells in the model for given source location. Source can be located in one or more grid cells with proper time. For example, an interface can be assumed as source for reflected wave. Single simulation requires about 20GB of operating memory and takes 2 hours to complete. By average about 200.000 grid cells are calculated every second on modern workstation. Unfortunately, due to algorithm construction, it is not possible to migrate to GPU or any other parallel solution. Comparison of vertical travel times through our 3D model and other models. (a) Vertical pass time through sedimentary cover in the 3D model and (b) time difference in relation to homogeneous 2-km-thick layer with a velocity of 3 km/s. (c) Vertical pass time through crust in the 3D model and (d) time difference to the iasp91 model. (e) Vertical pass time through the whole 3D 60-km-thick model and (f) time difference to 60-km-thick of the iasp91 model."||2016|10.1515/acgeo-2016-0091|M. Pólkowski, M. Wilde-Piórko|0.2222222222222222|2
1175|Uniform acquisition modelling across PET imaging systems: Unified scatter modelling|PET imaging is an important tool commonly used for studying disease by research consortia which implement multi-centre studies to improve the statistical power of findings. The UK government launched the Dementias Platform UK to facilitate one of the world's largest dementia population study involving national centres equipped with state-of-the-art PET/MR scanners from two major vendors. However, the difference in PET detector technology between the two scanners involved makes the standardisation of data acquisition and image reconstruction necessary. We propose a new approach to PET acquisition system modelling across different PET systems and technologies, focusing in particular on unified scatter estimation across TOF (time-of-flight) and non-TOF PET systems. The proposed scatter modelling is fully 3D and voxel based, as opposed to the popular line-of-response driven methods. This means that for each emitting voxel an independent 3D scatter estimate is found, inherently preserving the necessary information for TOF calculations as well as accounting for the large axial field of view. With adequate sampling of the input images, the non-TOF scatter estimate is identical to the summed TOF estimates across TOF bins, without an additional computational cost used for the TOF estimation. The model is implemented using the latest NVIDA GPU CUDA platform, allowing finer sampling of image space which is more essential for accurate TOF modelling. The high accuracy of the proposed scatter model is validated using Monte Carlo simulations. The model is deployed in our stand-alone image reconstruction pipeline for the Biograph mMR scanner, demonstrating accurate 3D scatter estimates resulting in uniform reconstruction for a high statistics phantom scan.|2016 IEEE Nuclear Science Symposium, Medical Imaging Conference and Room-Temperature Semiconductor Detector Workshop (NSS/MIC/RTSD)|2016|10.1109/NSSMIC.2016.8069584|S. Arridge, Matthias Joachim Ehrhardt, P. Markiewicz, S. Ourselin, B. Hutton, D. Atkinson|0.2222222222222222|2
1313|Breast Cancer Prediction by Logistic Regression with CUDA Parallel Programming Support|Objective: The present article shows the development and the simulation of a machine learning model created with logistic regression to predict breast cancer tumor. \nMethods: The software is developed under Linux Ubuntu, with Theano Framework. It uses Python programming language and Nvidia CUDA parallel GPU programming mechanism. It uses Nvidia CUDA programming approach to take advantage of multiple GPUs. \nResults: From the results we can say that the model is very efficient. We developed two versions. The first version gives, in 85% of cases, the right response while last and more optimized version is able to give 91% of good responses. They are significant values and the differences between the versions may open better scenario for the future. \nConclusion: The good responses of the method developed could be open better scenario for breast cancer disease to avoid, sometimes, invasive diagnostic analysis. Furthermore, with a different sample of study is possibile to improve the efficiency of the methods mixing some different input dataset. Create a web database to train the algorithm behind the model to create a sort of open data for consultations.||2016|10.4172/2572-4118.1000111|F. Amenta, Aless, R. Peretti|0.2222222222222222|2
1350|Lattice QCD simulations using the OpenACC platform|In this article we will explore the OpenACC platform for programming Graphics Processing Units (GPUs). The OpenACC platform offers a directive based programming model for GPUs which avoids the detailed data flow control and memory management necessary in a CUDA programming environment. In the OpenACC model, programs can be written in high level languages with OpenMP like directives. We present some examples of QCD simulation codes using OpenACC and discuss their performance on the Fermi and Kepler GPUs.||2016|10.1088/1742-6596/759/1/012070|P. Majumdar|0.2222222222222222|2
1369|CPU/GPU COMPUTING FOR AN IMPLICIT MULTI-BLOCK COMPRESSIBLE NAVIER-STOKES SOLVER ON HETEROGENEOUS PLATFORM|CPU/GPU computing allows scientists to tremendously accelerate their numerical codes. In this paper, we port and optimize a double precision alternating direction implicit (ADI) solver for three-dimensional compressible Navier-Stokes equations from our in-house Computational Fluid Dynamics (CFD) software on heterogeneous platform. First, we implement a full GPU version of the ADI solver to remove a lot of redundant data transfers between CPU and GPU, and then design two fine-grain schemes, namely “one-thread-one-point” and “one-thread-one-line”, to maximize the performance. Second, we present a dual-level parallelization scheme using the CPU/GPU collaborative model to exploit the computational resources of both multi-core CPUs and many-core GPUs within the heterogeneous platform. Finally, considering the fact that memory on a single node becomes inadequate when the simulation size grows, we present a tri-level hybrid programming pattern MPI-OpenMP-CUDA that merges fine-grain parallelism using OpenMP and CUDA threads with coarse-grain parallelism using MPI for inter-node communication. We also propose a strategy to overlap the computation with communication using the advanced features of CUDA and MPI programming. We obtain speedups of 6.0 for the ADI solver on one Tesla M2050 GPU in contrast to two Xeon X5670 CPUs. Scalability tests show that our implementation can offer significant performance improvement on heterogeneous platform.||2016|10.1142/S2010194516601630|Liang Deng, F. Wang, Qinggang Xu, Hanli Bai|0.2222222222222222|2
1440|Frontiers of Molecular Simulation in China|Molecular simulation has become an important and powerful method dealing with complex systems in the fields of chemistry, materials and bioscience. Chinese scholars have been important contributors to the field of molecular simulation. Their molecular simulation studies have been well received and have played an important role in developing novel materials for specific applications, such as gas storage and separation, catalysis and drug delivery in the past decade. The aim of this special issue on ‘Frontiers of Molecular Simulation in China’ is to provide a resource for the latest advances in the field of molecular simulation in China. Due to small scales and complex molecular structures, many properties of materials cannot be understood purely from experimental data. Accordingly, molecular simulation which is able to probe materials at the atomic scale is a useful tool in the fields of biology, materials science, chemistry, engineering and physics. The methods of molecular simulation include Monte Carlo (MC) and Molecular Dynamics (MD) as well as others, such as Brownian Dynamics (BD), Dissipative Particle Dynamics (DPD) and GPU-enabled MD. In the past decades, the study of molecular simulation in China has moved on from ‘simply’ simulating individual structures to simulating complex systems in different environments. More recently, a lot of Chinese scholars have focused on large-scale collaborative studies of novel proteins, biomolecules, membranes and tissues by multi-scale modelling and simulation. This special issue consists of a selection of eight papers reflecting the frontiers of molecular simulation in China as follows:||2016|10.1080/08927022.2016.1167443|D. Cheng|0.2222222222222222|2
1487|Towards real-time two-dimensional wave propagation for articulatory speech synthesis|The precise simulation of voice production is a challenging task, especially when real-time performances are sought. To fulfill real-time constraints, most articulatory vocal synthesizers have to rely on highly simplified acoustic and anatomical models, based on 1D wave propagation and on the usage of vocal tract area functions. In this work, we present a 2D propagation model, designed to simulate the air flow traveling through the midsagittal contour of the vocal tract. Building on the work by Allen et al. [Andrew Allen and Nikunj Raghuvanshi, “Aerophones in flatland: Interactive wave simulation of wind instruments,” ACM Trans. Graph. 34, Article 134 (2015)], we leverage OpenGL and GPU parallelism for a real-time precise 2D airwave simulation. The domain is divided into cells according to a Finite-Difference Time-Domain scheme and coupled with a self-oscillating two-mass vocal fold model. To investigate the system’s ability to simulate the physiology of the vocal tract and its aerodynamics, two studies are presented. First, we compare the performances in vowel production of our 2D approach with other 1D wave propagation systems in literature, using area functions. Subsequently, this case is extended by replacing area functions with 2D vocal tract contours derived from 3D MRI data.||2016|10.1121/1.4949912|S. Fels, A. Vasudevan, Victor Zappi|0.2222222222222222|2
287|Ebb: A DSL for Physical Simluation on CPUs and GPUs|Designing programming environments for physical simulation is challenging because simulations rely on diverse algorithms and geometric domains. These challenges are compounded when we try to run efficiently on heterogeneous parallel architectures. We present Ebb, a domain-specific language (DSL) for simulation, that runs efficiently on both CPUs and GPUs. Unlike previous DSLs, Ebb uses a three-layer architecture to separate (1) simulation code, (2) definition of data structures for geometric domains, and (3) runtimes supporting parallel architectures. Different geometric domains are implemented as libraries that use a common, unified, relational data model. By structuring the simulation framework in this way, programmers implementing simulations can focus on the physics and algorithms for each simulation without worrying about their implementation on parallel computers. Because the geometric domain libraries are all implemented using a common runtime based on relations, new geometric domains can be added as needed, without specifying the details of memory management, mapping to different parallel architectures, or having to expand the runtime's interface. We evaluate Ebb by comparing it to several widely used simulations, demonstrating comparable performance to hand-written GPU code where available, and surpassing existing CPU performance optimizations by up to 9$\times$ when no GPU code exists.|arXiv.org|2015|10.1007/978-3-319-24126-5_14|P. Hanrahan, Matthew Fisher, Chinmayee Shah, Zach DeVito, G. Bernstein, P. Levis, Crystal Lemire|0.2|2
340|Systems Biology Approaches to Mining High Throughput Biological Data|With advances in high throughput measurement techniques, large-scale biological data have been and will continuously be produced, for example, gene expression data, protein-protein interaction (PPI) data, tandem mass spectra data, microRNA expression data, lncRNA expression data, and biomolecule-disease association data. Such data contain insightful information for understanding the mechanism of molecular biological systems and have proved useful in diagnosis, treatment, and drug design for genetic disorders or complex diseases. For this focus issue, we have invited the researchers to contribute original research articles which develop or improve systems biology approaches to mining high throughput biological data. \n \nWith high throughput data, it is appealing to develop systems biology approaches to understand important biological processes. In the paper “Differential Expression Analysis in RNA-Seq by a Naive Bayes Classifier with Local Normalization,” Y. Dou et al. developed a new tool for the identification of differentially expressed genes with RNA-Seq data, named GExposer. This tool introduced a local normalization algorithm to reduce the bias of nonrandomly positioned read depth. The Naive Bayes classifier was employed to integrate fold change, transcript length, and GC-content to identify differentially expressed genes. Results on several independent tests showed that GExposer had better performance than other methods. In the paper “K-Profiles: A Nonlinear Clustering Method for Pattern Detection in High Dimensional Data,” K. Wang et al. designed the nonlinear K-profiles clustering method, which can be seen as the nonlinear counterpart of the K-means clustering algorithm. The method had a built-in statistical testing procedure that ensures genes not belonging to any cluster do not impact the estimation of cluster profiles. Results from extensive simulation studies showed that K-profiles clustering outperformed traditional linear K-means algorithm. In addition, K-profile clustering generated biologically meaningful results from a gene expression dataset. \n \nReplicative senescence is of fundamental importance for the process of cellular aging. In the paper “Similarities in Gene Expression Profiles during In Vitro Aging of Primary Human Embryonic Lung and Foreskin Fibroblasts,” S. Diekmann et al. elucidated cellular aging process by comparing gene expression changes, measured by RNA-Seq, in fibroblasts originating from two different tissues, embryonic lung (MRC-5) and foreskin (HFF), at five different time points during their transition into senescence. Their results showed that a number of monotonically up- and downregulated genes had a novel strong functional link to aging and senescence related processes. \n \nMore and more studies have shown that many complex diseases are contributed jointly by alterations of numerous genes. Genes often coordinate together as a functional biological pathway or network and are highly correlated. In the paper “Module Based Differential Coexpression Analysis Method for Type 2 Diabetes,” L. Yuan et al. proposed a gene differential coexpression analysis algorithm and applied it to a publicly available type 2 diabetes (T2D) expression dataset. Two differential coexpression gene modules about T2D were detected and were expected to be useful for exploring the biological functions of the related genes. \n \nOral mucosa is a useful material for regeneration therapy with the advantages of its accessibility and versatility regardless of age and gender. In the paper “Gene Signature of Human Oral Mucosa Fibroblasts: Comparison with Dermal Fibroblasts and Induced Pluripotent Stem Cells,” K. Miyoshi et al. reported the comparative profiles of the gene signatures of human oral mucosa fibroblasts (hOFs), human dermal fibroblasts (hDFs), and hOF-derived induced pluripotent stem cells (hOF-iPSCs), linking these with biological roles by functional annotation and pathway analyses. Their findings demonstrated that hOFs had unique cellular characteristics in specificity and plasticity. These data may provide useful insight into application of oral fibroblasts for direct reprograming. \n \nPredicting disease genes for a particular genetic disease is very challenging. However, this challenge can be tackled via exploring high throughput data. In the paper “ProSim: A Method for Prioritizing Disease Genes Based on Protein Proximity and Disease Similarity,” G. U. Ganegoda et al. proposed a new algorithm called proximity disease similarity algorithm (ProSim), which took use of two types of data: disease similarity data and protein-protein interaction data. The computational results have shown that their proposed method outperformed existing methods. \n \nIn order to learn the protein structures and functions via computational methods, it is important to predict the solvent accessibility and the contact number of protein residues from protein sequence. In the paper “AcconPred: Predicting Solvent Accessibility and Contact Number Simultaneously by a Multitask Learning Framework under the Conditional Neural Fields Model,” J. Ma and S. Wang presented a method AcconPred for predicting solvent accessibility and contact number simultaneously, which was based on a shared weight multitask learning framework under the CNF (Conditional Neural Fields) model. Trained on a 5729 monomeric soluble globular protein dataset, AcconPred could reach 0.68 three-state accuracy for solvent accessibility and 0.75 correlation for the contact number. Tested on the 105 CASP11 domain dataset for solvent accessibility, AcconPred could reach 0.64 accuracy, which outperformed existing methods. \n \nThe Smith-Waterman algorithm is one of the key sequence search algorithms for sequence alignment and has gained popularity due to improved implementations and rapidly increasing compute power. Recently, the Smith-Waterman algorithm has been successfully mapped onto the emerging general-purpose graphics processing units (GPUs). In the paper “Improving the Mapping of Smith-Waterman Sequence Database Searches onto CUDA-Enabled GPUs,” L.-T. Huang et al. employed the CUDA-enabled GPU to improve the mapping of Smith-Waterman algorithm, especially for short query sequences. The computational results showed that the proposed method significantly improved Smith-Waterman algorithm on CUDA-enabled GPUs in proper allocation of block and thread numbers. \n \nProtein interaction article classification is a text classification task in the biological domain to determine which articles describe protein-protein interactions. In the paper “Improving Classification of Protein Interaction Articles Using Context Similarity-Based Feature Selection,” Y. Chen et al. proposed new context similarity-based feature selection methods. Their performances were evaluated on two protein interaction article collections and compared against the frequency-based methods. The experimental results revealed that the context similarity-based methods performed better in terms of the F1 measure and the dimension reduction rate. \n \nRecent studies suggest that posttranscriptional RNA modifications play a crucial role in regulating gene expression. In practice, a single methylation site can contain multiple RNA methylation residuals, some of which can be regulated by different enzymes and thus differentially methylated between two conditions. However, existing peak-based methods could not effectively differentiate multiple methylation residuals located within a single methylation site. In the paper “Spatially Enhanced Differential RNA Methylation Analysis from Affinity-Based Sequencing Data with Hidden Markov Model,” Y.-C. Zhang et al. proposed a hidden Markov model (HMM) based approach to address this issue. The proposed algorithms were tested on both simulated data and real data. Results suggested that their proposed algorithm clearly outperformed existing peak-based approach on simulated systems and could detect differential methylation regions with higher statistical significance on real data, indicating an improved performance. \n \nPregnane X Receptor (PXR) and drug-metabolizing target genes are involved in most of inductive herb-drug interactions. To predict this kind of herb-drug interactions, the protocol could be simplified to only screen agonists of PXR from herbs because the relations of drugs with their metabolizing enzymes are well studied. In the paper “Screening Ingredients from Herbs against Pregnane X Receptor in the Study of Inductive Herb-Drug Interactions: Combining Pharmacophore and Docking-Based Rank Aggregation,” Z. Cui et al. employed a combinational in silico strategy of pharmacophore modelling and docking-based rank aggregation (DRA) to identify PXR's agonists. To validate their method, a curated herb-drug interaction database was built, which recorded 380 herb-drug interactions. The results showed that, among the top 10 herb ingredients from the ranking list, 6 ingredients were reported to involve herb-drug interactions. \n \nIn summary, this focus issue has reported the recent progress in systems biology approaches to analyzing high throughput data such as gene expression data, various biomolecular interaction data, and sequencing data. We hope that the readers of this focus issue could get some benefits from these newly developed methods. \n \n \nFang-Xiang Wu \n \nMin Li \n \nJishou Ruan \n \nFeng Luo|BioMed Research International|2015|10.1155/2015/504362|J. Ruan, F. Luo, Min Li, Fang-Xiang Wu|0.2|2
377|GPU-based inverse rendering with multi-objective particle swarm optimization|"We present a novel, GPU-accelerated per-pixel inverse rendering (IR) optimization algorithm based on Particle Swarm Optimization (PSO), IRPSO. IRPSO estimates the per-pixel scene attributes including reflectance properties of a 3D model, and is fast enough to do in situ visualization of the optimization in real-time. We utilize the GPU framebuffer as a computational domain, where each pixel is treated as an independent computational ""swarm"". We propose a parallel framework that recovers the reflectance at each pixel location in a massively parallel fashion. The algorithm's high parallel efficiency is demonstrated through our GPU/GLSL shader implementation of the method. IRPSO is validated experimentally on examples including simulated ground truth images."|SIGGRAPH Asia Visualization in High Performance Computing|2015|10.1145/2818517.2818523|Chi-An Chen, T. Collins, Koki Nagano, A. Nakano|0.2|2
387|A progressive mesh method for physical simulations using lattice Boltzmann method on single-node multi-gpu architectures|In this paper, a new progressive mesh algorithm is introduced in order to perform fast physical simulations by the use of a lattice Boltzmann method (LBM) on a single-node multi-GPU architecture. This algorithm is able to mesh automatically the simulation domain according to the propagation of fluids. This method can also be useful in order to perform various types of simulations on complex geometries. The use of this algorithm combined with the massive parallelism of GPUs allows to obtain very good performance in comparison with the static mesh method used in literature. Several simulations are shown in order to evaluate the algorithm.|arXiv.org|2015|10.5121/ijdps.2015.6501|J. Duchateau, C. Renaud, G. Roussel, N. Maquignon, F. Rousselle|0.2|2
416|Performance examinations of multiple time-stepping algorithms on stampede supercomputer|Our examinations of the methodical implementation of the multiple time-stepping algorithms on the Stampede supercomputer reveal a speedup factor of 23 over single time-stepping algorithm, for the same problem, with the combined algorithmic and hardware accelerations. More specifically, the MTS algorithm is 11.5 times faster than the STS algorithm; the GPU-enabled system performs 2 times faster than the CPU-only system. Combining these speedups and using MTS algorithm on the Stampede with 16 GPU nodes, we can simulate 1- ms multiscale phenomena of flowing platelets in blood vessels within approximate 37 days, enabling practical modeling of millisecond-scale biological phenomena with spatial resolutions at the nanoscales. The mathematical algorithms and the advances of computer hardware, such as the Stampede supercomputer, that can be leveraged allow us to explore the new frontiers of cutting-edge applications in medical and life sciences.|Extreme Science and Engineering Discovery Environment|2015|10.1145/2792745.2792753|Yuefan Deng, Li Zhang, Xiao Zhu, Lei Huang, Peng Zhang, N. Zhang|0.2|2
443|Accelerating Physical Simulations from a Multicomponent Lattice Boltzmann Method on a Single-Node Multi-GPU Architecture|In this paper, we introduce an efficient method to accelerate flow simulations for an isothermal multiphase and multicomponent (MPMC) Lattice Boltzmann method (LBM) on a single-node multi-GPU architecture. Our objective is to propose an efficient way to improve performance of multiphase and multicomponent Lattice Boltzmann simulations by the use of Nvidia GPUDirect technology and Peer-to-Peer (P2P) data transfers. Optimization of Peer-to-Peer communications is also studied in this work by the use of a clustering algorithm. Several simulations are shown and performance is discussed in order to validate the method.|International Conference on P2P, Parallel, Grid, Cloud and Internet Computing|2015|10.1109/3PGCIC.2015.41|J. Duchateau, C. Renaud, G. Roussel, N. Maquignon, F. Rousselle|0.2|2
452|Big data in drug design|Big data collection in the pharmaceutical research industry has four sources, high-throughput scientific experiments, high-performance computations, automated information acquisition and office automation, and scientific publications and patents. Big data is the product and the promoter of high-performance scientific experiments, therefore the technology for mining big data is the key to future drug discovery. However, big data brings greater challenges, such as, storage, retrieval, curation and quality assurance, sharing/transfer, analysis, visualization, modeling and computing complexities. This review outlines the current progress of processing big data in the drug design field. These problems may be resolved by adopting cloud computing and high-performance computing technologies, and parallelizing existing chemoinformatics and bioinformatics programs. Machine-learning approaches involving Bayesian learning methods and other methods, such as support vector machine and recursive petitioning, can be used for big data mining. Recent progress includes parallelized and GPU-accelerated molecular dynamics simulation technology, enhanced molecular docking technology, new parallelized algorithms for shape-based virtual screening, free-energy landscape calculations, and machine-learning algorithms for big chemical structural data mining. Big data from drug discoveries will increase, so conventional drug design software and methods need to be upgraded. This is a long-term project and we highlight the tasks that need to be accomplished to meet this goal.||2015|10.1360/N972014-01144|Xin Yan, Chenzhong Liao, Ling Wang, P. Ding, Zhihong Liu, Qiong Gu, Jun Xu|0.2|2
559|PMLES: A Hybrid Open MP CUDA Source Code for LES of Turbulent Flows|The occurrence of turbulent flows is quite common in nature and several industrial applications. The accurate simulation of these complex flows is still a great challenge in science. Large Eddy Simulation (LES) is an efficient technique based on the elimination of all scales of a flow smaller than a characteristic length ∆, considering that the flow pattern in small scales is homogeneous and isotropic. Therefore, modeling of turbulence in such scales is universal and independent of the flow type. This work present PMLES, a new OpenMP CUDA Fortran solver for complex turbulent flows at high Reynolds numbers and large computational domains (about 1 × 108 cells), using a single GPU card. This was possible by using an economical numerical scheme associated with a robust and efficient solution method that requires little variable storage. Theoretical and numerical aspects are firstly discussed, and then details of the computational implementation are given. Finally, the developed code is tested and validated by simulating a turbulent jet, and comparing the results with experimental and computational data from the literature. An analysis of performance gain is also carried out, demonstrating the code’s ability to solve this class of problems with a considerable reduction in computational time.||2020|10.36884/jafm.13.04.30698|Porto Alegre Rs Brazil no, J. Pinho, A. Muniz|0.2|2
585|Monte Carlo modeling photon-tissue interaction using on-demand cloud infrastructure|Purpose: This work advances a Monte Carlo (MC) method to combine ionizing radiation physics with optical physics, in a manner which was implicitly designed for deployment with the most widely accessible parallelization and portability possible. Methods: The current work updates a previously developed optical propagation plugin for GEANT4 architecture for medically oriented simulations (GAMOS). Both virtual-machine (VM) and container based instances were validated using previously published scripts, and improvements in execution time using parallel simulations are demonstrated. A method to programmatically deploy multiple containers to achieve parallel execution using an on-demand cloud-based infrastructure is presented. Results: A container-based GAMOS deployment is demonstrated using a multi-layer tissue model and both optical and X-ray source inputs. As an example, the model was split into 154 simulations which were run simultaneously on 64 separate containers across 4 servers. Conclusions: The container-based model provides the ability to execute parallel simulations of applications which are not inherently thread-safe or GPU-optimized. In the current demonstration, this reduced the time by at most 97% compared to sequential execution. The code and examples are available through an interactive online interface through links at: this https URL|arXiv.org|2020|10.1117/1.jbo.25.11.116004|E. LaRochelle, P. Arce, B. Pogue|0.2|2
592|Depth-of-Field Rendering Using Progressive Lens Sampling in Direct Volume Rendering|Direct volume rendering is a widely used technique for extracting information from three-dimensional scalar fields acquired by measurement or numerical simulation. However, the translucency of direct volume rendering to express the internal structure of the volume often makes it difficult to recognize the depth of complex structures. In this paper, we propose a new method for applying depth-of-field effects to volume ray-casting to improve the depth perception. A thin lens camera model is used to simulate rays passing through different parts of lens. The proposed method is implemented in the GPU pipeline with no preprocessing, so any acceleration techniques of volume ray-casting can be applied without restrictions. We also propose a multi-pass rendering framework using progressive lens sampling. This new technique uses a different number of lens samples per pixel, depending on the size of the circle of confusion at the point where each ray intersects the volume data. In the experiments with various data, we demonstrated that higher quality images with better depth perception were generated up to 9x faster than the existing depth-of-field method in direct volume rendering.|IEEE Access|2020|10.1109/ACCESS.2020.2994378|Bohyoung Kim, Jeongjin Lee, Ji-Min Kang, Y. Shin|0.2|2
593|Pedal to the Bare Metal: Road Traffic Simulation on FPGAs Using High-Level Synthesis|The performance of Agent-based Traffic Simulations (ABTS) has been shown to benefit tremendously from offloading to accelerators such as GPUs. In the search for the most suitable hardware platform, reconfigurable hardware is a natural choice. Some recent work considered ABTS on Field-Programmable Gate Arrays (FPGAs), yet only implemented simplified cellular automaton-based models. The recent introduction of support for high-level synthesis from C, C++, and OpenCL in FPGA tool chains allows FPGA designs to be expressed in a form familiar to software developers. However, the performance achievable with this approach in a simulation context is not well-understood. In this work, to the best of our knowledge, we present the first FPGA-accelerated ABTS based on widely-accepted microscopic traffic simulation models, and the first to be generated from high-level code. The achieved speedup of up to 24.3 over a sequential CPU-based execution indicates that recent FPGA toolchains allow simulationists to unlock the performance benefits of reconfigurable hardware without the need to express the simulation models in low-level hardware description languages.|SIGSIM Principles of Advanced Discrete Simulation|2020|10.1145/3384441.3395979|A. Knoll, D. Eckhoff, Philipp Andelfinger, G. Kilinç, Jiajian Xiao, Wentong Cai|0.2|2
594|Modeling realistic virtual objects within a high-throughput x-ray simulation framework|X-ray simulation of realistic object models is relevant across all areas in which X-ray systems are employed, including medical, industrial, and security applications. A particularly exciting area of impact stems from the development of machine learning approaches to classification, detection, and data processing. The continued development of these techniques requires large labeled datasets. Traditionally, this data needed to be collected with physical machines, creating steep logistical challenges. Moreover, the testing and evaluation of such X-ray scanners present their own challenges, as machines need to be shipped to a site capable of handling certain anomalies. To help alleviate these burdens, virtual models and simulations can be used in lieu of empirical measurements. The confluence of powerful computers and advanced data processing techniques presents an opportunity to develop tools to aid in dataset creation as well as system analysis. We present efforts toward the maturity of such tools. Building on previous work to validate the performance of simulation software, we show how modeling realistic virtual objects can produce data representative of real-world measurements. Furthermore, we present the efficiency of such an approach that leverages advances in computer graphics, ray-tracing utilities, and GPU hardware.|Defense + Commercial Sensing|2020|10.1117/12.2558947|M. Gehm, J. Greenberg, David Coccarelli|0.2|2
598|GPU-accelerated Calculation of Acoustic Echo Characteristics of Underwater Targets|In this paper, we study the echo characteristics of submerged targets at far field. Based on the thought of shooting and bouncing ray (SBR), which uses geometrical optics (GO) principle and physical optics (PO) integration in combination, we propose a more efficient GPU-accelerated prediction method based on multiresolution grid algorithm in the SBR to calculate the target strength (TS) of concave complex targets. According to the simulation and experiment results on rigid sphere, cylinder and submarine model, the accuracy of the proposed approach is proved. Furthermore, the GPU-based SBR is more than 750x faster than CPU via GPU parallel computing architecture. And the proposed GPU-accelerated multiresolution SBR improves runtime performance at least 2. 4x over GPU-based SBR.|International Symposium on Parameterized and Exact Computation|2020|10.1109/IPEC49694.2020.9115122|Keyan Wang, Yunsong Li, Xianyun Wu, Lixue Su|0.2|2
624|Ten years after (no, not the band): advancements in optical engineering computations over the decade(s)|Global optimization of imaging lenses, non-sequential ray tracing for illumination (or stray light) analyses, FDTD modeling of 3D photonic devices, optical field calculations by the Gaussian beamlet method, and image simulations using very large 2D FFTs can be some of the most computer intensive tasks in optical engineering. An example of the first on reasonably–priced Intel/Microsoft platforms is used to test whether relevant single-core computational speeds have kept up with predictions, from the early 1990’s 32-bit 486s running extended DOS to today’s 64-bit CPUs running Windows 10. It is found that before around 2005, performance doubled every 18 months (equivalent to 100 times every decade) as predicted by House’s variant of Moore’s 1975 law. At that time thermal issues lead to a more efficient microarchitecture and a relative stagnation of CPU frequencies so progress slowed significantly, but could be mostly reclaimed by the effective utilization of a large number of computational cores. Therefore, previously published results comparing multi-core performance on the other tasks will be updated using nearly a decade newer CPUs and GPUs. Even though they have much higher clock rates and core counts than the old hardware, their relative performances fall short (and sometimes far short) of expectations.|Optical Engineering + Applications|2020|10.1117/12.2564723|A. W. Greynolds|0.2|2
632|Scalable Graph Networks for Particle Simulations|Learning system dynamics directly from observations is a promising direction in machine learning due to its potential to significantly enhance our ability to understand physical systems. However, the dynamics of many real-world systems are challenging to learn due to the presence of nonlinear potentials and a number of interactions that scales quadratically with the number of particles N, as in the case of the N-body problem. In this work we introduce an approach that transforms a fully-connected interaction graph into a hierarchical one which reduces the number of edges to O(N). This results in a linear time and space complexity while the pre-computation of the hierarchical graph requires O(N log (N)) time and O(N) space. Using our approach, we are able to train models on much larger particle counts, even on a single GPU. We evaluate how the phase space position accuracy and energy conservation depend on the number of simulated particles. Our approach retains high accuracy and efficiency even on large-scale gravitational N-body simulations which are impossible to run on a single machine if a fully-connected graph is used. Similar results are also observed when simulating Coulomb interactions. Furthermore, we make several important observations regarding the performance of this new hierarchical model, including: i) its accuracy tends to improve with the number of particles in the simulation and ii) its generalisation to unseen particle counts is also much better than for models that use all O(N^2) interactions.|AAAI Conference on Artificial Intelligence|2020|10.1609/aaai.v35i10.17078|Nathanael Perraudin, Aurélien Lucchi, Karolis Martinkus|0.2|2
646|Tensegrity representation of microtubule objects using unified particle objects and springs|There are limitations in interactions with molecular objects in laboratory experiments due to the very small size of the objects. Common media to show the experimental results of molecular objects is still lack of observer interaction to understand it intuitively. In order to overcome this lack of interaction, this research takes tensegrity representation of molecular objects reproducing experimental results and creates interactive 3D objects to be presented in a virtual reality (VR) environment. The tensegrity representation enables us to enhance the interaction experience with the natural user interface with haptic technology and hand tracking controller. A particle simulation system that utilizes multiple GPUs resources is used to fulfill haptic VR requirements. We developed a unified particle object model using springs and particles which we call anchors which act as tensegrity structure of the object to support conformation of filament-type objects such as microtubules. Some object parameters can be set to match the flexural rigidity of the object with some experimental results. The bending shape of the object is evaluated using the classic bending equation and the results show high compatibility. Viscoelastic behavior also shows similarities with the viscosity reported in other studies. The object's flexural rigidity can be adjusted to match the target value with the direction of the prediction equation. The object model provides a better insight about molecular objects with natural and real-time interactions to provide a more intuitive understanding with the molecular objects presented. The results show that this model can also be applied to any filament-type or rod-like molecular object. Chem-Bio Informatics Journal, Vol.20, pp.19–43 (2020) 20|Chem-Bio Informatics Journal|2020|10.1273/cbij.20.19|A. Konagaya, M. Yamamura, Arif Pramudwiatmoko, A. Kakugo, Y. Ueno, G. Gutmann|0.2|2
648|Accurate Simulation of Neutrons in Less Than One Minute Pt. 2: Sandman—GPU-Accelerated Adjoint Monte-Carlo Sampled Acceptance Diagrams|A computational method in the modelling of neutron beams is described that blends neutron acceptance diagrams, GPU-based Monte-Carlo sampling, and a Bayesian approach to efficiency. The resulting code reaches orders of magnitude improvement in performance relative to existing methods. For example, data rates similar to world-leading, real instruments can be achieved on a 2017 laptop, generating 10 6 neutrons per second at the sample position of a high-resolution small angle scattering instrument. The method is benchmarked, and is shown to be in agreement with previous work. Finally, the method is demonstrated on a mature instrument design, where a sub-second turnaround in an interactive simulation process allows the rapid exploration of a wide range of options. This results in a doubling of the design performance, at the same time as reducing the hardware cost by 40%.|Quantum Beam Science|2020|10.3390/qubs4020024|P. Bentley|0.2|2
650|Accelerated hydrologic modeling: ParFlow GPU implementation|ParFlow is known as a numerical model that simulates the hydrologic cycle from the bedrock to the top of the plant canopy. The original codebase provides an embedded Domain-Specific Language (eDSL) for generic numerical implementations with support for supercomputer environments (distributed memory parallelism), on top of which the hydrologic numerical core has been built. In ParFlow, the newly developed optional GPU acceleration is built directly into the eDSL headers such that, ideally, parallelizing all loops in a single source file requires only a new header file. This is possible because the eDSL API is used for looping, allocating memory, and accessing data structures. The decision to embed GPU acceleration directly into the eDSL layer resulted in a highly productive and minimally invasive implementation. This eDSL implementation is based on C host language and the support for GPU acceleration is based on CUDA C++. CUDA C++ has been under intense development during the past years, and features such as Unified Memory and host-device lambdas were extensively leveraged in the ParFlow implementation in order to maximize productivity. Efficient intraand inter-node data transfer between GPUs rests on a CUDA-aware MPI library and application side GPU-based data packing routines. The current, moderately optimized ParFlow GPU version runs a representative model up to 20 times faster on a node with 2 Intel Skylake processors and 4 NVIDIA V100 GPUs compared to the original version of ParFlow, where the GPUs are not used. The eDSL approach and ParFlow GPU implementation may serve as a blueprint to tackle the challenges of heterogeneous HPC hardware architectures on the path to exascale.||2020|10.5194/egusphere-egu2020-12904|S. Kollet, Jaro Hokkanen, A. Herten, D. Pleiter, J. Kraus|0.2|2
652|Disruption Management for Dial-A-Ride Systems|Mobility on demand has been gaining more attention from the research community as a way to offer smart and efficient transportation services to people. Despite the advancements in vehicular technologies, vehicle breakdown (VB) remains one of the major contributors to the disruption of fleet operations, which may inflict large recovery costs and damage the service provider?s reputation. However, modeling such dynamic events for dial-a-ride (DAR) systems has not been addressed until now; this is the research gap we attempt to address in this article. The following are the contributions of our work: 1) the formulation of a disruptive DAR problem (DDARP) with VB (DDARP-VB), 2) a GPU-based adaptive large neighborhood search (G-ALNS) algorithm to solve the DDARP-VB, and 3) a fleet size minimization (FSM) strategy that leads to reduced operational costs under disruptions. From our simulations, the proposed G-ALNS algorithm performs up to 52 times faster than its CPU counterpart and produces better solutions when compared to the existing GPU approach for DARPs. Overall, our FSM strategy leads to a 59% reduction in fleet idle time under normal operations and a 15% reduction in operational cost under disruption.|IEEE Intelligent Transportation Systems Magazine|2020|10.1109/MITS.2020.3014429|J. Dauwels, Twinkle Tripathy, Ramesh Ramasamy Pandi, Songguang Ho, Sarat Chandra Nagavarapu|0.2|2
661|GPU Acceleration for Computational Finance|Recent progress of graphics processing unit (GPU) computing with applications in science and technology has demonstrated tremendous impact over the last decade. However, financial applications by GPU computing are less discussed and may cause an obstacle toward the development of financial technology, an emerging and disruptive field focusing on the efficiency improvement of our current financial system. This chapter aims to raise the attention of GPU computing in finance by first empirically investigating the performance of three basic computational methods including solving a linear system, Fast Fourier transform, and Monte Carlo simulation. Then a fast calibration of the wing model to implied volatilities is explored with a set of traded futures and option data in high frequency. At least 60% executing time reduction on this calibration is obtained under the Matlab computational environment. This finding enables the disclosure of an instant market change so that a real-time surveillance for financial markets can be established for either trading or risk management purposes.||2020|10.1142/9789811202391_0039|Chuan-Hsiang Han|0.2|2
666|Scalable Deep-Learning-Accelerated Topology Optimization for Additively Manufactured Materials|Topology optimization (TO) is a popular and powerful computational approach for designing novel structures, materials, and devices. Two computational challenges have limited the applicability of TO to a variety of industrial applications. First, a TO problem often involves a large number of design variables to guarantee sufficient expressive power. Second, many TO problems require a large number of expensive physical model simulations, and those simulations cannot be parallelized. To address these issues, we propose a general scalable deep-learning (DL) based TO framework, referred to as SDL-TO, which utilizes parallel schemes in high performance computing (HPC) to accelerate the TO process for designing additively manufactured (AM) materials. Unlike the existing studies of DL for TO, our framework accelerates TO by learning the iterative history data and simultaneously training on the mapping between the given design and its gradient. The surrogate gradient is learned by utilizing parallel computing on multiple CPUs incorporated with a distributed DL training on multiple GPUs. The learned TO gradient enables a fast online update scheme instead of an expensive update based on the physical simulator or solver. Using a local sampling strategy, we achieve to reduce the intrinsic high dimensionality of the design space and improve the training accuracy and the scalability of the SDL-TO framework. The method is demonstrated by benchmark examples and AM materials design for heat conduction. The proposed SDL-TO framework shows competitive performance compared to the baseline methods but significantly reduces the computational cost by a speed up of around 8.6x over the standard TO implementation.|arXiv.org|2020|10.2172/1673390|Jiaxin Zhang, Guannan Zhang, Sirui Bi|0.2|2
685|High Performance Scientific Computing in Applications with Direct Finite Element Simulation|To predict separated flow including stall of a full aircraft with Computational Fluid Dynamics (CFD) is considered one of the problems of the grand challenges to be solved by 2030, according to NASA [1]. The nonlinear NavierStokes equations provide the mathematical formulation for fluid flow in 3dimensional spaces. However, classical solutions, existence, and uniqueness are still missing. Since brute-force computation is intractable, to perform predictive simulation for a full aircraft, one can use Direct Numerical Simulation (DNS); however, it is prohibitively expensive as it needs to resolve the turbulent scales of order Re 9 4 . Considering other methods such as statistical average Reynolds’s Average Navier Stokes (RANS), spatial average Large Eddy Simulation (LES), and hybrid Detached Eddy Simulation (DES), which require less number of degrees of freedom. All of these methods have to be tuned to benchmark problems, and moreover, near the walls, the mesh has to be very fine to resolve boundary layers (which means the computational cost is very expensive). Above all, the results are sensitive to, e.g. explicit parameters in the method, the mesh, etc. As a resolution to the challenge, here we present the adaptive timeresolved Direct FEM Solution (DFS) methodology with numerical tripping, as a predictive, parameter-free family of methods for turbulent flow. We solved the JAXA Standard Model (JSM) aircraft model at realistic Reynolds number, presented as part of the High Lift Prediction Workshop 3. We predicted lift Cl within 5% error vs. experiment, drag Cd within 10% error and stall 1◦ within the angle of attack. The workshop identified a likely experimental error of order 10% for the drag results. The simulation is 10 times faster and cheaper when compared to traditional or existing CFD approaches. The efficiency mainly comes from the slip boundary condition that allows coarse meshes near walls, goal-oriented adaptive error control that refines the mesh only where needed and large time steps using a Schur-type fixed-point iteration method, without compromising the accuracy of the simulation results. As a follow-up, we were invited to the Fifth High Order CFD Workshop, where the approach was validated for a tandem sphere problem (low Reynolds number turbulent flow) wherein a second sphere is placed a certain distance downstream from a first sphere. The results capture the expected slipstream phenomenon, with appx. 2% error. A comparison with the higher-order frameworks Nek500 and PyFR was done. The PyFR framework has demonstrated high effectiveness for GPUs with an unstructured mesh, which is a hard problem in this field. This is achieved by an explicit time-stepping approach. Our study showed that our large time step approach enabled appx. 3 orders of magnitude larger time steps than the explicit time steps in PyFR, which made our method more effective for solving the whole problem. We also presented a generalization of DFS to variable density and validated against the well-established MARIN benchmark problem. The results show good agreement with experimental results in the form of pressure sensors. Later, we used this methodology to solve two applications in multiphase flow problems. One has to do with a flash rainwater storage tank (Bilbao water consortium), and the second is about designing a nozzle for 3D printing.||2020|10.1111/coin.12381|E. Krishnasamy|0.2|2
686|Wrapping Up Viruses at Multiscale Resolution: Optimizing PACKMOL and SIRAH Execution for Simulating the Zika Virus|Simulating huge biomolecular complexes of million atoms at relevant biological time scales is becoming accessible to the broad scientific community. That proves to be crucial for urgent responses against emergent diseases in real time. Yet, there are still issues to sort regarding the system setup so that molecular dynamics (MD) simulations can be run in a simple and standard way. Here, we introduce an optimized pipeline for building and simulating enveloped virus-like particles (VLP). First, the membrane packing problem is tackled with new features and optimized options in PACKMOL. This allows preparing accurate membrane models of thousands of lipids in the context of a VLP within a few hours using a single CPU. Then, the assembly of the VLP system is done within the multiscale framework of the coarse-grained SIRAH force field. Finally, the equilibration protocol provides a system ready for production MD simulations within a few days on broadly accessible GPU resources. The pipeline is applied to study the Zika virus as a test case for large biomolecular systems. The VLP stabilizes at approximately 0.5 μs of MD simulation, reproducing correlations greater than 0.90 against experimental density maps from cryo-electron microscopy. Detailed structural analysis of the protein envelope also shows very good agreement in root-mean-square deviations and B-factors with the experimental data. The level of details attained shows for the first time a possible role for anionic phospholipids in stabilizing the envelope. Combining an efficient and reliable setup procedure with an accurate coarse-grained force field provides a valuable pipeline for simulating arbitrary viral systems or subcellular compartments, paving the way toward whole-cell simulations.|Journal of Chemical Information and Modeling|2020|10.26434/chemrxiv.13093454|Martín Sóñora, L. Martínez, S. Pantano, M. Machado|0.2|2
730|On the Development of a Synchronized Harmonic Balance Method for Multiple Frequencies and its Application to LPT Flows|\n The aim of this paper is to describe the development and application of a multi-frequency harmonic balance solver for GPUs, particularly suitable for the simulation of periodic unsteadiness in nonlinear turbomachinery flows comprised of a few dominant frequencies, with an unsteady multistage coupling that bolsters the flow continuity across the rotor/stator interface.\n The formulation is addressed with the time-domain reinterpretation, where several non-equidistant time instants conveniently selected are solved simultaneously. The set of required frequencies in each row is driven into the governing equations with the help of almost-periodic Fourier transforms for time derivatives and time shifted boundary conditions. The spatial repetitiveness inside each row can be exploited to perform single-passage simulations and the relative circumferential positioning of the rotors or stators and the different blade or vane counts is tackled by means of adding fictitious frequencies referring to non-adjacent rows therefore taking into account clocking and indexing effects.\n Existing multistage row coupling techniques of harmonic methods rely on the use of non-reflecting boundary conditions, based on linearizations, or time interpolation, which may lead to Runge phenomenon with the resulting numerical instabilities and non-preserving flux exchange. Different sets of time instants might be selected in each row but the interpolation in space and time across their interfaces gives rise to robustness issues due to this phenomenon. The so-called synchronized approach, developed in this work, consist of having the same time instances among the whole ensemble of rows, ensuring that flux transfer at sliding planes is applied more robustly. The combination of a set of shared non-equidistant time instances plus the use of unequal frequencies (real and fictitious) may spoil the Fourier transforms conditioning but this can be dramatically improved with the help of oversampling and instants selection optimization. The resulting multistage coupling naturally addresses typical numerical issues such as flow that might reverse locally across the row interfaces by means of not using boundary conditions but a local flux conservation scheme in the sliding planes.\n Some examples will be given to illustrate the ability of this new approach to preserve accuracy and robustness while resolving them. A brief analysis of results for a fan stage and a LPT multi-row case is presented to demonstrate the correctness of the method, assessing the impact in the modeling accuracy of the present approach compared with a time-domain conventional analysis. Regarding the computational performance, the speedup compared to a full annulus time-domain unsteady simulation is a factor of order 30 combining the use of single-passage rows and time spectral accuracy.|Turbomachinery|2020|10.1115/GT2020-14952|J. Crespo, Jesús Contreras|0.2|2
1160|Efficient localization and spectral estimation of an unknown number of ocean acoustic sources using a graphics processing unit.|This paper develops a matched-field approach to localization and spectral estimation of an unknown number of ocean acoustic sources employing massively parallel implementation on a graphics processing unit (GPU) for real-time efficiency. A Bayesian formulation is developed in which the locations and complex spectra of multiple sources and noise variances are considered unknown random variables, and the Bayesian information criterion is minimized to estimate these parameters, as well as the number of sources present. Optimization is carried out using simulated annealing and includes steps that attempt to add/delete sources to/from the model. Closed-form maximum-likelihood (ML) solutions for source spectra and noise variances in terms of the source locations allow these parameters to be sampled implicitly, substantially reducing the dimensionality of the inversion. Source sampling, addition, and deletion are based on joint conditional probability distributions for source range and depth, which incorporate the ML spectral estimates. Computing these conditionals requires solving a very large number of systems of equations, which is carried out in parallel on a GPU, improving efficiency by 2 orders of magnitude. Simulated examples illustrate localizations and spectral estimation for a large number of sources (up to eight), and investigate mitigation of environmental mismatch via efficient multiple-frequency inversion.|Journal of the Acoustical Society of America|2015|10.1121/1.4934517|M. Wilmut, J. Dettmer, S. Dosso|0.2|2
1306|Fluid Simulation and Generating Textures with Reaction-Diffusion Systems on Surfaces in the GPU|In recent years, many researchers have used the Navier-Stokes equations and Reaction-Diffusion systems for fluid simulation and for the creation of textures on surfaces, respectively. For this purpose it is necessary to obtain information about operators defined on surfaces. We obtained the metric information of the distortion caused by the parametrization of Catmull-Clark subdivision surfaces. Then the Navier-Stokes equations and the systems of Reaction-Diffusion on surfaces are solved in the domain of parametrization of each surface patch. The solution can be computationally expensive, but this process can be done in parallel for each point in the discretization of the surface, so a GPU implementation can heavily speed up the computation. CR Categories: I.3.3 [Computer Graphics]: Picture/Image Generation—; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Color, shading, shadowing, and texture; I.6.5 [Simulation and Modeling]: Model Development—; I.6.8 [Simulation and Modeling]: Types of Simulation—Parallel;||2015|10.1002/spe.2340|L. Velho, Leonardo Carvalho, Maria Andrade|0.2|2
1318|SIMD and GPU-Accelerated Rendering of Implicit Models|Implicit models inherently support automatic blending and trivial collision detection which makes them an effective tool for designing complex organic shapes with many applications in various areas of research including surgical simulation systems. However, slow rendering speeds can adversely affect the performance of simulation and modelling systems. In addition, when the models are incorporated in a surgical simulation system, interactive and smooth cutting becomes a required feature for many procedures. In this book, we build a comprehensive framework for high-performance rendering and physically-based animation of tissues modelled using implicit surfaces. Our goal is to address performance and scalability issues that arise in rendering complex implicit models as well as in dynamic interactions between surgical tool and models.||2015|10.1109/mise.2015.10|Pourya Shirazian|0.2|2
1323|High performance ultrasonic field simulation on complex geometries|Ultrasonic field simulation is a key ingredient for the design of new testing methods as well as a crucial step for NDT inspection simulation. As presented in a previous paper [1], CEA-LIST has worked on the acceleration of these simulations focusing on simple geometries (planar interfaces, isotropic materials). In this context, significant accelerations were achieved on multicore processors and GPUs (Graphics Processing Units), bringing the execution time of realistic computations in the 0.1 s range.In this paper, we present recent works that aim at similar performances on a wider range of configurations. We adapted the physical model used by the CIVA platform to design and implement a new algorithm providing a fast ultrasonic field simulation that yields nearly interactive results for complex cases. The improvements over the CIVA pencil-tracing method include adaptive strategies for pencil subdivisions to achieve a good refinement of the sensor geometry while keeping a reasonable number of ray-tracing o...||2015|10.1063/1.4940501|S. Chatillon, Jean-Philippe Farrugia, Hamza Chouh, J. Iehl, G. Rougeron, V. Ostromoukhov|0.2|2
1445|IMPROVEMENT OF 3D MONTE CARLO LOCALIZATION USING A DEPTH CAMERA AND TERRESTRIAL LASER SCANNER|Effective and accurate localization method in three-dimensional indoor environments is a key requirement for indoor navigation and lifelong robotic assistance. So far, Monte Carlo Localization (MCL) has given one of the promising solutions for the indoor localization methods. Previous work of MCL has been mostly limited to 2D motion estimation in a planar map, and a few 3D MCL approaches have been recently proposed. However, their localization accuracy and efficiency still remain at an unsatisfactory level (a few hundreds millimetre error at up to a few FPS) or is not fully verified with the precise ground truth. Therefore, the purpose of this study is to improve an accuracy and efficiency of 6DOF motion estimation in 3D MCL for indoor localization. Firstly, a terrestrial laser scanner is used for creating a precise 3D mesh model as an environment map, and a professional-level depth camera is installed as an outer sensor. GPU scene simulation is also introduced to upgrade the speed of prediction phase in MCL. Moreover, for further improvement, GPGPU programming is implemented to realize further speed up of the likelihood estimation phase, and anisotropic particle propagation is introduced into MCL based on the observations from an inertia sensor. Improvements in the localization accuracy and efficiency are verified by the comparison with a previous MCL method. As a result, it was confirmed that GPGPU-based algorithm was effective in increasing the computational efficiency to 10-50 FPS when the number of particles remain below a few hundreds. On the other hand, inertia sensor-based algorithm reduced the localization error to a median of 47mm even with less number of particles. The results showed that our proposed 3D MCL method outperforms the previous one in accuracy and efficiency.||2015|10.5194/ISPRSARCHIVES-XL-4-W5-61-2015|R. Hatakeyama, H. Date, S. Kanai|0.2|2
445|Adaptive Third Order Adams-Bashforth Time Stepping Scheme for 2D Extended Boussinesq Equations|We develop the third-order adaptive Adams-Bashforth time stepping scheme and the second-order finite difference discretization equation for variable time steps. We incorporate these schemes in the Celeris Advent software to discretize and solve the extended Boussinesq equations. This software uses a hybrid finite volume - finite difference scheme and leverages GPU to solve the equations faster than real-time as well to concurrently visualize them. We simulate several benchmarks using the adaptive time stepping scheme of Celeris Advent and demonstrate the capability of the software in modeling wave-breaking, wave runup, irregular waves, and rip currents. The adaptive scheme significantly improves the robustness of the model without slowing it down.|arXiv.org|2019|10.1587/elex.16.20181064|P. Lynett, S. Tavakkol, S. Son|0.16666666666666666|2
464|Interactive Evidence Detection: train state-of-the-art model out-of-domain or simple model interactively?|Finding evidence is of vital importance in research as well as fact checking and an evidence detection method would be useful in speeding up this process. However, when addressing a new topic there is no training data and there are two approaches to get started. One could use large amounts of out-of-domain data to train a state-of-the-art method, or to use the small data that a person creates while working on the topic. In this paper, we address this problem in two steps. First, by simulating users who read source documents and label sentences they can use as evidence, thereby creating small amounts of training data for an interactively trained evidence detection model; and second, by comparing such an interactively trained model against a pre-trained model that has been trained on large out-of-domain data. We found that an interactively trained model not only often out-performs a state-of-the-art model but also requires significantly lower amounts of computational resources. Therefore, especially when computational resources are scarce, e.g. no GPU available, training a smaller model on the fly is preferable to training a well generalising but resource hungry out-of-domain model.|Conference on Empirical Methods in Natural Language Processing|2019|10.18653/v1/D19-6613|C. Stahlhut|0.16666666666666666|2
489|Integrating Intra-and Intercellular Simulation of a 2D HL-1 Cardiac Model Based on Embedded GPUs|Simulation of electrophysiological cardiac models enables researchers to investigate the activity of heart under various circumstances. Fortunately, recent development in embedded parallel computing architectures has made it possible for one to efficiently simulate sophisticated electrophysiological models that match up to real conditions on embedded computing devices, which typically relies on large scale CPU or GPU clusters in the past. In this paper, a simultaneous implementation of a 2D Takeuchi-HL-1 cardiac model combining unicellular and intercellular solver is proposed and conducted on NVIDIA Jetson Tegra X2 embedded computer. The experiment results demonstrate that our implementation yields considerable efficiency improvement compared with that using non-simultaneous methods, without loss of simulation accuracy. Moreover, it's also proved that embedded devices are much more energy-efficient than conventional systems on the simulation.|International Symposium on Embedded Multicore/Many-core Systems-on-Chip|2019|10.1109/MCSoC.2019.00041|Xin Zhu, Xingyu Wangchen, W. Shen, Baohua Liu|0.16666666666666666|2
490|GPU-accelerated mesh-based Monte Carlo photon transport simulations|The mesh-based Monte Carlo (MMC) algorithm is increasingly used as the gold-standard for developing new biophotonics modeling techniques in 3-D complex tissues, including both diffusion-based and various Monte Carlo (MC) based methods. Compared to multi-layered and voxel-based MCs, MMC can utilize tetrahedral meshes to gain improved anatomical accuracy, but also results in higher computational and memory demands. Previous attempts of accelerating MMC using graphics processing units (GPUs) have yielded limited performance improvement and are not publicly available. Here we report a highly efficient MMC – MMCL – using the OpenCL heterogeneous computing framework, and demonstrate a speedup ratio up to 420× compared to state-of-the-art single-threaded CPU simulations. The MMCL simulator supports almost all advanced features found in our widely disseminated MMC software, such as support for a dozen of complex source forms, wide-field detectors, boundary reflection, photon replay and storing a rich set of detected photon information. Furthermore, this tool supports a wide range of GPUs/CPUs across vendors and is freely available with full source codes and benchmark suites at http://mcx.space/#mmc.|bioRxiv|2019|10.1101/815977|Q. Fang, Shijie Yan|0.16666666666666666|2
502|Roofline Analysis and Performance Optimization of the MGB Hydrological Model|The Roofline model gives insights about the performance behavior of applications bounded by either memory or processor limits, providing useful guidelines for performance improvements. This work uses the Roofline model on the analysis of the MGB model that simulates hydrological processes in largescale watersheds. Real-world input data are used to characterize the performance on two multicore architectures, one with only CPUs and one with CPUs/GPU. The MGB model performance is improved with optimizations for better memory use, and also with shared-memory (OpenMP) and GPU (OpenACC) parallelism. CPU performance achieves 42.51 % and 50.17 % of each system’s peak, whereas GPU performance is low due to overheads caused by the MGB model structure.|Symposium on High Performance Computing Systems|2019|10.5753/wscad.2019.8657|C. Mendes, H. Freitas|0.16666666666666666|2
503|A GPU-Based Implementation of ADMIRE|Multipath and off-axis scattering are two of the primary mechanisms for ultrasound image degradation. To address their impact, we have proposed Aperture Domain Model Image REconstruction (ADMIRE). This algorithm utilizes a model-based approach in order to identify and suppress sources of acoustic clutter. The ability of ADMIRE to suppress clutter and improve image quality has been demonstrated in previous works, but its use for real-time imaging has been infeasible due to its significant computational requirements. However, in recent years, the use of GPUs for general-purpose computing has enabled significant acceleration of compute-intensive algorithms. This is due to the fact that many modern GPUs have thousands of computational cores that can be utilized to perform massively parallel processing.Therefore, in this work, we have developed a GPU-based implementation of ADMIRE. The computations were distributed across two GPUs, and speedups of almost three orders of magnitude were achieved when compared to a serial CPU implementation. The frame rate depends upon various imaging parameters, and we demonstrate this using a small cyst simulation dataset and a large in-vivo kidney dataset. However, even for the large dataset, the implementation still provides the ability to process multiple frames of data per second. Due to this, it has the capability to serve as a real-time imaging framework.|IUS|2019|10.1109/ULTSYM.2019.8925842|Kazuyuki Dei, B. Byram, Christopher Khan|0.16666666666666666|2
504|Variational Bayesian Approach in Model-Based Iterative Reconstruction for 3D X-Ray Computed Tomography with Gauss-Markov-Potts Prior|3D X-ray Computed Tomography (CT) is used in medicine and non-destructive testing (NDT) for industry to visualize the interior of a volume and control its healthiness. Compared to analytical reconstruction methods, model-based iterative reconstruction (MBIR) methods obtain high-quality reconstructions while reducing the dose. Nevertheless, usual Maximum-A-Posteriori (MAP) estimation does not enable to quantify the uncertainties on the reconstruction, which can be useful for the control performed afterwards. Herein, we propose to estimate these uncertainties jointly with the reconstruction by computing Posterior Mean (PM) thanks to Variational Bayesian Approach (VBA). We present our reconstruction algorithm using a Gauss-Markov-Potts prior model on the volume to reconstruct. For PM calculation in VBA, the uncertainties on the reconstruction are given by the variances of the posterior distribution of the volume. To estimate these variances in our algorithm, we need to compute diagonal coefficients of the posterior covariance matrix. Since this matrix is not available in 3D X-ray CT, we propose an efficient solution to tackle this difficulty, based on the use of a matched pair of projector and backprojector. In our simulations using the Separable Footprint (SF) pair, we compare our PM estimation with MAP estimation. Perspectives for this work are applications to real data as improvement of our GPU implementation of SF pair.||2019|10.3390/proceedings2019033004|A. Mohammad-Djafari, Camille Chapdelaine, Estelle Parra, N. Gac|0.16666666666666666|2
511|Simulation of Deployment of Multiple Stents Within Deformable Artery|One of the most common clinical treatments of arterial stenosis is the implantation of an endovascular prosthesis called a stent. Computer modeling represents a useful tool that enables the analysis of the complex behavior of the stent during implantation within a patient specific artery. The numerical model presented in this paper can simulate the expansion of multiple stents, as well as the interaction of the stents with the arterial wall and the behavior of the arterial wall due to the forces caused by the stents. Also, GPU principles are used during implementation, to enable the execution of simulations in real time. In this paper, two stents are deployed within patient-specific artery and the results of this simulation are presented. The numerical models that are originally applied in engineering are used in the presented numerical model, in order to create a useful tool that can be used in bio-medicine. This software can be used for a more detailed prediction of the shape of the stents and artery after implantation and thus this software can improve the techniques used for treatment of the arterial stenosis and pre-operative patient-specific planning.|International Conferences on Biological Information and Biomedical Engineering|2019|10.1109/BIBE.2019.00088|T. Djukić, G. Pelosi, N. Filipovic, O. Parodi, I. Šaveljić|0.16666666666666666|2
514|Anatomically realistic simulation framework for ultrasound localization microscopy|Ultrasound Localization Microscopy (ULM) can map vessels at the capillary scale (<10 μm) by acquiring tens of thousands of images in which injected microbubbles are detected and tracked to generate super-resolved blood vessel maps. However, to our knowledge, there are no validation frameworks for ULM image formation algorithms. Herein, we developed a 3-D, anatomically realistic simulation tool based on serial two-photon microscopy (STPM). Rodent brain vasculature was segmented from STPM [1] into a graph-based model, which was then used to generate microbubble flow trajectories according to a velocity-diameter relationship derived from in vivo ULM data [2]. Ultrasound signals were obtained using a fast GPU-based simulator and reconstructed using our ULM image formation algorithm. We then proceeded to a parametric study. We first compared 2-D with 3-D imaging. Despite the latter's lower contrast, the gain in spatial information led to a more complete vasculature map. Localization precision was also enhanced since it was freed from the intrinsic localization error in the elevation direction. Also, increasing the number of transmits enhanced localization precision. From this study, we could establish a relationship between microbubble concentration and acquisition time for full network filling and optimal localization. This simulation tool provides thus a ground truth for the validation of ULM image formation algorithms and enables testing of new hypotheses. [1] R. Damseh et al., IEEE JBHI(2018). [2] V. Hingot et al., Sci Rep. (2019).|Journal of the Acoustical Society of America|2019|10.1121/1.5137507|H. Belgharbi, R. Damseh, J. Porée, F. Lesage, J. Provost, P. Delafontaine-Martel|0.16666666666666666|2
516|New collision detection method for simulating virtual plant populations|Detecting and resolving the collision of organs between different plants or the collision of different organs of a single plant are key issues in the realistic construction of a virtual plant population. A suitable collision detection scheme is necessary to prevent a reduction in realism caused by organ penetration. A mixed bounding volume tree construction scheme based on the growth characteristics of tomato plants is proposed in this paper, and the construction mode of the bounding at all levels is simplified by using a digital tomato model. Using a parallel GPU approach, we designed a tomato plant population collision detection program with CUDA acceleration. The proposed method reduces the total collision detection time by 92%-96%. \nKeywords: plant simulation, collision detection, bounding volume, GPU processing \nDOI: 10.25165/j.ijabe.20191206.4888 \n \nCitation: Ding W L, Wan Z X, Xu Y, Max N. New collision detection method for simulating virtual plant populations. Int J Agric & Biol Eng, 2019; 12(6): 156–151.|International Journal of Agricultural and Biological Engineering|2019|10.25165/j.ijabe.20191205.4888|Xu Yan, N. Max, Zang-xin Wan, Weilong Ding|0.16666666666666666|2
554|Optimization and Acceleration of 5G Link Layer Simulator|Requirements for higher data rates and wider bandwidth are increasing day by day with the rise and number of digital devices. Modern digital devices and systems such as IoT, autonomous vehicles, smartphones, entertainment systems require a good data rate and quality service. LTE was presented as a successor of 4G to fulfill this requirement, but it is getting more and more congested with a rapid rise in a number of connected devices and bandwidth requirements. To address these challenges, 3GPP announced the specification of a 5G network that will overcome all these issues and provide higher data rates and wider bandwidth by using higher frequencies. This architecture uses millimeter waves instead of frequencies used up to 4G. The frequency range for millimeter waves is from 30 to 300GHz. The problem with millimeter waves is that they are absorbed and dispersed easily by obstacles and thus can travel less distance. To overcome this issue, base stations need to be spaced closely. This results in a higher number of antennas spaced closely to each other. When using higher data rates and a large number of antennas, parameter tracking becomes computationally more and more complex. These parameters include angle of arrival(AoA), angle of departure(AoD), Zenith Angles of Arrival(ZOA) with Zenith Angles of Departure(ZOD), user speed, user direction, antenna correlation. Each time these parameters change, the channel needs to reconfigure its parameters. The case is even more complex for an object moving with high speed. This results in performance degradation due to this bottleneck. This work focuses on optimization and acceleration of parameter tracking, calculation, and reconfiguration for a 5G channel model. Parameter tracking and re-calculation requires high computation time and a large amount of memory. Conventionally, FPGAs are programmed using HDL-based design methodology to accelerate computationally intense applications. FPGAs are frequently used in many real-world applications to speed up the performance alongside lower power consumption as compared to GPUs and multi-core CPUs. The OpenCL Synthesis tools for Xilinx and Intel FPGAs enable us to accelerate applications with short development time. This thesis presents an optimized implementation of a 5G NR(New-Radio) link-layer simulation model using OpenCL. Performance and power consumption for CPU and FPGA platform is presented for various dimensions of the kernel.||2019|10.1115/1.2019-feb2|N. Shah|0.16666666666666666|2
563|Pattern-Adaptive Time Series Prediction via Online Learning and Paralleled Processing using CUDA|In this paper we study the problem of prediction of time series data in a computational efficient way. We proposed a SARIMA (Seasonal AutoRegressive Integrated Moving Average) based-online learning algorithm to update the model parameters via Gradient Descent based algorithm every time when a new data comes, based on the gap between prediction and true value of the time series data. In this way, our model can adapt to the new pattern of the time series if the pattern changes, and generates more accurate prediction results comparing with existing SARIMA implementations. Besides, it has the capability of learning the true pattern and converge to optimal model even if the start point is far from optimal. We also implement our algorithm on CUDA (Compute Unified Device Architecture) platform to support training millions of different online learning models concurrently by leveraging the GPU's capability of parallel computing. The performance of our online learning algorithm is evaluated with simulation results.|2019 IEEE 16th International Conference on Mobile Ad Hoc and Sensor Systems Workshops (MASSW)|2019|10.1109/MASSW.2019.00013|Zhifeng He, Bing Han, Meng Han|0.16666666666666666|2
577|Performant and High-Fidelity Solution for Complex and Large Radar Scene Simulation including Wind Turbines|This paper presents a state-of-the-art solution, called SE-Workbench-RF, for generating raw data \ndedicated to radar simulation based on the exploitation of synthetic environments, which means \nphysical modelling of targets and backgrounds (terrains, buildings, vegetation and other entities). \nThe paper gives an overview of the models and their implementation in SE-RAY-EM, which is the \nrendering module of SE-Workbench-RF, including specific features related to radar simulation in the \nmaritime environment with targets moving on the dynamic sea surface. Several technical topics are \nthen discussed, such as the rendering technique (ray tracing vs. rasterization), the implementation \n(CPU vs. GP GPU) and the trade-off between physical accuracy and computational performance. \nFinally, typical examples of results are shown with a focus on the problematic of the disturbances \nthat may be generated by wind turbines installed close to radar systems, with a specific focus on sea- \nshore environment.||2019|10.23919/irs.2019.8768180|N. Douchin, C. Ruiz|0.16666666666666666|2
579|Generation of IQ data simulating a SAR acquisition: targets in motion, clutter and shadows|In order to test our radar algorithms in a more and more complex environment, IQ data obtained through a SAR acquisition campaign may not be enough: they are limited to predefined trajectories and beams, and are dependent of current weather conditions and flight authorizations. These acquisition campaigns are also expensive and hard to set up, thus few data are produced and are insufficient to establish the validity domain of algorithms. In order to complete these campaigns, this paper describes a method of generating the SAR acquisition through IQ data in various environments, with mobile targets modeled via MOCEM and complex trajectories. This method dealing with a large amount of data, it has been optimized to run efficiently on a GPU.|2019 International Radar Conference (RADAR)|2019|10.1109/RADAR41533.2019.171242|Alan Girault, Joan Broussolle, Jacques Petit-Frère|0.16666666666666666|2
1418|The Pipeline Performance Model: A Generic Executable Performance Model for GPUs|This paper presents the pipeline performance model, a generic GPU performance model, which helps understand the performance of GPU code by using a code representation that is very close to the source code. The code is represented by a graph in which the nodes correspond to the source code instructions and the edges to data dependences between them. Furthermore, each node is enhanced with two latencies that characterize the instruction's time behavior on the GPU. This graph, together with a simple characterization of the GPU and the execution configuration, is used by a simulator to mimic the execution of the code. We validate the model on the micro-benchmarks used to determine the latencies and on a matrix multiplication kernel, both on an NVIDIA Fermi and an NVIDIA Pascal GPU. Initial results show that the simulated times follow the measured times, with acceptable errors, for a wide occupancy range. We argue that to achieve better accuracies it is necessary to further refine the model to take into account the complexity of memory access and warp scheduling, especially for more recent GPUs.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2019|10.1109/EMPDP.2019.8671606|Jan G. Cornelis, J. Lemeire|0.16666666666666666|2
1422|A framework for realistic virtual clinical trials in photon counting computed tomography|Although photon counting systems have shown strong clinical potential, this technology has not yet been fully evaluated or optimized for specific clinical applications. The purpose of this study was to develop a framework for realistic virtual clinical trials (VCTs) in photon counting CT (PCCT) imaging. We developed a photon counting CT simulator based on the geometry and physics of an existing research prototype scanner. The developed simulator models primary, scatter, and noise signals, detector responses, vendor-specific bowtie filters and X-ray spectra, axial/helical trajectories, vendor-specific acquisition modes, and multiple energy thresholds per detector pixel. The simulation procedure is accelerated by parallel processing using multiple GPUs. The generated projection images can be reconstructed using generic reconstruction algorithms as well as a commercial reconstruction software (ReconCT Siemens). A computational model of a physical Mercury phantom was imaged at multiple energy thresholds (25 and 75 keV) and dose levels (36, 72, 144, and 216 mAs). Noise magnitude was measured in the simulated images and compared against noise measurements in a real scan acquired with a research prototype photon counting scanner (Siemens Healthcare). The results showed that our simulator was capable of synthesizing realistic photon counting CT data. The simulator can be combined with realistic 4D high-resolution XCAT phantoms with intra-organ heterogeneities to conduct VCTs for specific clinical applications. This framework can greatly facilitate the evaluation, optimization, and eventual clinical use of PCCT.|Medical Imaging|2019|10.1117/12.2512898|J. Ramírez, K. Stierstorfer, E. Abadi, W. Paul Segars, Jayasai R. Rajagopal, B. Harrawood, Shobhit Sharma, M. Sedlmair, A. Kapadia, E. Samei|0.16666666666666666|2
1423|Performance Optimization of Physics Simulations Through Genetic Algorithms|The GeantV R&D approach is revisiting the standard particle transport simulation approach to be able to benefit from “Single Instruction, Multiple Data” (SIMD) computational architectures or extremely parallel systems like coprocessors and GPUs. The goal of this work is to develop a mechanism for optimizing the programs used for High-Energy Physics (HEP) particle transport simulations using a “black-box” optimization approach. Taking in account that genetic algorithms are among the most widely used “black-box” optimization methods, we analyzed a simplified model that allows precise mathematical definition and description of the genetic algorithm. The work done in this article is focused on the studies of evolutionary algorithms and particularly on stochastic optimization algorithms and unsupervised machine learning methods for the optimization of the parameters of the GeantV applications.|Journal of Computer Science|2019|10.3844/JCSSP.2019.57.66|O. Shadura, F. Carminati, A. Petrenko|0.16666666666666666|2
1512|A comprehensive GPU-based framework for scatter estimation in single source, dual source, and photon-counting CT|Scattered radiation is one of the leading causes of image quality degradation in computed tomography (CT), leading to decreased contrast sensitivity and inaccuracy of CT numbers. The established gold-standard technique for scatter estimation in CT is Monte Carlo (MC) simulation, which is computationally expensive, thus limiting its utility for clinical applications. In addition, the existing MC tools are generalized and often do not model a realistic patient and/or a scanner-specific scenario, including lack of models for alternative CT configurations. This study aims to fill these gaps by introducing a comprehensive GPU-based MC framework for estimating patient and scanner-specific scatter for single-source, dual-source, and photon-counting CT using vendor-specific geometry/components and anatomically realistic XCAT phantoms. The tool accurately models the physics of photon transport and includes realistic vendor-specific models for x-ray spectra, bowtie filter, anti-scatter grid, and detector response. To demonstrate the functionality of the framework, we characterized the scatter profiles for a Mercury and an XCAT phantom using multiple scanner configurations. The timing information from the simulations was tallied to estimate the speedup over a comparable CPU-based MC tool. We also utilized the scatter profiles from the simulations to enhance the realism of primary-only ray-traced images generated for the purpose of virtual clinical trials (VCT). A speedup as high as 900x over a CPU-based MC tool was also observed for our framework. The results indicate the capability of this framework to quantify scatter for different proposed CT configurations and the significance of scatter contribution for simulating realistic CT images.|Medical Imaging|2019|10.1117/12.2513198|Shobhit Sharma, A. Kapadia, E. Samei, E. Abadi, W. Paul Segars|0.16666666666666666|2
1526|A Simulation Method for LIDAR of Autonomous Cars|Vehicle-mounted LIDAR simulation module is an important part of autonomous car simulation test platform. According to the working principle of LIDAR, its working process and interface can be simulated in the digital simulation environment. The environment includes LIDAR emulation module, high precision 3D scene model and data interface module. The LIDAR simulation module uses GPU(Graphic Processing Unit)to calculate the depth of high-precision 3D scene and output the results through data interface. On the PC workstation, 64 lines vehicle-mounted LIDAR can be implemented for real-time simulation of large-scale road scene.|IOP Conference Series: Earth and Environment|2019|10.1088/1755-1315/234/1/012055|H. Su, Rui Wang, Kaixin Chen, Yizhan Chen|0.16666666666666666|2
1546|Applications of finite-difference time-domain for architectural acoustics consulting|Acoustics consultants have many tools in their arsenal to evaluate and design rooms and architectural elements; the computational resources available to this point have made the use of wave-propagation models impractical for the common user. Threshold acoustics has found it both useful and now computationally feasible to supplement more traditional, geometric analysis with the simulation of wave-propagation using finite-difference time-domain (FDTD). Our group has developed first-order leapfrog FDTD routines in MATLAB for simulating wave propagation in an isotropic medium in two- and three-dimenision with perfectly matched layers being the boundary condition. The placement of solid elements within the test space allows analysis of arbitrary geometries. For additional computational power, our group has utilized GPU computing clusters available through Amazon Web Services accessed directly through MATLAB. Our method is based on the simulation of an impulse response and subsequent analysis of the impulse response consistent with traditional in situ testing methods. Applications to date include analysis of the scattering behavior of acoustically shaped surfaces and evaluation of the array behavior of architectural reflector panels. Acoustics consultants have many tools in their arsenal to evaluate and design rooms and architectural elements; the computational resources available to this point have made the use of wave-propagation models impractical for the common user. Threshold acoustics has found it both useful and now computationally feasible to supplement more traditional, geometric analysis with the simulation of wave-propagation using finite-difference time-domain (FDTD). Our group has developed first-order leapfrog FDTD routines in MATLAB for simulating wave propagation in an isotropic medium in two- and three-dimenision with perfectly matched layers being the boundary condition. The placement of solid elements within the test space allows analysis of arbitrary geometries. For additional computational power, our group has utilized GPU computing clusters available through Amazon Web Services accessed directly through MATLAB. Our method is based on the simulation of an impulse response and subsequent analysis of the impulse res...|Journal of the Acoustical Society of America|2019|10.1121/1.5101513|J. T. Strong, Laura C. Brill|0.16666666666666666|2
1587|SE-Workbench-RF: Performant and High-Fidelity Raw Data Generation for Various Radar Applications|Guidance of weapon systems relies on sensors to analyze targets signature. Defense weapon systems also need to detect and then identify threats also using sensors. One important class of sensors is radar systems that are very efficient for all weather detection. High frequency radars can produce high quality images with very complex features such as dihedral and corner reflector high scattering contributors, shadows and layover effects. Besides, image quality is very dependent on the carrier velocity and trajectory. Such sensors systems are so complex that they need simulation to be tested in a large variety of operational conditions. This paper presents a state-of-the-art solution, called SE-Workbench-RF, for generating raw data dedicated to radar simulation based on the exploitation of synthetic environments, which means physical modelling of targets and backgrounds (terrains, buildings, vegetation and other entities). The paper gives an overview of the models and their implementation in SE-RAY-EM, which is the rendering module of SE-Workbench-RF, including specific features related to radar simulation in the maritime environment with targets moving on the dynamic sea surface. Several technical topics are also discussed, such as the rendering technique (ray tracing vs. rasterization), the implementation (CPU vs. GP GPU) and the tradeoff between physical accuracy and computational performance.|International Radar Symposium|2019|10.23919/IRS.2019.8768180|C. Ruiz, H. Mametsa, N. Douchin, J. Israel|0.16666666666666666|2
1592|Technology Focus: Simulation (July 2019)|\n The original realm of this Technology Focus was reservoir simulation, but the scope has been expanded to include all simulation. This is, indeed, a broad sweep, and I fear that I may not be able to do full justice to technical domains in which I am not fully conversant. Nonetheless, muddling through, I have selected three papers with reasonably broad coverage. The one I particularly like describes an open-source 3D-printing micromodel tool kit. This highlights the need to validate simulation through experimental observation, and the work provides a practical means to do so.\n This will be my last editorial (it has been 6 years now), and I leave you with two main thoughts, one positive and one less so. My optimistic remark concerns the advances I have observed in the field of simulation of complex reservoirs, especially fracturing of tight reservoirs. While the topic remains challenging, the advances made have been quite significant over the past 5 years. Nonetheless, in my view, we still do not possess a full understanding of oil production in unconventional fractured reservoirs. Our ability to forecast such assets remains elusive, even with copious amounts of analytics, mountains of data, and an arsenal of machine-learning tools. We still cannot ascribe the level of confidence to such assets as we wish would be possible. More fundamental experimental investigation is necessary here, and, while we are gradually increasing our understanding, the journey has some way to go.\n My final comment concerns buttons. Specifically, I refer to these so-called big green “simulate” buttons: the ones that entice a user to blindly “press it, and for-get it” (with apologies to Ron Popeil). Well-crafted, user-experience-optimized, appealingly designed interfaces are now standard. Nothing new in that. Nonetheless, I cannot help but feel that, rather than assisting the engineer, such interfaces form a metaphorical barrier between the user and the simulation engine. I am of the generation that was quite happy navigating large keyword-driven ASCII files with the “vi” editor (remember that?). While these were awkward, slow, and often excruciatingly painful to operate, being forced to work directly with keywords and ASCII files yields one very significant advantage: an unavoidable and direct connection with the data. One had no choice but to become acquainted with all aspects of an important keyword and its input requirements. This ensured consistency of data input and facilitated a closer bond between user and simulator (greater transparency of what was going on under the hood).\n Being unashamedly old school, I feel that “optimized user-interface (UI) dashboards” often cast a misty veil over human/machine connectedness and sometimes may even impede the pathway to understanding of simulation behavior and the solution itself. My point here is this: Do not hesitate to dive into the files typically generated by these UIs and be unafraid to be old school, even if only for a few moments. The insight this affords is well worth the effort. Saying this, I am clearly showing my age, so it’s time for a fresh face to take over this editorial. I thank you for your patience over the past few years. Meanwhile, I feel an overwhelming urge to write another technical paper (that no one will read), written in TeX and coded in FORTRAN77, using my trusty “vi” editor—happiness awaits.\n Recommended additional reading at OnePetro: www.onepetro.org.\n SPE 191213 Application of Memory Formalism and Fractional Derivative in Reservoir Simulation by Mahamudul Hashan, Memorial University of Newfoundland, et al.\n SPE 193880 A Massively Parallel Algebraic Multiscale Solver for Reservoir Simulation on the GPU Architecture by A.M. Manea, Saudi Aramco, et al.\n SPE 193844 A Bayesian Sampling Framework With Seismic Priors for Data Assimilation and Uncertainty Quantification by Siavash Nejadi, University of Calgary, et al.|Journal of Petroleum Technology|2019|10.2118/0719-0043-JPT|W. Bailey|0.16666666666666666|2
1602|GPGPU implementation of a lattice Boltzmann methodology for particle transport and deposition in complex flow|\nPurpose\nThis study aims to modify the standard probabilistic lattice Boltzmann methodology (LBM) cellular automata (CA) algorithm to enable a more realistic and accurate computation of the ensemble rather than individual particle trajectories that need to be updated from one time step to the next (allowing, as such, a fraction of the collection of particles in any lattice grid cell to be updated in a time step, rather than the entire collection of particles as in the standard LBM-CA algorithm leading to a better representation of the dynamic interaction between the particles and the background flow). Exploitation of the inherent parallelism of the modified LBM-CA algorithm to provide a computationally efficient scheme for computation of particle-laden flows on readily available commodity general-purpose graphics processing units (GPGPUs).\n\n\nDesign/methodology/approach\nThis paper presents a framework for the implementation of a LBM for the simulation of particle transport and deposition in complex flows on a GPGPU. Towards this objective, the authors have shown how to map the data structure of the LBM with a multiple-relaxation-time (MRT) collision operator and the Smagorinsky subgrid-scale turbulence model (for turbulent fluid flow simulations) coupled with a CA probabilistic method (for particle transport and deposition simulations) to a GPGPU to give a high-performance computing tool for the calculation of particle-laden flows.\n\n\nFindings\nA fluid-particle simulation using our LBM-MRT-CA algorithm run on a single GPGPU was 160 times as computationally efficient as the same algorithm run on a single CPU.\n\n\nResearch limitations/implications\nThe method is limited by the available computational resources (e.g. GPU memory size).\n\n\nOriginality/value\nA new 3D LBM-MRT-CA model was developed to simulate the particle transport and deposition in complex laminar and turbulent flows with different hydrodynamic characteristics (e.g. vortex shedding, impingement, free shear layer, turbulent boundary layer). The solid particle information is encapsulated locally at the lattice grid nodes, allowing for straightforward mapping of the datastructure onto a GPGPU enabling a massive parallel execution of the LBM-MRT-CA algorithm. The new particle transport algorithm was based on the local (bulk) particle density and velocity and provides more realistic results for the particle transport and deposition than the standard LBM-CA algorithm.\n|International journal of numerical methods for heat & fluid flow|2019|10.1108/HFF-09-2018-0485|F. Lien, E. Yee, Ali Abdulkadhim|0.16666666666666666|2
1604|Connecting Diffusion MRI to Skeletal Muscle Microstructure: Leveraging Meta-Models and GPU-acceleration|Due to its non-invasive nature, diffusion-weighted MRI (dMRI) has shown promise as a method to quantify skeletal muscle's microstructure; however, connecting the dMRI signal of muscle to the underlying microstructure is difficult. Numerical models of dMRI can parameterize this relationship, but the associated computational expense has prohibited extensive use. Here, efficient numerical methods are presented to address this problem. In particular, a meta-model representation of a lattice Boltzmann model of dMRI is formulated and shown to be both accurate and several orders of magnitude faster to evaluate. It is also demonstrated how such a meta-model can help inform dMRI pulse profile selection in encoding microstructural information into the dMRI signal. Additionally, histologically-informed simulations are performed, allowing comparison of the numerical model's simplified parameterization with the more complex topology of skeletal muscle. Finally, an efficient inversion method is proposed to infer microstructural parameters of muscle from dMRI signal using a GPU-accelerated numerical model. The inversion method is able to infer microstructural parameters from dMRI signal when the underlying geometry matches the numerical model's, however, the simplified numerical model does not agree with simulations of more complex muscle tissue.|Practice and Experience in Advanced Research Computing|2019|10.1145/3332186.3333054|J. Georgiadis, Noel M. Naughton|0.16666666666666666|2
1610|NeuroGPU: Accelerating multi-compartment, biophysically detailed neuron simulations on GPUs|The membrane potential of individual neurons depends on a large number of interacting biophysical processes operating on spatial-temporal scales spanning several orders of magnitude. The multi-scale nature of these processes dictates that accurate prediction of membrane potentials in specific neurons requires utilization of detailed simulations. Unfortunately, constraining parameters within biologically detailed neuron models can be difficult, leading to poor model fits. This obstacle can be overcome partially by numerical optimization or detailed exploration of parameter space. However, these processes, which currently rely on central processing unit (CPU) computation, often incur exponential increases in computing time for marginal improvements in model behavior. As a result, model quality is often compromised to accommodate compute resources. Here, we present a simulation environment, NeuroGPU, that takes advantage of the inherent parallelized structure of graphics processing unit (GPU) to accelerate neuronal simulation. NeuroGPU can simulate most of biologically detailed models 800x faster than traditional simulators when using multiple GPU cores, and even 10-200 times faster when implemented on relatively inexpensive GPU systems. We demonstrate the power of NeuoGPU through large-scale parameter exploration to reveal the response landscape of a neuron. Finally, we accelerate numerical optimization of biophysically detailed neuron models to achieve highly accurate fitting of models to simulation and experimental data. Thus, NeuroGPU enables the rapid simulation of multi-compartment, biophysically detailed neuron models on commonly used computing systems accessible by many scientists.|bioRxiv|2019|10.1101/727560|Kyung Geun Kim, Roy Ben-Shalom, A. Korngreen, K. Bender, Alexander Ladd, Kristofer E. Bouchard, Nikhil S. Artherya, Hersh Sanghevi, Christopher Cross|0.16666666666666666|2
1620|SMODERP2D SOIL EROSION MODEL ENTERING AN OPEN SOURCE ERA WITH GPU-BASED PARALLELIZATION|Abstract. SMODERP2D is a runoff-soil erosion physically-based distributed episodic model used for calculation and prediction processes at agricultural areas and small watersheds. The core of the model is a raster based cell-by-cell mass balance calculation which includes the key hydrological processes, such as effective precipitation, surface runoff and stream network routing. Effective precipitation, the forcing of the runoff and erosion processes, is reduced by surface retention and infiltration. Surface runoff consists of two components: slower sheet and concentrated rapid rill flow. Stream network routing is performed line-by-line in the user predefined polyline layer.SMODERP is a long-term project driven by the Department of Landscape Water Conservation at the Czech Technical University in Prague. At the beginning, SMODERP has been developed as a surface runoff simulated by profile model (1D). Later the model has been redesigned using a spatially distributed method. This version is named SMODERP2D. Ongoing development is focused on obtaining parameters of the hydrological models, incorporating new infiltration and flow routing routines, and conceptualization of a rill flow and rill development. The model belongs to a family of so-called GIS-based hydrological models utilizing capabilities of GIS software for geospatial data processing. Importantly, the SMODERP2D project is currently entering an open source world. Originally the model could be run only in proprietary Esri ArcGIS platform. A new version of the model presented by this manuscript adds support for two key open source GIS platforms, GRASS GIS and QGIS. A newly developed GRASS module and QGIS plugin significantly increases the accessibility of the SMODERP2D model for research purposes and also for engineering practice.Middle scale distributed hydrological models often encounter with high computation costs and long model runtime. Long runtime is caused by high-resolution input data which is easily available nowadays. The project also includes an experimental version of the SMODERP2D model enabling the parallelization of computations. This parallelization is done using TensorFlow, and its goal is to decrease the time needed for its run. It is supported by both CPU and GPU. Parallelization of computations is an important step towards providing SMODERP2D web processing services in order to allow quick and easy integration to highly specialized platforms such as Atlas Ltd.\n|ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences|2019|10.5194/ISPRS-ARCHIVES-XLII-4-W14-143-2019|P. Kavka, J. Jeřábek, M. Landa, O. Pešek|0.16666666666666666|2
413|Advantages and pitfalls of OpenCL in computational physics|OpenCL offers many advantages in computational physics in comparison to traditional MPI/OpenMP parallelization. We present an MPI/OpenCL based plasma simulation code, as an example of how computational physics can benefit from OpenCL. The code utilizes a hybrid modeling approach which combines elements from both fluid and particle-in-cell (PIC) methods. Most applications in computational physics and engineering are based on either of these models. Typical application range from fluid simulations for the characterization of flow behavior to PIC simulations of different plasma applications like nuclear fusion. Hence the hybrid model includes many of the common problems encountered in computational science. This includes for example solving differential equations, systems of linear equations and parallel reduction. Therefore it is well suited to show the common advantages and problems that arise with an OpenCL based approach. For this purpose we compare performance results between CPUs and GPUs for the above mentioned problems as they arise in the simulation code. Overall the runtime per FLOP using GPUs is significantly lower compared to CPUs. One reason for this advantage in performance is that numerical solvers can be implemented more efficiently on GPUs compared to CPUs due to the inherent advantage of GPU architecture. An additional advantage of OpenCL is that it allows for modular structuring of the code by encapsulating numerical aspects into separate OpenCL kernels. This separates the MPI based host code from the numerical problem and allows for easy change of numerical modules for example solving algorithms while keeping a single code base for multiple platforms. This is an important feature that sets OpenCL apart from CUDA/OpenMP. Furthermore combination of MPI and OpenCL makes deployment on modern heterogeneous systems, including office workstations, possible. A drawback of OpenCL is, that it does not support dynamic memory handling. This is required by typical PIC or hybrid approaches to dynamically remove or insert particles at every step of the simulation. As dynamic memory allocation is supported in the usual CPU based approach this does not pose a problem. With OpenCL the memory allocation has to be done by the Host, which in turn introduces a significant copy penalty as the particle data on the device has to be copied by the Host at every time step.|International Workshop on OpenCL|2018|10.1145/3204919.3204929|P. Heinisch, Hendrik Ranocha, K. Ostaszewski|0.14285714285714285|2
1194|A parallel iterative approach for efficient full chip thermal analysis|Efficient full-chip thermal simulation is among the most challenging problems facing the EDA industry today, especially for modern 3D integrated circuits, due to the huge linear systems resulting from thermal modeling approaches that require unreasonably long computational times. Direct linear solvers are not capable of handling such huge problems and iterative methods are the only feasible approach. In this paper, we propose a computationally efficient iterative method with a parallel preconditioned technique that exploits the resources of massively parallel architectures such as Graphic Processor Units (GPUs). Experimental results, demonstrate that the proposed method achieves a speedup of 2.2X in CPU execution and a 26.93X speedup in GPU execution over the state of the art iterative method.|International Conference on Modern Circuits and Systems Technologies|2018|10.1109/MOCAST.2018.8376634|Konstantis Daloukas, N. Evmorfopoulos, G. Stamoulis, G. Floros|0.14285714285714285|2
1230|Thoroughly Exploring GPU Buffering Options for Stencil Code by Using an Efficiency Measure and a Performance Model|"Stencil computations form the basis for computer simulations across almost every field of science, such as computational fluid dynamics, data mining, and image processing. Their mostly regular data access patterns potentially enable them to take advantage of the high computation and data bandwidth of GPUs, but only if data buffering and other issues are handled properly. Finding a good code generation strategy presents a number of challenges, one of which is the best way to make use of memory. GPUs have several types of on-chip storage including registers, shared memory, and a read-only cache. The choice of type of storage and how it’s used, a <italic>buffering strategy</italic>, for each stencil array (<italic>grid function</italic>, [GF]) not only requires a good understanding of its stencil pattern, but also the efficiency of each type of storage for the GF, to avoid squandering storage that would be more beneficial to another GF. For a stencil computation with <inline-formula><tex-math notation=""LaTeX"">$N$</tex-math> <alternatives><inline-graphic xlink:href=""hu-ieq1-2705139.gif""/></alternatives></inline-formula> GFs, the total number of possible assignments is <inline-formula><tex-math notation=""LaTeX"">$\beta ^{N}$</tex-math><alternatives> <inline-graphic xlink:href=""hu-ieq2-2705139.gif""/></alternatives></inline-formula> where <inline-formula> <tex-math notation=""LaTeX"">$\beta$</tex-math><alternatives><inline-graphic xlink:href=""hu-ieq3-2705139.gif""/> </alternatives></inline-formula> is the number of buffering strategies. Our code-generation framework supports five buffering strategies (<inline-formula><tex-math notation=""LaTeX"">$\beta =5$</tex-math><alternatives> <inline-graphic xlink:href=""hu-ieq4-2705139.gif""/></alternatives></inline-formula>). Large, complex stencil kernels may consist of dozens of GFs, resulting in significant search overhead. In this work, we present an analytic performance model for stencil computations on GPUs and study the behavior of read-only cache and L2 cache. Next, we propose an efficiency-based assignment algorithm which operates by scoring a change in buffering strategy for a GF using a combination of (a) the predicted execution time and (b) on-chip storage usage. By using this scoring, an assignment for <inline-formula><tex-math notation=""LaTeX"">$N$</tex-math><alternatives> <inline-graphic xlink:href=""hu-ieq5-2705139.gif""/></alternatives></inline-formula> GFs can be determined in <inline-formula><tex-math notation=""LaTeX"">$(\beta -1)N(N+1)/2$</tex-math><alternatives> <inline-graphic xlink:href=""hu-ieq6-2705139.gif""/></alternatives></inline-formula> steps. Results show that the performance model has good accuracy and that the assignment strategy is highly efficient."|IEEE Transactions on Multi-Scale Computing Systems|2018|10.1109/TMSCS.2017.2705139|David M. Koppelman, Steven R. Brandt, Yue Hu|0.14285714285714285|2
1231|Breast tomosynthesis reconstruction using TIGRE software tool|This article shows the feasibility of using the open source Tomographic Iterative GPU-based Reconstruction (TIGRE) Toolbox, originally developed for cone-beam x-ray computed tomography (CBCT), to reconstruct images from a Digital Breast Tomosynthesis (DBT) system. We present reconstructed images of simple simulated phantoms as well as the commercially available breast phantoms CIRS models 013 and 073; acquired by a Hologic Selenia Dimensions system. Initial results have shown the ability of TIGRE to reconstruct images using several reconstruction algorithms (FDK, OSSART, MLEM), although a wider variety of iterative algorithms could be also considered. This is the first work that uses the TIGRE reconstruction tool for DBT geometries, opening new possibilities for free, fast and reliable reconstruction algorithms to other research groups.|Other Conferences|2018|10.1117/12.2317933|Josep Fernández, A. Malet, R. Martí, Diego García Pinto, Oliver Díaz|0.14285714285714285|2
1240|GPU-Accelerated Simulation of Elastic Wave Propagation|Modeling of ultrasound waves propagation in hard biological materials such as bones and skull has a rapidly growing area of applications, e.g. brain cancer treatment planing, deep brain neurostimulation and neuromodulation, and opening blood brain barriers. Recently, we have developed a novel numerical model of elastic wave propagation based on the Kelvin-Voigt model accounting for linear elastic wave proration in heterogeneous absorption media. Although, the model offers unprecedented fidelity, its computational requirements have been prohibitive for realistic simulations. This paper presents an optimized version of the simulation model accelerated by the Nvidia CUDA language and deployed on the best GPUs including the Nvidia P100 accelerators present in the Piz Daint supercomputer. The native CUDA code reaches a speed-up of 5.4 when compared to the Matlab prototype accelerated by the Parallel Computing Toolbox running on the same GPU. Such reduction in computation time enables computation of large-scale treatment plans in terms of hours.|International Symposium on High Performance Computing Systems and Applications|2018|10.1109/HPCS.2018.00044|Kristian Kadlubiak, J. Jaros, B. Treeby|0.14285714285714285|2
1256|A Technique for Large-Scale 2D Seismic Field Simulations on Supercomputers|An algorithm for the simulation of elastic wave propagation in 2D isotropic inhomogeneous media with complex geometrical structure is presented. A parallel implementation of the FDTD method of fourth order in space to perform calculations on high-performance computing systems with different architectures (CPUs, GPUs, or Xeon Phi coprocessors) is discussed. The proposed technique of mathematical modeling with the use of MPI and CUDA is used to design program codes for computations for a realistic long-distance model. Program codes for single device and multi device use are developed. A large-scale geophysical model of the Baikal rift zone is reconstructed. New synthetic data depicting the structure of the seismic field for the rift zone are presented.|IEEE International Conference on Actual Problems of Electronics Instrument Engineering|2018|10.1109/APEIE.2018.8545842|D. Karavaev, V. Kovalevsky|0.14285714285714285|2
1284|Accelerating Numerical Simulations of Supernovae with GPUs|To understand the mechanism of supernova explosions, large-scale numerical simulations are essential because of their complex dynamics described by a coupled equations of neutrino radiation transport and hydrodynamics of dense matter. In this work, we employ GPUs to accelerate such simulations. By adopting the implicit scheme for the evolution equation, an iterative linear equation solver for the coefficient matrix is the most time consuming part, which has been shown to be efficiently offloaded to GPUs. There are still several secondary bottlenecks which cost substantial time in the simulations, such as computation of the collision term of the Boltzmann equation of neutrinos, and parameter tuning of the matrices in the iterative solver. This paper focuses on these parts and offloads them to GPUs by employing CUDA in the case of spherically symmetric system. As a result, the time evolution is sufficiently accelerated for desirable model sizes toward systematic survey of stellar models with better grid resolution than that adopted so far.|2018 Sixth International Symposium on Computing and Networking Workshops (CANDARW)|2018|10.1109/CANDARW.2018.00056|H. Matsufuru, K. Sumiyoshi|0.14285714285714285|2
1299|Data visualization for vegetal landscapes: Building 3D representations of organ biomass compartments: How plant production could constrain 3D lollypop-like representations|In the past decade, with the power of graphical boards, visualization of virtual natural scene become popular in multimedia applications. It usually relies on degraded virtual single plant geometrical representations and massive use of textures. Such scenes show however poor variability in terms of the number of species and individual plasticity. They also show strong animation constraints, many limited to viewpoint moves, ignoring growth processes. We propose here a frame for future works related to virtual plant visualization. It aims to drop down the classical geometrical descriptions of individual plants for the benefits of functional representations, aggregated up to a single crown. We then show that a wide range of parametric virtual crown shapes can be built from the knowledge of leaf area. Moreover, production outputs computed at each simulation time step by crop models or FSPM models define strong constraints in the organ positioning. Such constraints can be used to build hierarchical virtual geometries underlying the plant main axis and its crown decomposition according to its axis typology. Conversely, similar representations can also be generated from an exhaustive representation of virtual or real trees. On such plants, we first build a point cloud from the organ 3D coordinates. A statistical hierarchical dynamic clustering analysis can then be applied to the leaf cloud. It allows obtaining the statistical ellipsoid decomposition that can then be used for comparisons or shape fitting optimization. We finally introduce some technical elements showing that basic 3D shapes used for functional visualization (cone frustum and ellipsoids) can be fully generated and rendered by GPU techniques. As a summary, simulated plant functional visualization appears as a promising research track, freeing models from complex and costly geometrical computations. These representations also propose a new frame of discussions on tree crown modeling and descriptions for diagnosis purposes.|2018 6th International Symposium on Plant Growth Modeling, Simulation, Visualization and Applications (PMA)|2018|10.1109/PMA.2018.8611608|S. Sabatier, V. Letort, M. Jaeger, X.P. Zhang, P. de Reffye, Y. Gang, M. Kang, P. Borianne|0.14285714285714285|2
1393|Challenges on Porting Lattice Boltzmann Method on Accelerators: NVIDIA Graphic Processing Units and Intel Xeon Phi|Currently NVIDIA GPUs and Intel Xeon Phi accelerators are alternatives of computational architectures to provide high performance. This chapter investigates the performance impact of these architectures on the lattice Boltzmann method. This method is an alternative to simulate fluid flows iteratively using discrete representations. It can be adopted for a large number of flows simulations using simple operation rules. In the experiments, it was considered a three-dimensional version of the method, with 19 discrete directions of propagation (D3Q19). Performance evaluation compare three modern GPUs: K20M, K80, and Titan X; and two architectures of Xeon Phi: Knights Corner (KNC) and Knights Landing (KNL). Titan X provides the fastest execution time of all hardware considered. The results show that GPUs offer better processing time for the application. A KNL cache implementation presents the best results for Xeon Phi architectures and the new Xeon Phi (KNL) is two times faster than the previous model (KNC). Challenges on Porting Lattice Boltzmann Method on Accelerators: NVIDIA Graphic Processing Units and Intel Xeon Phi||2018|10.4018/978-1-5225-4760-0.CH002|M. Serpa, C. Schepke, J. F. Lima|0.14285714285714285|2
1407|Parallelizing Combinatorial Optimization Heuristics with GPUs|Article history: Received: 12 August, 2018 Accepted: 09 November, 2018 Online: 18 November, 2018 Combinatorial optimization problems are often NP-hard and too complex to be solved within a reasonable time frame by exact methods. Heuristic methods which do not offer a convergence guarantee could obtain some satisfactory resolution for combinatorial optimization problems. However, it is not only very time consuming for Central Processing Units (CPU) but also very difficult to obtain an optimized solution when solving large problem instances. So, parallelism can be a good technique for reducing the time complexity, as well as improving the solution quality. Nowadays Graphics Processing Units (GPUs) have evolved supporting general purpose computing. GPUs have become many core processors, multithreaded, highly parallel with high bandwidth memory and tremendous computational power due to the market demand for high definition and real time 3D graphics. Our proposed work aims to design an efficient GPU framework for parallelizing optimization heuristics by focusing on the followings: distribution of data processing efficiently between GPU and CPU, efficient memory management, efficient parallelism control. Our proposed GPU accelerated parallel models can be very efficient to parallelize heuristic methods for solving large scale combinatorial optimization problems. We have made a series of experiments with our proposed GPU framework to parallelize some heuristic methods such as simulated annealing, hill climbing, and genetic algorithm for solving combinatorial optimization problems like Graph Bisection problem, Travelling Salesman Problem (TSP). For performance evaluation, we’ve compared our experiment results with CPU based sequential solutions and all of our experimental evaluations show that parallelizing combinatorial optimization heuristics with our GPU framework provides with higher quality solutions within a reasonable time.||2018|10.25046/AJ030635|L. Tao, M. H. Rashid|0.14285714285714285|2
1442|Research on the simulation of PF-LBM model based on MPI+CUDA mixed granularity parallel|"A microstructure numerical model is an intensive computational problem, for which the simulation time is too long and the simulation scale is too small. To solve these two problems, in this article, we use MPI+CUDA hybrid particle heterogeneous parallel computing to implement the dendrite growth simulation of a PF-LBM phase-field 3D model. Message Passing Interface (MPI) can be used to conduct coarse granularity division, to break through the limitation of the simulate scale in a single machine. In each node, fine-grained division is implemented by the Compute Unified Device Architecture (CUDA) parallel way to realize the completely parallelism intra-node, and to improve overall computational efficiency. At the same time, in this article, the ""pseudo three-dimensional array"" programming method is brought up in CUDA programming, and also to improve the CUDA random number generation method, in order to simplify the CUDA array programming and reduce the CUDA random number generation time purposes. Experiments show that at the same simulation scale, the speed-up ratio with 21 nodes MPI+CUDA was 57, which was increased 54% over the 21 nodes MPI. Under the condition of computing efficiency close, the largest simulation scale with 21 nodes MPI+CUDA was 4203, which is 13 times to single GPU. Therefore, the MPI + CUDA hybrid granularity parallel method proposed in this paper also has the advantages of high computational efficiency of the GPU and MPI to expand the simulation scale.A microstructure numerical model is an intensive computational problem, for which the simulation time is too long and the simulation scale is too small. To solve these two problems, in this article, we use MPI+CUDA hybrid particle heterogeneous parallel computing to implement the dendrite growth simulation of a PF-LBM phase-field 3D model. Message Passing Interface (MPI) can be used to conduct coarse granularity division, to break through the limitation of the simulate scale in a single machine. In each node, fine-grained division is implemented by the Compute Unified Device Architecture (CUDA) parallel way to realize the completely parallelism intra-node, and to improve overall computational efficiency. At the same time, in this article, the ""pseudo three-dimensional array"" programming method is brought up in CUDA programming, and also to improve the CUDA random number generation method, in order to simplify the CUDA array programming and reduce the CUDA random number generation time purposes. Experiment..."||2018|10.1063/1.5032155|Xin Deng, Li Feng, Changsheng Zhu, Jieqiong Liu|0.14285714285714285|2
1505|Spectral-element simulation of two-dimensional elastic wave propagation in fully heterogeneous media on a GPU cluster|We present an implementation of the spectral-element method for simulation of two-dimensional elastic wave propagation in fully heterogeneous media. We have incorporated most of realistic geological features in the model, including surface topography, curved layer interfaces, and 2-D wave-speed heterogeneity. To accommodate such complexity, we use an unstructured quadrilateral meshing technique. Simulation was performed on a GPU cluster, which consists of 24 core processors Intel Xeon CPU and 4 NVIDIA Quadro graphics cards using CUDA and MPI implementation. We speed up the computation by a factor of about 5 compared to MPI only, and by a factor of about 40 compared to Serial implementation.||2018|10.1088/1742-6596/1011/1/012034|I. Rudianto, Sudarmaji|0.14285714285714285|2
1553|Applying GPU parallel technology to accelerate FARSITE forest fire simulator|Forest fires are one of the most common natural hazards in the countries of the Mediterranean region. The forest fires spread simulators have proven to be very effective tools in the fight against these disasters. But to be able to use these simulators in an effective way it is necessary to be able to obtain realistic predictions of the behavior of the fire in a relatively short time. In the last decades, the advances in the field of computing have allowed different strategies to improve the effectiveness of simulators. With this objective, it has been tried to take advantage of the computing power of the GPUs ( Graphic Processors Units ) to accelerate the simulation of the propagation of fires. The problem with the use of GPUs is that the data transfer between the memory of the system and the GPU memory increases the execution time. For this reason, the computation operations have to compensate the transfer time of the data from the CPU ( Central Processor Unit ) to the memory of the GPU; which means, that the data volume must be sufficient to compensate this effect. The main objective of this work is to analyze the execution time of the forest fire spread simulation in a CPU and in a GPU, and to see from what data volume the execution in GPU is more efficient than in the CPU. For this study, a fire simulator has been designed based on the basic model for one point evolution in the simulator FARSITE. As study case, a synthetic fire was used where the fire progresses in a linear front and in a single direction, maintaining constant wind, terrain and vegetation conditions for all points of the fire front and throughout the simulation.||2018|10.14195/978-989-26-16-506_100|Antonio Espinosa, A. Cencerrado, A. Cortés, T. Margalef, Carlos Carrillo|0.14285714285714285|2
1560|Reducing redundancy of real time computer graphics in mobile systems|The goal of this thesis is to propose novel and effective techniques to eliminate redundant computations that waste energy and are performed in real-time computer graphics applications, with special focus on mobile GPU micro-architecture. Improving the energy-efficiency of CPU/GPU systems is not only key to enlarge their battery life, but also allows to increase their performance because, to avoid overheating above thermal limits, SoCs tend to be throttled when the load is high for a large period of time. Prior studies pointed out that the CPU and especially the GPU are the principal energy consumers in the graphics subsystem, being the off-chip main memory accesses and the processors inside the GPU the primary energy consumers of the graphics subsystem. \nFirst, we focus on reducing redundant fragment processing computations by means of improving the culling of hidden surfaces. During real-time graphics rendering, objects are processed by the GPU in the order they are submitted by the CPU, and occluded surfaces are often processed even though they will end up not being part of the final image. When the GPU realizes that an object or part of it is not going to be visible, all activity required to compute its color and store it has already been performed. We propose a novel architectural technique for mobile GPUs, Visibility Rendering Order (VRO), which reorders objects front-to-back entirely in hardware to maximize the culling effectiveness of the GPU and minimize overshading, hence reducing execution time and energy consumption. VRO exploits the fact that the objects in graphics animated applications tend to keep its relative depth order across consecutive frames (temporal coherence) to provide the feeling of smooth transition. VRO keeps visibility information of a frame, and uses it to reorder the objects of the following frame. VRO just requires adding a small hardware to capture the visibility information and use it later to guide the rendering of the following frame. Moreover, VRO works in parallel with the graphics pipeline, so negligible performance overheads are incurred. We illustrate the benefits of VRO using various unmodified commercial 3D applications for which VRO achieves 27% speed-up and 14.8% energy reduction on average. \nThen, we focus on avoiding redundant computations related to CPU Collision Detection (CD). Graphics applications such as 3D games represent a large percentage of downloaded applications for mobile devices and the trend is towards more complex and realistic scenes with accurate 3D physics simulations. CD is one of the most important algorithms in any physics kernel since it identifies the contact points between the objects of a scene and determines when they collide. However, real-time accurate CD is very expensive in terms of energy consumption. We propose Render Based Collision Detection (RBCD), a novel energy-efficient high-fidelity CD scheme that leverages some intermediate results of the rendering pipeline to perform CD, so that redundant tasks are done just once. Comparing RBCD with a conventional CD completely executed in the CPU, we show that its execution time is reduced by almost three orders of magnitude (600x speedup), because most of the CD task of our model comes for free by reusing the image rendering intermediate results. Although not necessarily, such a dramatic time improvement may result in better frames per second if physics simulation stays in the critical path. However, the most important advantage of our technique is the enormous energy savings that result from eliminating a long and costly CPU computation and converting it into a few simple operations executed by a specialized hardware within the GPU. Our results show that the energy consumed by CD is reduced on average by a factor of 448x (i.e., by 99.8\%). These dramatic benefits are accompanied by a higher fidelity CD analysis (i.e., with finer granularity), which improves the quality and realism of the application.||2018|10.1007/978-3-319-91590-6_14|Enrique de Lucas|0.14285714285714285|2
1581|GPU-accelerated depth map generation for X-ray simulations of complex CAD geometries|Interactive x-ray simulations of complex computer-aided design (CAD) models can provide valuable insights for better interpretation of the defect signatures such as porosity from x-ray CT images. Generating the depth map along a particular direction for the given CAD geometry is the most compute-intensive step in x-ray simulations. We have developed a GPU-accelerated method for real-time generation of depth maps of complex CAD geometries. We preprocess complex components designed using commercial CAD systems using a custom CAD module and convert them into a fine user-defined surface tessellation. Our CAD module can be used by different simulators as well as handle complex geometries, including those that arise from complex castings and composite structures. We then make use of a parallel algorithm that runs on a graphics processing unit (GPU) to convert the finely-tessellated CAD model to a voxelized representation. The voxelized representation can enable heterogeneous modeling of the volume enclosed by the CAD model by assigning heterogeneous material properties in specific regions. The depth maps are generated from this voxelized representation with the help of a GPU-accelerated ray-casting algorithm. The GPU-accelerated ray-casting method enables interactive (> 60 frames-per-second) generation of the depth maps of complex CAD geometries. This enables arbitrarily rotation and slicing of the CAD model, leading to better interpretation of the x-ray images by the user. In addition, the depth maps can be used to aid directly in CT reconstruction algorithms.Interactive x-ray simulations of complex computer-aided design (CAD) models can provide valuable insights for better interpretation of the defect signatures such as porosity from x-ray CT images. Generating the depth map along a particular direction for the given CAD geometry is the most compute-intensive step in x-ray simulations. We have developed a GPU-accelerated method for real-time generation of depth maps of complex CAD geometries. We preprocess complex components designed using commercial CAD systems using a custom CAD module and convert them into a fine user-defined surface tessellation. Our CAD module can be used by different simulators as well as handle complex geometries, including those that arise from complex castings and composite structures. We then make use of a parallel algorithm that runs on a graphics processing unit (GPU) to convert the finely-tessellated CAD model to a voxelized representation. The voxelized representation can enable heterogeneous modeling of the volume enclosed by t...||2018|10.1063/1.5031636|A. Krishnamurthy, R. Grandin, S. Holland, Gavin Young|0.14285714285714285|2
249|Understanding the Impact of Fine-Grained Data Sharing and Thread Communication on Heterogeneous Workload Development|The conventional OpenCL 1.x style CPU-GPU heterogeneous computing paradigm treats the CPU and GPU processors as loosely connected separate entities. At best each executes independent tasks, but, more commonly, the CPU idles while waiting for results from the GPU. No data-sharing and communications are allowed during kernel execution. This model limits the number of applications that can harness the tremendous computing power of two processors. OpenCL 2.x and compliant hardware introduce a new memory model that enables a new computing paradigm where the task-parallel CPU and data-parallel GPU are tightly coupled and can closely cooperate on shared data in a lock-based or non-blocking fashion. This new model maximizes hardware utilization and performance, and opens more applications to GPU acceleration. The most significant new OpenCL 2.x features are fine-grained data sharing and thread communication through shared virtual memory, CPU-GPU cache coherence support, and system-level atomics. However, few applications that can exploit the benefits of tightly coupled CPU-GPU heterogeneous processors have emerged. Programming and debugging in this new environment have proven challenging. The resulting lack of benchmark workloads have also left hardware architects uninformed. To facilitate truly heterogeneous workload development and hardware architecture research, this paper focuses on understanding the impact of fine-grained data sharing between the CPU and GPU for future heterogeneous workload development. To that goal, we identify three CPU-GPU cooperation paradigms, demonstrate their performance benefits on real hardware using both in-house and publicly available benchmarks, and profile their detailed behavior and characteristics using an architectural simulator. Our experiments demonstrate that truly heterogeneous implementations of our studied benchmarks outperform their corresponding conventional CPU or GPU versions by up to 59.5% and 36.6% respectively. We analyze thread contention problems, latency of synchronization operations and inter-cluster memory traffic in each cooperation paradigm using a timing architectural simulator.|International Symposium on Parallel and Distributed Computing|2017|10.1109/ISPDC.2017.16|Xiaoqi Hu, B. Jang, T. Ta, David Troendle|0.125|2
251|Study of GPU Acceleration in Genetic Algorithms for Quantum Circuit Synthesis|In this work we present a comparative study of several GPU accelerated elements of a Genetic Algorithm (GA) for the synthesis of quantum circuits on the level of Electro-Magnetic (EM) pulses. The novelty in our approach is in the implementation: a) a completely GPU accelerated quantum simulator, b) GPU accelerated genetic operators and fitness evaluation and finally c) a set of GPU implemented optimizations for GPU accelerated evolutionary search optimization for the synthesis of quantum circuits. The reason for using EM pulses model for synthesis is the observation that this model requires the largest amount of elementary rotations to implement quantumlogic gates and thus provides a good measure to evaluate the efficiency of the acceleration by the GPU processor. The reason to use a GA is the advantage of pseudo evolutionary search in very large problem space such as the one defined by the Ising model where the EM realized quantum circuits are evolved. As a result of the several GPU optimizations several new circuits implementations are presented and their cost is compared to thecurrently known Ising model implementations.|IEEE International Symposium on Multiple-Valued Logic|2017|10.1109/ISMVL.2017.24|Georgiy Krylov, M. Lukac|0.125|2
260|Modeling and Rendering of convective Cumulus Clouds for Real-Time Graphics Purposes|The paper presents a simulation and rendering model of three dimensional covective cloud evolution. The model is physically based, however its purpose is graphical. The main stress is put on balancing two parts of a model: the atmsphere simulation with convective motion of air and water vapor combined with rendering of semi-transparent and light-scattering clouds, in order to achieve realistic animation in real-time. We examine and compare two algorithmic approaches based on CPU and GPU computations.|Computer Science|2017|10.7494/csci.2017.18.3.1491|W. Alda, Pawel Kobak|0.125|2
313|A Micro-Macro Framework for Analyzing Steric and Hydrodynamic Interactions in Gliding Assays|Macroscopic flows of filament-motor mixtures, driven by the hydrolysis of ATP, are important to many cellular processes such as cytoplasmic streaming in Drosophila oocytes and cortical flow in the first cell division of C. elegans. Gliding assays, reduced in vitro model systems where motor proteins adsorbed onto a planar substrate bind to and move filaments, recreate large-scale dynamic patterns like coherent swarming motion and density waves. These systems are sensitive to the microscopic behavior such as motor protein binding and unbinding dynamics, which take place on a faster timescale than the direct and fluid-mediated filament interactions. In this work, we present a multiscale modeling and simulation framework for gliding assays that allows detailed microscopic motor modeling as well as both steric and hydrodynamic interactions between filaments. Our model is based on continuum kinetic theory, and our implementation utilizes CPU and GPU parallelism to track the sparse but high-dimensional state spa...|Multiscale Modeling & simulation|2017|10.1137/17M1113503|Christel Hohenegger, Tamar Shinar, Steven C. Cook|0.125|2
314|Simulation of Deformable Objects using GLSL 4.3|In this research, we implement a deformable object simulation system using OpenGL's shader language, GLSL4.3. Deformable object simulation is implemented by using volumetric mass-spring system suitable for real-time simulation among the methods of deformable object simulation. The compute shader in GLSL 4.3 which helps to access the GPU resources, is used to parallelize the operations of existing deformable object simulation systems. The proposed system is implemented using a compute shader for parallel processing and it includes a bounding box-based collision detection solution. In general, the collision detection is one of severe computing bottlenecks in simulation of multiple deformable objects. In order to validate an efficiency of the system, we performed the experiments using the 3D volumetric objects. We compared the performance of multiple deformable object simulations between CPU and GPU to analyze the effectiveness of parallel processing using GLSL. Moreover, we measured the computation time of bounding box-based collision detection to show that collision detection can be processed in real-time. The experiments using 3D volumetric models with 10K faces showed the GPU-based parallel simulation improves performance by 98% over the CPU-based simulation, and the overall steps including collision detection and rendering could be processed in real-time frame rate of 218.11 FPS.|KSII Transactions on Internet and Information Systems|2017|10.3837/tiis.2017.08.021|Nak-Jun Sung, Min Hong, Seung-Hyun Lee, Yoo-Joo Choi|0.125|2
321|Kinetic Compressive Sensing|Parametric images provide insight into the spatial distribution of physiological parameters, but they are often extremely noisy, due to low SNR of tomographic data. Direct estimation from projections allows accurate noise modeling, improving the results of post-reconstruction fitting. We propose a method, which we name kinetic compressive sensing (KCS), based on a hierarchical Bayesian model and on a novel reconstruction algorithm, that encodes sparsity of kinetic parameters. Parametric maps are reconstructed by maximizing the joint probability, with an Iterated Conditional Modes (ICM) approach, alternating the optimization of activity time series (OS-MAP-OSL), and kinetic parameters (MAP-LM). We evaluated the proposed algorithm on a simulated dynamic phantom: a bias/variance study confirmed how direct estimates can improve the quality of parametric maps over a post-reconstruction fitting, and showed how the novel sparsity prior can further reduce their variance, without affecting bias. Real FDG PET human brain data (Siemens mMR, 40min) images were also processed. Results enforced how the proposed KCS-regularized direct method can produce spatially coherent images and parametric maps, with lower spatial noise and better tissue contrast. A GPU-based open source implementation of the algorithm is provided.|Nuclear Science Symposium and Medical Imaging Conference|2017|10.1109/NSSMIC.2017.8532954|D. Greve, L. Landini, C. Catana, J. Price, M. Santarelli, S. Pedemonte, M. Scipioni|0.125|2
335|Assessing the feasibility of OpenCL CPU implementations for agent-based simulations|Agent-based modeling (ABM) is a bottom-up modeling approach, where each entity of the system being modeled is uniquely represented as a self-determining agent. Large scale emergent behavior in ABMs is population sensitive. As such, it is advisable that the number of agents in a simulation is able to reflect the reality of the system being modeled. This means that in domains such as social modeling, ecology, and biology, systems can contain millions or billions of individuals. Such large scale simulations are only feasible in non-distributed scenarios when the computational power of commodity processors, such as GPUs and multi-core CPUs, is fully exploited. In this paper we evaluate the feasibility of using CPU-oriented OpenCL for high-performance simulations of agent-based models. We compare a CPU-oriented OpenCL implementation of a reference ABM against a parallel Java version of the same model. We show that there are considerable gains in using CPU-based OpenCL for developing and implementing ABMs, with speedups up to 10x over the parallel Java version on a 10-core hyper-threaded CPU.|International Workshop on OpenCL|2017|10.1145/3078155.3078174|Nuno Fachada, A. Rosa|0.125|2
360|Solving global shallow water equations on heterogeneous supercomputers|The scientific demand for more accurate modeling of the climate system calls for more computing power to support higher resolutions, inclusion of more component models, more complicated physics schemes, and larger ensembles. As the recent improvements in computing power mostly come from the increasing number of nodes in a system and the integration of heterogeneous accelerators, how to scale the computing problems onto more nodes and various kinds of accelerators has become a challenge for the model development. This paper describes our efforts on developing a highly scalable framework for performing global atmospheric modeling on heterogeneous supercomputers equipped with various accelerators, such as GPU (Graphic Processing Unit), MIC (Many Integrated Core), and FPGA (Field Programmable Gate Arrays) cards. We propose a generalized partition scheme of the problem domain, so as to keep a balanced utilization of both CPU resources and accelerator resources. With optimizations on both computing and memory access patterns, we manage to achieve around 8 to 20 times speedup when comparing one hybrid GPU or MIC node with one CPU node with 12 cores. Using a customized FPGA-based data-flow engines, we see the potential to gain another 5 to 8 times improvement on performance. On heterogeneous supercomputers, such as Tianhe-1A and Tianhe-2, our framework is capable of achieving ideally linear scaling efficiency, and sustained double-precision performances of 581 Tflops on Tianhe-1A (using 3750 nodes) and 3.74 Pflops on Tianhe-2 (using 8644 nodes). Our study also provides an evaluation on the programming paradigm of various accelerator architectures (GPU, MIC, FPGA) for performing global atmospheric simulation, to form a picture about both the potential performance benefits and the programming efforts involved.|PLoS ONE|2017|10.1371/journal.pone.0172583|L. Gan, W. Xue, Guangwen Yang, H. Fu, Xinliang Wang, Chao Yang, Xiaomeng Huang, Lanning Wang|0.125|2
366|Accessing CUDA Features in the OpenGL Rendering Pipeline: A Case Study Using N-Body Simulation|The advances of the graphics programing unit (GPU) architecture and its rapidly evolving towards general purpose GPU make a series of applications adopt a general purpose (GPGPU) and a graphics computing interoperability approach in which the first is used for heavy calculations and the second for 3D graphics rendering. Because GPGPU exposes several hardware features, such as shared memory and thread synchronization mechanism, it allows a developer to write more efficient code. Nevertheless, we conjecture that such hardware features are also available in the graphics computing interface OpenGL 4.5 or later through the graphics concepts: blending, transform feedback, tessellation and instancing. In this paper we assess our conjecture by implementing an N-body simulation with both approaches. We indeed devise a novel non-graphics application to the tessellation hardware and the instanced rendering circuit. Instead of refining a mesh, we use the abstract patch for gaining direct accesses to shared memory. In the place of drawing multiple objects, we apply the instanced rendering technology for improving sequential data accesses. Comparative timing analysis is provided. We believe that these results provide better understanding of the graphics features that are useful for closing the performance gap between OpenGL and a GPGPU architecture, and open a new perspective on implementing solely with the OpenGL graphics applications that require both intense, but pre-specified, memory accesses and 3D graphics rendering.|SIBGRAPI Conference on Graphics, Patterns and Images|2017|10.1109/SIBGRAPI.2017.48|M. S. Camillo, Shin-Ting Wu|0.125|2
370|GPU-accelerated fault dictionary generation for the TRAX fault model|This paper presents the design and implementation of a fault simulator for the TRAnsition-X fault model (TRAX for short) on a graphics processing unit (GPU). Fault dictionaries are an important aspect of on-chip fault detection and diagnosis. Generating a fault dictionary requires fault simulation with no fault dropping, requiring extensive computational resources. The inherent parallelism of the fault simulation problem maps well to the large number of concurrent threads supported by a modern GPU, and a GPU can be used to accelerate the construction of a fault dictionary. Our approach employs both pattern-parallel and fault-parallel algorithms in the GPU kernel implementations. Experiments involving various circuits, including the OpenSPARC T2 processor, demonstrate a speed-up of over 42x.|International Test Conference in Asia|2017|10.1109/ITC-ASIA.2017.8097107|R. D. Blanton, Matthew Beckler|0.125|2
398|Parallel Algorithm for Dynamic Community Detection|Many real systems can be naturally modeled by complex networks. A complex network represents an abstraction of the system regarding its components and their respective interactions. Thus, by scrutinizing the network, interesting properties of the system can be revealed. Among them, the presence of communities, which consists of groups of densely connected nodes, is a significant one. For instance, a community might reveal patterns, such as the functional units of the system, or even groups correlated people in social networks. Albeit important, the community detection process is not a simple computational task, in special when the network is dynamic. Thus, several researchers have addressed this problem providing distinct methods, especially to deal with static networks. Recently, a new algorithm was introduced to solve this problem. The approach consists of modeling the network as a set of particles inspired by a N-body problem. Besides delivering similar results to state-of-the-art community detection algorithm, the proposed model is dynamic in nature; thus, it can be straightforwardly applied to time-varying complex networks. However, the Particle Model still has a major drawback. Its computational cost is quadratic per cycle, which restricts its application to mid-scale networks. To overcome this limitation, here, we present a novel parallel algorithm using many-core high-performance resources. Through the implementation of a new data structure, named distance matrix, was allowed a massive parallelization of the particles interactions. Simulation results show that our parallel approach, running both traditional CPUs and hardware accelerators based on multicore CPUs and GPUs, can speed up the method permitting its application to large-scale networks.|2017 International Symposium on Computer Architecture and High Performance Computing Workshops (SBAC-PADW)|2017|10.1109/SBAC-PADW.2017.18|Á. Fazenda, M. G. Quiles, Hugo Resende|0.125|2
408|Performance Improvement of a Two-Dimensional Flood Simulation Application in Hybrid Computing Environments|This paper presents a practical implementation of a 2D flood simulation model using hybrid distributed-parallel technologies including MPI, OpenMP, CUDA, and evaluations of its performance under various configurations that utilize these technologies. The main objective of this research work was to improve the computational performance of the flood simulation in a hybrid architecture. Modern desktops and small cluster systems owned by domain researchers are able to perform these simulations efficiently due to multicore and GPU computing devices, but lack the expertise needed to fully utilize software libraries designed to take advantage of the latest hardware. By leveraging knowledge of our experimentation environment, we were able to incorporate MPI and multiple GPU devices to improve performance over a single-process OpenMP version up to 18x, depending on the size of the input data. We discuss some observations that have significant effects on overall performance, including process-to-device mapping, communication strategies and data partitioning. The limitations of this work are discussed, and we propose some ideas to relieve or overcome such limitations in future work.|International Symposium on Computing and Networking - Across Practical Development and Theoretical Research|2017|10.1109/CANDAR.2017.106|A. Kalyanapu, Mike Rogers, T. Dullo, Ryan J. Marshall, S. Ghafoor|0.125|2
496|Analysis of the 3D glasses application in a spatial simulation on the basis of the architectural visualization|The arcticle describes preparation of architectural visualization of the virtual museum. It purpose was to explore possibilities of using VR glasses, 3D scans and design aplications. During the study VR glasses showed great opportunities in the field of architectural visualisation. The results of the performance tests confirmed potencial in 3D scans and models from design applications, although they require mesh prcoessing and simplification. However, the tests showed insufficient performance of tested smartphone’s GPU, which shows that market needs to advance in this direction. This would make it possible to create better VR applications.||2017|10.35784/jcsi.619|Mateusz Karol Smaga, M. Plechawska-Wójcik|0.125|2
1244|Development of a simultaneous PET/Ultrasound imaging system with near real-time reconstruction capability for point-of-care applications|In this project, we propose to investigate the feasibility of a novel technology that will bring both PET and ultrasound imaging to the patient bedside to support point-ofcare(PoC) molecular imaging applications. The system will comprise of a panel detector placed behind the patient and a maneuverable probe that consists of a PET detector and an ultrasound transducer. The probe can be moved around a region-of-interest in the patients’ body to collect both PET coincidence events and ultrasound signals. The location of the maneuverable probe relative to the back panel detectors is tracked in real-time as coincidence events are recorded. These events are used for list-mode image reconstruction in near realtime to provide visual feedback to the operator who can interactively control the probe to collect additional counts from the most critical locations and/or angles in order to dynamically optimize the image quality. To prove the concept, we developed a prototype that consists of a single channel SiPM(SensL FB30035) coupled to a 3.0 $\times$3.0$\times$20.0 mm$^{ 3}$ LSO crystal as the back detector. The maneuverable probe consists of an ultrasound transducer and a PMT(Hamamatsu H8500) coupled to 48$\times$48 LSO crystals of 1.0$\times$1.0$\times$10.0 mm$^{ 3}$ each. A robotic arm allows us to position the probe at arbitrary locations. Coincidence timing resolution of 470 ps FWHM has been achieved. We have implemented a GPUbased fully 3D list-mode Time-Of-Flight image reconstruction algorithm that can model the dynamically changing geometry of this PoC system. In this study, we report preliminary results from both actual experiments using this prototype and simulations using GATE. Detector modules with larger sensitive volume will be fabricated and tested to perform more imaging studies in order to explore the capability of this class of system.|Nuclear Science Symposium and Medical Imaging Conference|2017|10.1109/NSSMIC.2017.8532788|R. Wahidi, Y. Tai, Quing Zhu, Ke Li, J. O’Sullivan, Jianyong Jiang, Beichuan Qi, Yifeng Zeng, S. Komarov|0.125|2
1378|Simulation of partially coherent light propagation using parallel computing devices|Light acquires or loses coherence and coherence is one of the few optical observables. Spectra can be derived from coherence functions and understanding any interferometric experiment is also relying upon coherence functions. Beyond the two limiting cases (full coherence or incoherence) the coherence of light is always partial and it changes with propagation. We have implemented a code to compute the propagation of partially coherent light from the source plane to the observation plane using parallel computing devices (PCDs). In this paper, we restrict the propagation in free space only. To this end, we used the Open Computing Language (OpenCL) and the open-source toolkit PyOpenCL, which gives access to OpenCL parallel computation through Python. To test our code, we chose two coherence source models: an incoherent source and a Gaussian Schell-model source. In the former case, we divided into two different source shapes: circular and rectangular. The results were compared to the theoretical values. Our implemented code allows one to choose between the PyOpenCL implementation and a standard one, i.e using the CPU only. To test the computation time for each implementation (PyOpenCL and standard), we used several computer systems with different CPUs and GPUs. We used powers of two for the dimensions of the cross-spectral density matrix (e.g. 324, 644) and a significant speed increase is observed in the PyOpenCL implementation when compared to the standard one. This can be an important tool for studying new source models.|Applications of Optics and Photonics|2017|10.1117/12.2272339|Tiago E. C. Magalhães, J. Rebordão|0.125|2
1400|Combined particle and compartmental dynamics of cell-biological models using hollow spheres on the GPU|In this work the ML-Force method for spatial particle-based cell-biological simulation and the gHSX algorithm and implementation for the novel HSX collision problem are presented. After an overview of existing methods of spatial cell-biological simulation, it becomes clear that there is a need for a new simulation approach, as no existing spatial approach can combine all aspects of space in one unified method. This new approach combines forces, nesting, excluded volumes, changing sizes and arbitrary functions, which is new to the literature. This new approach is well founded in fundamental principles of thermodynamics and can build on a strong existing algorithmic base in molecular dynamics. A simple example model is also investigated. In the particle-based simulation of cell-biological systems in continuous space, a key performance bottleneck is the computation of all possible intersections between particles. This typically rely on solid sphere approaches for collision detection. Existing collision detection algorithms are foud not to be designed for spatial cell-biological modells' hollow spheres (that allow for nesting), because nearly all existing high performance parallel algorithms are focusing on solid sphere interactions. In this work the new problem of computing the intersections among arbitrarily nested hollow spheres of possibly different sizes, thicknesses, positions, and nesting levels is defined. A new algorithm to solve this nested hollow sphere intersection problem is described and implemented for massively parallel execution on graphical processing units (GPUs). A performance study shows 1.8-4x improvements compared to existing GPU approaches.||2017|10.1145/3064911.3064920|Till Köster|0.125|2
1492|Implementation of meso-scale radioactive dispersion model for GPU|Abstract Lagrangian Particle Dispersion Method (LPDM) is applied to model atmospheric dispersion of radioactive material in a meso-scale of a few tens of kilometers for site study purpose. Empirical relationships are used to determine the dispersion coefficient for various atmospheric stabilities. Diagnostic 3-D wind-field is solved based on data from one meteorological station using mass-conservation principle. Particles representing radioactive pollutant are dispersed in the wind-field as a point source. Time-integrated air concentration is calculated using kernel density estimator (KDE) in the lowest layer of the atmosphere. Parallel code is developed for GTX-660Ti GPU with a total of 1 344 scalar processors using CUDA. A test of 1-hour release discovers that linear speedup is achieved starting at 28 800 particles-per-hour (pph) up to about 20 × at 14 4000 pph. Another test simulating 6-hour release with 36 000 pph resulted in a speedup of about 60 ×. Statistical analysis reveals that resulting grid doses are nearly identical in both CPU and GPU versions of the code.||2017|10.3139/124.110646|Z. Suud, Sunarko|0.125|2
1499|Accelerated Adjoint Algorithmic Differentiation with Applications in Finance|Adjoint Differentiation’s (AD) ability to calculate Greeks efficiently and to machine precision while scaling in constant time to the number of input variables is attractive for calibration and hedging where frequent calculations are required. Algorithmic adjoint differentiation tools automatically generates derivative code and provide interesting challenges in both Computer Science and Mathematics. In this dissertation we focus on a manual implementation with particular emphasis on parallel processing using Graphics Processing Units (GPUs) to accelerate run times. Adjoint differentiation is applied to a Call on Max rainbow option with 3 underlying assets in a Monte Carlo environment. Assets are driven by the Heston stochastic volatility model and implemented using the Milstein discretisation scheme with truncation. The price is calculated along with Deltas and Vegas for each asset, at a total of 6 sensitivities. The application achieves favourable levels of parallelism on all three dimensions implemented by the GPU: Instruction Level Parallelism (ILP), Thread level parallelism (TLP), and Single Instruction Multiple Data (SIMD). We estimate the forward pass of the Milstein discretisation contains an ILP of 3.57 which is between the average range of 2-4. Monte Carlo simulations are embarrassingly parallel and are capable of achieving a high level of concurrency. However, in this context a single kernel running at low occupancy can perform better with a combination of Shared memory, vectorized data structures and a high register count per thread. Run time on the Intel Xeon CPU with 501 760 paths and 360 time steps takes 48.801 seconds. The GT950 Maxwell GPU completed in 0.115 seconds, achieving an 422⇥ speedup and a throughput of 13 million paths per second. The K40 is capable of achieving better performance. Acknowledgements With thanks to my supervisors Associate Professor Peter Ouwehand and Associate Professor Michelle Kuttel. GPU computing has been a fascinating journey, and without your help would not have been possible. Further, thank you to the MPhil team for the immensely satisfying body of work and guidance presented during the year. Also, I wish to thank my parents for their support over the years.||2017|10.1109/intlec.2017.8214132|J. D. Beer|0.125|2
214|Using www.sigma-delta.de to rapidly obtain ELD compensated CT ΣΔ modulators|For two decades, Excess-Loop-Delay (ELD) is known to degrade the performance of continuous-time (CT) ΣΔ A/D converters. Many methods have been proposed to compensate for this effect, but for their implementation sophisticated knowledge in loop-filter design is necessary. The web-based design tool for CT ΣΔ modulators www.sigma-delta.de offers a straightforward integration of commonly used ELD compensation techniques in an early stage of the design process without the need of in depth knowledge of ΣΔ loop-filters. As the tool uses a heuristic search, based on a genetic algorithm within a parallel implementation on a GPU, it provides results with a very short response time. This paper presents the compensation techniques which are commonly implemented within the state of the art and shows their automatic application on architectural level by the design tool. Exemplary modulators are created and evaluated in a circuit level simulator. Further, the calculation of device parameters for a circuit-level simulator model, based on the coefficients obtained by the tool, is illustrated. Thereby, the usage of the developed and publicly available design tool for CT ΣΔ modulator is practically shown.|International Symposium on Circuits and Systems|2016|10.1109/ISCAS.2016.7527542|J. Wagner, M. Ortmanns, R. Ritter|0.1111111111111111|2
232|A Comparative Study of Heterogeneous Processor Simulators|In 1970’s, Gordon Moore perceived that the number of transistors in a processor would double after every 18 months. With the addition of more transistors on a single-chip, a processor’s energy consumption increases exponentially. The solution to this problem is heterogeneous processors and machines. Heterogeneous machine is the combination of CPU and GPU platforms. Computer architecture is shifting from multi-core to heterogeneous era. Generally, computer architects practice of software simulation to model and analyze their ideas. Today, computer architects are using cycle-level simulators to discover and analyze new processor designs. To search the heterogeneous system design-space, we review and practically analyze heterogeneous simulators and their performance. In this study, we present a detailed comparative analysis of gem5-gpu, gem5, and multi2sim simulators.||2016|10.5120/IJCA2016911316|Muhammad Aleem, Muhammad Arshad Islam, Shagufta, M. Iqbal|0.1111111111111111|2
237|TU-H-CAMPUS-IeP2-04: Efficient Binary Tree Description of High Resolution Breast Models for in Silico Imaging.|PURPOSE\nWe introduce a new geometric model for Monte Carlo (MC) simulation of breast tomosynthesis and mammography images with very high resolution digital breast phantoms voxelized at resolutions down to 50 microns or less. This fine sampling of the anatomy is required to reproduce the spiculated lesions and microcalcifications visible in clinical mammograms with pixel sizes under 100 microns. Phantoms voxelized at this resolution require such a large amount of computer memory that it might not be possible to execute the simulation in regular computers or in Graphics Processing Units (GPU), limiting the realism that can be achieved in virtual clinical trials.\n\n\nMETHODS\nThe standard voxel-based geometry model used in MC codes is inefficient in terms of memory use, because uniform regions are described with a large number of identical voxels. We have developed a new geometric model based on a binary tree partition of space. The binary tree recursively divides the object in two equal halves, alternating the x, y, and z dimensions, until a uniform volume or a final voxel are found. A top-down search of the tree structure is used to determine the exact composition of a particular voxel during the ray-tracing process.\n\n\nRESULTS\nThe binary tree geometry was implemented in PENELOPE and its GPU-accelerated version MC-GPU. An example tomosynthesis scan was simulated using a computational breast phantom voxelized at 50 micron. This phantom was too large to fit in the GPU memory directly, but its binary tree version required 10 times less memory. The time spend searching the binary tree reduced the simulation speed in half approximately.\n\n\nCONCLUSION\nWe have introduced a new geometry model for in silico imaging of very high resolution phantoms. The binary tree model increases the computation time but, more importantly, enables the simulation of more detailed phantoms than previously possible.|Medical Physics (Lancaster)|2016|10.1118/1.4957682|A. Badano, R. Zeng, F. Samuelson, C. Graff, A. Badal|0.1111111111111111|2
273|Accelerating Numerical Ice Engineering Tools Using GPGPU|C-CORE is engaged in understanding the iceberg and sea ice design loads needs of the energy sector. As the energy industry ventures into oceans with greater ice cover and more icebergs, there is a significant need for efficient engineering tools to plan and manage operations in exploration, production, and safety. Industry requires a range of scenarios for their risk assessments, where existing simulations can be computationally and time intensive. C-CORE has recently started using the benefit of the General Purpose Computing on Graphical Processing Units (GPGPU) approach. This approach has shown significant speed up of several numerical ice engineering applications related to icebergs and sea ice. The investigated model types are Monte-Carlo type approaches for probabilistic design method, and quadratic discriminant. GPU computing with Compute Unified Device Architecture (CUDA) is a new approach to solve complex problems and transform the GPU into a massively parallel processor. The present study applies the GPGPU technology to a Monte-Carlo simulation, used for a sea ice load application. The objective of this study is to measure the performance of the GPU using CUDA, and compare against the serial Central Processing Unit (CPU) using C++ and MATLAB implementations. Results show a speedup of up to 2,600 times of the GPGPU implementation compared to the MATLAB implementation, reducing the elapsed time from about 1.5 hour to about 2 seconds. This strongly indicates that the GPGPU approach can help the industry to significantly reduce the time required for the simulations.||2016|10.4043/27386-MS|Jan Thijssen, Shadi G. Alawneh, M. Richard|0.1111111111111111|2
295|GPU Delegation: Toward a Generic Approach for Developping MABS using GPU Programming|Using Multi-Agent Based Simulation (MABS), computing resources requirements often limit the extent to which a model could be experimented. As the number of agents and the size of the environment are constantly growing in these simulations, using General-Purpose Computing on Graphics Units (GPGPU) appears to be very promising as it allows to use the massively parallel architecture of the GPU (Graphics Processing Unit) to do High Performance Computing (HPC). Considering the use of GPGPU for developing MABS, the conclusions of Perumalla and Aaby's work \cite{Perumalla2008} in 2008 was twofold: (1) data parallel execution capabilities of GPU can be used effectively in MABS and afford excellent speedup on models and (2) effective use of data parallel execution requires resolution of modeling and execution challenges at the cost of a decrease in modularity, ease of programmability and reusability. In this paper, we propose to study through experiments if the conclusions and issues outlined by Perumalla and Aaby are still true despite the evolution of GPGPU and MABS. To this end, we use the GPU environmental delegation principle on four models in order to compare CPU and GPU implementations. Then, we discuss and analyze the results from both a conceptual and a performance point of view.|Adaptive Agents and Multi-Agent Systems|2016|10.1007/978-3-319-39387-2_11|F. Michel, Emmanuel Hermellin|0.1111111111111111|2
332|Energy based dissolution simulation using smoothed particle hydrodynamic sampling|Fluid simulation plays an important role in Computer Graphics and has wide applications in film and games. The desire for an improved physically-based fluid simulation solver has grown hand in hand with the advances made in Computer Graphics. Interesting fluid behaviours emerge when solid objects are added to a simulation: when fluid and solid make contact, they do not only have a physical interaction (e.g., buoyancy), but also a chemical reaction (e.g., dissolution) under the right conditions. Dissolution is one of the most common natural phenomena which is an important visual effect in fluid simulation. However this phe- nomenon is difficult to simulate due to the complexity of the behaviour and there are only few techniques available. A novel unified particle-based method for approximating chemical dis- solution is introduced in this thesis which is fast, predictable and visually plausible. The dissolution algorithm is derived using chemical Collision Theory and integrated into a Smoothed Particle Hydrodynamics (SPH) framework. The Collision Theory of chemistry is used as an analogy to the dissolution process modelling. Dissolution occurs when solute submerges into solvent. Physical laws govern the local excitation of so- lute particles based on the relative motion with solvent particles. When the local excitation energy exceeds a user specified threshold (activation energy), the particle will be dislodged from the solid. Unlike previous methods, this dissolution model ensures that the dissolution result is in- dependent of solute sampling resolution. A mathematical relationship is also established between the activation energy, the interfacial surface area, and the total dissolution time — allowing for intuitive artistic con- trol over the global dissolution rate. Applications of this method are demonstrated using a number of practical examples, including antacid pills dissolving in water and hydraulic erosion of non-homogeneous ter- rains. Both solutes and solvents are represented by particles, and the dis- tribution of the solute particles greatly affects the plausibility of the dissolution simulation. An even but stochastic distribution of particles on both the surface and within the volume of the solute is essential for a good visual simulation of the dynamic process of dissolution. A new iterative particle-based sampling method derived from SPH is introduced in this thesis which can generate a range of blue noise pat- terns and is computationally efficient, controllable and has a variety of applications. This approach resolves many of the limitations of classic blue noise methods, such as the lack of controllability or varying the dis- tribution properties of the generated samples. Fast sampling is achieved in general dimensions for curves, surfaces and volumes. By varying a sin- gle parameter, the proposed method can generate a range of controllable blue noise samples with different distribution properties which are suit- able for various applications such as adaptive sampling and multi-class sampling. The SPH sampling approach is used for solute particle distribution which guarantees a predictable and smooth dissolution process thanks to the evenly distributed density and also gives the user control of the volume change during the phase transition. The proposed SPH sampling method achieves better visual effects compared with simple grid sampling and other blue noise sampling methods. Our energy based dissolution simulation with SPH sampled solute and solvent ensures that the dissolution behaviour is physically and chemi- cally plausible, while supporting features such as object separation and sharp feature rounding. The simulation is parallelized per particle on a GPU to enhance the performance.||2016|10.7498/aps.65.244701|Min Jiang|0.1111111111111111|2
343|An initial investigation of the performance of GPU-based swept time-space decomposition|Simulations of physical phenomena are essential to the expedient design of precision components in aerospace and other high-tech industries. These phenomena are often described by mathematical models involving partial differential equations (PDEs) without exact solutions. Modern design problems require simulations with a level of resolution that is difficult to achieve in a reasonable amount of time even in effectively parallelized solvers. Though the scale of the problem relative to available computing power is the greatest impediment to accelerating these applications, significant performance gains can be achieved through careful attention to the details of memory accesses. Parallelized PDE solvers are subject to a trade-off in memory management: store the solution for each timestep in abundant, global memory with high access costs or in a limited, private memory with low access costs that must be passed between nodes. The GPU implementation of swept time-space decomposition presented here mitigates this dilemma by using private (shared) memory, avoiding internode communication, and overwriting unnecessary values. It shows significant improvement in the execution time of the PDE solvers in one dimension achieving speedups of 6-2x for large and small problem sizes respectively compared to naive GPU versions and 7-300x compared to parallel CPU versions.|arXiv.org|2016|10.25080/majora-629e541a-00c|Kyle E. Niemeyer, Daniel J. Magee|0.1111111111111111|2
359|Accelerating the 3D euler atmospheric solver through heterogeneous CPU-GPU platforms|In climate change studies, the atmospheric model is an essential component for building a high-resolution climate simulation system. While the accuracy of atmospheric simulations has long been limited by the computational capabilities of CPU platforms, the heterogeneous platforms equipped with accelerators are becoming promising candidates for achieving high simulating performance. However, due to the complex algorithms and the heavy communications, atmospheric developers have to face to the tough challenges from both the algorithmic and architectural aspects. In this paper, we propose a hybrid algorithm to accelerate the solver of Euler atmospheric equations, which are the most essential equation sets to simulate the mesoscale atmospheric dynamics. Based on the heterogeneous CPU-GPU platform, we develop a 3-dimensional domain decomposition mechanism, which can achieve more efficient utilization of the computing resources. Furthermore, an extensive set of optimization techniques is applied to boost the performance of the solver on both the host and accelerator side. Compared with the performance of fully-optimized two 6-core CPU version, the optimized Euler solver can achieve a speedup of 6.64x when running on a hybrid node with two 6-core Intel Xeon E5645 CPUs and one Tesla K20c GPU. In addition, a nearly linear weak scaling result is achieved on a cluster with 12 CPU-GPU nodes. The experimental results demonstrate promising possibility to apply heterogeneous architecture in the study of the atmospheric simulation.|Conf. Computing Frontiers|2016|10.1145/2903150.2903480|Chao Yang, Wei Xue, Jingheng Xu, L. Gan, Guangwen Yang, H. Fu|0.1111111111111111|2
365|Predicting performance of Smoothed Particle Hydrodynamics codes at large scales|We present performance prediction studies and trade-offs of Smoothed Particle Hydrodynamics (SPH) codes that rely on a Hashed OctTree data structure to efficiently respond to neighborhood queries. We use the Performance Prediction Toolkit (PPT) to (i) build a loop-structure model (SPHSim) of an SPH code, where parameters capture the specific physics of the problem and method controls that SPH offers, (ii) validate SPHSim against SPH runs on mid-range clusters, (iii) show strong- and weak-scaling results for SPHSim, which test the underlying discrete simulation engine, and (iv) use SPHSim to run design parameter scans showing trade-offs of interconnect latency and physics computation costs across a wide range of values for physics, method and hardware parameters. SPHSim is intended to be a computational physicist tool to quickly predict the performance of novel algorithmic ideas on novel exascale-style hardware such as GPUs with a focus on extreme parallelism.|Online World Conference on Soft Computing in Industrial Applications|2016|10.1109/WSC.2016.7822229|R. Pavel, David Nicholaeff, S. Eidenbenz, Guillaume Chapuis|0.1111111111111111|2
384|ODoST: Automatic Hardware Acceleration for Biomedical Model Integration|Dynamic biomedical systems are mathematically described by Ordinary Differential Equations (ODEs) and their solution is often one of the most computationally intensive parts in biomedical simulations. With high inherent parallelism, hardware acceleration based on Field-Programmable Gate Arrays (FPGAs) has great potential to increase the computational performance of the model simulations, while being very power-efficient. However, the manual hardware implementation is complex and time consuming. The advantages of FPGA designs can only be realised if there is a general solution to automate the process. In this article, we propose a domain-specific high-level synthesis tool called ODoST that automatically generates an FPGA-based Hardware Accelerator Module (HAM) from a high-level description. In this direct approach, ODE equations are directly mapped to processing pipelines without any intermediate architecture layer of processing elements. We evaluate the generated HAMs on real hardware based on their resource usage, processing speed, and power consumption, and compare them with CPUs and a GPU. The results show that FPGA implementations can achieve 15.3 times more speedup compared to a single core CPU solution and perform similarly to an auto-generated GPU solution, while the FPGA implementations can achieve 14.5 times more power efficiency than the CPU and 3.1 times compared to the optimised GPU solution. Improved speedups are foreseeable based on further optimisations.|ACM Transactions on Reconfigurable Technology and Systems|2016|10.1145/2870639|C. Bradley, Ting-Rong Yu, O. Sinnen|0.1111111111111111|2
386|Simulation-Based Sailboat Trajectory Optimization using On-Board Heterogeneous Computers|A dynamic programming-based algorithm adapted to on-board heterogeneous computers for simulation-based trajectory optimization was studied in the context of high-performance sailing. The algorithm can efficiently utilize all OpenCL-capable devices, starting the computation (if necessary, in singleprecision) on a GPU and finalizing it (if necessary, in double-precision) with the use of a CPU. The serial and parallel versions of the algorithm are presented in detail. Possible extensions of the basic algorithm are also described. The experimental results show that contemporary heterogeneous on-board/mobile computers can be treated as micro HPC platforms. They offer high performance (the OpenCL-capable GPU was found to accelerate the optimization routine 41 fold) while remaining energy and cost efficient. The simulation-based approach has the potential to give very accurate results, as the mathematical model upon which the simulator is based may be as complex as required. The black-box represented performance measure and the use of OpenCL make the presented approach applicable to many trajectory optimization problems.|Computer Science|2016|10.7494/csci.2016.17.4.461|Roman Dębski|0.1111111111111111|2
392|Bit-vectorized GPU implementation of a stochastic cellular automaton model for surface growth|Stochastic surface growth models aid in studying properties of universality classes like the Kardar-Parisi-Zhang class. High precision results obtained from large scale computational studies can be transferred to many physical systems. Many properties, such as roughening and some two-time functions can be studied using stochastic cellular automaton (SCA) variants of stochastic models. Here we present a highly efficient SCA implementation of a surface growth model capable of simulating billions of lattice sites on a single GPU. We also provide insight into cases requiring arbitrary random probabilities which are not accessible through bit-vectorization.|International Conference on Intelligent Engineering Systems|2016|10.1109/INES.2016.7555127|G. Ódor, J. Kelling, S. Gemming|0.1111111111111111|2
396|Expérimentation du principe de délégation GPU pour la simulation multiagent. Les boids de Reynolds comme cas d'étude|General-Purpose Computing on Graphics Processing Units (GPGPU) allows to extend the scalability and performances of Multi-Agent Based Simulations (MABS). However, GPGPU requires the underlying program to be compliant with the specific architecture of GPU devices, which is very constraining. In this context, the GPU Environmental Delegation of Agent Perceptions principle has been proposed to ease the use of GPGPU for MABS. The idea is to identify in the model some computations which can be transformed into environmental dynamics and then translated into GPU modules. In this paper, we further trial this principle by testing its feasibility and genericness on a classic ABM, namely Reynolds's boids. The paper then shows that applying GPU delegation not only speeds up boids simulations but also produces an ABM which is easy to understand, thanks to a clear separation of concerns.|Rev. d'Intelligence Artif.|2016|10.3166/ria.30.109-132|Emmanuel Hermellin, F. Michel|0.1111111111111111|2
423|Computational Framework for in-Silico Study of Virtual Cell Biology via Process Simulation and Multiscale Modeling|The behavior of biological cells is governed by a myriad of bio-chemical pathways, whose function within the cell is to elicit a specific behavior. Each biological pathway is defined by dozens of interacting molecular species and hundreds of chemical reactions. A computational framework for high-fidelity simulation study of such complex pathways is invaluable to understanding the behavior of cells, effective drug design, studying various pathological ailments, etc. All-molecule stochastic simulation of such pathways is more accurate in reproducing the behavior of biological cells than the simulation tools based on ODE and PDE (Ordinary and Partial Differential Equations) modeling. However, the process is computationally demanding as just a single cell's function may be governed by dozens of biochemical pathways, hundreds of reactions, thousands of chemical species, and millions of molecules. The all-molecule process simulation framework presented in our previous work captures the stochastic and dynamic behavior of biological systems in ways which cannot be studied via traditional ODE/PDE-based simulations and is computational more efficient and scalable compared to Kinetic Monte-Carlo (KMC) and Chemical Master Equation based approaches. This paper is aimed at studying the framework for the purpose of improving both the over-arching algorithm and implementation. The properties studied include weak and strong scalability, performance analysis on massively parallel platforms (various GPU devices), accuracy of molecular concentrations, and error analysis of the simulation framework. These results demonstrate the computational efficiency as well as help identify the bottlenecks in the pipeline. Lastly, the accuracy of the framework is shown to be in excellent agreement with that of ODE-based simulations, and the error analysis demonstrates the granularity of time step is closely tied with the instantaneous standard deviation of concentration.|ACM International Conference on Bioinformatics, Computational Biology and Biomedicine|2016|10.1145/2975167.2975207|Luka Djapic, Morisa Manzella, Hanyu Jiang, N. Ganesan|0.1111111111111111|2
441|Protein folding prediction|Protein folding is the process that a protein molecule transforms from the linear polymer of peptides to a three-dimensional native structure with specific biological function. By now, the protein folding problem has been studied for more than 50 years and already became a broad and active research field. To answer the 58th question raised by Science in 2005, in this article we briefly reviewed the background and research history of the protein folding problem, and introduced the progresses of protein folding prediction research from four aspects: the protein folding process prediction (protein folding simulation), the folding process related parameter prediction, the protein folding result prediction (protein structure prediction), and the folding result related parameter prediction. The studies on the protein folding problem began in the 60s of 20th century, with the efforts to seek a solution to the paradox that a protein can actually form a native 3D structure in only several seconds but the time scale estimated by a thermodynamic ergodic hypothesis would be longer than the age of universe. Computer simulation is an important approach for protein folding study. The protein models can be classified into 3 categories: lattice model, off-lattice model and all-atom model. The current knowledge about protein folding mechanism is based on the concept of folding funnel on a free-energy landscape, and the current opinion is that the protein folding mechanism is not unique for the whole protein universe and that there may exist a continuum between the two extreme ends of hierarchical folding and nucleation folding scenarios. The hardware for protein folding simulation was becoming more powerful; distributed systems (e.g, Folding@home), special-purpose machines (e.g, ANTON), and GPU-based platforms have been developed for protein folding simulation. Meanwhile, the folding simulation software was continuously enhanced. An important issue in protein folding simulation is to overcome the local energy barrier to find the global energy minimum; several approaches such as replica-exchange, multi-scale modeling and Modeling Employing Limited Data (MELD) were developed to tackle this issue; human intelligence involvement (e.g, “Foldit” Game) is another interesting effort. During the past two decades, the ability of protein folding simulation was continuously rising. For now, the folding simulation for the proteins with dozens of amino acids can reach a time scale of millisecond, while the protein size able to do effective folding simulation is around 100 amino acids. The targets of protein folding simulation have been largely expanded and now include both the in vitro and the in vivo folding such as co-translational folding, chaperone-assistant folding, small-molecule- induced folding and metal-coupled folding. Folding rate and folding type are two important parameters related with the protein folding process and now they can be predicted by statistical and machine-learning approaches based on different levels of structural features such as the topological properties of tertiary structure, the contents of secondary structure and the amino acid frequencies of primary structure. The result of a protein folding process is the formation of a protein structure. According to the hierarchy of structural organization, the protein structure prediction problem includes secondary structure prediction, tertiary structure prediction and quaternary structure prediction. By now, the secondary structure prediction algorithm has experienced five generations and the current accuracy is about 80% for 3-classes prediction. The tertiary structure prediction approaches mainly include two categories: template-based modeling and free modeling, with the former having higher accuracy and the latter having larger application scope. The quaternary structure prediction includes the prediction of complex structure and the prediction of the possibility of protein-protein interaction, and these predictions can be performed based on protein 3D structure or merely amino acid sequence. Structure related parameter prediction also attracted research interests, including the predictions of protein structural classes, secondary structure contents, disordered regions, solvent accessible surface region and the amino acid contacting pairs in the interface of protein-protein interaction. In the end, some possible development directions worth noticing in the future of protein folding research were suggested and they are: the coupling between protein folding and binding, the fusion of protein folding research with systems biology and the application of deep-learning techniques in the field of protein folding prediction.||2016|10.1360/N972016-00658|Bin-Guang Ma|0.1111111111111111|2
1159|GPU accelation of parallel range alignment in ISAR real-time imaging|In the traditional range alignment algorithm of inverse synthetic aperture radar (ISAR) imaging, strong data coupling reduces the efficiency of the algorithm extremely. To decouple the data to the largest extent, a fine-grained optimized parallel range alignment algorithm based on logarithmic steps is proposed in this paper, and the parallel implement of ISAR imaging on Graphic Processing Unit (GPU) is designed as well, which provides a new way for real-time ISAR imaging. Experiments were carried out on a blade server with the GPU accelerator (NVidia K80) based on simulated target model. Results show that the GPU-based parallel algorithm is faster (up to 5 times) than the original serial algorithm, and the time consuming almost keep constant under increasing sample rate or longer accumulation time, which make it suitable for real-time imaging.|Radar|2016|10.1109/RADAR.2016.8059449|Jie Xu, Kai Wu, T. Su|0.1111111111111111|2
1188|Strategies for Phylogenetic Reconstruction - For the Maximum Parsimony Problem|The phylogenetic reconstruction is considered a central underpinning of diverse field of biology like: ecology, molecular biology and physiology. The main example is modeling patterns and processes of evolution. Maximum Parsimony (MP) is an important approach to solve the phylogenetic reconstruction by minimizing the total number of genetic transformations, under this approach different metaheuristics have been implemented like tabu search, genetic and memetic algorithms to cope with the combinatorial nature of the problem. In this paper we review different strategies that could be added to existing implementations to improve their efficiency and accuracy. First we present two different techniques to evaluate the objective function by using CPU and GPU technology, then we show a Path-Relinking implementation to compare tree topologies and finally we introduces the application of these techniques in a Simulated Annealing algorithm looking for an optimal solution.|Bioinformatics|2016|10.5220/0005702902260236|K. Ortiz, D. Lesaint, Jean-Michel Richer|0.1111111111111111|2
1261|Coarse-grained component concurrency in Earth System modeling|Climate models represent a large variety of processes on a variety of time and space scales, a canonical example of multi-physics multi-scale modeling. Current hardware trends, such as GPUs and MICs, are based on marginal increases in clock speed, coupled with vast increases in concurrency, particularly at the fine grain. Multi-physics codes face particular challenges in achieving fine-grained concurrency, as different physics and dynamics components have different computational profiles, and universal solutions are hard to come by. 5 We propose here one approach for multi-physics codes. These codes are typically structured as components interacting via software frameworks. The component structure of a typical Earth system model consists of a hierarchical and recursive tree of components, each representing a different climate process or dynamical system. This recursive structure generally encompasses a modest level of concurrency at the highest level (e.g atmosphere and ocean on different processor sets) with serial organization underneath. 10 We propose to extend concurrency much further by running more and more lower- and higher-level components in parallel with each other. Each component can further be parallelized on the fine grain, potentially offering a major increase in scalability of Earth system models. We present here first results from this approach, called Coarse-grained Component Concurrency. Within the GFDL Flexible Modeling System, the atmospheric radiative transfer component has been configured to run in parallel with the atmospheric 15 dynamics and all other atmospheric physics components. We will explore the algorithmic challenges involved in such an approach, and present results from such simulations. Plans to achieve even greater levels of coarse-grained concurrency by extending this approach within other components such as the ocean, will be discussed.||2016|10.5194/GMD-2016-114|V. Balaji, I. Held, R. Benson, B. Wyman|0.1111111111111111|2
1342|Forest of octree DSMC simulations of flow through porous media|In this work, a linear space filling Morton Z-curve is employed to represent the three dimensional octree structure in an array. The advantages and implementation of this linearized octree for the Direct Simulation Monte Carlo (DSMC) method is demonstrated. A hybrid MPI-CUDA multi-GPU solver is used to model gas flow through two types of immersed bodies, a fractal-like spherical aggregate and a fibrous microstructure of a Morgan carbon Felt material. The permeability of this material is calculated by modeling the diffusion of argon gas and the calculated continuum permeability values match very well with other published data. Strong scaling has shown that the multi-GPU octree -based DSMC solve is 85% effcient with 16 GPUs for a large-scale problem.||2016|10.1063/1.4967559|R. Jambunathan, D. Levin|0.1111111111111111|2
1346|Higher-order finite difference time domain algorithms for room acoustic modelling|The acoustic qualities of indoor spaces are fundamental to the intelligibility of speech, the quality of musical performances, and perceived noise levels. Computationally heavy wave-based acoustic modelling algorithms have gained momentum in the field of room acoustic modelling, as ever-increasing computational power makes their use more feasible. Most notably the Finite Difference Time Domain (FDTD) method is often employed for rendering the low- and mid-frequency part of room impulse responses (RIRs). However, this algorithm has known disadvantages, most prominently dispersion error, which renders a large part of the simulated RIR invalid. \n \nThis thesis is concerned with the implementation and analysis of higher-order FDTD stencils as a means to improve the current state-of-art FDTD methods that solve the room acoustic wave equation. A detailed analysis of dispersive properties, stability, and required grid spacing of current and higher-order stencils is presented, and has been verified using a GPU implementation of the different algorithms. It is argued that the 4th-order stencil gives the best result in terms of output quality versus computational effort. In addition, this thesis focusses on the derivation of absorbing boundaries for the 4th-order scheme, its stability analysis, and detailed analysis of absorptive properties compared to established boundary models for 2nd-order schemes. \n \nThe newly proposed 4th-order scheme and its boundaries are tested in two case studies: a large shoebox model, in order to test the validity against a common benchmark and a complex acoustic space. For the latter study, impulse responses were measured in the National Centre for Early Music in York, UK, and computationally generated using the current state-of-the-art as well as the proposed 4th-order FDTD algorithm and boundaries. It is shown that the 4th-order stencil gives at least as good as, or better results than those achieved using the 2nd-order stencil, at lower computational costs.||2016|10.1049/cp.2016.0296|J. V. Mourik|0.1111111111111111|2
1371|Combination of Cloud Computing and High Performance Computing|Cloud Computing is a remote location technology which provides Platform as a Service (PaaS), Software as a Service (SaaS) and Infrastructure as a Service (IaaS). High Performance Computing (HPC) allows scientists and engineers to solve complex science, engineering, and business problems using applications that require high bandwidth, enhanced networking, and very high compute capabilities. The combination of Cloud Computing and High Performance Computing allows you to increase the speed of research by running high performance computing in the cloud and to reduce costs by providing Cluster Compute or Cluster GPU server’s on-demand without large capital investments. Tap into unlimited resources to scale your High Performance Computing jobs analyzing large-scale data, running simulations and financial models and experimenting while reducing time to market. Build your personalized end to end High Performance Computing solution on Cloud which tailored to your organization’s needs.||2016|10.18535/ijecs/v5i12.51|Mandeep Kumar|0.1111111111111111|2
183|MINIME-GPU|We introduce MINIME-GPU, a novel automated benchmark synthesis framework for graphics processing units (GPUs) that serves to speed up architectural simulation of modern GPU architectures. Our framework captures important characteristics of original GPU applications and generates synthetic GPU benchmarks using the Open Computing Language (OpenCL) library from those applications. To the best of our knowledge, this is the first time synthetic OpenCL benchmarks for GPUs are generated from existing applications. We use several characteristics, including instruction throughput, compute unit occupancy, and memory efficiency, to compare the similarity of original applications and their corresponding synthetic benchmarks. The experimental results show that our synthetic benchmark generation framework is capable of generating synthetic benchmarks that have similar characteristics with the original applications from which they are generated. On average, the similarity (accuracy) is 96% and the speedup is 541 ×. In addition, our synthetic benchmarks use the OpenCL library, which allows us to obtain portable human readable benchmarks as opposed to using assembly-level code, and they are faster and smaller than the original applications from which they are generated. We experimentally validated that our synthetic benchmarks preserve the characteristics of the original applications across different architectures.|ACM Transactions on Architecture and Code Optimization (TACO)|2015|10.1145/2818693|A. Sen, Etem Deniz|0.1|2
190|GPGPU implementation of information theoretic algorithms for the analysis of granular layer neurons|Methods originally developed for communication systems are widely used in computational neuroscience to understand the information representation and processing performed by neurons and neural circuits in the brain. Information theoretic quantities Entropy and Mutual Information (MI) have been used in neuroscience as a metric to estimate the efficiency of information representation by neurons. These quantities are used here to measure the stimulus discrimination reliability of the cerebellar granule neurons using simulated response trains produced by a multi-compartmental model of Wistar rat neuron. With ~1011 granule neurons in the cerebellum, understanding spatio-temporal processing in such structures demands efficient, fast algorithms. Since the serial version of the algorithm had multiple estimation loops which increased the process time considerably with the problem size, we re-implemented the MI algorithm in GPGPU hardware as an efficient way of parallelizing the MI computations. Task-level parallelism and GPU optimizations were used to improve the process time. Estimates on GPGPUs showed 15X time efficiency compared to the CPU version of the algorithm. In order to understand learning inside the cerebellar circuit, synaptic plasticity conditions were simulated in the neuron model. We were able to quantify the stimulus discrimination reliability of granule neurons under control, LTP and LTD conditions and the analysis revealed that stimulus discrimination capability of the neuron was increased during high plasticity state.|International Conference on Computing, Networking and Communications|2015|10.1109/COCONET.2015.7411162|Vyshnav Mohan, Shyam Diwakar, B. Nair, Prasanth Madhu, Arathi G. Rajendran, Manjusha Nair|0.1|2
193|Reciprocal abstraction for computer architecture co-simulation|Co-simulation of computer architecture elements at different levels of abstraction and fidelity is becoming an increasing necessity for efficient experimentation and research. We propose reciprocal abstraction for computer architecture cosimulation, which allows the integration of simulation methods that utilize different levels of abstraction and fidelity of simulation. Further, reciprocal abstraction avoids the need to conduct detailed evaluations of individual computer architecture components entirely in a vacuum, which can lead to significant inaccuracies from ignoring the system context. Moreover, it allows an exploration of the impact on the full system resulting from design choices in the detailed component model. We demonstrate the potential inaccuracies of isolated component simulation. Using reciprocal abstraction, we integrate a parallel cycle-level networkon- chip (NoC) component into a detailed but more coarse-grain full system simulator.We show that co-simulation using reciprocal abstraction of the cycle-level network model reduces packet latency error compared to the more abstract network model by 69% on average. Additionally, as simulating a detailed network at the cycle-level can greatly increase simulation time over an abstract model, we implemented detailed network simulator using a GPU coprocessor. The CPU+GPU can reduce simulation time for the reciprocal abstraction co-simulation by 16% for a 256-core target machine and 65% for a 512-core target machine.|IEEE International Symposium on Performance Analysis of Systems and Software|2015|10.1109/ISPASS.2015.7095812|Michael Moeng, R. Melhem, A. Jones|0.1|2
217|High performance computing of fiber scattering simulation|Cellulose is one of the most promising energy resources that is waiting to be tapped. Harvesting energy from cellulose requires decoding its atomic structure. Some structural information can be exposed by modeling data produced by X-ray scattering. Forward simulation can be used to explore structural parameters of cellulose, including the diameter, twist and coiling, but modeling fiber scattering is computationally challenging. In this paper, we explore how to accelerate a molecular scattering algorithm by leveraging a modern high-end Graphic Processing Unit (GPU). A step-wise optimization approach is described in this work that considers memory utilization, math intrinsics, concurrent kernel execution and workload partitioning. Different caching strategies to manage the state of the atom volume in memory are taken into account. We have developed optimized cluster solutions for both CPUs and GPUs. Different workload distribution schemes and con- current execution approaches for both CPUs and GPUs have been investigated. Leveraging accelerators hosted on a cluster, we have reduced days/weeks of intensive simulation to parallel execution of just a few minutes/seconds. Our GPU-integrated cluster solution can potentially support concurrent modeling of hundreds of cellulose fibril structures, opening up new avenues for energy research.|GPGPU@PPoPP|2015|10.1145/2716282.2716285|Leiming Yu, Xiang Gong, D. Kaeli, L. Makowski, Yan Zhang, Nilay K. Roy|0.1|2
245|Hybrid multi-threaded simulation of agent-based pandemic modeling using multiple GPUs|Epidemiology computation models are crucial for the assessment and control of public health crises. Agent-based simulations of pandemic influenza forecast the infectious disease spreading in order to help public health policy makers during emergencies. In such emergencies, decisions are required for public health preparedness in cycles of less than a day, and the agent-based model should be adaptable and tractable for quick and simple calibration with low computational overhead. GPU accelerated computing involves the use of graphics processing units (GPUs) in combination with the CPU to perform heterogeneous computing by offloading compute-intensive portions of the program to the GPU while the remaining program runs on the CPU. In this paper, we demonstrate the utilization of the hardware environment and software tools and discuss strategies for porting agent-based simulations to multiple GPUs. We further compare the performance of simulations using two or four GPUs with the sequential execution on the CPU, in terms of time and speedup. The multi-GPU implementations exhibit great performance and support populations with up to 100 million individuals.|IEEE International Conference on Bioinformatics and Biomedicine|2015|10.1109/BIBM.2015.7359894|Barzan Shekh, E. Doncker, D. Prieto|0.1|2
284|A System-Level Simulation Framework for Evaluating Resource Management Policies for Heterogeneous System Architectures|Nowadays, heterogeneous system architectures, integrating CPUs and one or more kinds of accelerators (e.g., GPUs or HW accelerators), are a promising solution to achieve high performance for data-intensive workloads while fulfilling other system-level requirements on the available power/energy budgets. However, heterogeneity comes at the cost of greater design and management complexity leading to an increasing quest for the definition of innovative runtime resource management policies. We propose a system-level simulation framework implemented in SystemC and Transaction Level Modeling for a fast evaluation of resource management policies for such systems to provide a quick feedback to the middleware designer. A set of case studies shows the efficiency of the proposed framework in supporting a fast analysis of the investigated policies.|2015 Euromicro Conference on Digital System Design|2015|10.1109/DSD.2015.99|Gianluca Durelli, M. Santambrogio, A. Miele, C. Bolchini|0.1|2
362|Energy Modeling and Optimization for Tiled Nested-Loop Codes|We develop a methodology for modeling the energy efficiency of tiled nested-loop codes running on a graphics processing unit (GPU) and use it for energy efficiency optimization. % We use the polyhedral model, a We assume that a highly optimized and parametrized version of a tiled nested -- loop code, either written by an expert programmer or automatically produced by a polyhedral compilation tool -- is given to us as an input. We then model the energy consumption as an analytical function of a set of parameters characterizing the software and the GPU hardware. Most previous attempts at GPU energy modeling were based on low-level machine models that were then used to model whole programs through simulations, or were analytical models that required low level details. In contrast, our approach develops analytical models based on (i) machine and architecture parameters, (ii) program size parameters as found in the polyhedral model and (iii) tiling parameters, such as those that are chosen by auto-or manual tuners. Our model therefore allows efficient optimization of the energy efficiency with respect to a set of parameters of interest. We illustrate the framework on three nested-loop codes: Smith-Waterman, and one-dimensional and two-dimensional Jacobi stencils, and analyze the accuracy of the resulting models. We also show that the models can be used for optimal tile-size selection for energy efficiency. With optimal choice of model parameters the RMS error is less than 4%. Two factors allow us to attain this high accuracy. The first is domain-specificity: we focus only on tile-able nested-loop codes. The second is that we decouple the energy model from a model of the execution time, a known hard problem.|2015 IEEE International Parallel and Distributed Processing Symposium Workshop|2015|10.1109/IPDPSW.2015.94|W. Ranasinghe, Vamshi Tandrapati, Nirmal Prajapati, S. Rajopadhye, R. Andonov, H. Djidjev|0.1|2
417|On the Way to Future's High Energy Particle Physics Transport Code|High Energy Physics (HEP) needs a huge amount of computing resources. In addition data acquisition, transfer, and analysis require a well developed infrastructure too. In order to prove new physics disciplines it is required to higher the luminosity of the accelerator facilities, which produce more-and-more data in the experimental detectors. Both testing new theories and detector R&D are based on complex simulations. Today have already reach that level, the Monte Carlo detector simulation takes much more time than real data collection. This is why speed up of the calculations and simulations became important in the HEP community. The Geant Vector Prototype (GeantV) project aims to optimize the most-used particle transport code applying parallel computing and to exploit the capabilities of the modern CPU and GPU architectures as well. With the maximized concurrency at multiple levels the GeantV is intended to be the successor of the Geant4 particle transport code that has been used since two decades successfully. Here we present our latest result on the GeantV tests performances, comparing CPU/GPU based vectorized GeantV geometrical code to the Geant4 version.|arXiv.org|2015|10.1088/1674-1056/24/11/118901|E. Futo, G. Bíró, G. G. Barnaföldi|0.1|2
458|Real-time Water Simulation with Wave Particles on the GPU|When rendering large bodies of water in real-time an efficient method is required to model water waves. This article describes a method for real-time interactive generation of such waves. We use the wave particle method to describe wave propagation in a fluid medium. The method allows to simulate interactions of water with general shaped rigid bodies in real-time. We present a GPU implementation of the method and show results in scenarios such as open ocean waters or pools with water boundaries.||2015|10.20906/cps/cob-2015-0316|Daniel Mike|0.1|2
1101|Interactive ultrasonic field simulation for nondestructive testing|This paper presents an ultrasonic field simulation software, dedicated to Non Destructive Testing, aiming at interactivity. This work relies on Civa Software semi-analytical model. By restricting its scope to homogeneous isotropic specimens with simple geometry and half-skip modes, an almost completely regular algorithm, well suited to modern hardware, can be derived. The performance of three implementations on multicore SIMD general purpose processors (GPP), manycore accelerators (MIC) and graphical processing units (GPU) over a set of 18 realistic configurations (a standard one plus 17 variations) are presented and analysed. For GPP and the GPU, interactive performances with almost 30 fps have been reached on the standard configuration. This is, to our knowledge, the very first time for a NDT ultrasonic field simulation software.|International Conference on Quality Control by Artificial Vision|2015|10.1117/12.2182840|Sylvain Chatillon, Jason Lambert, G. Rougeron, Lionel Lacassagne|0.1|2
1146|First experience of vectorizing electromagnetic physics models for detector simulation|The recent emergence of hardware architectures characterized by many-core or accelerated processors has opened new opportunities for concurrent programming models taking advantage of both SIMD and SIMT architectures. The GeantV vector prototype for detector simulations has been designed to exploit both the vector capability of mainstream CPUs and multi-threading capabilities of coprocessors including NVidia GPUs and Intel Xeon Phi. The characteristics of these architectures are very different in terms of the vectorization depth, parallelization needed to achieve optimal performance or memory access latency and speed. An additional challenge is to avoid the code duplication often inherent to supporting heterogeneous platforms. In this paper we present the first experience of vectorizing electromagnetic physics models developed for the GeantV project.||2015|10.1088/1742-6596/664/9/092013|D. Elvira, G. Amadio, G. Bitzes, J. Apostolakis, O. Shadura, S. Jun, F. Carminati, L. Duhem, M. Bandieramonte, G. Lima, R. Seghal, A. Gheata, C. Bianchini, R. Brun, J. D. F. Licht, S. Wenzel, M. Novak, M. Presbyterian, P. Canal|0.1|2
1171|Advanced lighting for unstructured-grid data visualization|The benefits of using advanced illumination models in volume visualization have been demonstrated by many researchers. Interactive volume rendering incorporated with advanced lighting has been achieved with GPU acceleration for regular-grid volume data, making volume visualization even more appealing as a tool for 3D data exploration. This paper presents an interactive illumination strategy, which is specially designed and optimized for volume visualization of unstructured-grid data. The basis of the design is a partial differential equation based illumination model to simulate the light propagation, absorption, and scattering within the volumetric medium. In particular, a two-level scheme is introduced to overcome the challenges presented by unstructured grids. Test results show that the added illumination effects such as global shadowing and multiple scattering not only lead to more visually pleasing visualization, but also greatly enhance the perception of the depth information and complex spatial relationships for features of interest in the volume data. This volume visualization enhancement is introduced at a time when unstructured grids are becoming increasingly popular for a variety of scientific simulation applications.|IEEE Pacific Visualization Symposium|2015|10.1109/PACIFICVIS.2015.7156383|Yubo Zhang, Min Shih, K. Ma|0.1|2
1185|Innovative Algorithms for Extreme Scale Computing|For the past thirty years, the need for ever greater supercomputer performance has driven the development of many computing technologies which have subsequently been exploited in the mass market. Delivering an exaflop (or a million million million calculations per second) by the end of this decade is the challenge that the supercomputing community worldwide has set itself. Developing techniques and solutions which address the most difficult challenges that computing at the exascale can provide is a real challenge. Equipment vendors, programming tools providers, academics, and end users must all work together to build and to develop the development and debugging environment, algorithms and libraries, user tools, and the underpinning and cross-cutting technologies required to support the execution of applications at the exascale. This special issue of the journal is dedicated to advances in high performance computing in engineering and the way to exascale. It contains some papers which have been selected from the Exascale Applications and Software Conference (EASC2013) held on 9–11 April 2013 in Edinburgh, United Kingdom. The issue contains five papers, illustrates the recent advances made in the exascale path and covers algorithms, implementations and applications to solve large scale engineering problems. The first paper, by Reverdy et al., reports the realisation of the first cosmological simulations on the scale of the whole observable universe. The paper first focuses on the numerical aspects of two new simulations. In practice, each one of these simulations has evolved 550 billion dark matter particles in an adaptive mesh refinement grid, and one of the new simulations has pushed back the total number of grid points from 2000 billion for the L Cold Dark Matter (L CDM) model to 2200 billion due to the formation of a larger number of structures. The authors highlight the optimisations and adjustments required to run such a set of simulations and then summarise some important lessons learnt toward future exascale computing projects. Numerical examples illustrate the effectiveness of the procedure on 4752 nodes of the Curie Bull supercomputer. The second paper, by Mozdzynski et al., presents the European Centre for Medium-Range Weather Forecasts (ECMWF) Integrated Forecasting System (IFS) enhanced to use Fortran2008 coarrays to overlap computation and communication in the context of OpenMP parallel regions. Today ECMWF runs a 16 km global T1279 operational weather forecast model using 1536 cores. Following the historical evolution in resolution upgrades, ECMWF could expect to be running a 2.5 km global forecast model by 2030 on an exascale system that should be available and hopefully affordable by then. To achieve this would require IFS to run efficiently on about 1000 times the number of cores it uses today. This is a significant challenge that is addressed in this paper, where, after implementing an initial set of improvements, ECMWF is demonstrated running a 10 km global model efficiently on over 40,000 cores on the HECToR Cray XE6 supercomputer. The third paper, by Gray et al., describes a multiGPU implementation of the Ludwig application, which specialises in simulating a variety of complex fluids via lattice Boltzmann fluid dynamics coupled to additional physics describing complex fluid constituents. The authors describe the methodology in augmenting the original CPU version with GPU functionality in a maintainable fashion. After presenting several optimisations that maximise performance on the GPU architecture through tuning for the GPU memory hierarchy, they describe how to implement particles within the fluid in such a way as to avoid a major diversion of the CPU and GPU codebases, whilst minimising data transfer at each timestep. Numerical results show that the application demonstrates excellent scaling to at least 8192 GPUs in parallel (the largest system tested at the time of writing) on the Titan Cray XK7 supercomputer. Exascale computers are expected to have highly hierarchical architectures with nodes composed of multiple core processors (CPU) and accelerators (GPU). The different programming levels generate new difficulties and algorithms issues. The paper written by Magoules et al., presents Alinea, which stands for Advanced LINEar Algebra, a library well suited for hybrid CPU/|The international journal of high performance computing applications|2015|10.1177/1094342015576772|Lorna Smith, Mark I. Parsons, F. Magoulès|0.1|2
1276|Computer simulation implementations and optimization of the right atrium of the heart based on GPU|For heart right atrium electrophysiology simulation computation huge, time-consuming problem, a method based on high-performance graphics processing unit (GPU) to achieve parallel computing and optimization. First, consider the differences between right atrial heart cell center and the edge of the constructed one-dimensional non-homogeneous cardiac tissue model right atrium; operator split method enables solution of the model calculation task with parallelism. Presents three parallel strategies based on the specific solver process, and which takes the shortest thread blocks from the policy settings were further optimized data storage mode switching frequencies and so on. The results show that: for simulation 500 cells CUDA program execution time than the serial program fell by 60%, after further optimized CUDA program execution time can be reduced by 84%; the greater the right atrial cardiac tissue, GPU acceleration effect more obviously. The results verify the GPU acceleration solution method can significantly increase the heart rate right atrium solver model, reduce the actual execution time.||2015|10.2991/MEITA-15.2015.181|Chun-jiang Zhao|0.1|2
1316|Parallel and Distributed Programing for Data Computation Intensive Applications|Scientific Computing requires high computation power where large volumes of data are processed quickly usually in gigaFLOPS and teraFLOPS. Supercomputers, grid or cluster based systems are always the preferred choice for running such massively parallel scientific computing jobs. Due to its high performance and low cost GPUs are the preferred choice in High Performance Computing. The GPUs though originally were designed for rendering graphics in high resolution games, are now a days extensively used for computation intensive general purpose applications by the name GPGPU (General Purpose Graphic Processing Unit). Various programming tools and APIs have been developed for GPU computing with greater attention received by CUDA, OpenCL and OpenGL. This work uses OpenCL as parallel programming tool because of its open standard and heterogeneity. GPU Computing power has been exploited in running various applications such as sorting large data sets, design and implementation of parallel FFT library and the FFT based fast Magnetostatic field computation in the area of Micromagnetics. Sorting algorithms arrange a given sequence of input data into a certain order (monotonic increase or decrease) and are categorized by their computational complexity for best, average and worst case analysis. The time complexity is not the only deciding parameter, but other factors like stability, robustness, scalability, input distribution, memory storage and access patterns decide the applicability of a sorting algorithm for a certain application domain. The portion of the thesis work is devoted to the design and implementation of new parallel sorting techniques well suited for multi-processor architectures like GPUs and other multi-core systems. The novel sorting technique, Butterfly Network Sort, exploit high parallelism in its design and thus achieve considerable speedup against state-of-the-art sorting techniques. Fast Fourier Transforms library (named ToPe-FFT) is implemented using OpenCL. ToPe-FFT is based on the well-known Cooley-Tukey algorithm with auto-tuning for multiple GPUs. The open source ToPe-FFT implements several base radices along side the support for mixed-radices making it an almost arbitrary length FFT library. The library takes Complex-to-Complex (C2C) input type with dimension sizes up-to 3D. The design and interface of ToPe-FFT is similar to cuFFT and FFTW. The supported features of arbitrary input length, better accuracy in high dimension transforms, load balancing on multiple GPUs and above all significant speedup against cuFFT and FFTW makes ToPe-FFT promising in delivering maximum performance. An optimized version is tested in Micromagnetic simulations for performance improvement. In Micromagnetic simulations the computation of Magnetostatic field is the most time consuming part of the overall simulation time. In the case of a ferromagnetic region discretized into N number of elementary cells, the computation of Magnetostatic field at a particular location has a functional relationship with the magnetization at all other elements in the whole region. This long range elementary dipole interactions has high computation cost. In the FFT based Magnetostatic field computation, the given model is treated as discrete convolution problem with a reduced complexity. We have used an optimized version of our ToPe-FFT library for accelerating Magnetostatic field computation. Our GPU based optimized field solver has significant speedup against OOMMF Magnetostatic field computation time||2015|10.1109/fit.2015.34|J. Bilal|0.1|2
1349|ASC-ATDM Performance Portability Requirements for 2015-2019|This report outlines the research, development, and support requirements for the Advanced Simulation and Computing (ASC ) Advanced Technology, Development, and Mitigation (ATDM) Performance Portability (a.k.a., Kokkos) project for 2015 - 2019 . The research and development (R&D) goal for Kokkos (v2) has been to create and demonstrate a thread - parallel programming model a nd standard C++ library - based implementation that enables performance portability across diverse manycore architectures such as multicore CPU, Intel Xeon Phi, and NVIDIA Kepler GPU. This R&D goal has been achieved for algorithms that use data parallel pat terns including parallel - for, parallel - reduce, and parallel - scan. Current R&D is focusing on hierarchical parallel patterns such as a directed acyclic graph (DAG) of asynchronous tasks where each task contain s nested data parallel algorithms. This five y ear plan includes R&D required to f ully and performance portably exploit thread parallelism across current and anticipated next generation platforms (NGP). The Kokkos library is being evaluated by many projects exploring algorithm s and code design for NGP. Some production libraries and applications such as Trilinos and LAMMPS have already committed to Kokkos as their foundation for manycore parallelism an d performance portability.more » These five year requirements includes support required for current and antic ipated ASC projects to be effective and productive in their use of Kokkos on NGP. The greatest risk to the success of Kokkos and ASC projects relying upon Kokkos is a lack of staffing resources to support Kokkos to the degree needed by these ASC projects. This support includes up - to - date tutorials, documentation, multi - platform (hardware and software stack) testing, minor feature enhancements, thread - scalable algorithm consulting, and managing collaborative R&D.« less||2015|10.2172/1177389|C. Trott, H. Edwards|0.1|2
1357|Energie- und Ausführungszeitmodelle zur effizienten Ausführung wissenschaftlicher Simulationen|Computer simulation as a part of the scientific computing has established as third pillar in scientific methodology, besides theory and experiment. The task of computer science in the field of scientific computing is the development of efficient simulation algorithms as well as their efficient implementation. The thesis focuses on the efficient implementation of two important methods in scientific computing: the Fast Multipole Method (FMM) for particle simulations, and the Finite Element Method (FEM), which is, e.g., used for deformation problems of solids. The efficiency of the implementation considers the execution time of the simulations and the energy consumption of the computing systems needed for the execution. The method used for increasing the efficiency is model-based autotuning. For model-based autotuning, a model for the substantial parts of the algorithm is set up which estimates the execution time or energy consumption. This model depends on properties of the computer used, of the input data and of parameters of the algorithm. The properties of the computer are determined by executing the real code for different implementation variants. These implementation variantss comprise a CPU and a graphics processor implementation for the FEM, and implementations of near field and far field interaction calculations for the FMM. Using the models, the execution costs for each variant are predicted. Thus, the optimal algorithm parameters can be determined analytically for a minimisation of the desired target value, i.e. execution time or energy consumption. When the simulation is executed, the most efficient implementation variants are used depending on the prediction of the model. While for the FMM the performance measurement takes place independently from the execution of the simulation, for the FEM a method for dynamically distributing the workload to the CPU and the GPU is presented, which takes into account execution times measured at runtime. By measuring the real execution times, it is possible to response to changing conditions and to adapt the distribution of the workload accordingly. The results of the thesis show that model-based autotuning makes it possible to increase the efficiency of applications in scientific computing regarding execution time and energy consumption. Especially, the consideration of the energy consumption of alternative execution paths, i.e. the energy adaptivity, will be of great importance in scientific computing in the near future.||2015|10.1007/978-3-658-05734-3_48|Jens M. Lang|0.1|2
1381|Real Time Simulation of Cloth Like Object|CSE, DACOE, Karad, CSE, DACOE, Karad CSE, DACOE, Karad Abstract— Modeling and animation of cloth has experienced important developments in recent years. Concept regarding to simulation of cloth can be implemented by using both OPENGL and CUDA C. So we can examine how to incorporate effects of wind fields in cloth simulations. Use of CUDA C language helps to do execution faster. CUDA C is a GPU Programming language we have used which run on our nvidia graphics processor. We perform fast and stable time integration of a physically-based model, as well as wind drag effects to enhance realism. Keywords-Wind Simulation, Cloth Animation, Physically Based Modeling, Parallel computing, Computational model, CUDA (Compute unified device architecture).||2015|10.1109/iccubea.2015.172|Amit Ghadage, Akash Shinde, A. Kumbhar|0.1|2
1468|SU‐E‐T‐499: Initial Developments of An OpenCL‐Based Cross‐Platform Monte Carlo Dose Engine for Carbon Ion Therapy|Purpose Dose calculation is of critical importance for carbon ion therapy. Monte Carlo (MC) simulation is considered to be the most accurate method for calculation of absorbed dose and of all the more fundamental physical quantities related to biological effects. The long computation time, however, limits its routine clinical applications. We have recently started developing a fast MC package, gCMC for carbon therapy on a parallel processing platform, e.g. GPU, aiming at achieving sufficient efficiency to enable MC in clinically important tasks. This abstract reports our progress. Methods gCMC was developed in OpenCL environment. Our initial developments focused on water material. gCMC supported carbon ion transport in the energy range of 1–450 MeV/u. A Class II condensed history algorithm was implemented for charged particle transport simulations with stopping power computed via Bethe-Bloch equation. Energy straggling and multiple scattering were modeled. Total cross section of nuclear interaction was extracted from Geant4. At present, nuclear interaction events were sampled but transports of secondary particles were not included. Results We tested cases with a homogeneous water phantom and a pencil carbon ion beam with energy of 200–400 MeV/u. When only electro-magnetic channel was included, dose/fluence difference between gCMC and Geant4 results averaged within 10% isodose line was <0.5% of the maximum dose/fluence. After enabling nuclear interactions without transporting secondary particles, dose and fluence agreed with the corresponding results computed by Geant4 with <1% difference. Due to the support for multiple platforms of OpenCL, gCMC was executable on NVidia and AMD GPUs, and Intel CPUs. It took ∼50 sec to transport 107 200MeV/u source carbon ions on an NVidia Titan GPU card. Conclusion Preliminary studies have demonstrated the accuracy and efficiency of gCMC. With further developments in near future, gCMC will potentially achieve clinically acceptable fast and accurate MC simulations for carbon ion therapy.||2015|10.1118/1.4924861|G. Dedes, M. Pinto, Z. Tian, N. Qin, K. Parodi, X. Jia, A. Pompoš, Steve B. Jiang|0.1|2
1486|Towards the Implementation of Wind Turbine Simulations on Many-Core Systems|We are concerned with alternative approaches to accelerate the matrix construction step that is a computationally intensive portion of the Finite Element Method (FEM) framework. Our target application is part of a wind turbine simulation tool-chain modeled using the Navier-Stokes equations for incompressible flow and discretized with the discontinuous Galerkin (DG) finite element method and implicit time-stepping. The Poisson pressure correction equation and the structural part of the fluid-structure interaction are formulated and solved numerically in the continuous Galerkin framework. The construction of the required matrix is performed by exploiting multiple Graphics Processing Units of a cluster, using the CUDA programming model. The performance results indicate that our approach scales well as more nodes of the cluster, as well as more GPUs within each node are exploited.||2015|10.2514/6.2015-0050|Ioannis E. Venetis, Efstratios Gallopoulos, J. Ekaterinaris, Nikolaos Nikoloutsakos|0.1|2
1531|Accelerating mono-domain cardiac electrophysiology simulations using OpenCL|Abstract Using OpenCL, we developed a cross-platform software to compute electrical excitation conduction in cardiac tissue. OpenCL allowed the software to run parallelized and on different computing devices (e.g., CPUs and GPUs). We used the macroscopic mono-domain model for excitation conduction and an atrial myocyte model by Courtemanche et al. for ionic currents. On a CPU with 12 HyperThreading-enabled Intel Xeon 2.7 GHz cores, we achieved a speed-up of simulations by a factor of 1.6 against existing software that uses OpenMPI. On two high-end AMD FirePro D700 GPUs the OpenCL software ran 2.4 times faster than the OpenMPI implementation. The more nodes the discretized simulation domain contained, the higher speed-ups were achieved.||2015|10.1515/cdbme-2015-0100|E. M. Wülfers, Zhasur Zhamoliddinov, O. Dössel, G. Seemann|0.1|2
187|3D seismic modeling using hybrid arquitectures|This paper explains the wave phenomenon that was worked and the medium in which the propagation occurs. A differential equation of the elastic wave in isotropic and heterogeneous media is solved using a finite difference method with staggered mesh with second order accuracy in time and fourth grade in space, aiming for greater stability and efficiency. The solution of the equation was implemented in C. Scons was used for compiling the programs and Madagascar (seismic data processing software) to control the flow of data and perform seismic simulation. The testbed was the GUANE-1 architecture. OpenMP was used to accelerate the application, running the algorithm on 24 CPUs on one node. CUDA was used to run the solution in a hybrid architecture with CPU and GPU, using only global memory on the graphic processors. Finally an analysis and comparison of metrics and runtime acceleration of the two models of parallelization is used to check the processing power that can be achieved with hybrid architectures.|Cybersecurity and Cyberforensics Conference|2015|10.1109/COLUMBIANCC.2015.7333472|Herling Gonzalez, Diego Nino, William Trigos, Sergio Gélvez, Carlos Barrios, J. Carreño|0.0|2
192|Application of MW-FDTD to simulate the EM wave propagation over ocean with OpenCL|To simulate the EM wave propagation over ocean with FDTD, large memory and large computational quantity is needed. To reduce the memory usage, moving-window FDTD (MW-FDTD) is involved. By moving the calculation window which includes most energy of the EM wave, the necessary memory is much less than traditional FDTD method. To increase the simulation speed, an AMD R7 260X GPU is used and OpenCL is used as the GPU programming interface. The simulation acceleration ratio can be over 10 while the simulation code of a 4-core CPU is compiled with OpenMP. The FFT method is used to model oceans specified with a certain wind speed and a wind direction.|2016 IEEE/ACES International Conference on Wireless Information Technology and Systems (ICWITS) and Applied Computational Electromagnetics (ACES)|2016|10.1109/ROPACES.2016.7465400|Yongsheng Zhao, Zhaoqing Sun, T. Jiang, Hanlin Duan|0.0|2
197|Effect of heart failure-induced electrical remodeling on the initiation of atrial arrhythmias|Heart failure (HF) is a particularly prevalent clinical condition promoting atrial arrhythmias. However, the underlying mechanism is rarely studied. In this study, using a GPU-based simulation, a biophysically detailed computational model of the three-dimensional (3D) sheep atria was implemented to investigate the mechanism by which HF-induced electrical remodeling promoting atrial arrhythmia. At both the single cell and the 3D levels of the sheep atrial model, effects of such HF-induced electrical remodeling on the electrical properties were evaluated. At the cellular level, simulation results demonstrated that the action potential duration (APD) and the amplitude of systolic Ca2+ transient were decreased in all cell types except the PV cell in the HF condition. At the 3D whole organ level, simulation results showed that though localized APDs were shortened, the spatial electrical heterogeneity was maintained in the HF condition, resulting in an increased vulnerability of the tissue for the initiation of the conduction block in response to a premature stimulus. This study provided new insights into understanding the mechanism by which HF promoted atrial arrhythmias.|International Conference on Computing in Cardiology|2016|10.22489/CINC.2016.215-160|Xiangyun Bai, Qince Li, Runnan He, Yong Xia, Henggui Zhang, Kuanquan Wang, Na Zhao|0.0|2
198|Building a Nature-Inspired Computer|Summary form only given. Since the before birth of computers we have strived to make intelligent machines that share some of the properties of our own brains. We have tried to make devices that quickly solve problems that we find time consuming, that adapt to our needs, and that learn and derive new information. In more recent years we have tried to add new capabilities to our devices: self-adaptation, fault tolerance, self-repair, even self-programming, or self-building. In pursing these challenging goals we push the boundaries of computer and software architectures. We invent new parallel processing approaches or we exploit hardware in new ways. For the last decade Peter Bentley and his group have made their own journey in this area. In order to overcome the observed incompatibilities between conventional architectures and biological processes, Bentley created the Systemic Computer [1] -- a computing paradigm and architecture designed to process information in a way more similar to natural systems. The computer uses a systemic world-view. Instead of the traditional centralized view of computation, here all computation is distributed. There is no separation of data and code/functionality into memory, ALU, and I/O. Everything in systemic computation is composed of systems, which may not be destroyed, but may transform each other through their interactions, akin to collision-based computing. Two systems interact in the context of a third system, which defines the result of their interaction. All interactions may be separated and embedded within scopes, which are also systems, enabling embedded hierarchies. Systemic computation makes the following assertions: · Everything is a system. · Systems can be transformed but never destroyed or created from nothing. · Systems may comprise or share other nested systems. · Systems interact, and interaction between systems may cause transformation of those systems, where the nature of that transformation is determined by a contextual system. · All systems can potentially act as context and affect the interactions of other systems, and all systems can potentially interact in some context. · The transformation of systems is constrained by the scope of systems, and systems may have partial membership within the scope of a system. · Computation is transformation. Computation has always meant transformation in the past, whether it is the transformation of position of beads on an abacus, or of electrons in a CPU. But this simple definition also allows us to call the sorting of pebbles on a beach, or the transcription of protein, or the growth of dendrites in the brain, valid forms of computation. Such a definition is important, for it provides a common language for biology and computer science, enabling both to be understood in terms of computation. The systemic computer is designed to enable many features of natural computation and provide an effective platform for biological modeling and bio-inspired algorithms. Several different implementations of the systemic computer have been created, each with their own advantages and disadvantages. Simulators on conventional computers have enabled the demonstration of bio-inspired algorithms, fault tolerance, self-repair, and modeling of biological processes [2]. To improve speed, an FPGA-based hardware implementation was created and shown to be several orders of magnitude faster [3]. A GPU-based implementation was also created in order to combine flexibility, scalability, and speed [4]. Through this work, many important lessons have been learned. In addition to the advances in bio-inspired computing, it is increasingly possible to see parallels between systemic computing and other techniques and architectures under development. High performance graph-based computing or novel hardware based on memristors or neural modeling may provide excellent new substrates for systemic-style computation in the future.|Symposium on Symbolic and Numeric Algorithms for Scientific Computing|2015|10.1109/SYNASC.2015.12|P. Bentley|0.0|2
201|Spatial data processing with MapReduce|The current development of high performance parallel supercomputing infrastructures are pushing the boundaries of applications of science and are bringing new paradigms into engineering practices and simulations. Earthquake engineering is also one of the major fields, which benefits from above by looking for solutions in grid computing and cloud computing techniques. Generally, earthquake simulations involve analysis of petabytes of data. Analyzing these large amounts of data in parallel in thousands of nodes in computer clusters results in gaining high performances. Open source cloud solutions such as Hadoop MapReduce, which is highly scalable and capable of processing large amount of data rapidly in parallel on large clusters provide better solution compared to RDBDM. Both GPUs and MapReduce are designed to support vast data parallelism. For performance considerations, GPU computing could be adopted over low performing CPU systems. This paper discusses MapReduce system using Hadoop and Mars. Mars is a MapReduce framework on graphics processor. Hence, the proposition is to use GPU based systems for earthquake simulations in which Digital elevation model 3D data sets are fully materialized where scientist can make use of these data for various analysis and simulations.|International Conference on Industrial and Information Systems|2015|10.1109/ICIINFS.2015.7399060|G. Mecca, A. Vicari, T. Gunawardena|0.0|2
203|A Performance Model and Efficiency-Based Assignment of Buffering Strategies for Automatic GPU Stencil Code Generation|Stencil computations form the basis for computer simulations across almost every field of science, such as computational fluid dynamics, data mining, and image processing. Their mostly regular data access patterns potentially enable them to take advantage of the high computation and data bandwidth of GPUs, but only if data buffering and other issues are handled properly. Finding a good code generation presents a number of challenges, one of which is the best way to make use of memory. GPUs have three types of on-chip storage: registers, shared memory, and read-only cache. The choice of type of storage and how it's used, a buffering strategy, for each stencil array (grid function, (GF)) not only requires a good understanding of its stencil pattern, but also the efficiency of each type of storage for the GF, to avoid squandering storage that would be more beneficial to another GF. Our code-generation framework supports five buffering strategies. For a stencil computation with N GFs, the total number of possible assignments is 5N. Large, complex stencil kernels may consist of dozens of GFs, resulting in significant search overhead. In this work, we present an analytic performance model for stencil computations on GPUs, and study the behavior of readonly cache and L2 cache. Next, we propose an efficiency-based assignment algorithm, which operates by scoring a change in buffering strategy for a GF using a combination of (a) the predicted execution time and (b) on-chip storage usage. By using this scoring an assignment for N GFs and b strategy types can be determined in (b - 1)N(N + 1)/2 steps. Results show that the performance model has good accuracy and that the assignment strategy is highly efficient.|International Symposium on Embedded Multicore/Many-core Systems-on-Chip|2016|10.1109/MCSoC.2016.37|Yue Hu, David M. Koppelman, Steven R. Brandt|0.0|2
207|WE-AB-202-09: Feasibility and Quantitative Analysis of 4DCT-Based High Precision Lung Elastography.|PURPOSE\nThe purpose of this project is to derive high precision elastography measurements from 4DCT lung scans to facilitate the implementation of elastography in a radiotherapy context.\n\n\nMETHODS\n4DCT scans of the lungs were acquired, and breathing stages were subsequently registered to each other using an optical flow DIR algorithm. The displacement of each voxel gleaned from the registration was taken to be the ground-truth deformation. These vectors, along with the 4DCT source datasets, were used to generate a GPU-based biomechanical simulation that acted as a forward model to solve the inverse elasticity problem. The lung surface displacements were applied as boundary constraints for the model-guided lung tissue elastography, while the inner voxels were allowed to deform according to the linear elastic forces within the model. A biomechanically-based anisotropic convergence magnification technique was applied to the inner voxels in order to amplify the subtleties of the interior deformation. Solving the inverse elasticity problem was accomplished by modifying the tissue elasticity and iteratively deforming the biomechanical model. Convergence occurred when each voxel was within 0.5 mm of the ground-truth deformation and 1 kPa of the ground-truth elasticity distribution. To analyze the feasibility of the model-guided approach, we present the results for regions of low ventilation, specifically, the apex.\n\n\nRESULTS\nThe maximum apical boundary expansion was observed to be between 2 and 6 mm. Simulating this expansion within an apical lung model, it was observed that 100% of voxels converged within 0.5 mm of ground-truth deformation, while 91.8% converged within 1 kPa of the ground-truth elasticity distribution. A mean elasticity error of 0.6 kPa illustrates the high precision of our technique.\n\n\nCONCLUSION\nBy utilizing 4DCT lung data coupled with a biomechanical model, high precision lung elastography can be accurately performed, even in low ventilation regions of the lungs. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1144087.|Medical Physics (Lancaster)|2016|10.1118/1.4957750|D. Low, K. Hasse, A. Santhanam, J. Neylon|0.0|2
211|Modelling angiogenesis in three dimensions|The process through which new blood vessels are formed within the body is known as angiogenesis. An essential part of our survival, it has also been implicated more recently in many diseases both in terms of induced growth, and abnormal vascular structure. \n \nAngiogenesis is characterized as two processes, the development of a vascular network during embryonic growth and the production of new blood vessels. This work focuses on the latter, and seeks to develop a robust, three-dimensional model for simulating blood vessel growth and the attendant processes of blood flow and mass transfer within the simulated system. A system was developed which utilises medical imaging scan data (specifically, MicroCT) as the initial conditions from which a network of vessels is grown. This is combined with GPU accelerated simulations of fluid dynamics, with the intention of providing a technique for future use in predictive medicine and therapeutic simulation.||2015||L. Parsonson|0.0|2
212|Exploiting Task-Parallelism on GPU Clusters via OmpSs and rCUDA Virtualization|OmpSs is a task-parallel programming model consisting of a reduced collection of OpenMP-like directives, a front-end compiler, and a runtime system. This directive-based programming interface helps developers accelerate their application's execution, e.g. in a cluster equipped with graphics processing units (GPUs), with a low programming effort. On the other hand, the virtualization package rCUDA provides seamless and transparent remote access to any CUDA GPU in a cluster, via the CUDA Driver and Runtime programming interfaces. In this paper we investigate the hurdles and practical advantages of combining these two technologies. Our experimental study targets two cluster configurations: a system where all the GPUs are located into a single cluster node, and a cluster with the GPUs distributed among the nodes. Two applications, the N-body particle simulation and the Cholesky factorization of a dense matrix, are employed to expose the bottlenecks and performance of a remote virtualization solution applied to these two OmpSs task-parallel codes.|2015 IEEE Trustcom/BigDataSE/ISPA|2015|10.1109/Trustcom.2015.626|Adrián Castelló, E. Quintana-Ortí, R. Mayo, Judit Planas|0.0|2
215|Editorial Issue 29.2|This issue contains five papers. In the first paper, Bartlomiej Waszak, from Quebec City, Canada, proposes a novel approach to limbless movement simulation. The author built a simulation framework using position-based dynamics. He describes the body configuration of snakes using different types of distance constraints. The limbless movement is based on the formulation of a friction constraint to model the behavior of a snake's scales. In this approach, it is easy to solve collisions between objects and self-collisions for simulated snakes. The model includes a dynamic geometrical environment colliding with simulated animals. Detailed patterns are presented for four main types of limbless movement: serpentine, rectilinear, concertina, and sidewinding. In the second paper, Min Jiang, Richard Southern, and Jian J Zhang, from Bournemouth University, UK, present a novel and unified particle-based method for real-time dissolution simulation, which is fast, predictable, independent of sampling resolution, and visually plausible. The dissolution model is derived from collision theory and integrated into a smoothed particle hydrodynamics (SPH) fluid solver. Dissolution occurs when a solute is submerged in solvent. Physical laws govern the local excitation of solute particles based on kinetic energy. Solute separation during dissolution is handled using a new GPU-based region growing method. The use of SPH sampling for both solute and solvent guarantees a predictable and smooth dissolution process and provides user control of the volume change during the phase transition. In the third paper. Ashish Dhiman, Dhaval Solanki, and Uttama Lahiri, from The Indian Institute of Technology Gandhinagar in Gujarat, India; Ashu Bhasin, from All India Institute of Medical Sciences, in Delhi, India; and Abhijit Das, from AMRI Institute of Neuroscience, in Kolkata, India, describe the design of a novel, multimodal, virtual-reality-based, and performance-sensitive exercise platform that can intelligently adapt its task presentation to one's performance. Here, the authors aim to address unilateral shoulder abduction and adduction that are essential for the performance of daily living activities. They designed an experimental study in which six chronic stroke survivors participated. While they interacted with the virtual-reality-based tasks, the authors recorded the physiological signals in a synchronized manner. In the fourth paper, Xiaoyong Zhang and Shiguang Liu, from Tianjin University, China, propose a novel interactive SPH fluid control framework with turbulent details. The authors run SPH fluid simulation on CUDA and greatly improve the efficiency of fluid control. The control particle with curvature framework was adapted in this paper. The authors specially designed spring forces to make the fluid match a fast moving control target. Moreover, fine fluid details were preserved by separately calculating the under-control fluid turbulence and the free fluid turbulence. This improved SPH fluid control can run in real time, which can enhance the visual quality of fluid animation as well. In the fifth paper, Dayal R.Parhi, Chinmaya Sahu, and Priyadarshi Biplab Kumar, from The National Institute of Technology Rourkela in Orissa, India, present a navigation strategy for humanoid robots using a hybridized technique consisting of adaptive particle swarm optimization and adaptive ant colony optimization. Here, the governing parameters of the adaptive ant colony optimization technique are optimized by using adaptive particle swarm optimization method. These optimized parameters are subsequently used by the adaptive particle swarm optimization technique to get the final turning angle by which the humanoid navigates in a cluttered environment. Here, navigation is performed in both static and dynamic environments. To avoid the intercollision among the humanoids, a Petri net controller has been designed and implemented along with the proposed hybridized method.|Comput. Animat. Virtual Worlds|2018|10.1002/cav.1809|D. Thalmann, N. Magnenat-Thalmann|0.0|2
216|New advances in large‐scale distributed simulation and real‐time applications|The overlap between large distributed simulations and real-time applications [1–3] has been always present, and it is increasing fast recently because of the circumstances raised from the current new trends, such as Cloud computing [4, 5], intelligent transportation systems [1,6], green computing [7, 8], and virtual environments [2,9]. As the overlap increases, new challenges appear, mostly for enabling applications, like serious gaming [10], virtual and augmented reality, and collaborative virtual environments [11], to execute seamlessly on distributed resources [12–14]. This special issue intends to collect the state-of-the-art research works that include a set of concepts on high performance computing; design, modeling, and validation of distributed real-time systems; and multi-agent control and simulation systems. This issue also presents as objective to enable access to future trends and directions in the scope of simulations and real-time systems, covering cloud-based simulations, simulation in GPUs, and implementation of distributed virtual environment applications. Nine articles have been chosen from a strict selective process; all of these articles have been thoroughly reviewed by highly qualified anonymous referees. The articles cover a variety of important and challenging topics in the areas of large-scale distributed simulations and real-time systems. Matthew Forshaw, et. al. [15] present a new scalable simulator of High-Throughput Computing systems (HTC-Sim) incorporating real workloads and distinguishing itself by accepting to model multi-use clusters and interactive users. Their model is specially built to work with HTCSim and allow different HTC system policies to be incorporated in the simulation system. Being a large-scale simulation software, HTC-Sim provides the ability to model multi-use clusters and the presence of interactive users; thus, it enables fault tolerance and evaluation of server’s energy consumption in HTC using different resource allocation policies. Arthur Valadares, Eugenia Gabrielova, and Cristina Lopes [16] introduce a conceptual framework for facilitating the design and testing of distributed real-time systems and applications. In their work, they define six properties and concerns while designing a Distributed Real Time (DRT) system. These properties are showcased in real-life, popular examples from different applicable domains in the field of distributed virtual environments. The re-evaluation of the concerns raised in this work shows the importance of some aspect in enhancing user quality of experience in such environments.|Concurrency and Computation|2016|10.1002/cpe.3806|M. S. Notare|0.0|2
230|Ultra-high fidelity radio frequency propagation modeling using distributed high performance graphical processing units: A simulator for multi-element non-stationary antenna systems|A newly-invented, distributed, high-performance graphical processing framework that simulates complex radio frequency (RF) propagation has been developed and demonstrated. The approach uses an advanced computer architecture and intensive multi-core system to enable highperformance data analysis at the fidelity necessary to design and develop modern sensor systems. This widely applicable simulation and modeling technology aids in the design and development of state-of-the-art systems with complex waveforms and more advanced downstream exploitation techniques, e.g., systems with arbitrary RF waveforms, higher RF bandwidths and increasing resolution. The recent breakthroughs in computing hardware, software, systems and applications has enabled these concepts to be tested and demonstrated in a large variety of environments and early in the design cycle. Improvements in simulation accuracies and simulation timescales have been made that immediately increase the value to the end user. A near-analytic RF propagation model increased the computational need by orders of magnitude. This model also increased required numerical precision. The new general purpose graphics processing units (GPGPUs) provided the capability to simulate the propagation effects and model it with the necessary information dependence, and floating point mathematics where performance matters. The relative performance improvement between the baseline MATLAB® parallelized simulation and the equivalent GPU based simulation using 12 NVIDIA Tesla K20m GPUs on the Offspring High-Performance Computer (HPC) using the AirWASP© framework decreased simulation and modeling from 16.5 days to less than 1 day.|IEEE Conference on High Performance Extreme Computing|2017|10.1109/HPEC.2017.8091082|Mark D. Barnell, Nathan Stokes, Jessie Grabowski, Jason Steeger|0.0|2
233|Accelerating frequency-domain simulations using small shared-memory CPU/GPU cluster|Numerical approach to frequency response problems usually requires that the system governing equation is solved repeatedly at many frequencies. The computational efficiency of the overall process can be increased by departing from traditional sequential computing model in favor of utilizing the parallel processing capability commonly offered by modern hardware. In this paper, we consider a hybrid programming pattern, OpenMP + CUDA, from the perspective of a user of a rather typical low-cost multi-core CPU-based workstation that can accommodate up to four GPUs. Such the small-scale heterogeneous platforms have recently gained wide popularity in scientific computing as an inexpensive massively parallel architecture. The relevant programming model issues and performance questions are addressed. Experimental results for the example physics problem, that is, the electromagnetic scattering from perfectly electrically conducting body, show that significant performance improvement can be attained with the OpenMP + CUDA programming model.|2016 21st International Conference on Microwave, Radar and Wireless Communications (MIKON)|2016|10.1109/MIKON.2016.7492098|A. Noga, A. Karwowski, T. Topa|0.0|2
234|Hardware-Centric Vision Processing for Mobile IoT Environment Exploiting Approximate Graph Cut in Resistor Grid|The Internet of things (IoT) has become a general trend in the electronic world. In this IoT era, various types of sensors gathering data are placed everywhere, and visual data is one of the most valuable information. While extracting important features from visual images, complex vision algorithms are often processed by high speed computing units like CPU or GPU. However, most IoT devices are mobile, so hardware resources are severely limited, which disturbs usage of the high performance processors in IoT appliances. Therefore, efficient vision computing becomes a crucial concern in this resource poor environment. Graph cut is a popular and widespread algorithm for image segmentation, image denoising, and stereo matching. However, it suffers from inefficient memory access patterns and massive digital computations. Due to the small hardware capacity not enough to overcome these obstacles, it is hardly feasible to run the algorithm in the IoT devices. In this paper, we propose an approximate version of the graph cut with the aid of the dedicated hardware design. By exploiting the analogy between the max flow and electric currents, we design the theoretical model of an approximate max flow solution and suggest the resistor grid as its processing circuit. Because this work utilizes electric potentials for the min cut computation, it is possible to find the cut at a low energy and a high speed. A prototype circuit is designed and simulated for evaluation. Using public GrabCut benchmark, algorithm evaluations are performed. As a result, we can verify a fast and an energy-efficient image processing with an acceptable accuracy. For supervised binary image segmentation, speed efficiency is 4.57 times higher, and energyefficiency is 9.11 times better when compared to other image/video segmentation works.|IEEE Workshop/Winter Conference on Applications of Computer Vision|2017|10.1109/WACV.2017.92|L. Kim, Yeongjae Choi, Jun-Seok Park|0.0|2
241|Utilizing Computational Techniques to Accelerate Discovery in Peanut Allergenicity: A Case Study|Computational tools and techniques are attractive due to the low-cost and scope of application and are becoming more prevalent and accepted across a variety of fields. With the advancement of high performance computing resources and the recent utility of GPU architecture, molecular dynamics (MD) simulations have become a nearly ubiquitous tool to interrogate protein structure-function relationships. Understanding these structure-function relationships is fundamental to protein biochemistry and facilitates avenues to modulate activity in disease states. Herein, we studied and compared the ability of several GPU- and CPU-based clusters to accelerate mechanistic and therapeutic discovery with MD simulations. MD simulations on the peanut protein Ara h 2, the causative agent of peanut allergenicity, from 10 different species of the genus Arachis were conducted in triplicate for 1 μs to probe structural dynamics and epitope solvent accessibility relevant to allergenicity. These simulations provide molecular insight into the causes and potential potency of the allergenic response. Ultimately, these computational findings and insights can guide experimental design, increasing the impact of therapeutics and understanding of disease states. Further, integrating GPUs into these parallel computing operations accelerates computations and provides researchers with tools to perform MD simulations without large CPU centers. We have thus provided an increased understanding of protein structure-function relationships through the usage of high performance computing and have shown GPU scaling power with MD simulations.|Practice and Experience in Advanced Research Computing|2017|10.1145/3093338.3104154|A. M. Brown, IV L.R.Hollingsworth, D. Bevan|0.0|2
242|Accelerating sino-atrium computer simulations with graphic processing units.|Sino-atrial node cells (SANCs) play a significant role in rhythmic firing. To investigate their role in arrhythmia and interactions with the atrium, computer simulations based on cellular dynamic mathematical models are generally used. However, the large-scale computation usually makes research difficult, given the limited computational power of Central Processing Units (CPUs). In this paper, an accelerating approach with Graphic Processing Units (GPUs) is proposed in a simulation consisting of the SAN tissue and the adjoining atrium. By using the operator splitting method, the computational task was made parallel. Three parallelization strategies were then put forward. The strategy with the shortest running time was further optimized by considering block size, data transfer and partition. The results showed that for a simulation with 500 SANCs and 30 atrial cells, the execution time taken by the non-optimized program decreased 62% with respect to a serial program running on CPU. The execution time decreased by 80% after the program was optimized. The larger the tissue was, the more significant the acceleration became. The results demonstrated the effectiveness of the proposed GPU-accelerating methods and their promising applications in more complicated biological simulations.|Bio-medical materials and engineering|2015|10.3233/BME-151365|Hong Zhang, Zheng Xiao, Shien‐Fong Lin|0.0|2
252|Shader-based physical modelling for the design of massive digital musical instruments|Physical modelling is a sophisticated synthesis technique, often used in the design of Digital Musical Instruments (DMIs). Some of the most precise physical simulations of sound propagation are based on Finite-Difference TimeDomain (FDTD) methods, which are stable, highly parameterizable but characterized by an extremely heavy computational load. This drawback hinders the spread of FDTD from the domain of off-line simulations to the one of DMIs. With this paper, we present a novel approach to real-time physical modelling synthesis, which implements a 2D FDTD solver as a shader program running on the GPU directly within the graphics pipeline. The result is a system capable of running fully interactive, massively sized simulation domains, suitable for novel DMI design. With the help of diagrams and code snippets, we provide the implementation details of a first interactive application, a drum head simulator whose source code is available online. Finally, we evaluate the proposed system, showing how this new approach can work as a valuable alternative to classic GPGPU modelling.|New Interfaces for Musical Expression|2017|10.5281/zenodo.1176203|S. Fels, Andrew Allen, Victor Zappi|0.0|2
256|NVIDIA CUDA technology application to numerical simulation of electromagnetic pulse scattering by dual layered structure|In this paper numerical solution of electromagnetic pulse scattering by dual layered dielectric structure has been considered. For calculation convenience propagation in two-dimensional rectangular area bounded perfectly conducting walls has been treated. Numerical algorithm has been developed using finite difference in time-domain with NVIDIA CUDA technology implementation. A finite difference method implementation for the case of computation by graphical processor has been considered. The results of numerical modeling Gaussian function pulse scattering has been shown. Comparative analysis of the CPU and GPU performance has been retrieved out. As shown, the FDTD method computation by graphical processor allows to achieve a sufficient gain of the performance.|International Siberian Conference on Control and Communications|2015|10.1109/SIBCON.2015.7147200|V. Scherbinin, Ilya P. Molostov, A. Rykshin|0.0|2
258|GPU-Based Parallelized Solver for Large Scale Vascular Blood Flow Modeling and Simulations|Cardio-vascular blood flow simulations are essential in understanding the blood flow behavior during normal and disease conditions. To date, such blood flow simulations have only been done at a macro scale level due to computational limitations. In this paper, we present a GPU based large scale solver that enables modeling the flow even in the smallest arteries. A mechanical equivalent of the circuit based flow modeling system is first developed to employ the GPU computing framework. Numerical studies were employed using a set of 10 million connected vascular elements. Run-time flow analysis were performed to simulate vascular blockages, as well as arterial cut-off. Our results showed that we can achieve ~100 FPS using a GTX 680m and ~40 FPS using a Tegra K1 computing platform.|Medicine Meets Virtual Reality|2016|10.3233/978-1-61499-625-5-345|J. Eldredge, P. Benharash, A. Santhanam, E. Dutson, J. Neylon, J. Teran|0.0|2
263|A 3D pseudospectral solver for nonlinear wave propagation in inhomogeneous attenuating media|The present paper describes an approach to solve a forward problem of nonlinear wave propagation in inhomogeneous and linear attenuating media. The implementation was written in Matlab code, but the equations and algorithm described can be implemented in other programming languages. The object of this work is to create an algorithm of nonlinear wave solver that is useful for ultrasound research and the instruction of nonlinear and ultrasound acoustics, by simulations. In other cases, the model can be quickly and easily modified to address a range of simulation tasks or including improvements with GPUs or Cluster PCs. Validation of acoustics field absorption is showed and compared with Finite Element simulations.|Pan American Health Care Exchanges|2015|10.1109/PAHCE.2015.7173323|I. Bazán, M. Gutiérrez, L. Castellanos, A. Ramírez-García|0.0|2
264|PDSEC Introduction and Committees|Welcome to the 17th IEEE International Workshop on Parallel and Distributed Scientific and Engineering Computing (PDSEC-16), held on May 27, 2016 in Chicago, USA, in conjunction with the 30th IEEE International Parallel and Distributed Processing Symposium (IPDPS 2016). The field of high performance computing has earned prominence through advances in electronic and integrated technologies. Current times are very exciting and the years to come will witness a proliferation in the use of parallel and distributed systems. Again, we saw a continuation in the increase in the use of board-level massively parallel processors for scientific applications, with one third of the papers of the workshop utilizing GP-GPUs and manycore processors for this purpose. We also saw an extent stronger emphasis on the orientation to Exascale computing and related concerns from previous, with several papers dealing with the issue of resilience. The scientific and engineering application domains have a key role in shaping future research and development activities in academia and industry, especially when the solution of large and complex problems must cope with tight timing schedules. One of the most challenging issues facing scientific and engineering computing today is designing the infrastructure required to support massively parallel applications to the meet ever-evolving systems expected over the coming decades. This year we were especially delighted to have Dr. George Bosilca from University of Tennessee, Knoxville, and Dr. Jeremiah Wilke from Sandia National Laboratories, California deliver the two PDSEC-16 keynote talks on parallel programming models, and their research challenges and future. For this year’s workshop we have received many highquality submissions from Australia, Asia Pacific, Europe, North Africa and North America. In a peer-reviewing phase with at least 3 reviews per paper, the submissions were judged by originality, relevance, technical quality, and clarity of presentation. Based on the reviews, we decided to accept 7 high-quality papers for presentation in the technical program of PDSEC-16 out of 14 papers submitted. The annual PDSEC workshop brings together researchers from computer science, applied mathematics and other application areas of high-performance computing to present, discuss and exchange ideas, results, work in progress and experiences in the area of parallel and distributed computing for problems in science and engineering applications. For the contributed papers, there is one application paper on electron dynamics simulation. Two papers focus on task parallelism, one on a CFD code and the other one on radiative heat transport with many GPUs. Two papers are oriented on the area of resilience, both are illustrated with linear algebra applications. Finally two papers are oriented on performance: the former on linear algebra performance using CPUs and GPUs and the latter on an application profiler and emulator. The program for this workshop is the result of hard and excellent work of many others. We would like to express our sincere appreciation to all authors for their valuable contributions and to all program committee members and external reviewers for their cooperation and diligent work in completing the workshop program under a very tight schedule. Last but not least, we thank Bora Uçar (CNRS and ENS Lyon, France) and Ramachandran Vaidyanathan (Louisiana State University, USA) the IPDPS 2016 Workshops Chair and Proceedings chair, for helping and encouraging the inclusion of the PDSEC-16 in IPDPS 2016.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2016|10.1109/IPDPSW.2014.222|K. Teranishi, M. Strout, P. Strazdins, L. Yang, G. Rünger, T. Rauber, R. Couturier|0.0|2
269|Editorial Issue 28.2|This issue contains six papers. In the first paper, Jong‐Hyun Kim and Chang‐Hun Kim, Korea University, Seoul, and Jung Lee, Hallym University, Gangwon‐do, Republic of Korea, present a new method for the fast simulation of interactions between fluids and solids by incorporating particle‐based water flow into an adaptive signed distance field (ADF). This paper addresses the problem of “tunneling” artifact, in which particles with high velocities skip across the layer of leaf nodes by (i) considering particles only on the leaf nodes in the adaptive structure to improve the processing time required for the water–solid coupling and (ii) considering the water flow to avoid the tunneling artifact by incorporating particle motion into the tree structure of the ADF. In the second paper, Rodrigo Guillermo Baravalle and Leonardo Scandolo, Universidad Nacional de Rosario, Argentina; Claudio Delrieux, Universidad Nacional del Sur, Buenos Aires, Argentina; Cristian García Bauza, Universidad Nacional del Centro de la Provincia de Buenos Aires, Argentina; and Elmar Eisemann, Delft University of Technology, The Netherlands, propose an algorithm for the procedural generation of porous materials based on a simulation of the growth of self‐avoiding bubbles inside a volume, by means of dynamical systems. This simulation generates 3D textures that adequately represent porous materials, which are then rendered by means of a GPU‐based direct volume rendering method. The patterns induced by the bubbles can be intuitively controlled. The bubbles adapt to any given shape and have convincing global and local fluid‐like patterns as seen in bread and sponges. In the third paper, Andre Possani‐Espinosa, J. Octavio Gutierrez‐Garcia, and Isaac Vargas Gordillo, Instituto Tecnológico Autónomo de México, lay the foundations for the design of believable virtual drivers by proposing a methodology for profiling players using the open racing car simulator. Data collected from 125 players about their driving behaviors and personality traits give insights into how personality traits should model the behavior of believable virtual drivers. The data analysis was conducted using a correlation analysis and the J48 decision tree algorithm. In addition, this work also (i) gives preliminary insights into the relationship between the driving behavior and personality of racing game players and actual car drivers and (ii) presents evidence of the relevance of gender as a predictor of personality traits of racing game players.|Comput. Animat. Virtual Worlds|2017|10.1002/cav.1753|D. Thalmann, N. Magnenat-Thalmann|0.0|2
280|Efficient Reliability Support for Hardware Multicast-Based Broadcast in GPU-enabled Streaming Applications|Streaming applications, which are data-intensive, have been extensively run on High-Performance Computing (HPC) systems to seek the higher performance and scalability. These applications typically utilize broadcast operations to disseminate in real-time data from a single source to multiple workers, each being a multi-GPU based computing site. State-of-the-art broadcast operations take advantage of InfiniBand (IB) hardware multicast (MCAST) and NVIDIA GPUDirect features to boost inter-node communications performance and scalability. The IB MCAST feature works only with the IB Unreliable Datagram (UD) mechanism and consequently provides unreliable communication for applications. Higher-level libraries and/or runtime environments must handle and provide reliability explicitly. However, handling reliability at that level can be a performance bottleneck for streaming applications. In this paper, we analyze the specific requirements of streaming applications and the performance bottlenecks involved in handling reliability. We show that the traditional Negative-Acknowledgement (NACK) based approach requires the broadcast sender to perform retransmissions for lost packets, degrading streaming throughput. To alleviate this issue, we propose a novel Remote Memory Access (RMA) based scheme to provide high-performance reliability support at the MPI-level. In the proposed scheme, the receivers themselves (as opposed to the sender) retrieve lost packets through RMA operations. Furthermore, we provide an analytical model to illustrate the memory requirements of the proposed RMA-based scheme. Our experimental results show that the proposed scheme introduces nearly no overhead compared to the existing solutions. In a micro-benchmark with injected failures (to simulate unreliable network environments), the proposed scheme shows up to 45% reduction in latency compared to the existing NACK-based scheme. Moreover, with a synthetic streaming benchmark, our design also shows up to a 56% higher broadcast rate compared to the traditional NACK-based scheme on a GPU-dense Cray CS-Storm system with up to 88 NVIDIA K80 GPU cards.|2016 First International Workshop on Communication Optimizations in HPC (COMHPC)|2016|10.1109/COM-HPC.2016.9|Ching-Hsiang Chu, Khaled Hamidouche, Akshay Venkatesh, H. Subramoni, D. Panda, B. Elton|0.0|2
289|HAMEX: heterogeneous architecture and memory exploration framework|The increasing amount of computation in heterogeneous architectures (including CPU and GPU cores) puts a big burden on memory subsystem. With the gap between compute units and the memory performance getting wider, designing a platform with a responsive memory system becomes more challenging. This issue is exacerbated when memory systems have to satisfy a high volume of traffic generated from heterogeneous compute units. Furthermore, as emerging memory technologies are being introduced to address these issues, a rapid and flexible mechanism is needed to evaluate these technologies in the context of heterogeneous architectures. This paper proposes HAMEX, a framework that enables early design space exploration of heterogeneous systems with a focus on resolving memory access bottlenecks. This framework first allows system designers to easily model heterogeneous architectures that can run both CPU and GPU workloads. Next, given a set of workloads partitioned on various compute units, traffic generated by these units are captured in order to explore different memory systems. We show the feasibility of design space exploration using HAMEX by simulating a contemporary commercial heterogeneous platform and explore the opportunities for power and performance improvements by adopting different memory technologies.|RSP|2016|10.1145/2990299.2990316|Kasra Moazzemi, N. Dutt, Chen-Ying Hsieh|0.0|2
345|Accelerating Molecular Structure Determination Based on Inter-Atomic Distances Using OpenCL|Fast and accurate determination of the 3D structure of molecules is essential for better understanding their physical, chemical, and biological properties. We focus on an existing method for molecular structure determination: restrained molecular dynamics with simulated annealing. In this method a hybrid function, composed by a physical model and experimental restraints, is minimized by simulated annealing. Our goal is to accelerate computation time using commodity multi-core CPUs and GPUs in a heterogeneous computing model. We present a parallel and portable OpenCL implementation of this method. Experimental results are discussed in terms of accuracy, execution time, and parallel scalability. With respect to the XPLOR-NIH professional software package, compared to the single CPU core implementation, we obtain speedups of three to five times (increasing with problem size) on commodity GPUs. We achieve these performances by writing specialized kernels for different problem sizes and hardware architectures.|IEEE Transactions on Parallel and Distributed Systems|2015|10.1109/TPDS.2014.2385712|I. Lorentz, L. Fabry-Asztalos, Razvan Andonie|0.0|2
347|GPU implementation of a constrained hyperspectral coded aperture algorithm for compressive sensing|In this paper, a parallel implementation of a previously constrained hyperspectral coded aperture (CHYCA) algorithm for compressive sensing on graphics processing units (GPUs) is proposed. CHYCA method combines the ideas of spectral unmixing and compressive sensing exploiting the high spatial correlation that can be observed in the data and the generally low number of endmembers needed in order to explain the data. The performance of CHYCA relies which does not depend on the tuning of a regularization parameter, which is a time consuming task offering good performance compared with a previously hyperspectral coded aperture (HYCA) method. The proposed implementation exploits the GPU architecture at low level, thus taking full advantage of the computational power of GPUs using shared memory and coalesced accesses to memory. Experimental results using simulated data reveals speedups up to 56 times, with regards to serial implementation.|Workshop on Hyperspectral Image and Signal Processing|2015|10.1109/WHISPERS.2015.8075456|G. Martín, Vítor Silva, A. Plaza, J. Bioucas-Dias, S. Bernabé, J. Nascimento|0.0|2
352|Reproducible large-scale social simulations on various computing environment|In this paper, we propose parallel computing techniques for reproducible large-scale social simulations on various computing environments including CPU (Central Processing Unit) or GPU (Graphic Processing Unit). When we use computing resources for large-scale social simulations, the reproducibility of a simulation should be considered. “Reproducibility” means the same trial of a simulation can be repeated. If the same computing resources are available to repeat the trial, it is easy to reproduce the same simulation results. When not all the same computing resources are available, however, it becomes difficult to obtain the same trial since random number generators may become different from the original computation resources. In this study, we employ multi-thread computing on CPU or GPU. We propose two models to run reproducible social simulations on CPU or GPU. One is to parallelize trials (Trial Parallelization). The other is to parallelize agents of a single simulation (Agent Parallelization). These models can be ensured reproducibility even in different computing resources. Our experimental results show that the same computing processes are obtained on CPU or GPU. When we parallelize large-scale social simulation on CPU or GPU, we can accelerate the simulation as a secondary effect.|2017 Joint 17th World Congress of International Fuzzy Systems Association and 9th International Conference on Soft Computing and Intelligent Systems (IFSA-SCIS)|2017|10.1109/IFSA-SCIS.2017.8023303|T. Murata, Takuya Harada|0.0|2
353|Decoupled Vector-Fetch Architecture with a Scalarizing Compiler|"Author(s): Lee, Yunsup | Advisor(s): Asanovic, Krste | Abstract: As we approach the end of conventional technology scaling, computer architects are forced to incorporate specialized and heterogeneous accelerators into general-purpose processors for greater energy efficiency. Among the prominent accelerators that have recently become more popular are data-parallel processing units, such as classic vector units, SIMD units, and graphics processing units (GPUs). Surveying a wide range of data-parallel architectures and their parallel programming models and compilers reveals an opportunity to construct a new data-parallel machine that is highly performant and efficient, yet a favorable compiler target that maintains the same level of programmability as the others.In this thesis, I present the Hwacha decoupled vector-fetch architecture as the basis of a new data-parallel machine. I reason through the design decisions while describing its programming model, microarchitecture, and LLVM-based scalarizing compiler that efficiently maps OpenCL kernels to the architecture. The Hwacha vector unit is implemented in Chisel as an accelerator attached to a RISC-V Rocket control processor within the open-source Rocket Chip SoC generator. Using complete VLSI implementations of Hwacha, including a cache-coherent memory hierarchy in a commercial 28 nm process and simulated LPDDR3 DRAM modules, I quantify the area, performance, and energy consumption of the Hwacha accelerator. These numbers are then validated against an ARM Mali-T628 MP6 GPU, also built in a 28 nm process, using a set of OpenCL microbenchmarks compiled from the same source code with our custom compiler and ARM's stock OpenCL compiler."||2016|10.1016/b978-0-12-800386-2.00006-7|Yunsup Lee|0.0|2
358|Maritime scene design tools for realistic radar performance predictions|The GPU computing has recently opened news efficient ways for radar simulation. For small target detection and for advanced processings such as imagery SAR/ISAR, the simulation of realistic raw data is crucial. It concerns maritime patrol applications from unmanned and manned platforms, such as presented in [1]. The aim of this paper is to present works driven by DGA and to illustrate the recent progress and capabilities offered by two recent tools: - “MOCVIEW” for target raw data generator of maritime scenes (using “MOCEM” model [2] of the ship). - “PHYS_IQ” for clutter radar IQ signal generator. Maritime scenes need to take into account: - For the target, realistic 3D ship models with right Electromagnetic (EM) materials, ship motions - For the sea clutter, it is essential to deal with the grazing angle challenge or effects associated to High sea states (shadows, sea spikes, heavy tail distributions) [3].|IEEE International Geoscience and Remote Sensing Symposium|2015|10.1109/IGARSS.2015.7326478|J. Louvigne, C. Cochin|0.0|2
371|Advanced Optimizations of An Implicit Navier-Stokes Solver on GPGPU|General-purpose computing on graphics processing units (GPGPU) is a massive fine-grain parallel computation platform, which is is particularly attractive for CFD tasks due to its potential of one or two magnitudes of performance improvement with relatively low capital investment. Many successful attempts have been reported in recent years (see, for example [1, 2, 3, 4, 5, 6]). Although early attempts of utilizing GPGPU for CFD has been hampered by the heterogeneous nature of GPGPU hardware and complex programming tools, recent GPU technology has seen significant improvement on programming toolchain. The emergence of OpenACC [7], a directive-based programming model closely resembling OpenMP, further reduces the obstacles for efficient GPGPU programming. By allowing programmers to use a collection of compiler directives to specify loops and regions of their codes to be offloaded from a host CPU to GPGPU, this programming model offers a good balance between porting efforts and performance gain. As shown in [8, 6], our previous attempts of porting existing CFD codes to GPGPU using OpenACC and MVAPICH2 [9] have proven the advantages on portability and maintainability of this approach. Even with the assistance of convenient development tools many of the challenges in GPGPU can be resolved, including memory contingency and efficient data exchange between multiple GPU’s. A more fundamental performance restriction is encountered when porting implicit time integration schemes. Employing an implicit time marching instead of explicit time marching can often significantly improve simulation speed. This is particularly true when the physical time scale is relatively large, as is often the case in incompressible flows. From a more general point of view, governing equations of many complex systems are so stiff that an explicit method may never converge under any reasonable CFL number. One prime example is multiphase reactive flows, which, due to their incorporation of a large number of variables and equations for describing the complex thermodynamical and chemical processes, often result in highly stiff linear systems. Another example is highly streched computational grids due to unusual boundary shapes of the domain, which also result in stiff linear systems. Even formulated as an implicit problem, the resulting system matrix is often non-diagonally||2015|10.2514/6.2015-0052|H. Luo, Lixiang Luo, J. Edwards, F. Mueller|0.0|2
379|Fireflies: New Software for Interactively Exploring Dynamical Systems Using GPU Computing|"In non-linear systems, where explicit analytic solutions usually can't be found, visualisation is a powerful approach which can give insights into the dynamical behaviour of models; it is also crucial for teaching this area of mathematics. In this paper we present new software, Fireflies, which exploits the power of graphical processing unit (GPU) computing to produce spectacular interactive visualisations of arbitrary systems of ordinary differential equations. In contrast to typical phase portraits, Fireflies draws the current position of trajectories (projected onto 2D or 3D space) as single points of light, which move as the system is simulated. Due to the massively parallel nature of GPU hardware, Fireflies is able to simulate millions of trajectories in parallel (even on standard desktop computer hardware), producing ""swarms"" of particles that move around the screen in real-time according to the equations of the system. Particles that move forwards in time reveal stable attractors (e.g. fixed points and limit cycles), while the option of integrating another group of trajectories backwards in time can reveal unstable objects (repellers). Fireflies allows the user to change the parameters of the system as it is running, in order to see the effect that they have on the dynamics and to observe bifurcations. We demonstrate the capabilities of the software with three examples: a two-dimensional ""mean field"" model of neuronal activity, the classical Lorenz system, and a 15-dimensional model of three interacting biologically realistic neurons."|International Journal of Bifurcation and Chaos in Applied Sciences and Engineering|2015|10.1142/S0218127415501813|Robert Merrison-Hort|0.0|2
381|Modeling and simulation of tumor-influenced high resolution real-time physics-based breast models for model-guided robotic interventions|Breast radiation therapy is typically delivered to the patient in either supine or prone position. Each of these positioning systems has its limitations in terms of tumor localization, dose to the surrounding normal structures, and patient comfort. We envision developing a pneumatically controlled breast immobilization device that will enable the benefits of both supine and prone positioning. In this paper, we present a physics-based breast deformable model that aids in both the design of the breast immobilization device as well as a control module for the device during every day positioning. The model geometry is generated from a subject’s CT scan acquired during the treatment planning stage. A GPU based deformable model is then generated for the breast. A mass-spring-damper approach is then employed for the deformable model, with the spring modeled to represent a hyperelastic tissue behavior. Each voxel of the CT scan is then associated with a mass element, which gives the model its high resolution nature. The subject specific elasticity is then estimated from a CT scan in prone position. Our results show that the model can deform at >60 deformations per second, which satisfies the real-time requirement for robotic positioning. The model interacts with a computer designed immobilization device to position the breast and tumor anatomy in a reproducible location. The design of the immobilization device was also systematically varied based on the breast geometry, tumor location, elasticity distribution and the reproducibility of the desired tumor location.|SPIE Medical Imaging|2016|10.1117/12.2217028|K. Sheng, K. Hasse, A. Santhanam, J. Neylon|0.0|2
391|Hierarchical Parallelism in a Physical Modelling Synthesis Code|Modern computer hardware provides parallelism at various different levels most obviously, multiple multicore processors allow many independent threads to execute at once. At a finer-grained level, each core contains a vector unit allowing multiple integer or floating point calculations to be performed with a single instruction. Additionally, GPU hardware is highly parallel and performs best when processing large numbers of independent threads. At the same time, tools such as CUDA have become steadily more abundant and mature, allowing more of this parallelism to be exploited. In this paper we describe the process of optimising a physical modelling sound synthesis code, the Multiplate 3D code, which models the acoustic response of a number of metal plates embedded within a box of air. This code presented a number of challenges and no single optimisation technique was applicable to all of these. However, by exploiting parallelism at several different levels (multithreading, GPU acceleration, and vectorisation), as well as applying other optimisations, it was possible to speed up the simulation very significantly.|International Conference on Parallel Computing|2015|10.3233/978-1-61499-621-7-207|S. Bilbao, A. Torin, James Perry|0.0|2
395|FPGA Implementation of Non-Uniform DFT for Accelerating Wireless Channel Simulations (Abstract Only)|FPGAs have been used as accelerators in a wide variety of domains such as learning, search, genomics, signal processing, compression, analytics and so on. In recent years, the availability of tools and flows such as high-level synthesis has made it even easier to accelerate a variety of high-performance computing applications onto FPGAs. In this paper we propose a systematic methodology for optimizing the performance of an accelerated block using the notion of compute intensity to guide optimizations in high-level synthesis. We demonstrate the effectiveness of our methodology on an FPGA implementation of a non-uniform discrete Fourier transform (NUDFT), used to convert a wireless channel model from the time-domain to the frequency domain. The acceleration of this particular computation can be used to improve the performance and capacity of wireless channel simulation, which has wide applications in the system level design and performance evaluation of wireless networks. Our results show that our FPGA implementation outperforms the same code offloaded onto GPUs and CPUs by 1.6x and 10x respectively, in performance as measured by the throughput of the accelerated block. The gains in performance per watt versus GPUs and CPUs are 15.6x and 41.5x respectively.|Symposium on Field Programmable Gate Arrays|2017|10.1145/3020078.3021800|Srinivas Siripurapu, Aman Gayasen, P. Gopalakrishnan, N. Chandrachoodan|0.0|2
400|Enhanced Preemptive Global Utility Accrual Real Time Scheduling Algorithms in Multicore Environment|This paper proposed an efficient real time scheduling algorithm using global scheduling paradigm running in multicore environment known as Global Preemptive Utility Accrual Scheduling (GPUAS) algorithm. The existing TUF/UA multiprocessor scheduling algorithms known as Greedy-Global Utility Accrual (G-GUA) and Non Greedy-Global Utility Accrual (NG-GUA) algorithms is seen to overlook the efficiency on its task scheduling algorithm. These algorithms have adapted the task migration attribute considering the load balancing problem in multi core platform. The existing PUAS uniprocessor scheduling algorithm is mapped into the multicore scheduling environment that consists of the global scheduling schemes considering the migration attribute of the executed tasks. The main principal of global scheduling is that it allows the executed tasks to migrate from one processor to the other processors whenever a scheduling event occurs in the system. The proposed GPUAS algorithm inherits the characteristics of PUAS in uniprocessor where it can preempt the highest PUD task at any event that occurs in the system. In this research, the proposed GPUAS algorithm enhanced the existing NG-GUA and G-GUA algorithms. The developed simulator has derived the set of parameter, events and performance metrics according to a detailed analysis of the base model. The proposed GPUAS algorithm achieved the highest accrued utility for the entire load range. The proposed GPUAS algorithm is more efficient than the existing algorithms, producing the highest accrued utility ratio and less abortion ratio making it more suitable and efficient for real time application domain.|Journal of Computer Science|2015|10.3844/jcssp.2015.1099.1107|Idawaty Ahmad, Z. Zulkarnain, M. Othman|0.0|2
401|Benchmarking neuromorphic systems with Nengo|Nengo is a software package for designing and simulating large-scale neural models. Nengo is architected such that the same Nengo model can be simulated on any of several Nengo backends with few to no modifications. Backends translate a model to specific platforms, which include GPUs and neuromorphic hardware. Nengo also contains a large test suite that can be run with any backend and focuses primarily on functional performance. We propose that Nengo's large test suite can be used to benchmark neuromorphic hardware's functional performance and simulation speed in an efficient, unbiased, and future-proof manner. We implement four benchmark models and show that Nengo can collect metrics across five different backends that identify situations in which some backends perform more accurately or quickly.|Frontiers in Neuroscience|2015|10.3389/fnins.2015.00380|T. Stewart, Trevor Bekolay, C. Eliasmith|0.0|2
406|Optimizing the Bayesian Inference of Phylogeny on Graphic Processors|Searching for the evolutionary relationships between groups of organism has become a routine procedure in molecular biology. MrBayes is a popular model based phylogenetic inference tool using Bayesian statistics. Unfortunately, the computational cost is very high, resulting in undesirably long execution time. In this paper, we present what we believe the fastest solution of the MrBayes MC3 algorithm running on off-the-shelf graphic processors. The performance benefits are offered by the multi-granularity parallelism model, coarse-grained GPU kernel system, efficient thread arrangement strategy and GPU code level optimizations. MrBayes goMC3 (proposed herein) provides a significant performance improvement over the sequential MrBayes MC3 by a speedup of up to 48× when using single Tesla C2075 GPU card, whereas a speedup factor of 77× can be achieved when using dual GPUs. In comparison to the state-of-the-art version of other publicly available GPU implementations of MrBayes MC3, the cumulative optimizations adopted in goMC3 resulted in a speedup of up 2.5× over oMC3 (v1.0), 1.75× over tgMC3 (v1.0) and 1.46× over nMC3(v2.1.1) for realistic empirical biological datasets. Besides, experimental results indicated that goMC3 outstrips these GPU implementations on the analysis of simulated datasets composed of ultra-large-scale sequences. As a consequence, the reported performance improvement of goMC3 is significant and appears to scale well with increasing dataset sizes.|2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing|2015|10.1109/CCGrid.2015.13|A. Luo, T. Hamada, Cheng Ling, Guoguang Zhao, Chunbao Zhou, Xiaoyan Zhu|0.0|2
421|Accelerating the D3Q19 Lattice Boltzmann Model with OpenACC and MPI|Multi-GPU implementations of the Lattice Boltzmann method are of practical interest as they allow the study of turbulent flows on large-scale simulations at high Reynolds numbers. Although programming GPUs, and in general power-efficient accelerators, typically guarantees high performances, the lack of portability in their low-level programming models implies significant efforts for maintainability and porting of applications. Directive-based models such as OpenACC look promising in tackling these aspects. In this work we will evaluate the performances of a Multi-GPU implementation of the Lattice Boltzmann method accelerated with OpenACC. The implementation will allow for multi-node simulations of fluid flows in complex geometries, also supporting heterogeneous clusters for which the load balancing problem is investigated.||2015||A. Gabbana|0.0|2
424|MAGC: A Mapping Approach for GPU Clusters|GPU accelerators have been increasingly used in modern heterogeneous HPC clusters by offering high performance and energy efficiency. Such heterogeneous GPU clusters consisting of multiple CPU cores and GPU devices have become the platform of choice for many HPC applications. The communication channels among these processing elements expose different latency and bandwidth characteristics. Thus, efficient utilization of communication channels becomes an important factor for achieving higher inter-process communication performance. In this paper, we exploit topology awareness for a better utilization of communication channels in GPU clusters. We first discuss the challenges associated with topology-aware mapping in GPU clusters, and then propose MAGC, a Mapping Approach for GPU Clusters. MAGC seeks to improve the total communication performance by a joint consideration of both CPU-to-CPU and GPU-to-GPU communications of the application, and CPU and GPU physical topologies of the underlying GPU cluster. It provides a unified framework for topology-aware process-to-core mapping and GPU-to-process assignment across a GPU cluster. We study the potential benefits of MAGC with two different mapping algorithms: a) the Scotch graph mapping library, and b) a heuristic designed to explicitly consider maximum congestion. We evaluate our design through extensive experiments at micro-benchmark and application levels on two GPU clusters with different GPU types and topologies. We have developed a micro-benchmark suite to model various communication patterns among CPU cores and among GPU devices. For application results, we use the molecular dynamics simulator, HOOMD-blue. Micro-benchmark results show that we can achieve up to 91.4% improvement in communication time. At the application level, we can achieve up to 8% performance improvement.|Symposium on Computer Architecture and High Performance Computing|2016|10.1109/SBAC-PAD.2016.15|Iman Faraji, A. Afsahi, S. Mirsadeghi|0.0|2
425|Performance Analysis of CPU-GPU Cluster Architectures|"High performance computing (HPC) encompasses advanced computation over parallel processing, enabling faster execution of highly compute intensive tasks such as climate research, molecular modeling, physical simulations, cryptanalysis, geophysical modeling, automotive and aerospace design, financial modeling, data mining and more. High performance simulations require the most efficient compute platforms. The execution time of a given simulation depends upon many factors, such as the number of CPU/GPU cores and their utilization factor and the interconnect performance, efficiency, and scalability. CPU and GPU clusters are one of the most progressive branches in a field of parallel computing and data processing nowadays. GPUs have become increasingly common in supercomputing, serving as accelerators or ""co-processors"" in every node CPU-GPU to help CPUs get work done faster. In this paper I use the Multiclass Closed Product-Form Queueing Network (MCPFQN) and Mean Value Analysis (MVA) to analyze effects of the CPU-GPU cluster interconnect on the performance of computer systems."||2015|10.11648/J.AJNC.20150403.18|Ho Khanh Lam|0.0|2
431|Current challenges in simulations of HPC systems|Simulation of many-core HPC systems is nowadays an active and fruitful area of research. Recent and future proposals are driven by the need of a fast, efficient, and comprehensive simulation framework. This simulation framework should be complete in several ways. First, it should model a wide range of components and provide the mechanisms necessary to plug-in more components as needed. Second, it should allow the designer to focus on critical components while avoiding a large part of the simulation complexity. Each of these components should be able to be evaluated with multiple models with distinct detail levels, ranging from simply analytical models to detailed cycle-accurate simulations. Third, a complete simulation framework should provide a wide range of metrics of interest for the designer and the market. Finally, support for heterogeneous architectures combining CPU and GPU, as well as some degree of reconfigurability is surely required. Building such titanic framework is and will be a collaborative process between researchers around the globe and it is expected to be a hot research topic for the next years.|International Symposium on High Performance Computing Systems and Applications|2015|10.1109/HPCSim.2015.7237110|S. Petit|0.0|2
433|Z2 traversal order for VR stereo rendering on tile-based mobile GPUs|With increasing demands of virtual reality (VR) applications, efficient VR rendering techniques are becoming essential because VR stereo rendering requires increased computational costs to separately render views for the left and right eyes. To reduce the rendering cost in VR applications, we present a novel traversal order for tile-based mobile GPU architectures, called the Z2 traversal order. In tile-based mobile GPU architectures, a tile traversal order that maximizes spatial locality can increase the GPU cache efficiency. For VR applications, our approach improves the traditional Z-curve order; we render two screen tiles in the left and right views by turns or simultaneously, as a result, we can exploit spatial locality between the two tiles. To evaluate our approach, we conducted a trace-driven hardware simulation using Mesa and a hardware simulator. The experimental results show that the Z2 traversal order can reduce external memory bandwidth requirements and can increase rendering performance.|SIGGRAPH Asia Technical Briefs|2016|10.1145/3005358.3005374|Yeongkyu Lim, Sunho Ki, Jae-Ho Nah, Chulho Shin|0.0|2
457|Mixed Precision Vector Processors|Mixed-Precision Vector Processors by Albert Ji-Hung Ou Master of Science in Computer Science University of California, Berkeley Professor Krste Asanovic, Chair Mixed-precision computation presents opportunities for programmable accelerators to improve performance and energy efficiency while retaining application flexibility. Building on the Hwacha decoupled vector-fetch accelerator, we introduce high-occupancy vector lanes (HOV), a set of mixed-precision hardware optimizations which support dynamic configuration of multiple architectural register widths and high-throughput operations on packed data. We discuss the implications of HOV for the programming model and describe our microarchitectural approach to maximizing register file utilization and datapath parallelism. Using complete VLSI implementations of HOV in a commercial 28 nm process technology, featuring a cache-coherent memory hierarchy with L2 caches and simulated LPDDR3 DRAM modules, we quantify the impact of our HOV enhancements on area, performance, and energy consumption compared to the baseline design, a decoupled vector architecture without mixed-precision support. We observe as much as a 64.3% performance gain and a 61.6% energy reduction over the baseline vector machine on half-precision dense matrix multiplication. We then validate the HOV design against the ARM Mali-T628 MP6 GPU by running a suite of microbenchmarks compiled from the same OpenCL source code using our custom HOV-enabled compiler and the ARM stock compiler.||2015|10.1016/b978-0-12-418663-7.00006-x|Albert J. Ou|0.0|2
461|A GPU-based micromechanical simulation model for submerged geotechnical problems|Driven steel piles are commonly used as deep foundations for a wide range of engineering structures, particularly in the offshore branch. They are also an interesting example among the broad spectrum of geotechnical applications where the fluid-solid interaction at the pore-scale can play a major role for the macromechanical behaviour of the whole system. \n \nIn the context of the geotechnical practice for offshore wind-farm structures, both the industrial design and the actual dimensions of the large piles used as foundations in the seabed are often driven by factors such as the soil resistance to driving (SRD), which are still not well understood and often estimated based on mere empirical correlations or overly simplified one-dimensional models. In particular, the role of the micromechanical effects during the installation process (e.g. local dilatancy or contractancy) and their consequences on the pore pressure levels at the pile-tip and on the effective resistance to driving, are generally either disregarded or at most assumed to be covered by the simplified engineering “black-box” solutions. \n \nHere, we propose a general framework to address such local aspects of a geotechnical application involving fluid-saturated soils while retaining the focus on the micro-scale phenomena. We advocate for an approach that combines the relative simplicity of the Discrete Element Method (DEM) for the solid mechanics with the capabilities of the Lattice Boltzmann Method (LBM) for the fluid dynamics. In this sense, we aim to compile some useful techniques and practical recommendations for an efficient GPU-based implementation of a micromechanical LBM-DEM simulation tool.||2019|10.30884/jogs/2019.01.09|P. Cuéllar|0.0|2
462|Simulation of Multimedia Data Parallelization Process when Processing on CPU and GPU in Online Learning Cloud Platforms|The article describes the principle of operation of a parallel algorithm for generating terrain (landscape) and the superposition method based on the architecture of computing unified devices. The presented solutions were implemented in the design and creation of a software module for the implementation and research of virtual models of environmental objects in online courses. An innovative approach is proposed, which allows solving the problem of efficient distribution and use of computing resources in processing large streams of multimedia data by reducing the load on the central processor, video and RAM. A model of the process of multimedia data parallelization is presented when processing on a CPU and GPU in cloud-based online learning platforms. The result of visualization of a three-dimensional landscape obtained as a result of the multimedia data parallelization process during its generation is presented. It was concluded that the developed model and the algorithm are capable of conducting high-quality modeling of realistic three-dimensional images of various objects with a significant reduction in the requirements for the productive capacity and computer resources of the student.|2019 International Russian Automation Conference (RusAutoCon)|2019|10.1109/RUSAUTOCON.2019.8867796|V. Zaporozhko, V. Shardakov, D. Parfenov|0.0|2
479|Cost-efficiency of Large-scale Electronic Structure Simulations with Intel Xeon Phi Processors|Benefits of Intel Xeon Phi Knights Landing (KNL) systems in computing cost are studied with tight-binding simulations of large-scale electronic structures that involve sparse system matrices whose dimensions normally reach several tens of millions. Speed and energy usage of our in-house Schrödinger equation solver are benchmarked in KNL systems for realistic modelling tasks, and are discussed against the cost required by offload computing with P100 devices. Superiority in speed and energy-efficiency observed in KNL systems justify the practicality of bootable manycore processors that are adopted by nearly 30% of largest supercomputers in the world. With a demonstration of the strong scalability up to 2,500 nodes, this work serves as an useful case study that supports the utility of KNL systems for handling memory-bound applications including ours and other numerical problems that involve large-scale sparse matrix-vector multiplications, particularly compared to GPU-based systems.|IEEE International Conference on Cluster Computing|2019|10.1109/CLUSTER.2019.8891046|H. Ryu, Seungmin Lee|0.0|2
484|High Performance Agent-based Models with Real-Time in situ Visualization of inflammatory and Healing responses in Injured vocal folds|Title of dissertation: HIGH PERFORMANCE AGENT-BASED MODELS WITH REAL-TIME IN SITU VISUALIZATION OF INFLAMMATORY AND HEALING RESPONSES IN INJURED VOCAL FOLDS Nuttiiya Seekhao Doctor of Philosophy, 2019 Dissertation directed by: Professor Joseph JaJa Department of Electrical and Computer Engineering University of Maryland, College Park Dr. Nicole Yee-Key Li-Jessen School of Communication Sciences and Disorders McGill University, Montreal, Québec, Canada The introduction of clusters of multi-core and many-core processors has played a major role in recent advances in tackling a wide range of new challenging applications and in enabling new frontiers in BigData. However, as the computing power increases, the programming complexity to take optimal advantage of the machine’s resources has significantly increased. High-performance computing (HPC) techniques are crucial in realizing the full potential of parallel computing. This research is an interdisciplinary effort focusing on two major directions. The first involves the introduction of HPC techniques to substantially improve the performance of complex biological agent-based models (ABM) simulations, more specifically simulations that are related to the inflammatory and healing responses of vocal folds at the physiological scale in mammals. The second direction involves improvements and extensions of the existing state-of-the-art vocal fold repair models. These improvements and extensions include comprehensive visualization of large data sets generated by the model and a significant increase in user-simulation interactivity. We developed a highly-interactive remote simulation and visualization framework for vocal fold (VF) agent-based modeling (ABM). The 3D VF ABM was verified through comparisons with empirical vocal fold data. Representative trends of biomarker predictions in surgically injured vocal folds were observed. The physiologically representative human VF ABM consisted of more than 15 million mobile biological cells. The model maintained and generated 1.7 billion signaling and extracellular matrix (ECM) protein data points in each iteration. The VF ABM employed HPC techniques to optimize its performance by concurrently utilizing the power of multi-core CPU and multiple GPUs. The optimization techniques included the minimization of data transfer between the CPU host and the rendering GPU. These transfer minimization techniques also reduced transfers between peer GPUs in multi-GPU setups. The data transfer minimization techniques were executed with a scheduling scheme that aims to achieve load balancing, maximum overlap of computation and communication, and a high degree of interactivity. This scheduling scheme achieved optimal interactivity by hyper-tasking the available GPUs (GHT). In comparison to the original serial implementation on a popular ABM framework, NetLogo, these schemes have shown substantial performance improvements of 400x and 800x for the 2D and 3D model, respectively. Furthermore, the combination of data footprint and data transfer reduction techniques with GHT achieved highinteractivity visualization with an average framerate of 42.8 fps. This performance enabled the users to perform real-time data exploration on large simulated outputs and steer the course of their simulation as needed.||2019|10.13016/PONX-UMET|Nuttiiya Seekhao|0.0|2
485|Advancing the Multi-Solver Paradigm for Overset CFD Toward Heterogeneous Architectures|Title of dissertation: ADVANCING THE MULTI-SOLVER PARADIGM FOR OVERSET CFD TOWARD HETEROGENEOUS ARCHITECTURES Dylan Jude Doctor of Philosophy, 2019 Dissertation directed by: Professor James Baeder Department of Aerospace Engineering A multi-solver, overset, computational fluid dynamics framework is developed for efficient, large-scale simulation of rotorcraft problems. Two primary features distinguish the developed framework from the current state of the art. First, the framework is designed for heterogeneous compute architectures, making use of both traditional codes run on the Central Processing Unit (CPU) as well as codes run on the Graphics Processing Unit (GPU). Second, a framework-level implementation of the Generalized Minimal Residual linear solver is used to consider all meshes from all solvers in a single linear system. The developed GPU flow solver and framework are validated against conventional implementations, achieving a 5.35× speedup for a single GPU compared to 24 CPU cores. Similarly, the overset linear solver is compared to traditional techniques, demonstrating the same convergence order can be achieved using as few as half the number of iterations. Applications of the developed methods are organized into two chapters. First, the heterogeneous, overset framework is applied to a notional helicopter configuration based on the ROBIN wind tunnel experiments. A tail rotor and hub are added to create a challenging case representative of a realistic, full-rotorcraft simulation. Interactional aerodynamics between the different components are reviewed in detail. The second application chapter focuses on performance of the overset linear solver for unsteady applications. The GPU solver is used along with an unstructured code to simulate laminar flow over a sphere as well as laminar coaxial rotors designed for a Mars helicopter. In all results, the overset linear solver out-performs the traditional, de-coupled approach. Conclusions drawn from both the full-rotorcraft and overset linear solver simulations can have a significant impact on improving modeling of complex rotorcraft aerodynamics. ADVANCING THE MULTI-SOLVER PARADIGM FOR OVERSET CFD TOWARD HETEROGENEOUS ARCHITECTURES||2019|10.13016/KCEC-84KK|D. Jude|0.0|2
486|New methodologies and techniques on the development of virtual globes for mobile devices and the web|The thesis studies the field of computerized 3D mapping on mobile devices and web platforms. It addresses several topics related to the implementation of virtual globes systems on these platforms. Due to its nature, the thesis encompasses different areas, including software architecture, computer graphics, terrain modelling and tessellation and cartographic symbology. As the range of topics is diverse, each subject is enclosed on a different chapter.\nIn order of appearance, these are the contents of the thesis:\nChapter 2. The Glob3 Mobile project (G3M) is presented which is the experimentation and implementation platform utilized during the conduction of the thesis. The chapter describes software a novel modularized architecture and an original multiplatform development strategy.\nChapter 3. A graphics multiplatform architecture is described. As it is portrayed, the software implements a graphics engine utilized in the G3M project, aimed to facilitate the use of 3D graphics to unskilled developers. The engine features a novel algorithm for the automatic selection of GPU shaders.\nChapter 4. A study of the different alternatives to the multi-level-of-detail rendering and modelling of terrain is presented in this chapter. G3M follows a well-known hierarchical scheme similar to geometry mipmapping. Nonetheless, novel variations of the LoD test are presented. Likewise, the chapter proposes a “pluggable” software architecture of the rendering system, which allows a great degree of optimization and customization. \nChapter 5. This chapter presents a particular version of the terrain rendering strategy, which precedes the architecture presented on Chapter 4. The core of this system is a novel PoV-based LoD test also introduced in the chapter. The efficiency of this simpler version of the rendering system is thoroughly tested on different platforms, both in terms of memory and computing consumption. The results show that the earth model is stable and meets the fps requirements even on low-powered devices.\nChapter 6. Here, the problem is coarsening elevation models is addressed. A novel method based on the resolution of a linear system is proposed. The results of this approach are studied and compared with other alternatives. The results show that, for the assumed error metric, the method outperforms its competitors on several fields for the tested DEMs. \nChapter 7. In this part of the document, the virtual camera system is described. Many definitions of the camera managing are given, including a parametric definition utilized in the implementation based on the Tait-Bryan navigation angles. Moreover, the chapter presents several schemes of interaction and user interfaces, with special emphasis on the multitouch interaction. As a novelty a methodology to achieve accurate interaction with the globe model by using the pinch gesture is proposed.\nChapter 8. This chapter is a summary of the different aspects of presenting the virtual globe on VR and AR given the available mobile hardware. The achieved solutions to the implementation problems are explained in detail.\nChapter 9. This chapter focuses on the atmospheric rendering effects, based on the Rayleigh Scattering equations. A study of this volumetric rendering is undertaken. A simplification of the model has been successfully implemented on GLSL. The study follows with the proposing of several novel ideas to achieve a more efficient implementation of the full scattering model.\nChapter 10. A physics-based system for the on-screen reorganisation of cartographic markers for 3D virtual globes is proposed. The method successfully achieves its purposes, namely, the dynamic avoidance of cluttering and markers overlapping. \nFinally, the thesis summarized the different projects of technological transfer in which the novelties of this research have been successfully utilized.\n___\nLa tesis aqui presentada estudia la generacion de mapas 3D en dispositivos moviles y plataformas web. Se abordan varios temas relacionados con la implementacion de sistemas de globos virtuales en estas plataformas. Debido a su naturaleza, la tesis abarca diferentes areas, incluyendo arquitectura de software, computacion grafica, modelado y teselado de terreno y simbologia cartografica.\nLa tesis comienza con la presentacion proyecto Glob3 Mobile (G3M), plataforma de experimentacion, implementacion y transferencia tecnologica utilizada durante la conduccion de la tesis. Posteriormente, se describe una arquitectura multiplataforma de graficos, con diversas novedades destinadas a facilitar el uso de graficos 3D en aplicaciones GIS.\nSe prosigue con el tema del renderizado y modelado de terreno multi-nivel-de-detalle, explicando la implementacion seguida, basada en una variante de Geometrical Mipmapping, y las contribuciones en ambos campos. Diferentes implementaciones del LoD-test son estudiadas.\nSe hace un estudio es la navegacion en el entorno 3D, estudiando los distintos esquemas de manipulacion de la camara virtual, basada en angulos de navegacion Tait-Bryan y en interfaces multitouch, y proponiendo un sistema de manipulacion bi-puntero para una manipulacion fisica del globo. Asi mismo, se describen las implementaciones moviles de VR y AR usando dispositivos de mano.\nEl renderizado volumetrico de la atmosfera basado en un modelo de Rayleight scattering ha sido tambien objeto de estudio a fin de obtener una implementacion eficiente en moviles. Como ultima contribucion, la tesis presenta un novedoso sistema de reordenacion de marcas en pantalla basado en una simulacion fisica 2D que evita efectos de abarrotamiento y oclusion de los mapas.||2017|10.1007/978-3-319-59397-5_28|José Núñez|0.0|2
505|Feynman Machine: A Cortical Machine Intelligence for Path Detection in Advanced Driver-Assistance Systems|Mobility being a prime desire of mankind, the effort and resources plunged into the development of safe, efficient, low-cost and easily controllable vehicles has always remained at its peak. Several advancements have been brought about in the field, yet several challenges still exist while one major challenge is achieving robustness at a low computational footprint so as to reduce the power requirements and negate the need for heavy GPUs and other system pre-requisites. This research puts forth an application of Feynman Machine, a hierarchical temporal memory based cortical learning intelligence model analogous to the mammalian neocortex to learn the images and detect a suitable traversable path for navigation. The online learning ability of the Feynman Machine improves performance with time while negating the need for GPUs during training and prediction. The computation can be carried out in computers as small as a Raspberry Pi having low processor specifications, thereby reducing the power requirements of the model. A Unity based simulation and a Raspberry Pi based prototype were also built to substantiate the proposed idea in this research.|IEEE Region 10 Conference|2019|10.1109/TENCON.2019.8929604|Shafeeque K. Muhammedali, Rahul P. Bharathan, Dony S. Kurian, Irene Tenison, A. T. Imthias, J. Rajan|0.0|2
508|Design and Implementation of Multi Agent Simulation Library MasCUDA for Parallel Processing with GPU|This paper presents the design and implementation of parallel processing support library, primary for multi-agent simulation with GPU (Graphical Processing Unit). GPU provides highly parallel processing environment. However, in order to develop software for GPU, high level skill and knowledge of GPU, parallel processing and GPU architecture are required, and these requirements sometimes disturb to use GPU for specific application development. In this article we will provide the library for GPU programming named MasCUDA. Users can develop their own application by their familiar language such as Ruby. GPU specific programming is hidden by MasCUDA and users need not to understand the detail of GPU programming. Our experimental evaluation proved that MasCUDA accelerates the execution speed more than 5,000 times faster than Ruby program and the number of source code with MasCUDA is approximately half of GPU specific language.|International Conference on Electronics and Electrical Engineering Technology|2019|10.1145/3362752.3362770|H. Haga, A. Ohiwa|0.0|2
512|Planetary Surface Image Generation for Testing Future Space Missions with PANGU|Testing the image processing functions critical to the guidance systems of autonomous spacecraft landers for future space missions is a challenging task. One solution is to generate realistic synthetic images of the proposed planetary or asteroid surface to test the navigation and guidance systems in closed-loop tests. This paper presents a solution to this problem with the PANGU tool which creates multi-resolution models of planets and asteroids by enhancing the resolution of real terrain data with representative terrain and generates realistic, highquality, synthetic images in real-time using a custom GPU-based renderer which includes a parameterizable camera model. Introduction. Vision and LiDAR-based navigation systems can be used to support a wide variety of autonomous space applications including asteroid approach and landing missions1, planetary landers, and surface rovers. Verifying and validating these mission critical systems and their underlying image-processing algorithms2 is a challenging task that can be addressed by a test environment which can generate realistic images of the scenario to simulate the vision or LiDAR instruments used for autonomous navigation3. It is possible to use physical mock-ups to generate simulated images for testing vision-guided landers such as a camera mounted on a robotic arm taking images of a physical model of the expected terrain or a wall-mounted high-resolution image such as the Testbed for Robotic Optical Navigation4 (TRON) facility. These physical mockups can be used to generate images in real-time for closed-loop testing, but they are expensive, inflexible and have difficulty simulating realistic lighting and atmospheric conditions. Virtual simulations are therefore an increasingly valuable tool to support testing and verifying future autonomous approach and landing systems5. PANGU. PANGU (Planet and Asteroid Natural Scene Generation Utility) is a software package designed to aid the testing and verification of the approach and landing of autonomous spacecraft missions by providing realistic simulations of onboard vision and LiDAR guidance sensors. PANGU runs on standard, modern PCs but will have better performance on powerful PCs with fast GPUs and plenty of memory. PANGU contains surface modelling tools, a custom-designed visualization tool, flexible integration options to interface with other simulators, and a NAIF/SPICE interface6 which can be used to accurately simulate historical events or future mission missions where SPICE data is available. It provides a high degree of realism while producing images from large models in real-time at frame rates expected of navigation cameras on a planetary lander. These images can be used in a variety of test setups including off-line open loop, closed-loop and hardwarein-the-loop. PANGU functionality and the realism of the simulated images have been extensively verified and validated in preparation for use in all phases of operational missions. Figure 1: Multi-resolution PANGU lunar model with flight||2019|10.2514/6.2019-4234|M. S. Gestido, I. Martin, M. Dunstan|0.0|2
517|GPU-based Real-time Porous Medium Dynamic Network Simulation|Porous medium network simulation is to accurately describe the seepage law of multiphase fluids in porous media. Researchers have studied the relationship between various macroscopic physical properties of multiphase fluids (e.g. relative permeability, capillary pressure and fluid saturation) from theoretical, experimental, and computer simulation. This research introduces the construction method of stochastic pore network model and describes the calculation steps of pore network pressure field. In this research, the computational efficiency of the GPU-based pore network was compared with CPU. The calculation speed and simulation model scale of the pressure domain of the random pore network were improved.|DEStech Transactions on Computer Science and Engineering|2019|10.12783/dteees/iccis2019/31762|Min Li, Yanbing Tang, Chenxi Li|0.0|2
526|Actuator line simulations of wind turbine wakes using the lattice Boltzmann method|Abstract. The high computational demand of large-eddy simulations (LESs) remains the biggest obstacle for a wider applicability of the method in the field of wind energy. Recent progress of GPU-based (graphics processing unit) lattice Boltzmann frameworks provides significant performance gains alleviating such constraints. The presented work investigates the potential of LES of wind turbine wakes using the cumulant lattice Boltzmann method (CLBM). The wind turbine is represented by the actuator line model (ALM). The implementation is validated and discussed by means of a code-to-code comparison to an established finite-volume Navier–Stokes solver. To this end, the ALM is subjected to both laminar and turbulent inflow while a standard Smagorinsky sub-grid-scale model is employed in the two numerical approaches. The resulting wake characteristics are discussed in terms of the first- and second-order statistics as well the spectra of the turbulence kinetic energy. The near-wake characteristics in laminar inflow are shown to match closely with differences of less than 3 % in the wake deficit. Larger discrepancies are found in the far wake and relate to differences in the point of the laminar-turbulent transition of the wake. In line with other studies, these differences can be attributed to the different orders of accuracy of the two methods. Consistently better agreement is found in turbulent inflow due to the lower impact of the numerical scheme on the wake transition. In summary, the study outlines the feasibility of wind turbine simulations using the CLBM and further validates the presented set-up. Furthermore, it highlights the computational potential of GPU-based LBM implementations for wind energy applications. For the presented cases, near-real-time performance was achieved using a single, off-the-shelf GPU on a local workstation.\n|Wind Energy Science|2019|10.5194/wes-2019-94|H. Asmuth, H. Olivares-Espinosa, S. Ivanell|0.0|2
527|Simulation of the student enrollment system through the reaction-diffusion model and parallel computing with CUDA|The reduction of student segregation should be one of the priorities of any state in terms of educational legislation and regulation since there is empirical evidence that students belonging to families in vulnerable situations, who attend less segregated schools, tend to have better academic results. In this work, we implement the reaction-diffusion model to simulate decentralized student enrollment. Reaction-diffusion equations are used in conjunction with the finite volumes method to generate a discrete domain of space over which students search for a suitable school and the explicit Euler method for the approximation of time. The computational implementation, through CUDA, allows solving for the problem of wide territories, such as cities, taking advantage of the performance benefits offered by parallel computing by GPU at a low cost. In our tests, simulations have a low runtime, converging to the total assignment of students to schools.|International Conference of the Chilean Computer Science Society|2019|10.1109/SCCC49216.2019.8966403|Dominik Gutwein, Matias Poblete-Salgado, S. Berres, Julio Rojas-Mora|0.0|2
528|GPU Data Structures and Code Generation for Modeling, Simulation, and Visualization|Virtual prototyping, the iterative process of using computer-aided (CAx) modeling, simulation, and visualization tools to optimize prototypes and products before manufacturing the first physical artifact, plays an increasingly important role in the modern product development process. Especially due to the availability of affordable additive manufacturing (AM) methods (3D printing), it is becoming increasingly possible to manufacture customized products or even for customers to print items for themselves. In such cases, the first physical prototype is frequently the final product. \n \nIn this dissertation, methods to efficiently parallelize modeling, simulation, and visualization operations are examined with the goal of reducing iteration times in the virtual prototyping cycle, while simultaneously improving the availability of the necessary CAx tools. The presented methods focus on parallelization on programmable graphics processing units (GPUs). Modern GPUs are fully programmable massively parallel manycore processors that are characterized by their high energy efficiency and good price-performance ratio. Additionally, GPUs are already present in many workstations and home computers due to their use in computer-aided design (CAD) and computer games. However, specialized algorithms and data structures are required to make efficient use of the processing power of GPUs. \n \nUsing the novel GPU-optimized data structures and algorithms as well as the new applications of compiler technology introduced in this dissertation, speedups between approximately one (10×) and more than two orders of magnitude (> 100×) are achieved compared to the state of the art in the three core areas of virtual prototyping. Additionally, memory use and required bandwidths are reduced by up to nearly 86%. As a result, not only can computations on existing models be executed more efficiently but larger models can be created and processed as well. \n \nIn the area of modeling, efficient discrete mesh processing algorithms are examined with a focus on volumetric meshes. In the field of simulation, the assembly of the large sparse system matrices resulting from the finite element method (FEM) and the simulation of fluid dynamics are accelerated. As sparse matrices form the foundation of the presented approaches to mesh processing and simulation, GPU-optimized sparse matrix data structures and hardware- and domain-specific automatic tuning of these data structures are developed and examined as well. In the area of visualization, visualization latencies in remote visualization of cloud-based simulations are reduced by using an optimizing query compiler. By using hybrid visualization, various user interactions can be performed without network round trip latencies.||2020|10.25534/TUPRINTS-00011291|J. Mueller-Roemer|0.0|2
529|Compression Methods for Structured Floating-Point Data and their Application in Climate Research|The use of new technologies, such as GPU boosters, have led to a dramatic \nincrease in the computing power of High-Performance Computing (HPC) \ncentres. This development, coupled with new climate models that can better \nutilise this computing power thanks to software development and internal \ndesign, led to the bottleneck moving from solving the differential equations \ndescribing Earth’s atmospheric interactions to actually storing the variables. \nThe current approach to solving the storage problem is inadequate: either \nthe number of variables to be stored is limited or the temporal resolution \nof the output is reduced. If it is subsequently determined that another vari- \nable is required which has not been saved, the simulation must run again. \nThis thesis deals with the development of novel compression algorithms \nfor structured floating-point data such as climate data so that they can be \nstored in full resolution. \nCompression is performed by decorrelation and subsequent coding of \nthe data. The decorrelation step eliminates redundant information in the \ndata. During coding, the actual compression takes place and the data is \nwritten to disk. A lossy compression algorithm additionally has an approx- \nimation step to unify the data for better coding. The approximation step \nreduces the complexity of the data for the subsequent coding, e.g. by using \nquantification. This work makes a new scientific contribution to each of the \nthree steps described above. \nThis thesis presents a novel lossy compression method for time-series \ndata using an Auto Regressive Integrated Moving Average (ARIMA) model \nto decorrelate the data. In addition, the concept of information spaces and \ncontexts is presented to use information across dimensions for decorrela- \ntion. Furthermore, a new coding scheme is described which reduces the \nweaknesses of the eXclusive-OR (XOR) difference calculation and achieves \na better compression factor than current lossless compression methods for \nfloating-point numbers. Finally, a modular framework is introduced that \nallows the creation of user-defined compression algorithms. \nThe experiments presented in this thesis show that it is possible to in- \ncrease the information content of lossily compressed time-series data by \napplying an adaptive compression technique which preserves selected data \nwith higher precision. An analysis for lossless compression of these time- \nseries has shown no success. However, the lossy ARIMA compression model \nproposed here is able to capture all relevant information. The reconstructed \ndata can reproduce the time-series to such an extent that statistically rele- \nvant information for the description of climate dynamics is preserved. \nExperiments indicate that there is a significant dependence of the com- \npression factor on the selected traversal sequence and the underlying data \nmodel. The influence of these structural dependencies on prediction-based \ncompression methods is investigated in this thesis. For this purpose, the \nconcept of Information Spaces (IS) is introduced. IS contributes to improv- \ning the predictions of the individual predictors by nearly 10% on average. \nPerhaps more importantly, the standard deviation of compression results is \non average 20% lower. Using IS provides better predictions and consistent \ncompression results. \nFurthermore, it is shown that shifting the prediction and true value leads \nto a better compression factor with minimal additional computational costs. \nThis allows the use of more resource-efficient prediction algorithms to \nachieve the same or better compression factor or higher throughput during \ncompression or decompression. The coding scheme proposed here achieves \na better compression factor than current state-of-the-art methods. \nFinally, this paper presents a modular framework for the development \nof compression algorithms. The framework supports the creation of user- \ndefined predictors and offers functionalities such as the execution of bench- \nmarks, the random subdivision of n-dimensional data, the quality evalua- \ntion of predictors, the creation of ensemble predictors and the execution of \nvalidity tests for sequential and parallel compression algorithms. \nThis research was initiated because of the needs of climate science, but \nthe application of its contributions is not limited to it. The results of this the- \nsis are of major benefit to develop and improve any compression algorithm \nfor structured floating-point data.||2020|10.5445/IR/1000105055|Ugur Çayoglu|0.0|2
534|Recent Advances in Scalable Network Generation|Random graph models are frequently used as a controllable and versatile data source for experimental campaigns in various research fields. Generating such data-sets at scale is a non-trivial task as it requires design decisions typically spanning multiple areas of expertise. Challenges begin with the identification of relevant domain-specific network features, continue with the question of how to compile such features into a tractable model, and culminate in algorithmic details arising while implementing the pertaining model. \nIn the present survey, we explore crucial aspects of random graph models with known scalable generators. We begin by briefly introducing network features considered by such models, and then discuss random graphs alongside with generation algorithms. Our focus lies on modelling techniques and algorithmic primitives that have proven successful in obtaining massive graphs. We consider concepts and graph models for various domains (such as social network, infrastructure, ecology, and numerical simulations), and discuss generators for different models of computation (including shared-memory parallelism, massive-parallel GPUs, and distributed systems).|arXiv.org|2020|10.1515/itit-2019-0041|M. Penschuck, P. Sanders, Ilya Safro, M. Hamann, U. Meyer, S. Lamm, U. Brandes, Christian Schulz|0.0|2
536|Editorial issue 31.1|This issue contains four papers. In the first paper, Xiao Zhang and Deling Yang, from Guangzhou Academy of Fine Arts in China, propose a study to explore how online museums distribute the postproduction of cultural knowledge related to artworks in a network society. This study has implications for how online museums will adapt to the future development of knowledge societies. The core innovation of this study is exploring distributed knowledge postproduction in online museums from the perspective of Mode 2 knowledge production and online distributed knowledge production. This study also examines distributed knowledge postproduction in online museums, which coconstructs knowledge and meaning. In the second paper, Bin Wang, Weibin Liu, and Weiwei Xing, from Beijing Jiaotong University, China, propose an automatic segmentation method based on geodesic by introducing Riemannian manifold. They convert Mo-cap data from Euler angles into quaternions, calculate the intrinsic mean of the motion sequence, hemispherize quaternions, and use logarithmic and exponential mapping to calculate geodesic distances instead of quaternions. The experimental results show that the algorithms can achieve automatic segmentation and have a better segmentation effect. In the third paper, Jure Demšar and Iztok Lebar Bajec, from University of Ljubljana, Slovenia and Will Blewitt, from Coventry University, UK, present a hybrid model for the simulation of herds of grazing sheep. The novel approach called hybrid modeling tries to mix the best of both worlds—precision of individual based models and speed of flow based ones. Through Bayesian data analysis, the authors show that they can encompass several aspects of real-world sheep behavior. Their hybrid model is also extremely efficient, capable of simulating herds of more than 1000 individuals in real-time without resorting to GPU execution. In the last paper, Wei Cao, Zhixin Yang, Xiaohua Ren, Luan Lyu, and Bob Zhang, from University of Macau, China; Yanci Zhang, from Sichuan University in Chengdu, China; and Enhua Wu, from University of Macau and Chinese Academy of Sciences in Beijing, China, introduce an improved approach to simulate nonorthotropic geometric models under large deformation. The improvements are mainly twofold. First, a frame-field is specified on a given undeformed object, that is, each point of the object is equipped with a frame. Second, they design the deformation properties along each axis in the local nonorthogonal coordinate to get a local constitutive model. To improve the stability, they introduce a time-varying method to simultaneously track the local coordinates reorientation by pushing forward the original frame-field to the deformed frame-field.|Comput. Animat. Virtual Worlds|2020|10.1002/cav.1921|D. Thalmann, N. Magnenat-Thalmann|0.0|2
542|Exposing Fine-Grained Parallelism in Sequential Gaussian Simulation|Summary The implementation of computationally demanding algorithms on GPU architectures is becoming inevitable nowadays. In the geoscience domain, reverse time migration based on the wave equation provides the most evident example. Problems related to modeling of flow and transport of black-oil or compositional fluids in the subsurface have also been successfully addressed. However, less attention has been drawn to the population of petrophysical properties, where geostatistical simulation algorithms, such as Sequential Gaussian Simulation (SGS), are also computationally expensive. The path level parallelism approach in SGS assumes the simultaneous simulation of several values along a randomly chosen path, which traverses the simulation grid. The values simulated further down the path may depend on previously simulated values, hence efficient operation requires scheduling the simulation of each cell to maximize parallelism while avoiding race conditions. This can be tackled by relaxing the accuracy of the initial algorithm and ignoring a few dependencies leading to conflicts, however, the solution will diverge from the exact algorithm and the level of differences is difficult to control. The exact path-level parallelization strategy can be implemented using multi-coloring schemes, as it has been shown for a limited number of CPU threads. In our work, we demonstrate that fine-grained parallelism in sequential Gaussian simulation can be exposed to sufficient degree for efficient use of GPU architectures even for an exact strategy. We discuss several implementations of multi-coloring algorithms applied to path-level parallelism and benchmark the overall performance of the GPU implementations against CPU standards.|Fourth EAGE Workshop on High Performance Computing for Upstream 2019|2019|10.3997/2214-4609.201903296|K. Esler, M. Khait|0.0|2
552|Program realization of simulation tools of low-density codecs: effective architecture of a decoder for massive parallel computations on graphic processors|The article deals with the peculiarities of modeling the characteristics of low-density parity-check codes (LDPC) in terms of their decoding. When decoding low-density parity-check codes in various communication standards, the iterative belief propagation algorithm (BP) and its various modifications - Pearl’s algorithm, sum-of-products algorithm - are widely used. In the article, special attention is given to this algorithm. The BP algorithm is a message-passing algorithm on a graphical model and consists of two basic messaging procedures between the nodes in the Tanner graph for LDPC code. The issues of constructing an effective architecture for an iterative decoder operating on the belief propagation algorithm in a heterogeneous system with massively parallel computing on a graphics processing unit are considered. It is shown that the use of the GPU allows to achieve a significant gain in decoding time with a sufficient size of the test matrix (a sufficient number of code and test nodes), obtained due to the parallelism of the architecture. Expressions for calculation of the size of the “working group” are given. The benefit of using a modified decoder architecture is estimated.|Journal of Physics: Conference Series|2019|10.1088/1742-6596/1333/2/022001|A. Bashkirov, A. A. Pirogov, S. Beletskaya, V. Glotov, A. Muratov, O. Y. Makarov|0.0|2
553|Research on Beautification Method of 3D Face Model Based on Texture|"The beautification effect design method of the medical beauty industry mainly deals with the texture of the 3D face model, including texture local processing and texture overall processing method. In this paper, two new methods are proposed for texture local processing. One is the real-time texture local repair method based on fast marching method, the other is the ROI texture fusion method based on pixel extraction; the overall processing method for face-finished texture is lacking. A defect of model whitening effect is proposed, and an improved gamma transform texture pixel enhancement algorithm is proposed. Introduction 3D face model modeling and optimization techniques are widely used in multimedia, VR (virtual reality) games and medical fields. But even in today's mature medical aesthetics industry, experienced doctors sometimes can't make beauty seekers get the desired results after surgery, so good postoperative simulation is an essential step, which can be Both physicians and beauty seekers provide powerful visualization methods, and this technique requires efficient 3D face model beautification methods. The design of face beautification effect is a hot topic researched by scholars at home and abroad. Tian Tanghao [1] developed a 3D face simulation cosmetic system, which can realize 3D face analysis, free deformation and simulated facelift, but requires users to manually mark feature points, which requires professional knowledge to adjust 3D faces. Zhao Yan [2] studied a technique based on graphical personal face repair, which can simulate the effect of 3D face postoperative surgery, and improve a Gaussian distribution function to make the STL data of face model can be used. In the deformation of the area, but there are problems such as cumbersome operation and no texture display. Tang Yucheng [3] and others proposed a GPU-based 3D face data repair method, which can improve the low-quality part by scanning the face data scanning cavity, and obtains a good effect, and this method lacks the processing of the overall texture of the model. Bertalmio [4] proposed a repair algorithm based on Partial Differential Equations (PDE). The main idea of this algorithm is to iterate the information around the repaired area by the direction of the iso-illuminance line along the pixels in the image. It is in the area to be repaired until the image is completely filled, and the repair algorithm can only be applied to the two-dimensional scene, which is difficult to implement in the three-dimensional face model texture patching. Based on the above application and method analysis, this paper proposes two new local processing methods based on texture and optimization of an overall processing method. The combination of these two methods can realize local texture restoration and overall whitening function for 3D face model. It has been verified that the algorithm proposed in this paper has been used in the commercial software ""Beauty 100 3D Plastic Instructor"" and has great practical value."||2019|10.12783/dtcse/cscme2019/32528|Jin-Fang Li, Jiayuan Liu, Hanwen He, Jia-hong Cai, J. Mo, Erfang Li|0.0|2
556|GPU Accelerated Trajectory Simulation for Large Scale Monte-Carlo and Optimization|For large systems of systems modeling and simulation studies it is often required to perform a multitude of simulations with parameter variation to evaluate the control system strategy or overall system performance. Evaluations of these parametric studies for small systems is often trivial and may be completed on a desktop computers, whereas larger systems, with run times greater than real-time, porting to a cluster is often necessary. With the improvement of General Purpose Graphics Processing Units (GPGPU) computing capabilities, it is more tractable to port models to GPU kernels for parallel execution. This paper demonstrates the use of graphics processing units for Monte Carlo trajectory analysis with the intent of optimizing flight trajectories and improving guidance laws. A variety of approaches are attempted to port a legacy simulation for a guided mortar application to a code base which may be run on a GPGPU with the objective to minimize the introduction of error and improve workflow for the systems engineer. Historically, this process is time intensive and requires deep understanding of the implementation details for the hardware. The approach is to minimize the human translation of code from one language to another (Ex: Simulink to NVidia CUDA) and utilize modern methods and libraries to perform this translation and optimization. The paper will describe the integration of high performance codes and compilers to demonstrate large scale simulation and speedup of trajectory simulations.||2019|10.12783/ballistics2019/33133|M. Ilg|0.0|2
567|An FPGA based implementation of the Conjugate Gradient Kernels|The Conjugate Gradient (CG) is frequently used iterative methods to solve Systems of Linear Equations (SLEs). The CG has a faster convergence rate and higher accuracy. It is widely used for many scientific applications such as meteorology, groundwater flow problems, studying satellite data, ocean circulation modeling, molecular dynamics simulations, real-time power quality assessment, and a neural robot controller, etc. It can be implemented on CPUs, GPUs and in Field Programmable Gate Arrays (FPGAs). FPGAs have been shown to provide an order of magnitude to speed up for various computation-intensive applications. However, a Hardware Description Language (HDL) based FPGA implementation for all the arithmetic modules requires considerable development time and the designer needs to be knowledgeable in hardware design as well as in HDL programming. Using IP cores can reduce the development time and design complexity. Prominently, CG has basic three computational kernels and amongst them, Matrix-Vector Multiplication (MVM) is the most computationally intensive kernel. Optimizing MVM kernels with higher throughput can reduce the computation time required for each iteration of CG. In this research, three basic kernels of CG are implemented on FPGAs using floating-point IP cores. The results show that with an FPGA-based implementation of CG we achieved a significant order-of-magnitude over the software implementation (Intel Xenon ® CPU E5-2650 V2, 2.60 GHz) of the CG using Arria 10 1150 GX.|International Conference on Electrical Information and Communication Technologies|2019|10.1109/EICT48899.2019.9068774|Ahmad Sadman Sakib, Shrikant S. Jadhav, C. Gloster, C. Doss, Jannatun Naher|0.0|2
568|Portable and Performant GPU/Heterogeneous Asynchronous Many-Task Runtime System|Asynchronous many-task (AMT) runtimes are maturing as a model for computing simulations on a diverse range of architectures at large-scale. The Uintah AMT framework is driven by a philosophy of maintaining an application layer distinct from the underlying runtime while operating on an adaptive mesh grid. This model has enabled task developers to focus on writing task code while minimizing their interaction with MPI transfers, halo processing, data stores, coherency of simulation variables, and proper ordering of task execution. Further, Uintah is implementing an architecture portable solution by utilizing the Kokkos programming portability layer so that application tasks can be written in one codebase and performantly executed on CPUs, GPUs, Intel Xeon Phis, and other future architectures. Of these architectures, it is perhaps Nvidia GPUs that introduce the greatest usability and portability challenges for AMT runtimes. Specifically, Nvidia GPUs require code to adhere to a proprietary programming model, use separate high capacity memory, utilize asynchrony of data movement and execution, and partition execution units among many streaming multiprocessors. Numerous novel solutions to both Uintah and Kokkos are required to abstract these GPU features into an AMT runtime while preserving an application layer and enabling portability. The focus of this AMT research is largely split into two main parts, performance and portability. Runtime performance comes from 1) minimizing runtime overhead when preparing simulation variables for tasks prior to execution, and 2) executing a heterogeneous mixture of tasks to keep compute node processing units busy. Preparation of simulation variables, especially halo processing, receives significant emphasis as Uintah’s target problems heavily rely on local and global halos. In addition, this work covers automated data movement of simulation variables between host and GPU memory as well as distributing tasks throughout a GPU for execution. Portability is a productivity necessity as application developers struggle to maintain three sets of code per task, namely code for single CPU core execution, CUDA code for GPU tasks, and a third set of code for Xeon Phi parallel execution. Programming portability layers, such as Kokkos, provide a framework for this portability, however, Kokkos itself requires modifications to support GPU execution of finer grained tasks typical of AMT runtimes like Uintah. Currently, Kokkos GPU parallel loop execution is bulk-synchronous. This research demonstrates a model for portable loops that is asynchronous, nonblocking, and performant. Additionally, integrating GPU portability into Uintah required additional modifications to aid the application developer in avoiding Kokkos specific details. This research concludes by demonstrating a GPU-enabled AMT runtime that is both performant and portable. Further, application developers are not burdened with additional architecture specific requirements. Results are demonstrated using production task codebases written for CPUs, GPUs, and Kokkos portability and executed in GPU homogeneous and CPU/GPU heterogeneous environments.||2020|10.1201/9781003073536-2|B. Peterson|0.0|2
570|CityFFD – City Fast Fluid Dynamics Model for Urban Microclimate Simulations|In recent years, due to the rapid population growth and the preference to live in urban areas, urbanization has intensely increased. Currently, based on a United Nation report, 55% of the population live in the cities and the number is expected to reach about 68% by 2050. Urban microclimate has significant impacts on human life and health, and building energy performance. Urban microclimate information, such as wind velocity, temperature, humidity, pollutant dispersion levels, and local precipitation, are often important for accurate evaluations of building energy performance, indoor and outdoor human comfort, extreme events, and emergency situations. For example, it was reported that indoor temperature estimated with the microclimate information could be at least 5 °C different from that without it, which could be significant for the evaluations of indoor thermal comfort. The study of urban microclimate includes both observational and numerical approaches. The observational study is often related to field measurements, satellite imagery, and laboratory tests, e.g. in wind tunnels. The numerical approach is often based on computer models, such as CFD (computational fluid dynamics), for high-resolution and relatively small computing domains, compared to larger scale regional climate models, such as WRF and GEM-SURF. The latter two models are mostly used to model the domain size of 1~10 km with the resolution more than 100 m so they are not developed for urban microclimate and building-level simulations. In comparison, CFD has been applied to the urban microclimate of less than 1 km with a resolution less than 10 m down to the building level. However, conventional CFD solvers often perform unsatisfactorily for microscale and complicated problems because of numerical constraints such as stability issues associated with CFL condition, which is a necessary condition for convergence while solving certain partial differential equations (usually hyperbolic PDEs) numerically. Thus, conventional tools are often computationally expensive for modeling microclimates and consequently impractical for urban-scale problems. Recently, there are an increasing amount of efforts focusing on developing faster and accurate CFD techniques such as based on Fast Fluid Dynamics (FFD) methods. A FFD method relies on semi-Lagrangian and fractional step methods. FFD methods is fundamentally an explicit method without the CFL constraint so it is unconditionally stable even under large time steps and coarse grid resolutions, which are common for urban microclimate problems. In the meantime, the conventional FFD methods are often dependent on low-order interpolation schemes and thus with high numerical errors, which are the main drawbacks of this approach. \nThe main objective of this thesis is to develop a fast and accurate CFD solver with a series of new computing algorithms based on semi-Lagrangian approach for modeling urban/city scale microclimates. The new solver with the name of CityFFD (city Fast Fluid Dynamics), is designed for tackling the challenges of large domain, coarse grid, and/or large time step, which are typical for urban microclimate simulations, without a heavy reliance on computer resources, such as the possibility of running on personal computers. First, a novel high-order interpolation scheme is proposed to significantly reduce the numerical errors of conventional semi-Lagrangian solvers. The new interpolation scheme enables the possibility of obtaining fast and accurate results even on coarse grids. The second algorithm focuses on the simulation accuracy associated with the time step of the semi-Lagrangian method. A new scheme of an adaptive time step is developed to adjust the time step dynamically according to local truncation errors. To improve the estimation of the characteristic curves, a new algorithm is proposed by considering the acceleration of the flow particles inside the computational domain which can provide highly accurate results and capture the complicated flow fields even by using a large time step. The fourth algorithm is to speed up the simulation by eliminating the need for solving the Poisson equation, which is often the most time-consuming operation of conventional semi-Lagrangian models. The new scheme is based on the concept of the artificial compressibility of solving incompressible flows and makes it easier to implement parallel computing techniques, such as the NVIDIA GPU CUDA and the OpenMP. The last feature of CityFFD is adding Large Eddy Simulation (LES) model to capture the turbulence behavior of the flow in urban environments. In this section, a parallel OpenMP geometry reader is developed to read the city scale geometries in a fast manner. At the end, the proposed CityFFD model is demonstrated by a case study: the modeling of an extreme weather event, the snow-storm of the century in Montreal, for evaluating building resilience during the storm, to show the importance of urban microclimate and its impact on human health and indoor environment.||2019|10.31223/osf.io/74uhy|M. Dorostkar|0.0|2
573|Porting a Lattice Boltzmann Simulation to FPGAs Using OmpSs|. Reconﬁgurable computing, exploiting Field Programmable Gate Arrays (FPGA), has become of great interest for both academia and industry research thanks to the possibility to greatly accelerate a variety of applications. The interest has been further boosted by recent developments of FPGA programming frameworks which allows to design applications at a higher-level of abstraction, for example using directive based approaches. In this work we describe our ﬁrst experiences in porting to FPGAs an HPC application, used to simulate Rayleigh-Taylor instability of ﬂuids with different density and temperature using Lattice Boltzmann Methods. This activity is done in the context of the FET HPC H2020 EuroEXA project which is developing an energy-efﬁcient HPC system, at exa-scale level, based on Arm processors and FPGAs. In this work we use the OmpSs directive based programming model, one of the models available within the EuroEXA project. OmpSs is developed by the Barcelona Su-percomputing Center (BSC) and allows to target FPGA devices as accelerators, but also commodity CPUs and GPUs, enabling code portability across different architectures. In particular, we describe the initial porting of this application, evaluating the programming efforts required, and assessing the preliminary performances on a Trenz development board hosting a Xilinx Zynq UltraScale+ MPSoC embedding a 16nm FinFET+ programmable logic and a multi-core Arm CPU.|International Conference on Parallel Computing|2019|10.3233/APC200100|S. Schifano, E. Calore|0.0|2
582|Handling geophysical flows: Numerical modelling using Graphical Processing Units|Computational tools may help engineers in the assessment of sediment transport during the decision-making processes. The main requirements are that the numerical results have to be accurate and simulation models must be fast. The present work is based on the 2D shallow water equations in combination with the 2D Exner equation [1]. The resulting numerical model accuracy was already discussed in previous work. Regarding the speed of the computation, the Exner equation slows down the already costly 2D shallow water model as the number of variables to solve is increased and the numerical stability is more restrictive. On the other hand, the movement of poorly sorted material over steep areas constitutes a hazardous environmental problem. Computational tools help in the predictions of such landslides [2]. In order to overcome this problem, this work proposes the use of Graphical Processing Units (GPUs) for decreasing significantly the simulation time [3, 4]. The numerical scheme implemented in GPU is based on a finite volume scheme. The mathematical model and the numerical implementation are compared against experimental and field data. In addition, the computational times obtained with the Graphical Hardware technology are compared against Single-Core (sequential) and Multi-Core (parallel) CPU implementations. References [Juez et al.(2014)] Juez, C., Murillo, J., & Garca-Navarro, P. (2014) A 2D weakly-coupled and efficient numerical model for transient shallow flow and movable bed. Advances in Water Resources. 71 93–109. [Juez et al.(2013)] Juez, C., Murillo, J., & Garca-Navarro, P. (2013) . 2D simulation of granular flow over irregular steep slopes using global and local coordinates. Journal of Computational Physics. 225 166–204.||2016|10.1201/9781315644479-372|M. Morales-Hernández, A. Lacasta, P. García-Navarro, C. Juez|0.0|2
584|Estimating Silent Data Corruption Rates Using a Two-Level Model|High-performance and safety-critical system architects must accurately evaluate the application-level silent data corruption (SDC) rates of processors to soft errors. Such an evaluation requires error propagation all the way from particle strikes on low-level state up to the program output. Existing approaches that rely on low-level simulations with fault injection cannot evaluate full applications because of their slow speeds, while application-level accelerated fault testing in accelerated particle beams is often impractical. We present a new two-level methodology for application resilience evaluation that overcomes these challenges. The proposed approach decomposes application failure rate estimation into (1) identifying how particle strikes in low-level unprotected state manifest at the architecture-level, and (2) measuring how such architecture-level manifestations propagate to the program output. We demonstrate the effectiveness of this approach on GPU architectures. We also show that using just one of the two steps can overestimate SDC rates and produce different trends---the composition of the two is needed for accurate reliability modeling.|arXiv.org|2020|10.1017/ice.2020.265|Michael B. Sullivan, P. Shirvani, S. Hari, S. Keckler, Paul Racunas, P. Rech, M. Stephenson, J. Emer, Timothy Tsai, A. Zulfiqar|0.0|2
588|Comparison of different methods to compute Total Propagation Uncertainty for airborne bathymetric LiDAR (Conference Presentation)|Total Propagated Uncertainty (TPU) has been a very important topic of research and developments in the last five years in the bathymetric lidar community.\n\nTo date, two models coexist to estimate the TPU of a bathymetric lidar system. One would rely on an analytical approach based on the General Law Of Propagation Of Variance (GLOPOV) whereas the other relies on a probabilistic and modeling approach based on water surface simulation, Monte Carlo and Ray-tracing principles. We propose a new hybrid method that is based on the GLOPOV and the Law of Large Number (LLN). This new approach includes several innovative mathematical concepts and computational tools that help to overcome known challenges and limitations whilst improving the computation speed.\n\nFor instance, an analytical TPU estimation relies on the Jacobian and Covariance matrices associated to the LiDAR system. The estimation the Jacobian matrix, i.e. the partial derivatives of the LiDAR equation’s vector function, remains a difficult task. Therefore, we’ve developed an equation parser and a code generator tool that evaluate the partial derivatives of any equation system and produce the GPU code for its evaluation. The Covariance matrix also has its own challenges and we propose new approaches to estimate some of the sigma values.\nThe probabilistic model based on Monte Carlo simulation and ray tracing is inherently intensive in terms of computation and therefore slow. The use of the LLN offers a scalable and robust approach to estimate parameters and coefficients of the model. \n\nFinally, we will present an attempt to adapt the Quality Factor introduce by Lurton and Augustin in 2009 for the Multibeam echosounder systems. This factor has the potential to deliver an objective way to assess the bathymetric performance of any Airborne Bathymetric Lidar based on waveform analysis.||2020|10.1117/12.2558140|L. Soltész, Orestis Ioannou, Antoine Cottin, M. Pfennigbauer, A. Ullrich, A. Spitzer|0.0|2
589|Parallel Simulation of Steady State Light Transport Through Solid Media Using Logistic Map|The Monte Carlo algorithm has been extensively used for photon transport simulations in medical imaging to assists doctors in Photodynamic Therapy for treatment of wide range of medical conditions including varieties of cancer by eliciting phototoxicity in cells. Previously this was\n done using static 2 Dimensional models on traditional CPUs. With the advent of GPU Computing further work was done to extend this model by separating the PRNG.||2020|10.1166/jctn.2020.8409|K. Sathish, Anirudh Agarwal, Siddharth Singh, P. K. Shukla|0.0|2
590|Parallel computing applied to the molecular dynamics simulations|This paper discusses the use of high-performance algorithms for modeling the dynamics of dispersed systems using the method of molecular dynamics. Large-scale modeling, which makes it possible to determine various thermodynamic parameters and control the processes of phase transformations, requires consideration of a large number of particles, which imposes significant restrictions on the computational capabilities of the system. In work various problems of dynamics of disperse systems are considered. To solve the problems posed by the authors, a specialized data structure was developed, which reduces the computational complexity of the algorithm from quadratic to linear. The specificity of the molecular dynamics method makes it possible to achieve significant acceleration when using computations at heterogeneous stations equipped with a central processing unit (CPU) and graphic processing units (GPU). The authors showed that the presented technique can be effectively used for solving problems related to modeling processes occurring in areas with characteristic sizes of tens of nanometers on personal computers equipped with one or more GPUs.|Journal of Physics: Conference Series|2019|10.1088/1742-6596/1392/1/012055|V. Malyshev, E. Moiseeva|0.0|2
591|GPU-based parallel simulations of the Gatenby-Gawlinski model with anisotropic, heterogeneous acid diffusion|We introduce a variant of the Gatenby-Gawlinski model for acid-mediated tumor invasion, accounting for anisotropic and heterogeneous diffusion of the lactic acid across the surrounding healthy tissues. Numerical simulations are performed for two-dimensional data by employing finite volume schemes on staggered Cartesian grids, and parallel implementation through the modern CUDA GPUs technology is considered. The effectiveness of such approach is proven by reproducing biologically relevant results like the formation of propagating fronts and the emergence of an interstitial gap between normal and cancerous cells, which is driven by the pH lowering strategy and depends significantly on the diffusion rates. By means of a performance analysis of the serial and parallel execution protocols, we infer that exploiting highly parallel GPU-based computing devices allows to rehabilitate finite volume schemes on regularly-shaped meshes, together with explicit time discretization, for complex applications to interface diffusion problems of invasive processes.|arXiv.org|2020|10.2478/caim-2020-0001|C. Simeoni, D. Pera, C. Mascia|0.0|2
608|Increasing diversity and usability of crowd animation systems|Crowd systems are a vital part in virtual environment applications that are used in entertainment, education, training or different simulation systems such as evacuation planning. Because performance and scalability are key factors, the implementation of crowds poses many challenges in many of its aspects: behaviour simulation, animation, and rendering. This paper is focusing on a different model of animating crowd characters that support dynamically streaming of animation data between CPU and GPU. There are three main aspects that motivate this work. First, we want to provide a greater diversity of animations for crowd agents than was possible before, by not storing any predeﬁned animation data. Another aspect that stems from the ﬁrst improvement is that this new model allows the crowd simulation side to communicate more efﬁciently with the animation side by sending events back and forth at runtime, fulﬁlling this way use-cases that different crowd systems have. Finally, a novel technique implementation that blends between crowd agents’ animations is presented. The results section shows that these improvements are added with negligible cost.||2017||C. Paduraru|0.0|2
617|Acceleration of high-order combined compact finite difference scheme for simulating three-dimensional flow and heat transfer problems in GPUs|Abstract In this article, the high-order upwinding combined compact difference scheme developed in a three-point grid stencil is applied to solve the incompressible Navier-Stokes (NS) and energy equations in three dimensions. The time integrator with symplectic property is employed to approximate the temporal derivative term in inviscid Euler equation so as to numerically retain the embedded Hamiltions and Casimir to get long-time accurate solutions. For the sake of computational efficiency in solving the three-dimensional NS equations, all the calculations will be accelerated using the hybrid CUDA and OpenAcc GPU programing models. The parallel speedup performance compared to the multicore of an Intel Xeon E5-2690V5 CPU is reported.||2020|10.1080/10407790.2020.1783127|T. Sheu, Neo Shih-Chao Kao, Rex Kuan-Shuo Liu|0.0|2
620|Adaptation and Integration of GPU-Driven Physics for a Biology Research RIS|Developmental biology studies biophysical processes that lead to the development of tissues, organs, and organisms. Like other complex scientific domains, developmental biology can greatly benefit from real-time interactive systems (RIS). In addition to utilizing various innovative RIS technologies, highly efficient domain models have to be provided as well. In this paper, we present a prototypical RIS for developmental biology research that achieves this goal by adapting an existing, GPU-driven, position-based physics engine called FleX to support the required biological interaction mechanisms. The adapted cell model is further augmented by a GPU-based substance diffusion system that simulates biochemical signals that allow cells to communicate and react to their environment. We present model specifics with an emphasis on their efficient integration in an existing game engine, and we elaborate on future improvements.|Workshop on Software Engineering and Architectures for Realtime Interactive Systems|2018|10.1109/SEARIS44442.2018.9180233|A. Knote, S. Mammen|0.0|2
621|OpenAWSEM with Open3SPN2: A fast, flexible, and accessible framework for large-scale coarse-grained biomolecular simulations|We present OpenAWSEM and Open3SPN2, new cross-compatible implementations of coarse-grained models for protein (AWSEM) and DNA (3SPN2) molecular dynamics simulations within the OpenMM framework. These new implementations retain the chemical accuracy and intrinsic efficiency of the original models while adding GPU acceleration and the ease of forcefield modification provided by OpenMM’s Custom Forces software framework. By utilizing GPUs, we achieve more than a 100-fold speedup in protein and protein-DNA simulations over the existing LAMMPS-based implementations running on a CPU. We showcase the benefits of OpenMM’s Custom Forces framework by devising and implementing two new potentials that allow us to address important aspects of protein folding and structure prediction and by testing the ability of the combined OpenAWSEM and Open3SPN2 to model protein-DNA binding. The first potential is used to describe the changes in effective interactions that occur as a protein becomes partially buried in a membrane. We also introduced an interaction to describe proteins with multiple disulfide bonds. Using simple pairwise disulfide bonding terms results in unphysical clustering of cysteine residues, posing a problem when simulating the folding of proteins with many cysteines. We now can computationally reproduce Anfinsen’s early Nobel prize winning experiments [1] by using OpenMM’s Custom Forces framework to introduce a multi-body disulfide bonding term that prevents unphysical clustering. Our protein-DNA simulations show that the binding landscape is funneled towards structures that are quite similar to those found using experiments. In summary, this paper provides a simulation tool for the molecular biophysics community that is both easy to use and sufficiently efficient to simulate large proteins and large protein-DNA systems that are central to many cellular processes. These codes should facilitate the interplay between molecular simulations and cellular studies, which have been hampered by the large mismatch between the time and length scales accessible to molecular simulations and those relevant to cell biology. Author summary The cell’s most important pieces of machinery are large complexes of proteins often along with nucleic acids. From the ribosome, to CRISPR-Cas9, to transcription factors and DNA-wrangling proteins like the SMC-Kleisins, these complexes allow organisms to replicate and enable cells to respond to environmental cues. Computer simulation is a key technology that can be used to connect physical theories with biological reality. Unfortunately, the time and length scales accessible to molecular simulation have not kept pace with our ambition to study the cell’s molecular factories. Many simulation codes also unfortunately remain effectively locked away from the user community who need to modify them as more of the underlying physics is learned. In this paper, we present OpenAWSEM and Open3SPN2, two new easy-to-use and easy to modify implementations of efficient and accurate coarse-grained protein and DNA simulation forcefields that can now be run hundreds of times faster than before, thereby making studies of large biomolecular machines more facile.|bioRxiv|2020|10.1101/2020.09.07.285759|J. D. de Pablo, Joshua Moller, Mingchen Chen, Nicholas P. Schafer, Xun Chen, P. Wolynes, Xinyu Gu, Shikai Jin, Carlos Bueno, W. Lu|0.0|2
633|Efficient simulation of 5G Antenna platforms and Circuits using the Characteristic Basis Function Method (CBFM) and GPU Acceleration|Modeling and simulation of electromagnetic devices, such as mobile phones, operating at millimeter wavelengths, e.g., at 30GHz, is challenging because the dimensions of the platform are typically large as compared to the operating wavelength. In this work, we propose several strategies for addressing the issue of CPU time and memory consumption, while preserving the accuracy of the computation. This include geometry simplification as well as application of a numerical algorithm for memory reduction and GPU acceleration. Memory reduction is achieved by employing an efficient iteration-free technique called the Characteristic Basis Functions Method (CBFM) proposed in [1] , to solve problems involving a large number of degrees of freedom. The CBFM has been extensively investigated by a number of authors, including [2] , to analyze objects embedded in multi-layered media.|IEEE International Conference on Consumer Electronics|2020|10.1109/ICCEM47450.2020.9219472|Yang Su, R. Mittra|0.0|2
659|Hybrid rendering : in pursuit of real-time raytracing|Hybrid rendering combines ray-tracing and rasterization graphics techniques to generate visually accurate photorealistic computer-generated images at a tight realtime frame rate. This thesis presents contributions to the field of hybrid rendering by introducing an open-source ray-traced ambient occlusion workload, by quantifying the performance tradeoffs of this workload and by evaluating one promising hardware technique to accelerate real-time raytracing. In this thesis, we introduce the first open-source implementation of a viewpoint independent raytraced ambient occlusion GPU compute benchmark. We study this workload using a cycle-level GPU simulator and present the trade-offs between performance and quality. We show that some ray-traced ambient occlusion is possible on a real-time frame budget but that the full quality effect is still too computationally expensive for today’s GPU architectures. This thesis provides a limit study of a new promising technique, Hash-Based Ray Path Prediction (HRPP), which exploits the similarity between rays to predict leaf nodes to avoid redundant acceleration structure traversals. Our data shows that acceleration structure traversal consumes a significant proportion of the raytracing rendering time regardless of the platform or the target image quality. Our study quantifies unused ray locality and evaluates the theoretical potential for improved ray traversal performance for both coherent and seemingly incoherent rays. We show that HRPP can skip, on average, 40% of all hit-all traversal computations. We further show that the significant memory overhead, ranging on the order of megabytes, inhibits this technique from being feasible for current architectures.||2020|10.14288/1.0391894|Francois Demoullin|0.0|2
664|Performance Evaluation of a GPU-based Monte Carlo Simulation Package for Water Radiolysis with sub-MeV Electrons|The simulation of water radiolysis including three stages, physical, physico-chemical and chemical, modeling the interactions between water and radicals is essential to understand the radiobiological mechanisms and quantitatively test some hypotheses in related problem. Monte Carlo (MC) simulation is recognized as one of the most accurate approaches for the computations of the water radiolysis process. Geant4-DNA which extending the Geant4 Monte Carlo simulation toolkit provides accurate descriptions of the initial physical process of ionization, along with the pre-chemical production of ion species and subsequent chemistry, in a single application for water radiolysis. To accelerate the long execution time of Geant4-DNA simulation, an open source GPU code for water radiolysis simulation, gMicroMC, has been developed. In this paper, we focus on reviewing the GPU implementation architecture of each stage of gMicroMC and evaluating the computational performance in the sub-MeV range of incident electrons. The experimental results of gMicroMC show up to three orders of magnitude performance gain, up to 1690x, with recent generations of NVIDIA graphic cards compared with Geant4-DNA running on a single CPU thread.|Research in Adaptive and Convergent Systems|2020|10.1145/3400286.3418241|X. Jia, Y. Lai, Y. Chi, Shih-Hao Hung, Min-yu Tsai|0.0|2
665|Real-Time 3D Position-Based Dynamics Simulation for Hydrographic Printing|Hydrographic Printing, also called Hydrographics, is a viable method for coloring objects created with 3D printers. However, executing the hydrographic technique leads to a complex interaction between a thin film and a 3D printed object, in which the image in the film must adhere to the object. To handle the difficulty of predicting the final result of hydrographic printing, we propose a 3D computational simulation that uses Position-Based Dynamics, a popular technique for simulating deformable bodies and widely used in physics engines. We take advantage of this technique running in parallel a GPU-based simulation with suitable performance. We simulate the film behavior and its interaction with the printed object, as an interaction between a soft-body colliding with a rigid one. To evaluate the achieved performance consistency, we varied the number of vertices and voxels in the bodies involved and observed that the simulation kept running in real-time. We also execute the hydrographic technique in different printed models and compare these results with the simulated models.|SIBGRAPI Conference on Graphics, Patterns and Images|2020|10.1109/SIBGRAPI51738.2020.00012|L. Thomas, Karl Apaza-Agüero, A. Apolinario|0.0|2
667|A neural-network framework for modelling auditory sensory cells and synapses|In classical computational neuroscience, transfer functions are derived from neuronal recordings to derive analytical model descriptions of neuronal processes. This approach has resulted in a variety of Hodgkin-Huxley-type neuronal models, or multi-compartment synapse models, that accurately mimic neuronal recordings and have transformed the neuroscience field. However, these analytical models are typically slow to compute due to their inclusion of dynamic and nonlinear properties of the underlying biological system. This drawback limits the extent to which these models can be integrated within large-scale neuronal simulation frameworks and hinders an uptake by the neuro-engineering field which requires fast and efficient model descriptions. To bridge this translational gap, we present a hybrid, machine-learning and computational-neuroscience approach that transforms analytical sensory neuron and synapse models into artificial-neural-network (ANN) neuronal units with the same biophysical properties. Our ANN-model architecture comprises parallel and differentiable equations that can be used for backpropagation in neuro-engineering applications, and offers a simulation run-time improvement factor of 70 and 280 on CPU or GPU systems respectively. We focussed our development on auditory sensory neurons and synapses, and show that our ANN-model architecture generalizes well to a variety of existing analytical models of different complexity. The model training and development approach we present can easily be modified for other neuron and synapse types to accelerate the development of large-scale brain networks and to open up avenues for ANN-based treatments of the pathological system.|bioRxiv|2020|10.1101/2020.11.25.388546|Fotios Drakopoulos, Deepak Baby, S. Verhulst|0.0|2
668|Mixed precision matrix interpolative decompositions for model reduction|Renewed interest in mixed-precision algorithms has emerged due to growing data capacity and bandwidth concerns, as well as the advancement of GPUs, which enable significant speedup for low precision arithmetic. In light of this, we propose a mixed-precision algorithm to generate a double-precision accurate matrix interpolative decomposition approximation under a given set of criteria. Though low precision arithmetic suffers from quicker accumulation of round-off error, for many data-rich applications we nevertheless attain viable approximation accuracy, as the error incurred using low precision arithmetic is dominated by the error inherent to low-rank approximation. We then conduct several simulated numerical tests to demonstrate the efficacy of the algorithms and the corresponding error estimates. Finally, we present the application of our algorithms to a problem in model reduction for particle-laden turbulent flow.|arXiv.org|2020|10.1096/fasebj.2020.34.s1.06171|Alyson Fox, Alec M. Dunton|0.0|2
670|Optimizing IC Testing for Diagnosability, Effectiveness and Efficiency|Chip testing is an important step of integrated circuits (“chip”) manufacturing. It involves applying tests to each manufactured chip using expensive testers (automatic test equipment) to identify and reject bad (malfunctioning) chips. Various types of manufacturing defects (shorts, disconnects, missing vias, etc.) can occur during fabrication and cause a chip to malfunction. Testing not only needs to verify every gate, cell, interconnect, etc. are operational as expected, but also needs to help identify and analyze existing manufacturing defects so that improvements in fabrication, design and even test can be made in a timely manner.The cost of developing high-quality tests is being driven up by the increasing complexity of defect behaviors. New processing technologies introduce new types of defects, some of which only occur in certain circuit/layout configurations. It is no longer possible to detect all types of defects using only conventional stuck-at tests. More sophisticated fault models and test metrics have been developed to guide the test development toward better defect detection, but they also require a significantly larger volume of tests to achieve acceptable coverage. Test engineers need to reduce test volume in order to save test cost (i.e., achieving high test efficiency), and at the same time prevent most bad chips from escaping test (i.e., achieving high test effectiveness). The ability to diagnose a failing chip precisely and accurately (diagnosability) also depends on the tests applied. This important characteristic of test is often downplayed in production testing, but could be very important during yield ramp-up for quickly discovering major yield-loss contributors. In this dissertation, four new methods are developed to improve the state of the art for test development, either in terms of diagnosability, test effectiveness or test efficiency. These methods can be used in conjunction, or individually for achieving a specific prioritized, goal in test development.First, a test-reordering method is developed to improve the diagnosability of production tests. To our knowledge, this is the first-ever work that examine the impact of test order on logic diagnosis. Due to constraints such as limited test time or tester memory, a commonly-used practice during production testing is to only record the first few failing tests or pins for a failing chip. This recording of an incomplete tester response adversely affects the outcome of diagnosis because less information is provided for diagnosis. The proposed test-reordering method tries to find an optimal test ordering that can better distinguish stuck-at faults when recorded tester response is incomplete. Since the set of candidate defect sites is typically obtained based on stuck-at faults, faults that are distinguished from each other are unlikely to become the candidate defect site at the same time, which leads to better outcome for diagnosis.Second, a fault-model evaluation method (DELAY-METER) is developed to improve test effectiveness. Various delay fault models are proposed in previous work to capture defects that escape slow-speed testing, but which models should be used to guide the generation of tests for at-speed testing remain an open question. The conventional method is to evaluate fault-model effectiveness using test experiments involving actual fabricated chips, in other words, tests are developed using various fault models and applied to a population of chips to determine which tests are best at detecting defects. Alternatively, DELAY-METER evaluates the effectiveness of delay fault models using readily-available fail data from production testing, so that an optimal mix of delay fault models can be chosen for at-speed testing.Third, a defect-level prediction model (the DDP model) is developed to balance test effectiveness and test efficiency. Defect level (DL) represents the fraction of defective chips among all chips that pass tests. However DL is difficult to measure directly and be able to predict during test development is of critical importance. Conventional DL prediction models become insufficient when tests are generated from multiple fault models. The DDP model learns the defect detection probability (DDP) of multiple fault models from diagnosis, and combines it with the coverages of multiple fault models to provide a more accurate prediction. The more accurate prediction of DL by the DDP model thus enables a better trade-off analysis between test effectiveness and test efficiency.Finally, a test-selection method is developed to improve test efficiency. Test time reduction (TTR) is a focus of research in test development to save test cost and improve test efficiency. One method for TTR involves identifying a subset of tests from a large baseline test set. Test selection can be performed based on actual tester data measured from tested chips, or data taken from the simulation of the circuit design that has faults/defects injected. Previous work that uses simulation for test selection are only applied to archaic benchmark circuits that are too small to be meaningful. A one-pass test-selection method is developed in this dissertation that identifies a subset of tests that maximize fault-model coverage while requiring relatively limited CPU time and memory.To demonstrate the practical utility of the four methods developed in this dissertation, several real designs from industry are used in various experiment. Specifically, an ASIC and GPU designs and test data taken from a large population of actual fabricated chips are used to experiment the proposed test-reordering method and test-selection method. Experiments results demonstrate the improvement in diagnosability and test efficiency, respectively. The same ASIC design and test data is not only used by DELAY-METER to evaluate the effectiveness of different delay fault models, but also used by the DDP model as both a training data for building a DL-prediction model, and a verification data for verifying the prediction accuracy.||2016|10.1184/R1/7416389.V1|Cheng Xue|0.0|2
674|Architecture exploration of recent GPUs to analyze the efficiency of hardware resources|This study analyzes the efficiency of parallel computational applications with the adoption of recent graphics processing units (GPUs). We investigate the impacts of the additional resources of recent architecture on the popular benchmarks compared with previous architecture. Our simulation results demonstrate that Pascal GPU architecture improves the performance by 273% on average compared to old-fashioned Fermi architecture. To evaluate the performance improvement depending on specific hardware resources, we divide the hardware resources into two types: computing and memory resources. Computing resources have bigger impact on performance improvement than memory resources in most of benchmarks. For Hotspot and B+ tree, the architecture adopting only enhanced computing resources can achieve similar performance gains of the architecture adopting both computing and memory resources. We also evaluate the influence of the number of warp schedulers in the SM (Streaming Multiprocessor) to the GPU performance in relationship with barrier waiting time. Based on these analyses, we propose the development direction for the future generation of GPUs.|Bulletin of Electrical Engineering and Informatics|2021|10.11591/EEI.V10I2.2736|C. Kim, V. Vo|0.0|2
677|Efficient graphic processing unit implementation of the chemical-potential multiphase lattice Boltzmann method|The chemical-potential multiphase lattice Boltzmann method (CP-LBM) has the advantages of satisfying the thermodynamic consistency and Galilean invariance, and it realizes a very large density ratio and easily expresses the surface wettability. Compared with the traditional central difference scheme, the CP-LBM uses the Thomas algorithm to calculate the differences in the multiphase simulations, which significantly improves the calculation accuracy but increases the calculation complexity. In this study, we designed and implemented a parallel algorithm for the chemical-potential model on a graphic processing unit (GPU). Several strategies were used to optimize the GPU algorithm, such as coalesced access, instruction throughput, thread organization, memory access, and loop unrolling. Compared with dual-Xeon 5117 CPU server, our methods achieved 95 times speedup on an NVIDIA RTX 2080Ti GPU and 106 times speedup on an NVIDIA Tesla P100 GPU. When the algorithm was extended to the environment with dual NVIDIA Tesla P100 GPUs, 189 times speedup was achieved and the workload of each GPU reached 96%.|The international journal of high performance computing applications|2020|10.1177/1094342020968272|Yutong Ye, Binghai Wen, Chaoying Zhang, Hongyin Zhu|0.0|2
678|A Revolution in Performance using a GPU Based Simulator|\n Geological models are preferably built with very fine grids in a 3D dimensional geometry to capture reservoir complexity and heterogeneity. However, simulating such detailed models with sizes that can range to the 100’s of millions of cells is a huge challenge for current commercial CPU (Central Processing Unit) simulators as the results cannot be achieved within an acceptable time frame and simulation may last days or weeks.\n These long run times cause a delay in achieving a robust history matched or production forecast model as it constrains the ability to effectively characterize the uncertainty range of subsurface parameters and potential development solutions. Restricting the number of alternate scenarios in this manner diminishes the ability to arrive at solutions which may lead to loss of business opportunity and economic value. The alternatives have been used by oil and gas industry so far to reduce simulation run time, are either upscaling fine grid models to a coarser grid, consequently reducing the number of cells or increasing the number of nodes on a high-performance cluster so that simulation results can be achieved more rapidly by parallel computing using CPU based simulators. The former solution is limited by reservoir complexity and the latter by the scaling limits of the software. However, recent advances in high performance technical computing using Graphic Processing Units (GPU) has generated significant interest in the performance characteristics of a GPU-based simulator. This paper demonstrates performance of new generation of GPU based simulator as compared to CPU based simulators. A simulation run time of 9-11 hours on a 4-core CPU based simulator was reduced to 25-40 min on 1 GPU based simulator with tight bounds on results.||2020|10.2118/200748-ms|K. Mukundakrishnan, Jamal Siavoshi|0.0|2
687|Optimization of Building Patterns for Better Air Quality Using GPU-Based Large Eddy Simulation|Several earlier studies (e.g. [1–2]) pointed out the advantages of Large Eddy Simulation (LES) in modelling microscale dispersion, however, due to its high computational cost, most LES based studies focus on a single geometrical configuration without investigating the effects of different geometrical parameters. In this study, a virtual wind tunnel is presented, constructed in ANSYS Discovery Live, which is a GPU-based software originally developed for mechanical engineering applications. The applied software allows for altering the geometry or post-processing “on the fly”, while the results are being constantly updated by the solver. These features enable the rapid comparison of numerous design concepts with decent accuracy. The dispersion of traffic induced air pollutants is analyzed by utilizing the analogy between heat and mass transport processes. Using the presented methodology, converged LES results can be obtained for an urban dispersion scenario involving a few (< 20) buildings within an hour of computational time on a high-end gaming video card. Using passive turbulence generators at the inlet combined with a relatively short preparatory section, the proposed numerical wind tunnel configuration is capable of producing the mean velocity and turbulence intensity profiles characterizing the urban boundary layer. In our previous study [3] the numerical model has been validated by comparing the resultant flow and the pollutant fields with known wind tunnel data. In the present study, the application of the numerical wind tunnel is demonstrated. The flow and concentration fields are compared in the cases of various building arrangements of equal useful volumes. The building pattern of highest pollutant removal efficiency can be identified using the mass Stanton number resulting from the numerical model. The dependence of the Stanton number on various geometrical parameters is analyzed. In conclusion, building patterns containing higher objects in a staggered arrangement displayed superior pollutant removal performance and improved air quality at pedestrian level.||2019|10.1136/bmjopen-2019-qhrn.68|Papp Bálint, K. Gergely|0.0|2
697|Emulating and Verifying Sensing, Computation, and Communication in Distributed Remote Sensing Systems|This paper presents updates to the Virtual Constellation Engine (VCE), a simulator and emulator which enables modeling of sensing, computation, and communication of multi-platform remote sensing systems. Users can launch heterogeneous constellations, describe different on-board processing configurations, and simulate and verify autonomous operations. Updates in the past year include emulation of on-board computing utilizing cloud based processors, inclusion of the Delay Tolerant Networking (DTN) protocol in the communication modeling, and visualization and diagnostic tools which facilitate analysis of the distributed system. To illustrate the benefits, VCE was utilized in the development of on-board processing implementation of a new, high data rate sensor, Multispectral, Imaging, Detection, and Active Reflectance (MiDAR), for both an embedded Nvidia Tegra GPU and a Xilinx MPSoC FPGA achieving a 7.6x speedup over a desktop workstation and enabling this application for integration into a distributed sensing system.|IEEE International Geoscience and Remote Sensing Symposium|2020|10.1109/IGARSS39084.2020.9323610|Vivek V. Menon, M. French, Marco Paolieri, A. Schmidt|0.0|2
707|MoSES_2PDF: A GIS-Compatible GPU-accelerated High-Performance Simulation Tool for Grain-Fluid Shallow Flows|We introduce a GPU-accelerated simulation tool, named Modeling on Shallow Flows with Efficient Simulation for Two-Phase Debris Flows (MoSES_2PDF), of which the input and output data can be linked to the GIS system for engineering application. MoSES_2PDF is developed based on the CUDA structure so that it can well run with different NVIDIA GPU cards, once the CUDA vers. 9.2 (or higher) is installed. The performance of the MoSES_2PDF is evaluated, and it is found that the present GPU-CUDA implementation can enhance efficiency by up to 230 folds, depending on the PC/workstations, models of GPU card, and the mesh numbers in the computation domain. Two numerical examples are illustrated with two distinct initial inflow conditions, which are included in two modes of MoSES_2PDF, respectively. In the numerical example of a large-scale event, the 2009 Hsiaolin event, the results computed by two distinct NVIDIA GPU cards (RTX-2080-Ti and Tesla-V100) are found to be identical but only tiny deviation is figured out in comparison with the results computed by the conventional single-core CPU-code. It is speculated to be caused by the different structures in the source codes and some float/double operations. In addition to the illustration in the GIS system, the computed results by MoSES\_2PDF can also be shown with animated 3D graphics in the ANSI-Platform, where the user can interact with 3D scenes. The feasibility, features, and facilities of MoSES\_2PDF are demonstrated with respect to the two numerical examples concerning two real events.|arXiv.org|2021|10.5194/egusphere-egu21-10498|Chi-Jyun Ko, Po-Chih Chen, H. Wong, Y. Tai|0.0|2
711|Performance Optimization and Scalability Analysis of the MGB Hydrological Model|Hydrological models are extensively used in applications such as water resources, climate change, land use, and forecast systems. The focus of this paper is performance optimization of the MGB hydrological model, which is widely employed to simulate water flows in large-scale watersheds. The optimization strategies that we selected include AVX-512 vectorization, thread-parallelism on multi-core CPUs (OpenMP), and data-parallelism on many-core GPUs (CUDA). We conducted experiments for real-world input datasets on state-of-the-art HPC systems based on Intel's Skylake CPUs and NVIDIA GPUs. In addition, a Roofline model characterization for these datasets confirmed performance improvements of up to 37.5x on the most time-consuming part of the code and 8.6x on the full MGB model. The work proposed herein shows that careful optimizations are needed for hydrological models to achieve a significant fraction of the performance potential in modern processors.|International Conference on High Performance Computing|2020|10.1109/HiPC50609.2020.00017|C. Mendes, A. Ilic, H. Freitas|0.0|2
719|TRENDS IN THE DEVELOPMENT OF METHODS FOR DIAGNOSTICS OF THE TECHNICAL STATE OF THE BLADES OF GAS-PUMPING UNITS|Object of research: degradation processes occurring in the blades of a gas-pumping unit (GPU) during its long-term operation and cause the appearance of defects that lead to a change in their technical condition and breakage of the working blades and, as a result, to accidents. \nSolved problem: obtaining a method for diagnosing the GPU blades, which can be used to determine its technical condition during the GPU operation. \nThe main scientific results: a classification of methods for diagnosing the GPU blades and their analysis was developed, according to the results of which it was established that the improvement of a new method of aerodynamic calculation of profiles relative to the GPU blades by developing mathematical models of the deformation process and flow around the GPU blades and calculation formulas for assessing their aerodynamic characteristics, will allow to simulate possible options for changing the technical state of the blades (the amount of wear of the blades, their number, deformation, etc.), to study their influence on the parameters of the oscillatory processes of the blades and to compile a dictionary (base) of diagnostic signs of their state. The use of such a base will make it possible to quickly determine the technical condition of the blades during the GPU operation and prevent the occurrence of emergency situations \nThe area of practical use of the research results: the enterprises of the gas transmission system, which operate GPU with a gas turbine drive \nAn innovative technological product: a method of direct aerodynamic calculation of airfoils relative to GPU blades to determine their technical condition during operation. \nScope of application of the innovative technological product: gas-pumping units with a gas turbine drive with a capacity of 6.3 MW to 25.0 MW||2021|10.21303/2313-8416.2021.001678|Leonid Zamikhovsky, O. Zamikhovska, N. Ivanyuk|0.0|2
720|A quantitative prediction of the post-operative lobectomy lung physiology using a GPU-based linear elastic lung biomechanics model and a constrained generative adversarial learning approach|Lobectomy is a common and effective procedure for treating early-stage lung cancers. However, for patients with compromised pulmonary function (e.g. COPD) lobectomy can lead to major postoperative pulmonary complications. A technique for quantitatively predicting postoperative pulmonary function is needed to assist surgeons in assessing candidate’s suitability for lobectomy. We present a framework for quantitatively predicting the postoperative lung physiology and function using a combination of lung biomechanical modeling and machine learning strategies. A set of 10 patients undergoing lobectomy was used for this purpose. The image input consists of pre- and post-operative breath hold CTs. An automated lobe segmentation algorithm and lobectomy simulation framework was developed using a Constrained Adversarial Generative Networks approach. Using the segmented lobes, a patient-specific GPU-based linear elastic biomechanical and airflow model and surgery simulation was then assembled that quantitatively predicted the lung deformation during the forced expiration maneuver. The lobe in context was then removed by simulating a volume reduction and computing the elastic stress on the surrounding residual lobes and the chest wall. Using the deformed lung anatomy that represents the post-operative lung geometry, the forced expiratory volume in 1 second (FEV1) (the amount of air exhaled by a patient in 1 second starting from maximum inhalation), and forced vital capacity (FVC) (the amount of air exhaled by force from maximum inhalation), were then modeled. Our results demonstrated that the proposed approach quantitatively predicted the postoperative lobe-wise lung function at the FEV1 and FEV/FVC.||2021|10.1117/12.2582271|I. Barjaktarevic, D. Low, M. Lauria, A. Santhanam, J. Yanagawa, J. Goldin, B. Stiehl|0.0|2
726|Enabling Large-Scale Simulations of Quantum Transport with Manycore Computing|The non-equilibrium Green’s function (NEGF) is being utilized in the field of nanoscience to predict transport behaviors of electronic devices. This work explores how much performance improvement can be driven for quantum transport simulations with the aid of manycore computing, where the core numerical operation involves a recursive process of matrix multiplication. Major techniques adopted for performance enhancement are data restructuring, matrix tiling, thread scheduling, and offload computing, and we present technical details on how they are applied to optimize the performance of simulations in computing hardware, including Intel Xeon Phi Knights Landing (KNL) systems and NVIDIA general purpose graphic processing unit (GPU) devices. With a target structure of a silicon nanowire that consists of 100,000 atoms and is described with an atomistic tight-binding model, the effects of optimization techniques on the performance of simulations are rigorously tested in a KNL node equipped with two Quadro GV100 GPU devices, and we observe that computation is accelerated by a factor of up to ∼20 against the unoptimized case. The feasibility of handling large-scale workloads in a huge computing environment is also examined with nanowire simulations in a wide energy range, where good scalability is procured up to 2048 KNL nodes.|Electronics|2021|10.3390/ELECTRONICS10030253|Yosang Jeong, H. Ryu|0.0|2
740|Workflows Community Summit: Advancing the State-of-the-art of Scientific Workflows Management Systems Research and Development|Scientific workflows are a cornerstone of modern scientific computing, and they have underpinned some of the most significant discoveries of the last decade. Many of these workflows have high computational, storage, and/or communication demands, and thus must execute on a wide range of large-scale platforms, from large clouds to upcoming exascale HPC platforms. Workflows will play a crucial role in the data-oriented and post-Moore's computing landscape as they democratize the application of cutting-edge research techniques, computationally intensive methods, and use of new computing platforms. As workflows continue to be adopted by scientific projects and user communities, they are becoming more complex. Workflows are increasingly composed of tasks that perform computations such as short machine learning inference, multi-node simulations, long-running machine learning model training, amongst others, and thus increasingly rely on heterogeneous architectures that include CPUs but also GPUs and accelerators. The workflow management system (WMS) technology landscape is currently segmented and presents significant barriers to entry due to the hundreds of seemingly comparable, yet incompatible, systems that exist. Another fundamental problem is that there are conflicting theoretical bases and abstractions for a WMS. Systems that use the same underlying abstractions can likely be translated between, which is not the case for systems that use different abstractions. More information: https://workflowsri.org/summits/technical|arXiv.org|2021|10.5281/zenodo.4915801|L. Pottier, J. Wozniak, Bertram Ludäscher, A. Fouilloux, Douglas Lowe, Y. Babuji, S. Gesing, Rosa Filgueira, D. Ahn, S. Soiland-Reys, Renan Souza, B. Baliś, Kshitij Mehta, D. Thain, A. Gaignard, J. Ozik, U. Leser, T. Fahringer, Mehdi Roozmeh, A. Ganose, H. Casanova, M. Crusoe, A. Hasan, Alvaro Vidal-Torreira, S. Jha, K. Vahi, I. Altintas, M. Turilli, Silvina Caíno-Lores, T. Coleman, G. Fursin, Rosa M. Badia, K. De, D. Garijo, Rafael Ferreira da Silva, Frederik Coppens, F. Suter, R. Mayani, André Merzky, Frank Di Natale, K. Maheshwari, T. Do, Sebastiaan P. Huber, M. Wolf, S. Ristov, D. Laney, Dorran Howell, C. Goble, Michael Wilde, D. Katz, S. Callaghan, K. Chard, M. Malawski, Alan R. Williams, T. Munson, B. Enders, Benjamín Tovar, W. Whitcup|0.0|2
744|Massflow—A software for dynamic modeling and risk evaluation of earth-surfaced flow|<p>Massflow is based on the depth-integrated continuum and solved by second-order MacCormack-TVD finite difference method. Shared code and friendly GUI are provided for researchers and engineers. It adopted CPU and GPU accellerated algorithm to improve the efficiency. Now around 1000 people adopted Massflow to do their own research. Based the framework, we have done several insightful simulations of real landslides and debris flows. Meanwhile, we are developing a solution for catchment-based rainfall- flood-debris flow prediction. We will introduce the basic of the software, the mechanism and related model to modeling the real hazards, and the framework and finished work of forecasting of catchment flood or debris flow.&#160;</p>||2021|10.5194/EGUSPHERE-EGU21-14914|C. Ouyang|0.0|2
748|Scalable In Situ Lagrangian Flow Map Extraction: Demonstrating the Viability of a Communication-Free Model|We introduce and evaluate a new algorithm for the in situ extraction of Lagrangian flow maps, which we call Boundary Termination Optimization (BTO). Our approach is a communication-free model, requiring no message passing or synchronization between processes, improving scalability, thereby reducing overall execution time and alleviating the encumbrance placed on simulation codes from in situ processing. We terminate particle integration at node boundaries and store only a subset of the flow map that would have been extracted by communicating particles across nodes, thus introducing an accuracy-performance tradeoff. We run experiments with as many as 2048 GPUs and with multiple simulation data sets. For the experiment configurations we consider, our findings demonstrate that our communication-free technique saves as much as 2x to 4x in execution time in situ, while staying nearly as accurate quantitatively and qualitatively as previous work. Most significantly, this study establishes the viability of approaching in situ Lagrangian flow map extraction using communication-free models in the future.|EGPGV@EuroVis|2020|10.2312/pgv.20211040|H. Childs, C. Garth, K. Moreland, R. Bujack, S. Sane, A. Yenpure, Matthew Larsen|0.0|2
752|MCX Cloud—a modern, scalable, high-performance and in-browser Monte Carlo simulation platform with cloud computing|Significance Despite the ample progress made towards faster and more accurate Monte Carlo (MC) simulation tools over the past decade, the limited usability and accessibility of these advanced modeling tools remain key barriers towards widespread use among the broad user community. Aim An open-source, high-performance, web-based MC simulator that builds upon modern cloud computing architectures is highly desirable to deliver state-of-the-art MC simulations and hardware acceleration to general users without the need for special hardware installation and optimization. Approach We have developed a configuration-free, in-browser 3-D MC simulation platform – MCX Cloud – built upon an array of robust and modern technologies, including a Docker Swarm-based cloud-computing backend and a web-based graphical user interface (GUI) that supports in-browser 3-D visualization, asynchronous data communication, and automatic data validation via JavaScript Object Notation (JSON) schemas. Results The front-end of the MCX Cloud platform offers an intuitive simulation design, fast 3-D data rendering, and convenient simulation sharing. The Docker Swarm container orchestration backend is highly scalable and can support high-demand GPU MC simulations using Monte Carlo eXtreme (MCX) over a dynamically expandable virtual cluster. Conclusion MCX Cloud makes fast, scalable, and feature-rich MC simulations readily available to all biophotonics researchers without overhead. It is fully open-source and can be freely accessed at http://mcx.space/cloud.|bioRxiv|2021|10.1101/2021.06.28.450034|Q. Fang, Shijie Yan|0.0|2
754|Implementing the BBE Agent-Based Model of a Sports-Betting Exchange|In this paper we describe three independent implementations of a new agent-based model (ABM) that simulates a contemporary sports-betting exchange, such as those offered commercially by companies including Betfair, Smarkets, and Betdaq. The motivation for constructing this ABM, which is known as the Bristol Betting Exchange (BBE), is so that it can serve as a synthetic data generator, producing large volumes of data that can be used to develop and test new betting strategies via advanced data analytics and machine learning techniques. Betting exchanges act as online platforms on which bettors can find willing counterparties to a bet, and they do this in a way that is directly comparable to the manner in which electronic financial exchanges, such as major stock markets, act as platforms that allow traders to find willing counterparties to buy from or sell to: the platform aggregates and anonymises orders from multiple participants, showing a summary of the market that is updated in real-time. In the first instance, BBE is aimed primarily at producing synthetic data for in-play betting (also known as in-race or in-game betting) where bettors can place bets on the outcome of a track-race event, such as a horse race, after the race has started and for as long as the race is underway, with betting only ceasing when the race ends. The rationale for, and design of, BBE has been described in detail in a previous paper that we summarise here, before discussing our comparative results which contrast a single-threaded implementation in Python, a multi-threaded implementation in Python, and an implementation where Python header-code calls simulations of the track-racing events written in OpenCL that execute on a 640-core GPU – this runs approximately 1000 times faster than the single-threaded Python. Our source-code for BBE is being made freely available on GitHub.|Proceedings of the 33rd European Modeling & Simulation Symposium|2021|10.46354/i3m.2021.emss.032|Roberto Lau-Soto, James Hawkins, D. Cliff, James E. Keen|0.0|2
760|Plenary Talk 2 : Parallel Computing and AI: Impact and Opportunities|Parallel Computing has for many years now been needed in order to be able to get the desired performance for computational intensive tasks, ranging from image processing to astro-physics and weather simulations. Traditionally, the HPC field drove companies like Cray, IBM and others to develop processors for supercomputing. However, the market forces in other fields have-since the proliferation of COTS (commercial off-the shelf) processors, including GPUs for gaming and now more recently AI- driven the innovation in processor design. This means that algorithms, tools and applications now should adapt and take advantage of tensor processor, Machine Learning techniques, and other related technology, rather than expecting that old computational models will hold true. In this talk, we will discuss these issues, including how this is also an opportunity to help develop better graph algorithms for AI, and apply some of the techniques from AI to HPC challenges.|Ubiquitous Computing|2021|10.1109/MIUCC52538.2021.9447606|A. Elster|0.0|2
761|Using a Coarse-Grained Modeling Framework to Identify Oligomeric Motifs with Tunable Secondary Structure.|Coarse-grained modeling can be used to explore general theories that are independent of specific chemical detail. In this paper, we present cg_openmm, a Python-based simulation framework for modeling coarse-grained hetero-oligomers and screening them for structural and thermodynamic characteristics of cooperative secondary structures. cg_openmm facilitates the building of coarse-grained topology and random starting configurations, setup of GPU-accelerated replica exchange molecular dynamics simulations with the OpenMM software package, and features a suite of postprocessing thermodynamic and structural analysis tools. In particular, native contact analysis, heat capacity calculations, and free energy of folding calculations are used to identify and characterize cooperative folding transitions and stable secondary structures. In this work, we demonstrate the capabilities of cg_openmm on a simple 1-1 Lennard-Jones coarse-grained model, in which each residue contains 1 backbone and 1 side-chain bead. By scanning both nonbonded and bonded force-field parameter spaces at the coarse-grained level, we identify and characterize sets of parameters which result in the formation of stable helices through cooperative folding transitions. Moreover, we show that the geometries and stabilities of these helices can be tuned by manipulating the force-field parameters.|Journal of Chemical Theory and Computation|2021|10.26434/CHEMRXIV.14687463.V1|Michael R. Shirts, Theodore L. Fobe, G. A. Meek, Christopher C. Walker|0.0|2
765|Parallel simulation method of GNSS multipath signal based on GPU|In order to solve the problems of the traditional GNSS multipath signal simulation methods, such as the complexity increases linearly with the number of multipath signals, inconvenient parameter updating and long operation time, a GPU based GNSS multipath signal parallel simulation method is proposed. Based on Farrow variable fractional delay filter, an independent parallel computing model of multi-channel filter coefficients is established, and a large number of floating-point units in GPU are used for parallel computing and filtering. Aiming at the performance bottleneck of this method, the optimization strategy of asynchronous parallel execution is given. The computer simulation results show that the complexity and operation time of the proposed method will not increase with the increase of the number of multipath signals. For the GNSS direct signal with sampling rate of 75MHz and time length of 1s, the maximum time consuming of the proposed method to generate 50 ∼ 1000 multipath signals is 158.6ms. Compared with the method of serial simulation on CPU, the proposed method speeds up to about 9.5 times, and the real-time simulation of multipath signals in complex environment can be realized.|Journal of Physics: Conference Series|2021|10.1088/1742-6596/1971/1/012070|Rui Wang, Baiyu Li, Xin Kang, Zhicheng Lv, Weihua Mou|0.0|2
770|A hybrid system for the near real-time modeling and visualization of extreme magnitude earthquakes|<p>Among other natural hazards, the occurrence and impact of extreme magnitude earthquakes are of great interest both from the scientific and societal points of view. The scarcity of observational instrumental data for these type of events, as well as the urgent need to take mitigation measures to minimize their effects on human life and critical infrastructure have required the development of computational codes for the modeling of the propagation of these events.&#160;</p><p>Examples of the realistic modeling of the propagation of extreme magnitude earthquakes that can be achieved by the use of powerful HPC facilities and 3D finite difference Fortran codes have been presented by Cabrera et al. 2007 and Chavez et al. 2016. These large-scale scientific simulations generate vast amount of data, writing such data out to storage step-by-step is very slow and requires expensive I/O post-processing procedures for their analyses. However, the current and foreseen major advances occurring in Exascale HPC systems offer a transformational approach to the research community, as well as the possibility for the latter of contributing to the solution of urgent and complex problems that society is or will be facing in the years to come.</p><p>Taking into account the future exascale developments and in order to speed-up in situ analysis, i.e., analyze data at the same time simulations are running, in this ongoing research we present the main computational characteristics of the hybrid system we are developing for the near real-time simulation and visualization of the propagation of the realistic modeling of the 3D wave propagation of extreme magnitude earthquakes. The system is based on the updated version the staggered finite difference Fortran code 3DWPFD, coupled with an efficient visualization C++ code. The system is being developed in the hybrid HPC Miztli of UNAM, Mexico, made up of CPUs (8344 cores) + GPUs (16 NVIDIA m2090 and 8 V100). We expect to fully adapt the code for emerging hybrid Exascale architectures in the near future. Examples of the results obtained by using the hybrid system for the modeling of the propagation of the extreme magnitude Mw 8.2 earthquake occurred the 7 September 2017 in southern Mexico will be presented.</p>||2021|10.5194/EGUSPHERE-EGU21-16508|A. Salazar, M. Chavez, Eduardo C. Flores|0.0|2
773|A computational method for the covariance matrix associated with extracellular diffusivity on disordered models of cylindrical brain axons.|This study developed a method to approximate the covariance matrix associated with the simulation of water molecular diffusion inside the brain tissue. The computation implements the Discontinuous Galerkin method of the diffusion equation. A physically consistent numerical flux is applied to model the interaction between the axon walls and extracellular regions. This numerical flux yields an efficient GPU-CUDA implementation. We consider the two-dimensional case of high axon pack density, valid, for instance, in the brain's corpus callosum region.|Mathematical biosciences and engineering : MBE|2021|10.3934/MBE.2021252|Alonso Ramírez-Manzanares, D. Cervantes, Joaquín Peña, M. Moreles|0.0|2
774|Porting NEMO diagnostics to GPU accelerators|<p>This work makes part of an effort to make NEMO capable of taking advantage of modern accelerators. To achieve this objective we focus on port routines in NEMO that have a small impact on code maintenance and the higher possible overall time footprint reductions. Our candidates to port were the diagnostic routines, specifically <em>diahsb</em> (heat, salt, volume budgets) and <em>diawri</em> (Ocean variables) diagnostics. These two diagnostics correspond to 5% of the NEMO's runtime each on our test cases. Both can be executed in an asynchronous fashion allowing overlap between diagnostic GPU and other NEMO routines CPU computations. <br>We report a methodology to port runtime diagnostics execution on NEMO to GPU using CUDA Fortran and OpenACC. Both synchronous and asynchronous are implemented on <em>diahsb</em> and <em>diawri</em> diagnostics. Associated time step and stream interleave are proposed to allow the overlap of CPU execution of NEMO and data communication between CPU, and GPU.<br><br>In the case of constraint computational resources and high-resolution grids, synchronous implementation of <em>diahsb</em> and <em>diawri</em> show up to 3.5x speed-up. With asynchronous implementation we achieve a higher speed-up from 2.7x to 5x with <em>diahsb</em> in the study cases. The results for this diagnostic optimization point out that the asynchronous approach is profitable even in the case where plenty of computational resources are available and the number of MPI ranks is in the threshold of parallel effectiveness for a given computational workload. For <em>diawri</em> on the other hand, the results of the asynchronous implementation depart from the <em>diahsb</em>. In the <em>diawri</em> diagnostic module there are 30 times more datasets demanding pinned memory to overlap communication between CPU and GPU with CPU execution. Pinned memory attribute limits data management of datasets allocated on main memory, therefore makes possible to the GPU access to main memory, overlapping CPU computation. The result is a scenario where the improvement from offloading the diagnostic computation impacts on NEMO CPU general execution. Our main hypothesis is that the amount of pinned memory used decreases the performance on runtime data management, this is confirmed by the 7% increase of the L3 data cache misses in the study case. Although the necessity of evaluating the amount of datasets needed for asynchronous communication on a diagnostic port, the payout of asynchronous diagnostic may be worth given the higher speed-up values that we can achieve with this technique. This work proves that models such as NEMO, developed only for CPU architectures, can port some of their computation to accelerators. Additionally, this work explains a successful and simple way to implement an asynchronous approach, where CPU and GPU are working in parallel, but without modifying the CPU code itself, since the diagnostics are extracted as kernels for the GPU and the CPU is yet working in the simulation.</p>||2021|10.5194/EGUSPHERE-EGU21-10970|kim Serradell Maronda, M. Acosta, S. Palomas, Stella V. Paronuzzi Ticco, David Vicente Dorca, M. Castrillo, M. Faria|0.0|2
776|Communication-free and Parallel Simulation of Neutral Biodiversity Models|Biodiversity simulations are used in ecology and conservation to predict the effect of habitat destruction on biodiversity. We present a novel communication-free algorithm for individual-based probabilistic neutral biodiversity simulations. The algorithm transforms a neutral Moran ecosystem model into an embarrassingly parallel problem by trading off inter-process communication at the cost of some redundant computation. Specifically, by careful design of the random number generator that drives the simulation, we arrange for evolutionary parent-child interactions to be modelled without requiring knowledge of the interaction, its participants, or which processor is performing the computation. Critically, this means that every individual can be simulated entirely independently. The simulation is thus fully reproducible irrespective of the number of processors it is distributed over. With our novel algorithm, a simulation can be (1) split up into independent batch jobs and (2) simulated across any number of heterogeneous machines – all without affecting the simulation result. We use the Rust programming language to build the extensible and statically checked simulation package necsim-rust. We evaluate our parallelisation approach by comparing three traditional simulation algorithms against a CPU and GPU implementation of our Independent algorithm. These experiments show that as long as some local state is maintained to cull redundant individuals, our Independent algorithm is as efficient as existing sequential solutions. The GPU implementation further outperforms all algorithms on the CPU by a factor ranging from ∼ 2 to ∼ 80, depending on the model parameterisation and the analysis that is performed. Amongst the parallel algorithms we have investigated, our Independent algorithm provides the only non-approximate parallelisation strategy that can scale to large simulation domains. For example, while Thompson’s 2019 necsim simulation takes more than 48 hours to simulate 10 individuals, we have been able to simulate 7.1 · 10 individuals on an HPC batch system within a few hours.|arXiv.org|2021||Momo Langenstein|0.0|2
784|Novel Reconfigurable Hardware Systems for Tumor Growth Prediction|An emerging trend in biomedical systems research is the development of models that take full advantage of the increasing available computational power to manage and analyze new biological data as well as to model complex biological processes. Such biomedical models require significant computational resources, since they process and analyze large amounts of data, such as medical image sequences. We present a family of advanced computational models for the prediction of the spatio-temporal evolution of glioma and their novel implementation in state-of-the-art FPGA devices. Glioma is a rapidly evolving type of brain cancer, well known for its aggressive and diffusive behavior. The developed system simulates the glioma tumor growth in the brain tissue, which consists of different anatomic structures, by utilizing MRI slices. The presented models have been proved highly accurate in predicting the growth of the tumor, whereas the developed innovative hardware system, when implemented on a low-end, low-cost FPGA, is up to 85% faster than a high-end server consisting of 20 physical cores (and 40 virtual ones) and more than 28× more energy-efficient than it; the energy efficiency grows up to 50× and the speedup up to 14× if the presented designs are implemented in a high-end FPGA. Moreover, the proposed reconfigurable system, when implemented in a large FPGA, is significantly faster than a high-end GPU (i.e., from 80% and up to 250% faster), for the majority of the models, while it is also significantly better (i.e., from 80% to over 1,600%) in terms of power efficiency, for all the implemented models.|ACM Trans. Comput. Heal.|2021|10.1145/3454126|Pavlos Malakonakis, I. Papaefstathiou, M. Papadogiorgaki, Konstantinos Malavazos|0.0|2
791|Simulation of Motion of an Ensemble of Unmanned Ground Vehicles Using FLAME GPU|"A new approach to modeling the spatial dynamics of unmanned ground vehicles (UV) and conventional vehicles (CV) using the FLAME GPU supercomputer agent-based simulation platform is presented. A new simulation model of an artificial road network (ARN) of the ""Manhattan Grid"" type is proposed, within the framework of which the spatial dynamics of the UV and CV ensemble is studied under various scenario conditions. The effects of ""turbulence"" and ""crush"" (traffic congestion) resulting from intensive and dense traffic of vehicles are investigated."|INFORMACIONNYE TEHNOLOGII|2021|10.17587/IT.27.369-379|A. Beklaryan, Central Economics, F. A. Belousov, A. Akopov, L. Beklaryan|0.0|2
794|gprMax: An Open Source Electromagnetic Simulator for Generating Big Data for Ground Penetrating Radar Applications|<p>Lack of well-labelled and coherent training data is the main reason why machine learning (ML) and data-driven interpretations are not established in the field of Ground-Penetrating Radar (GPR). Non-representative and limited datasets lead to non-reliable ML-schemes that overfit, and are unable to compete with traditional deterministic approaches. To that extent, numerical data can potentially complement insufficient measured datasets and overcome this lack of data, even in the presence of large feature spaces.</p><p>Using synthetic data in ML is not new and it has been extensively applied to computer vision. Applying numerical data in ML requires a numerical framework capable of generating synthetic but nonetheless realistic datasets. Regarding GPR, such a framework is possible using gprMax, an open source electromagnetic solver, fine-tuned for GPR applications [1], [2], [3]. gprMax is fully parallelised and can be run using multiple CPU&#8217;s and GPU&#8217;s. In addition, it has a flexible scriptable format that makes it easy to generate big data in a trivial manner. Stochastic geometries, realistic soils, vegetation, targets [3] and models of commercial antennas [4], [5] are some of the features that can be easily incorporated in the training data.</p><p>The capability of gprMax to generate realistic numerical datasets is demonstrated in [6], [7]. The investigated problem is assessing the depth and the diameter of rebars in reinforced concrete. Estimating the diameter of rebars using GPR is particularly challenging with no conclusive solution. Using a synthetic training set, generated using gprMax, we managed to effectively train ML-schemes capable of estimating the diameter of rebar in an accurate and efficient manner [6], [7]. The aforementioned case studies support the premise that gprMax has the potential to provide realistic training data to applications where well-labelled data are not available, such as landmine detection, non-destructive testing and planetary sciences.</p><p><strong>References</strong></p><p>[1] Warren, C., Giannopoulos, A. & Giannakis, I., (2016). gprMax: Open Source software to simulate electromagnetic wave propagation for Ground Penetrating Radar, Computer Physics Communications, 209, 163-170.</p><p>[2] Warren, C., Giannopoulos, A., Gray, A., Giannakis, I., Patterson, A., Wetter, L. & Hamrah, A., (2018). A CUDA-based GPU engine for gprMax: Open source FDTD, electromagnetic simulation software. Computer Physics Communications, 237, 208-218.</p><p>[3] Giannakis, I., Giannopoulos, A. & Warren, C. (2016). A realistic FDTD numerical modeling framework of Ground Penetrating Radar for landmine detection. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 9(1), 37-51.</p><p>[4] Giannakis, I., Giannopoulos, A. & Warren, C., (2018). Realistic FDTD GPR antenna models optimized using a novel linear/non-linear full waveform inversion. IEEE Transactions on Geoscience and Remote Sensing, 207(3), 1768-1778.</p><p>[5] Warren, C., Giannopoulos, A. (2011). Creating finite-difference time-domain models of commercial ground-penetrating radar antennas using Taguchi&#8217;s optimization method. Geophysics, 76(2), G37-G47</p><p>[6] Giannakis, I., Giannopoulos, A. & Warren, C. (2021). A Machine Learning Scheme for&#160; Estimating the Diameter of Reinforcing Bars Using Ground Penetrating Radar. IEEE Geoscience and Remote Sensing Letters.</p><p>[7] Giannakis, I., Giannopoulos, A., & Warren, C. (2019). A machine learning-based fast-forward solver for ground penetrating radar with application to full-waveform inversion. IEEE Transactions on Geoscience and Remote Sensing. 57(7), 4417-4426.</p>||2021|10.5194/egusphere-egu21-10347|A. Giannopoulos, Craig Warren, I. Giannakis|0.0|2
797|System-level Early-stage Modeling and Evaluation of IVR-assisted Processor Power Delivery System|Despite being employed in numerous efforts to improve power delivery efficiency, the integrated voltage regulator (IVR) approach has yet to be evaluated rigorously and quantitatively in a full power delivery system (PDS) setting. To fulfill this need, we present a system-level modeling and design space exploration framework called Ivory for IVR-assisted power delivery systems. Using a novel modeling methodology, it can accurately estimate power delivery efficiency, static performance characteristics, and dynamic transient responses under different load variations and external voltage/frequency scaling conditions. We validate the model over a wide range of IVR topologies with silicon measurement and SPICE simulation. Finally, we present two case studies using architecture-level performance and power simulators. The first case study focuses on optimal PDS design for multi-core systems, which achieves 8.6% power efficiency improvement over conventional off-chip voltage regulator module– (VRM) based PDS. The second case study explores the design tradeoffs for IVR-assisted PDSs in CPU and GPU systems with fast per-core dynamic voltage and frequency scaling (DVFS). We find 2 μs to be the optimal DVFS timescale, which not only reaps energy benefits (12.5% improvement in CPU and 50.0% improvement in GPU) but also avoids costly IVR overheads.|ACM Transactions on Architecture and Code Optimization (TACO)|2021|10.1145/3468145|C. Gill, An Zou, V. Reddi, Xuan Zhang, Xin He, Jingwen Leng, Huifeng Zhu|0.0|2
799|Interactive Volume Rendering Method Using a 3D Texture|Volume rendering is a method for converting scalar fields into images. In the GPU, three-dimensional scalar fields are stored as 3D textures. Recently, WebGL2 has added support for 3D textures, which allows you to implement elegant and fast volume rendering in the browser. In this paper, we propose a method of interactive volume rendering that works completely in the browser, implemented on WebGL2. The method takes into account the absorption and emission of light by the volume, as well as scattering effects. To create a physically realistic image from volume data, the absorption, emission, and scattering of light rays by the medium was simulated. Although modeling the propagation of light through the medium at this level produces spectacular and physically correct results, it is too expensive for interactive rendering, which is the goal of volume visualization. Since a fully physical model with scattering would be too expensive for interactive rendering, the method uses a simplified emission-absorption model. The proposed model does not calculate the cost effects of scattering and their approximation. The model of emission-absorption only is considered.|Automation, Control, and Information Technology|2021|10.1109/ACIT52158.2021.9548629|I. Perun, T. Korobeinikova, R. Chekhmestruk, S. Vyatkin, P. Mykhaylov|0.0|2
802|GPU-Parallel Constant-Time Limit Evaluation of Catmull-Clark Solids|Subdivision solids, such as Catmull-Clark (CC) solids, are versatile volumetric representation schemes that can be employed for geometric modeling, physically based simulation, and multi-material additive manufacturing. With volumetric limit evaluation still being the performance bottleneck for these applications, we present a massively parallel approach to Altenhofen et al.’s constant-time limit evaluation method for CC solids. Our algorithm exploits the computational power of modern GPUs, while maintaining the mathematical concepts of Altenhofen et al.’s method. Distributing the computations for a single cell across multiple streaming multiprocessors (SMs) increases the utilization of the GPU’s resources compared to straightforward parallelization. Specialized compute kernels for different topological configurations optimize shared memory usage and memory access. Our hybrid approach dynamically chooses the best kernel based on the topology and the evaluation parameters, resulting in speedups of between 5.75× and 61.58× compared to a CPU-parallel implementation of Altenhofen et al.’s method.|International Symposium on Vision, Modeling, and Visualization|2021|10.2312/vmv.20211369|Sebastian Besler, A. Stork, C. Altenhofen, D. Fellner|0.0|2
803|A GPU-accelerated LiDAR Sensor for Generating Labelled Datasets|This paper presents a GPU-based LiDAR simulator to generate large datasets of ground-truth point clouds. LiDAR technology has signiﬁcantly increased its impact on academic and industrial environments. However, some of its applications require a large amount of annotated LiDAR data. Furthermore, there exist many types of LiDAR sensors. Therefore, developing a parametric LiDAR model allows simulating a wide range of LiDAR scanning technologies and obtaining a signiﬁcant number of points clouds at no cost. Beyond their intensity data, these synthetic point clouds can be classiﬁed with any level of detail.|Spanish Computer Graphics Conference|2021|10.2312/ceig.20211360|Alfonso López Ruiz, C. Ogáyar-Anguita, F. F. Higueruela|0.0|2
804|Distributed Control Algorithm for Cooperative Autonomous Driving Vehicles Inspired by Flocking Behaviour|Control strategies for cooperatively autonomous driving vehicles are becoming more and more complex to ensure safe operation while, when taking inspiration from nature, more simple mathematical descriptions exist which describe behaviour of animals moving in a flocking or herding manner. In the eighties Reynolds came up with a set of heuristic rules [1] which form a model for flocking behaviour of birds and his work has been an inspiration for constructing a control strategy based on flocking behaviour for self-driving vehicles. Previous research relied on more simple kinematic models of vehicles and thus using it directly in real vehicles would be difficult. This research provides an algorithm inspired by flocking behaviour as found in swarms of birds and a bridge between said algorithm and a vehicle. A dedicated simulation environment which uses parallelism on a GPU is written and is used to determine the performance of the proposed algorithm. It is found that the proposed algorithm performs relatively well under the tested situations and is found that a wide range of parameters could be used which result in stable behaviour.||2021|10.1109/access.2021.3070490|Joppe Blondel|0.0|2
808|ICLA Unit: Intra-Cluster Locality-Aware Unit to Reduce L2 Access and NoC Pressure in GPGPUs|As the number of streaming multiprocessors (SMs) in GPUs increases, in order to gain better performance, the reply network faces heavy traffic. This causes congestion on Network-on-Chip (NoC) routers and memory controller’s (MC) buffers. By taking advantage of cooperative thread arrays (CTAs) that are scheduled locally in clusters, there is a high probability of finding the same copy of data in other SM’s [Formula: see text] cache in the same cluster. In order to make this feasible, it is necessary for the SMs to have access to local [Formula: see text] cache of the neighboring SMs. There is a considerable congestion in NoC due to unique traffic pattern called many-to-few-to-many. Thanks to the reduced number of requests that is attained by our proposed Intra-Cluster Locality-Aware (ICLA) unit, this congested replying network traffic becomes many-to-many traffic pattern and the replied data goes through the less-utilized core-to-core communication that mitigates the NoC traffic. The proposed architecture in this paper has been evaluated using 15 different workloads from CUDA SDK, Rodinia, and ISPASS2009 benchmarks. The proposed ICLA unit has been modeled and simulated in the GPGPU-Sim. The results show about 23.79% (up to 49.82%) reduction in average network latency, 15.49% (up to 36.82%) reduction in average [Formula: see text] cache access, and 18.18% (up to 58.1%) average improvement in the instruction per cycle (IPC).|J. Circuits Syst. Comput.|2021|10.1142/s0218126622500153|Siamak Biglari Ardabili, G. Z. Fatin|0.0|2
810|Design and Implementation of a Simulated Betting Exchange|his thesis details the development and implementation of possibly the first agent-based simulated betting exchange, named BBE (Bristol Betting Exchange), whose design principals are laid out in [11]. I \nprove that one of the main bottlenecks of the design can be tackled through the utilisation of a GPUs \nparallel processing capabilities, and I provide a detailed break-down on the behaviour exhibited by six \nimplementations of betting agents that adopt some naive strategies. It is hoped that this implementation \nof BBE could be picked up by academics in order to aid in the research of profitable betting strategies, something that has been traditionally very difficult to do without the existence of an agent based \nsimulator. \nMany people are familiar with financial exchanges, but far less are aware of the existence of betting \nexchanges. As a result, the development of profitable betting strategies to deploy on these exchanges, \nmuch in the same way automated trading algorithms are deployed on financial exchanges, has gone \nmostly overlooked. Various tools exist for testing out trading algorithms in a simulated environment \nsuch as BSE [8], however there is no equivalent tool for testing betting strategies. Getting access to high \nresolution historical data from an exchange to back-test a betting strategy is not only costly, but also fails \nto capture the feedback loops that may form between bettors as a result of the bettor interacting with \nothers in real time on the exchange. In addition, there only exists as much historical data as there has \nbeen events that have been bet on, which may not meet the volume of data required to train a Machine \nLearning model in the hopes of discovering a profitable betting strategy. BBE aims to address this lack \nof data by generating plausibly realistic synthetic data modelled from the mechanics of a real betting \nexchange. \nIn my implementation, the parallel processing framework OpenCL is used to relieve one of the core \nbottlenecks of BBE’s design — the need to simulate many thousands of horse races at regular intervals \nto simulate the changing of bettor’s opinions during an in-play betting session. I test and compare the \nrun-times from simulating the races sequentially on a CPU and in parallel on a GPU to discover that \noutsourcing the task of simulating many races on a GPU results in a speed-up of over 100 times. \nMy implementation of BBE is intended to be a gift to academics, which is why it has been made \nopen source. It is hoped that BBE can be used to train a Machine Learning Model from the simulations \nof many thousands of betting sessions. Making BBE run as efficient as possible was a key focus for me, \nsince I wanted to make BBE more accessible to those who do not have access to powerful servers or super \ncomputers for the long periods of time that would be required to train a Machine Learning model on an \nunoptimized version of BBE.|Social Science Research Network|2021|10.2139/ssrn.3876312|James Hawkins|0.0|2
812|Counter-based pseudorandom number generators for CORSIKA 8|This document is devoted to the description of advances in the generation of high-quality random numbers for CORSIKA 8, which is being developed in modern C++17 and is designed to run on modern multi-thread processors and accelerators. CORSIKA 8 is a Monte Carlo simulation framework to model ultra-high energy secondary particle cascades in astroparticle physics. The aspects associated with the generation of high-quality random numbers on massively parallel platforms, like multi-core CPUs and GPUs, are reviewed and the deployment of counter-based engines using an innovative and multi-thread friendly API are described. The API is based on iterators providing a very well known access mechanism in C++, and also supports lazy evaluation. Moreover, an upgraded version of the Squares algorithm with highly efficient internal 128 as well as 256 bit counters is presented in this context. Performance measurements are provided, as well as comparisons with conventional designs are given. Finally, the integration into CORSIKA 8 is commented.|EPJ Web of Conferences|2021|10.1051/epjconf/202125103039|Anton Poctarev, A. A. Alves, R. Ulrich|0.0|2
819|Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning|We present a novel characterization of the mapping of multiple parallelism forms (e.g. data and model parallelism) onto hierarchical accelerator systems that is hierarchy-aware and greatly reduces the space of software-to-hardware mapping. We experimentally verify the substantial effect of these mappings on all-reduce performance (up to 448x). We offer a novel syntax-guided program synthesis framework that is able to decompose reductions over one or more parallelism axes to sequences of collectives in a hierarchy- and mapping-aware way. For 69% of parallelism placements and user requested reductions, our framework synthesizes programs that outperform the default all-reduce implementation when evaluated on different GPU hierarchies (max 2.04x, average 1.27x). We complement our synthesis tool with a simulator exceeding 90% top-10 accuracy, which therefore reduces the need for massive evaluations of synthesis results to determine a small set of optimal programs and mappings.|Conference on Machine Learning and Systems|2021|10.1109/ijcnn52387.2021.9533956|Dominik Grewe, Ningning Xie, Dimitrios Vytiniotis, Tamara Norman|0.0|2
824|Real-Time Modelling and Rendering of Sprayed Concrete|This paper presents a new real-time method to model and render how sprayed concrete is spread on a surface. The method not only models and renders deposits sprayed from any angle, any distance and with any concrete ﬂow, but it is also able to compute the amount of deposited volume taking into account the percentage of material that rebounds. The proposed method has been developed for a real-time training simulator for concrete spraying machinery, where most of the algorithm is parallelised and computed in the GPU, leaving the CPU free for other computations. In this research, the method has been validated for its use on plain surfaces and tunnel walls, but it can be extended to other types of surfaces.||2021|10.2312/PT.20111141|L. Matey, F. Ordás, G. Velez, Aiert Amundarain, J. Marín|0.0|2
825|GPU-Enabled High-Fidelity LES Simulations for Turbomachinery Flows|\n Engineers performing computational simulations of flow physics are often faced with a trade-off between turn-around time and accuracy. High-fidelity models that can accurately capture small details of flow, such as turbulent mixing, are typically too expensive and are therefore reserved for studying smaller, component level problems. Standard models, like Reynolds-Averaged Navier-Stokes (RANS) and Unsteady-RANS, are used to predict larger interactions without the ability to accurately compute the small scales, at a lower computational cost than high-fidelity models. However, with specific algorithmic choices and access to large-scale GPU systems, we can demonstrate high-fidelity simulations of large engine sections that can be completed within engineering design cycle turn-around times, instead of the typical weeks to months required for high fidelity simulations.\n In this paper we present the high-order GENESIS code, employed in the simulation of complex turbulent flows inside the high-pressure turbine of a jet engine. The code efficiently exploits GPU accelerators to execute high-fidelity simulations, while also demonstrating extraordinary accuracy validated by experimental data and previous RANS model predictions. This is demonstrated for a three-dimensional high-pressure turbine stator domain, for which the LES is able to accurately predict wake mixing and temperature distribution, factors that are critical for designing durable turbine components.\n The new capability allows for computational studies of phenomena such as laminar to turbulent transition and wake mixing, all applied to relevant three-dimensional geometries present in the high-pressure turbine, all within the time scale of a typical engineering design cycle.|Volume 2C: Turbomachinery — Design Methods and CFD Modeling for Turbomachinery; Ducts, Noise, and Component Interactions|2021|10.1115/gt2021-58711|Greg Sluyter, M. Osuský, Rathakrishnan Bhaskaran, S. Shankaran, Dheeraj Kapilavai|0.0|2
826|Optimal Motion Planning for Object Interception and Capture|The work presented in this thesis is motivated by the great strength of optimal control and numerical optimization in generating feasible and optimal trajectories for complex robot trajectory planning problems. The task of interest is the interception and capture of free-flying objects, to include the interception with a flying object on ground by means of a fixed-based robot and the capture of a free-tumbling satellite in orbit by means of a space robot. In the first application, dynamic constraints play an important role in the optimal solutions, which need to be computed in a short time. In the second application, the optimal motion planning is characterized by multiple motion and sensor-based constraints, as well as by the non-holonomic dynamics of the robot. The stringent safety requirements in the remote orbital operational environment call for methods which can provide guarantees of feasibility with respect to the constraints at hand. Given the non-convex and highly constrained nature of these planning problems, the performance of trajectory optimization methods heavily depends on the provided trajectory initialization and they are generally not guaranteed to find a feasible solution. This motivates generating solutions offline and retrieving them online with the aid of generalization via regression. A series of regression methods are applied and compared to the first problem, namely the interception with a flying object on ground, for purpose of analysis. The optimal solutions generated offline build a training set for the regression methods, which construct a mapping function between a suitable task space and the optimization parameter solution space. This mapping is then used in an online setting, to quickly provide an initial guess for warm starting an online planner. Statistical simulation results show a very high rate of convergence of the online planner, and give insight into the relation between the optimality of the solutions and the size of the training data set. For the second task, namely the robot trajectory planning for the capture of a freetumbling satellite in orbit, knowledge of the satellite motion in future time is required. The dynamics of a free-tumbling satellite in orbit can be modelled as a free rigid body. The rotational dynamics however still presents some challenges, when wanting to propagate the body’s orientation for a sufficient time, for planning purposes. These challenges are addressed here in detail, proposing a method to identify the state and inertial parameters necessary for a robust motion prediction, accounting for measurement noise, modelling errors and other dynamical effects pertinent to the free-body dynamics. Furthermore, a statistical propagation method is presented which provides an estimate of the dispersion of the motion prediction, which results from the same disturbances. This information is intended as input to robust control methods, which account for the given uncertainty. The OOS-SIM robotic experimental facility at the DLR reproduces orbital dynamic and illumination conditions on ground, and was used to validate the proposed methods. A complex trajectory planner is then presented for the task of capturing the free-tumbling satellite by means of a free-floating robot in a realistic operational scenario. Due to the long computation times necessary for generating a training data set, an initialization method was developed based on a look-up table combined with a motion propagation of the target satellite. A statistical simulation analysis shows a satisfactory convergence behavior of the online planner. Furthermore, in order to make use of the motion planning solutions for control purposes, a tracking controller is presented which combines the planner’s input to sensor feedback. This controller was also implemented and tested on the OOS-SIM facility. The methods presented in this thesis for the satellite capture task describe an autonomous operational strategy. The motion planning is combined with target satellite motion prediction and robot tracking control functionalities in a new fashion. The use of numerical trajectory optimization for control purposes is as such demonstrated for this application. The effectiveness of using an experimental facility on ground for validation purposes is also demonstrated. More generally, the results for the two addressed interception and capture tasks have shown that generalization via regression of feasible and optimal solutions generated offline has great potential for efficiently solving complex trajectory planning problems online. This potential is also recognizable from the described possible improvements of the adopted methods, as well as from the possible use of GPU and cluster technologies. A reference trajectory is argued \nto be necessary to provide guarantees of feasibility for a given task. However, its intrinsic uncertainty calls for methods which provide the same guarantees, in view of the necessary trajectory deviations in the tracking phase.||2021|10.26083/TUPRINTS-00017617|R. Lampariello|0.0|2
827|An Interactive, Virtual Wind Tunnel using Virtual Reality and Unreal Engine 4|This article presents an integrated, interactive modelling and simulation tool for aerodynamic design and analysis. The development and coupling of a GPU-accelerated Computational Fluid Dynamics (CFD) library, a 3D scanning library and a virtual-reality-powered video game are presented and the resulting virtual wind tunnel described. Users of the tunnel may interact with both geometry and solver from within the virtual environment providing a truly unique tool for performing high-level aerodynamic analysis around imported and scanned objects. Here we introduce the relevant technologies and approaches used to build the components and discuss the technical challenges of integrating them. The flow solver is validated against experimental data for a representative turbulent flow and demonstrates excellent agreement with available data. We also present the peak performance of the solver on current hardware. Limitations and potential expansions for this proof of concept are discussed as well as applications in a range of other scientific fields.||2021|10.48420/14816583.V1|A. Harwood, A. Revell|0.0|2
828|Demonstrating UPC++/Kokkos Interoperability in a Heat Conduction Simulation (Extended Abstract)|"Author(s): Waters, Daniel; MacLean, Colin A; Bonachea, Dan; Hargrove, Paul | Abstract: We describe the replacement of MPI with UPC++ in an existing Kokkos code that simulates heat conduction within a rectangular 3D object, as well as an analysis of the new code’s performance on CUDA accelerators. The key challenges were packing the halos in Kokkos data structures in a way that allowed for UPC++ remote memory access, and streamlining synchronization costs. Additional UPC++ abstractions used included global pointers, distributed objects, remote procedure calls, and futures. We also make use of the device allocator concept to facilitate data management in memory with unique properties, such as GPUs. Our results demonstrate that despite the algorithm’s good semantic match to message passing abstractions, straightforward modifications to use UPC++ communication deliver vastly improved performance and scalability in the common case. We find the one-sided UPC++ version written in a natural way exhibits good performance, whereas the message-passing version written in a straightforward way exhibits performance anomalies. We argue this represents a productivity benefit for one-sided communication models."||2021|10.25344/S4630V|Paul H. Hargrove, D. Waters, D. Bonachea, Colin MacLean|0.0|2
832|EnzyHTP: A High-Throughput Computational Platform for Enzyme Modeling|Molecular simulations, including quantum mechanics (QM), molecular mechanics (MM), and multiscale QM/MM modeling, have been extensively applied to understand the mechanism of enzyme catalysis and to design new enzymes. However, molecular simulations typically require specialized, manual operation ranging from model construction to data analysis to complete the entire life cycle of enzyme modeling. The dependence on manual operation makes it challenging to simulate enzymes and enzyme variants in a high-throughput fashion. In this work, we developed a Python software, EnzyHTP, to automate molecular model construction, QM, MM, and QM/MM computation, and analyses of modeling data for enzyme simulations. To test the EnzyHTP, we used fluoroacetate dehalogenase (FAcD) as a model system and simulated the enzyme interior electrostatics for 100 FAcD mutants with a random single amino acid substitution. For each enzyme mutant, the workflow involves structural model construction, 1 ns molecular dynamics (MD) simulations, and quantum mechanical calculations in 100 MD-sampled snapshots. The entire simulation workflow for 100 mutants was completed in 7 h with 10 GPUs and 160 CPUs. EnzyHTP improves the efficiency of computational enzyme modeling, setting a basis for high-throughput identification of function-enhancing enzymes and enzyme variants. The software is expected to facilitate the fundamental understanding of catalytic origins across enzyme families and to accelerate the optimization of biocatalysts for non-native substrates.|Journal of Chemical Information and Modeling|2021|10.26434/chemrxiv-2021-f9sf1|Qianzhen Shao, Yaoyukun Jiang, Zhongyue Yang|0.0|2
838|GPU-Accelerated Discrete Event Simulations: Towards Industry 4.0 Manufacturing|Discrete Event Simulations (DES) are the most commonplace tools for modelling today's manufacturing factories and their processes. DES are becoming steadfastly integrated into their corresponding physical counterparts to administer greater avenues for their analysis, control, forecasts and optimisations in real-time. However, this growth does not materialise without a penalty in the form of computational burden. The demand for flexible and alternate approaches to accelerate DES is made necessary. Hence, the utilisation of GPUs to comply with such acceleration presents a research topic of growing interest. This work investigates the use of the Machine Learning platform TensorFlow with GPUs to accelerate a variety of manufacturing-domain DES using the SimPy simulation framework. A range of results were gathered, of speed-ups spanning between x1.4 and x3.21, paving the way for further enhancements towards the vision of real-time communication between simulation and physical system in the form of a complete Digital Twin.|International Symposium on Computers and Communications|2021|10.1109/ISCC53001.2021.9631514|Moustafa Faheem, C. Reaño, A. Murphy|0.0|2
845|Elastic 3D Wavefield Simulation on budget GPUs using the GLSL shading language|Forward wavefield simulation is an important step in Full Waveform Inversion systems. Fast simulations are instrumental to get inversion result in reasonable time frames. Most of research and software aims towards utilizing costly computer clusters composed of multiple CPUs and numerous high end GPUs to shorten the forward simulation time. Using this type of hardware has some disadvantages as: high cost, complex programming models and unavailability of resources. In this work, we present a finite difference elastic 3D wavefield forward simulation that takes advantage of any modern low end GPU, by using the GLSL shading language. Some of the advantages of using GLSL are: runs in any modern GPU, has a simplified computing and memory model and provides state of art performance thanks to its very well optimized vendor developed drivers. We show that our GLSL implementation easily outperforms a multicore CPU implementation in a modern PC. We further benchmark our result using a real seismic event, and show that we can get accurate simulations in reasonable time using our system. 1 ar X iv :2 11 2. 15 07 1v 1 [ ee ss .S P] 3 0 D ec 2 02 1||2021|10.1109/fg52635.2021.9666967|Emanuel Trabes, C. Paez, J. Gazzano, Orlando Alvarez Pontoriero, S. Spagnotto|0.0|2
847|Large-Scale In Situ Visualization of Raleigh-Taylor Instability with Ascent and VTK-m|Highlight of a Large-Scale In Situ Visualization of Raleigh-Taylor Instability with Ascent and VTK-m. The 97.8 billion element simulation ran across 16,384 GPUs on 4,096 Sierra Compute Nodes to guide the development of sub-grid models to capture instability effects with less computation. The visualization and analysis used the same resources as the simulation and scaled to the entire machine.||2021|10.6084/M9.FIGSHARE.16926130.V1|K. Moreland, Matthew Larsen|0.0|2
864|Parallel Application Power and Performance Prediction Modeling Using Simulation|High performance computing (HPC) system runs compute-intensive parallel applications requiring large number of nodes. An HPC system consists of heterogeneous computer architecture nodes, including CPUs, GPUs, field programmable gate arrays (FPGAs), etc. Power capping is a method to improve parallel application performance subject to variable power constraints. In this paper, we propose a parallel application power and performance prediction simulator. We present prediction model to predict application power and performance for unknown power-capping values considering heterogeneous computing architecture. We develop a job scheduling simulator based on parallel discrete-event simulation engine. The simulator includes a power and performance prediction model, as well as a resource allocation model. Based on real-life measurements and trace data, we show the applicability of our proposed prediction model and simulator.|Online World Conference on Soft Computing in Industrial Applications|2021|10.1109/WSC52266.2021.9715340|S. Tasnim, Kishwar Ahmed, Kazutomo Yoshii|0.0|2
868|FishGym: A High-Performance Physics-based Simulation Framework for Underwater Robot Learning|Bionic underwater robots have demonstrated their superiority in many applications. Yet, training their intelligence for a variety of tasks that mimic the behavior of underwater creatures poses a number of challenges in practice, mainly due to lack of a large amount of available training data as well as the high cost in real physical environment. Alternatively, simulation has been considered as a viable and important tool for acquiring datasets in different environments, but it mostly targeted rigid and soft body systems. There is currently dearth of work for more complex fluid systems interacting with immersed solids that can be efficiently and accurately simulated for robot training purposes. In this paper, we propose a new platform called “FishGym”, which can be used to train fish-like underwater robots. The framework consists of a robotic fish modeling module using articulated body with skinning, a GPU-based high-performance localized two-way coupled fluid-structure interaction simulation module that handles both finite and infinitely large domains, as well as a reinforcement learning module. We leveraged existing training methods with adaptations to underwater fish-like robots and obtained learned control policies for multiple benchmark tasks. The training results are demonstrated with reasonable motion trajectories, with comparisons and analyses to empirical models as well as known real fish swimming behaviors to highlight the advantages of the proposed platform.|IEEE International Conference on Robotics and Automation|2022|10.48550/arXiv.2206.01683|Kai-Yi Bai, Changxi Zheng, Wenji Liu, Xuming He, Shuran Song, Xiaopei Liu|0.0|2
874|Context-LSTM: a robust classifier for video detection on UCF101|Video detection and human action recognition may be computationally expensive, and need a long time to train models. In this paper, we were intended to reduce the training time and the GPU memory usage of video detection, and achieved a competitive detection accuracy. Other research works such as Two-stream, C3D, TSN have shown excellent performance on UCF101. Here, we used a LSTM structure simply for video detection. We used a simple structure to perform a competitive top-1 accuracy on the entire validation dataset of UCF101. The LSTM structure is named Context-LSTM, since it may process the deep temporal features. The Context-LSTM may simulate the human recognition system. We cascaded the LSTM blocks in PyTorch and connected the cell state flow and hidden output flow. At the connection of the blocks, we used ReLU, Batch Normalization, and MaxPooling functions. The Context-LSTM could reduce the training time and the GPU memory usage, while keeping a state-of-the-art top-1 accuracy on UCF101 entire validation dataset, show a robust performance on video action detection.|arXiv.org|2022|10.48550/arXiv.2203.06610|Dengshan Li, Rujing Wang|0.0|2
878|Alternating Blind Identification of Power Sources for Mobile SoCs|The need for faster Systems on Chip (SoCs) has accelerated scaling trends, leading to a considerable power density increase and raising critical power and thermal challenges. The ability to measure power consumption of different hardware units is essential for the operation and improvement of mobile SoCs, as well as the enhancement of the power efficiency of the software that runs on them. SoCs are usually enabled with embedded thermal sensors to measure the temperature at the hardware unit level; however, they lack the ability to sense the power. In this paper we introduce an Alternating Blind Identification of Power sources (Alternating-BPI), a technique that accurately estimates the power consumption of individual SoC units without the use of any design based models. The proposed technique uses a novel approach to blindly identify the sources of power consumption, by relying only on the measurements from the embedded thermal sensors and the total power consumption. The accuracy and applicability of the proposed technique was verified using simulation and experimental data. Alternating-BPI is able to estimate the power at the SoC hardware unit level with up to 98.1% accuracy. Furthermore, we demonstrate the applicability of the proposed technique on a commercial SoC and provide a fine-grain analysis of the power profiles of CPU and GPU Apps, as well as Artificial Intelligence (AI), Virtual Reality (VR) and Augmented Reality (AR) Apps. Additionally, we demonstrate that the proposed technique could be used to estimate the power consumption per-process by relying on the estimated per-unit power numbers and per-unit hardware utilization numbers. The analysis provided by the proposed technique gives useful insights about the power efficiency of the different hardware units on a state-of-the-art commercial SoC.|International Conference on Performance Engineering|2022|10.1145/3489525.3511676|S. Reda, Michael Chen, Farrukh Hijaz, Abhinav Golas, Adel Belouchrani, Sofiane Chetoui|0.0|2
879|Revisiting Model-based Value Expansion|Model-based value expansion methods promise to improve the quality of value function targets and, thereby, the effectiveness of value function learning. However, to date, these methods are being outperformed by Dyna-style algorithms with conceptually simpler 1-step value function targets. This shows that in practice, the theoretical justification of value expansion does not seem to hold. We provide a thorough empirical study to shed light on the causes of failure of value expansion methods in practice which is believed to be the compounding model error. By leveraging GPU based physics simulators, we are able to efficiently use the true dynamics for analysis inside the model-based reinforcement learning loop. Performing extensive comparisons between true and learned dynamics sheds light into this black box. This paper provides a better understanding of the actual problems in value expansion. We provide future directions of research by empirically testing the maximum theoretical performance of current approaches.|arXiv.org|2022|10.48550/arXiv.2203.14660|M. Lutter, Daniel Palenicek, Jan Peters|0.0|2
880|A dynamic ancestral graph model and GPU‐based simulation of a community based on metagenomic sampling|In this paper, we present an ancestral graph model of the evolution of a guild in an ecological community. The model is based on a metagenomic sampling design in that a random sample is taken at the community, as opposed the taxon, level and species are discovered by genetic sequencing. The specific implementation of the model envisions an ecological guild that was founded by colonization at some point in the past that then potentially undergoes diversification by natural selection. Within the graph, species emerge and evolve through the diversification process and their densities in the graph are dynamic and governed by both ecological drift and random genetic drift, as well as differential viability. We employ the 3% sequence divergence rule at a marker locus to identify operational taxonomic units. We then explore approaches to see whether there are indirect signals of the diversification process, including population genetic and ecological approaches. In terms of population genetics, we study the joint site frequency spectrum of OTUs, as well its associated statistics. In terms of ecology, we study the species (or OTU) abundance distribution. For both, we observe deviations from neutrality, which indicates that there may be signals of diversifying selection in metagenomic studies under certain conditions. The model is available as a GPU‐based computer program in C/C++ and using OpenCL, with the long‐term goal of adding functionality iteratively to model large‐scale eco‐evolutionary processes for metagenomic data.|Molecular Ecology Resources|2022|10.1111/1755-0998.13613|C. Griswold|0.0|2
885|Tile-based multiple parallel triangle rasterization algorithm implemented in FPGA|With the progress of rendering technology, the industry has put forward higher requirements for GPU performance while rasterization algorithm plays an important role in GPU performance. In this paper, we first review some of the current mainstream rasterization algorithms. Then we propose a triangle rasterization algorithm—tile-based multiple parallel midpoint rasterization algorithm. This algorithm combines the characteristics of midpoint traversal and tiled traversal. At last, we use FPGA to implement the algorithm in order to verify the feasibility of the algorithm. And we use the triangle data set used in previous papers to simulate in Modelsim software. The results show that this algorithm can improve the rasterization speed and the highest frequency of this design is 200MHZ.|Journal of Physics: Conference Series|2022|10.1088/1742-6596/2253/1/012015|Junjie Xue, Mingjiang Wang, Dihan Ai|0.0|2
891|Understanding wafer-scale GPU performance using an architectural simulator|Wafer-Scale chips have the potential to break the die-size limitation and provide extreme performance scalability. Existing solutions have demonstrated the possibility of integrating multi-CPU and multi-GPU systems at a significantly larger scale on a wafer. This increased capability results in an increase in complexity in managing the memory and computing resources. To support the community studying wafer-scale systems, this paper develops an architectural simulator dedicated to modeling wafer-scale multi-device systems. Also, this work demonstrates an analysis of initial results from simulations on wafer-scale GPU systems, providing useful insight that can guide future system design.|GPGPU@PPoPP|2022|10.1145/3530390.3532736|Yifan Sun, C. Thames, Hang Yan|0.0|2
900|Design and implementation of simulation platform for dynamics on complex networks|Complex networks are widely applied for describing the topological structures of complex systems. Besides the exploration of the topologies for complex networks, the dynamics on the complex networks receives much attention from researchers in the field of complex systems, the techniques of computer simulations are used to intuitively understand the emergent behaviors of systems consisting of large number of interacting agents. In this paper, we aim to design to a platform for visualization of dynamics on complex networks: By setting parameters such as network topology, types of dynamics and running time, the platform can visualize agent-based micro-characters as well as system-based emergent micro-behavior for evolutionary game dynamics, opinion dynamics and infection dynamics on complex networks. In addition, it also has functions such as logic control operation, configuration of initial network state, and export of state data. Compared with other networks modeling and simulation platforms, the platform has the characteristics of multithreading, scalability and cross-platform. Especially, it can also enhance the computing efficiency by techniques of GPU parallel computation.|Other Conferences|2022|10.1117/12.2639385|Yi Zhou, Wuneng Zhou|0.0|2
908|A Massively-Parallel 3D Simulator for Soft and Hybrid Robots|Simulation is an important step in robotics for creating control policies and testing various physical parameters. Soft robotics is a field that presents unique physical challenges for simulating its subjects due to the nonlinearity of deformable material components along with other innovative, and often complex, physical properties. Because of the computational cost of simulating soft and heterogeneous objects with traditional techniques, rigid robotics simulators are not well suited to simulating soft robots. Thus, many engineers must build their own one-off simulators tailored to their system, or use existing simulators with reduced performance. In order to facilitate the development of this exciting technology, this work presents an interactive-speed, accurate, and versatile simulator for a variety of types of soft robots. Cronos, our open-source 3D simulation engine, parallelizes a mass-spring model for ultra-fast performance on both deformable and rigid objects. Our approach is applicable to a wide array of nonlinear material configurations, including high deformability, volumetric actuation, or heterogenous stiffness. This versatility provides the ability to mix materials and geometric components freely within a single robot simulation. By exploiting the flexibility and scalability of nonlinear Hookean mass-spring systems, this framework simulates soft and rigid objects via a highly parallel model for near real-time speed. We describe an efficient GPU CUDA implementation, which we demonstrate to achieve computation of over 1 billion elements per second on consumer-grade GPU cards. Dynamic physical accuracy of the system is validated by comparing results to Euler-Bernoulli beam theory, natural frequency predictions, and empirical data of a soft structure under large deformation.|arXiv.org|2022|10.48550/arXiv.2207.09334|Hod Lipson, A. Gaudio, Max Segan, Boxi Xia, Andrew Moshova, Joel Clay, J. Austin, S. Wyetzner|0.0|2
909|Dynamic computational resource allocation for CFD simulations based on pareto front optimization|Computational Fluid Dynamics (CFD) simulations can be extremely computationally demanding and usually rely on the use of High-performance computing (HPC) using both CPU and GPU resources. Modeling the behavior of bubbles size distribution often leads to a symmetric function evaluation problem. This paper proposes a dynamic computational resource allocation, based on Pareto-optimal solutions. The solutions are obtained from the formulation of the resource-constrained symmetric function evaluation problem as a multi-objective problem. After the Pareto-front is obtained, we suggest a dynamic selection method of the solutions that utilize the existing resources. To solve the multi-objective problem ϵ-MOEA, an algorithm known to obtain good diversity of Pareto-front solutions, is applied. As the problem formulated is new, brute-force search and two-specifically designed for this problem-heuristics are implemented and tested to serve as baselines. The methods are tested and compared to three dimensionalities of the problem. The results showed that ϵ-MOEA can successfully approximate the Pareto-front, allowing to utilize the resources optimally at each simulation time-step.|GECCO Companion|2022|10.1145/3520304.3534033|M. Antoniou, G. Petelin, G. Papa|0.0|2
913|Next-generation HPC models for future rotorcraft applications|. Rotorcraft technologies pose great scientific and industrial challenges for numerical computing. As available computational resources approach the exascale, finer scales and therefore more accurate simulations of engineering test cases become accessible. However, shifting legacy workflows and optimizing parallel efficiency and scalability of existing software on new hardware is often demanding. This paper reports preliminary results in CFD and structural dynamics simulations using the T106A Low Pressure Turbine (LPT) blade geometry on Leonardo S.p.A.’s davinci-1 high-performance computing (HPC) facility. Time to solution and scalability are assessed for commercial packages Ansys Fluent, STAR-CCM+, and ABAQUS, and the open-source scientific computing framework PyFR. In direct numerical simulations of compressible fluid flow, normalized time to solution values obtained using PyFR are found to be up to 8 times smaller than those obtained using Fluent and STAR-CCM+. The findings extend to the incompressible case. All models offer weak and strong scaling in tests performed on up to 48 compute nodes, each with 4 Nvidia A100 GPUs. In linear elasticity simulations with ABAQUS, both the iterative solver and the direct solver provide speedup in preliminary scaling tests, with the iterative solver outperforming the direct solver in terms of time-to-solution and memory usage. The results provide a first indication of the potential of HPC architectures in scaling engineering applications towards certification by simulation, and the first step for the Company towards the use of cutting-edge HPC toolkits in the field of Rotorcraft technologies.|8th European Congress on Computational Methods in Applied Sciences and Engineering|2022|10.48550/arXiv.2207.12269|L. Capone, Nicoletta Sanguini, Francesco Rondina, A. Sciarappa, Tommaso Benacchio, Daniele Malacrida, F. Cipolletta|0.0|2
914|Algorithmic Advancements and a Comparative Investigation of Left and Right Looking Sparse LU Factorization on GPU Platform for Circuit Simulation|Sparse LU factorization is a key tool in the solution of large linear set of algebraic equations encompassing a wide range of computing applications. Recent advances in this field exploit the massively parallel architecture of the GPUs via left-looking algorithm (LLA) and right-looking algorithm (RLA). In this paper, adaptive cluster mode is proposed to improve the state-of-the-art in LLA for GPU platforms. The proposed method takes into consideration of varying sparsity at different levels during cluster mode execution, to adaptively configure the GPU block size and the number of parallel columns. The new refinements for LLA are also integrated with the dynamic parallelism that is available in modern GPU architectures. The paper also provides a comprehensive performance comparison of the LLA and hybrid RLA along with state-of-the-art advances on the same GPU platform. The results indicate that, when implemented with similar refinements and on a same platform, LLA provides better performance compared to the hybrid-RLA. The results would be useful to the scientific community while making decision on adopting LLA or RLA algorithms for sparse LU factorization.|IEEE Access|2022|10.1109/access.2022.3193785|Ramachandra Achar, W. Lee|0.0|2
915|Herald: An Embedding Scheduler for Distributed Embedding Model Training|Given the ability to represent categorical features, embedding models have gained great success on many internet services. State-of-the-art training frameworks enable embedding cache in GPU workers to benefit from hardware acceleration while supporting massive category representations (embeddings) in the limited-capacity GPU device memory. However, based on our measurements, naively adopting a cache system in embedding model training leads to non-negligible communications overhead between caches and the global parameter server. We observe that many such communications are avoidable, given the predictability and sparsity natures of embedding cache accesses in distributed training. In this paper, we propose Herald, a runtime embedding scheduler that significantly reduces the cache overhead by leveraging information about the required embeddings in the input samples and the locations of those embeddings. Herald is composed of two key optimizations: It allocates samples in a training batch to proper workers for a high cache hit rate via a heuristic location-aware inputs partition mechanism, and applies an on-demand synchronization strategy for a low frequency of embedding synchronization. Preliminary simulation results show that Herald can reduce cache overhead by 39.3%-53.7% compared to a naive cache-enabled training system across different realistic datasets.|Asia-Pacific Workshop on Networking|2022|10.1145/3542637.3542645|Xiaodian Cheng, Hao Wang, Han Tian, Kai Chen, Chaoliang Zeng|0.0|2
916|Next Generation Computational Tools for the Modeling and Design of Particle Accelerators at Exascale|Particle accelerators are among the largest, most complex devices. To meet the challenges of increasing energy, intensity, accuracy, compactness, complexity and ef-ﬁciency, increasingly sophisticated computational tools are required for their design and optimization. It is key that con-temporary software take advantage of the latest advances in computer hardware and scientiﬁc software engineering practices, deliveringspeed, reproducibilityand feature composability for the aforementioned challenges. A new open source software stack is being developed at the heart of the Beam pLasma Accelerator Simulation Toolkit ( BLAST ) by LBNL and collaborators, providing new particle-in-cell modeling codes capable of exploiting the power of GPUs on Exascale supercomputers. Combined with advanced numerical techniques, such as mesh-reﬁnement, and intrinsic support for machine learning, these codes are primed to provide ultrafast to ultraprecise modeling for future accelerator design and operations.|arXiv.org|2022|10.18429/JACoW-NAPAC2022-TUYE2|J. Qiang, J. Vay, R. Sandberg, R. Ryne, R. Lehe, C. Mitchell, A. Huebl|0.0|2
918|Highly Portable C++ Based Simulator with Dual Parallelism and Spatial Decomposition of Simulation Domain using Floating Point Operations and More Flops Per Watt for Better Time-To-Solution on Particle Simulation|LAMMPS is a classical molecular dynamics (MD) code that models ensembles of particles in a solid, liquid, or gaseous state. LAMMPS performance has been optimized over the years. This paper focuses on the study of MD simulations using LAMMPS on the PARAM Siddhi-AI system. LAMMPS’s performance is analyzed with the number of the latest NVIDIA A100 co-processor (GPU) based on Ampere architecture. The faster and larger L1 cache and shared memory in Ampere architecture (192 KB per Streaming Multiprocessor (SM)) delivers additional speedups for High-Performance Computing (HPC) workloads. In this work, single-node multi-GPUs as well as multi-node multi-GPUs (up to 5 nodes) LAMMPS performance for two input datasets LJ 2.5 (intermolecular pair potential) and EAM (interatomic potential), on PARAM Siddhi-AI system, is discussed. Performance improvement of GPU-enabled LAMMPS run over CPU-only performance is demonstrated for both the inputs data sets. LAMMPS performance is analyzed for initialization, atoms communication, forces, thermodynamic state (Pair (non-bonded force computations), Neigh (neighbor list construction), Comm (inter-processor communication of atoms and their properties), Output (output of thermodynamic info and dump files), Modify (fixes and computes invoked by fixes) and others (all the remaining time forces and functions)). GPU utilization (in terms of computing and memory) is also discussed for both the input datasets. GPU enabled LAMMPS single-node performance shows 31x and 125x speed-up in comparison to single-node CPU-only performance for LJ2.5 and EAM input datasets respectively. LAMMPS scales well across multi-node and shows almost linear scalability. In comparison to single node GPU enabled LAMMPS run, the observed speedup is 4.1x and 3.8x on 5 nodes, for LJ2.5 and EAM input datasets respectively. LAMMPS performance comparison across GPU generation shows 1.5x to 1.9x speedup on A100 GPUs over V100 GPUs.|International Conference on Communication, Computing & Security|2022|10.1109/icccs55155.2022.9846280|A. Das, M. Modani, Nisha Agrawal, R. Pathak|0.0|2
923|Using a static naming approach to implement remote scope promotion|: GPUs employ simple coherence mechanisms and require explicit use of costly synchronization operations for data integrity. Local-scoped synchronization can be utilized to lower the performance penalty of synchronization when sharing is within a subgroup of threads. Unfortunately, in asymmetric sharing (which is an important dynamic sharing pattern), it is necessary to use global-scoped synchronization due to possible accesses by remote sharers. Remote Scope Promotion (RSP) was introduced to take advantage of local-scoped synchronization at regular accesses while using scope promotion at occasional remote accesses. First implementation of RSP makes use of a simple approach that performs costly cache operations on all L1 data caches when implementing scope promotion, and therefore, it performs poorly on large scale GPU systems. We present nRSP which utilizes a static naming mechanism to identify regularly accessing agent in asymmetric sharing and avoids applying costly coherence actions on every L1 data cache when implementing scope promotion. We evaluate nRSP using timing detailed Gem5-APU simulator modeling a GPU system with 128 Compute Units and show that nRSP lowers remote synchronization overhead greatly and improves performance considerably. On average, nRSP provides around 28% speedup on a 128 Compute Unit GPU device.|Turkish J. Electr. Eng. Comput. Sci.|2022|10.55730/1300-0632.3903|Ayse Yilmazer|0.0|2
924|Understanding the Power of Evolutionary Computation for GPU Code Optimization|Achieving high performance for GPU codes requires developers to have significant knowledge in parallel programming and GPU architectures, and in-depth understanding of the application. This combination makes it challenging to find performance optimizations for GPU-based applications, especially in scientific computing. This paper shows that significant speedups can be achieved on two quite different scientific workloads using the tool, GEVO, to improve performance over human-optimized GPU code. GEVO uses evolutionary computation to find code edits that improve the runtime of a multiple sequence alignment kernel and a SARS-CoV-2 simulation by 28.9% and 29% respectively. Further, when GEVO begins with an early, unoptimized version of the sequence alignment program, it finds an impressive 30 times speedup–a performance improvement similar to that of the hand-tuned version. This work presents an in-depth analysis of the discovered optimizations, revealing that the primary sources of improvement vary across applications; that most of the optimizations generalize across GPU architectures; and that several of the most important optimizations involve significant code interdependencies. The results showcase the potential of automated program optimization tools to help reduce the optimization burden for scientific computing developers and enhance performance portability for domain-specific accelerators.|IEEE International Symposium on Workload Characterization|2022|10.1109/IISWC55918.2022.00025|S. Hofmeyr, M. Awan, Carole-Jean Wu, S. Forrest, Jhe-Yu Liou|0.0|2
927|GPU-optimized Approaches to Molecular Docking-based Virtual Screening in Drug Discovery: A Comparative Analysis|COVID-19 has shown the importance of having a fast response against pandemics. Finding a novel drug is a very long and complex procedure, and it is possible to accelerate the preliminary phases by using computer simulations. In particular, virtual screening is an in-silico phase that is needed to filter a large set of possible drug candidates to a manageable number. This paper presents the implementations and a comparative analysis of two GPU-optimized implementations of a virtual screening algorithm targeting novel GPU architectures. The first adopts a traditional approach that spreads the computation required to evaluate a single molecule across the entire GPU. The second uses a batched approach that exploits the parallel architecture of the GPU to evaluate more molecules in parallel, without considering the latency to process a single molecule. The paper describes the advantages and disadvantages of the proposed solutions, highlighting implementation details that impact the performance. Experimental results highlight the different performance of the two methods on several target molecule databases while running on NVIDIA A100 GPUs. The two implementations have a strong dependency with respect to the data to be processed. For both cases, the performance is improving while reducing the dimension of the target molecules (number of atoms and rotatable bonds). The two methods demonstrated a different behavior with respect to the size of the molecule database to be screened. While the latency one reaches sooner (with fewer molecules) the performance plateau in terms of throughput, the batched one requires a larger set of molecules. However, the performances after the initial transient period are much higher (up to 5x speed-up). Finally, to check the efficiency of both implementations we deeply analyzed their workload characteristics using the instruction roof-line methodology.|Journal of Parallel and Distributed Computing|2022|10.48550/arXiv.2209.05069|G. Palermo, E. Vitali, M. Fatica, M. Bisson, A. Beccari, D. Gadioli, F. Ficarelli|0.0|2
931|Towards Exascale for Wind Energy Simulations|We examine large-eddy-simulation modeling approaches and computational performance of two open-source computational fluid dynamics codes for the simulation of atmospheric boundary layer (ABL) flows that are of direct relevance to wind energy production. The first is NekRS, a high-order, unstructured-grid, spectral element code. The second, AMR-Wind, is a block-structured, second-order finite-volume code with adaptive-mesh-refinement capabilities. The objective of this study is to co-develop these codes in order to improve model fidelity and performance for each. These features will be critical for running ABL-based applications such as wind farm analysis on advanced computing architectures. To this end, we investigate the performance of NekRS and AMR-Wind on the Oak Ridge Leadership Facility supercomputers Summit, using 4 to 800 nodes (24 to 4,800 NVIDIA V100 GPUs), and Crusher, the testbed for the Frontier exascale system using 18 to 384 Graphics Compute Dies on AMD MI250X GPUs. We compare strong- and weak-scaling capabilities, linear solver performance, and time to solution. We also identify leading inhibitors to parallel scaling.|arXiv.org|2022|10.48550/arXiv.2210.00904|A. Tomboulides, M. Min, Paul F. Fischer, M. Churchfield, M. Sprague, Michael J. Brazell|0.0|2
934|Parallel computing for mobilities in periodic geometries.|We examine methods for calculating the effective mobilities of molecules driven through periodic geometries in the context of particle-based simulation. The standard formulation of the mobility, based on the long-time limit of the mean drift velocity, is compared to a formulation based on the mean first-passage time of molecules crossing a single period of the system geometry. The equivalence of the two definitions is derived under weaker assumptions than similar conclusions obtained previously, requiring only that the state of the system at subsequent period crossings satisfy the Markov property. Approximate theoretical analyses of the computational costs of estimating these two mobility formulations via particle simulations suggest that the definition based on first-passage times may be substantially better suited to exploiting parallel computation hardware. This claim is investigated numerically on an example system modeling the passage of nanoparticles through the slit-well device. In this case, the traditional mobility formulation is found to perform best when the Péclet number is small, whereas the mean first-passage time formulation is found to converge much more quickly when the Péclet number is moderate or large. The results suggest that, given relatively modest access to modern GPU hardware, this alternative mobility formulation may be an order of magnitude faster than the standard technique for computing effective mobilities of biomolecules through periodic geometries.|Physical Review E|2022|10.1103/physreve.106.045304|Andrew M. Nagel, M. Magill, H. D. de Haan|0.0|2
938|Nek5000/RS Performance on Advanced GPU Architectures|We demonstrate NekRS performance results on various advanced GPU architectures. NekRS is a GPU-accelerated version of Nek5000 that targets high performance on exascale platforms. It is being developed in DOE's Center of Efficient Exascale Discretizations, which is one of the co-design centers under the Exascale Computing Project. In this paper, we consider Frontier, Crusher, Spock, Polaris, Perlmutter, ThetaGPU, and Summit. Simulations are performed with 17x17 rod-bundle geometries from small modular reactor applications. We discuss strong-scaling performance and analysis.|arXiv.org|2022|10.2172/1894022|John K. Holmen, T. Rathnayake, M. Min, Paul F. Fischer, Yu-Hsiang Lan|0.0|2
940|Sim-to-Real via Sim-to-Seg: End-to-end Off-road Autonomous Driving Without Real Data|Autonomous driving is complex, requiring sophisticated 3D scene understanding, localization, mapping, and control. Rather than explicitly modelling and fusing each of these components, we instead consider an end-to-end approach via reinforcement learning (RL). However, collecting exploration driving data in the real world is impractical and dangerous. While training in simulation and deploying visual sim-to-real techniques has worked well for robot manipulation, deploying beyond controlled workspace viewpoints remains a challenge. In this paper, we address this challenge by presenting Sim2Seg, a re-imagining of RCAN that crosses the visual reality gap for off-road autonomous driving, without using any real-world data. This is done by learning to translate randomized simulation images into simulated segmentation and depth maps, subsequently enabling real-world images to also be translated. This allows us to train an end-to-end RL policy in simulation, and directly deploy in the real-world. Our approach, which can be trained in 48 hours on 1 GPU, can perform equally as well as a classical perception and control stack that took thousands of engineering hours over several months to build. We hope this work motivates future end-to-end autonomous driving research.|Conference on Robot Learning|2022|10.48550/arXiv.2210.14721|J. Edlund, P. Abbeel, Stephen James, Ali-akbar Agha-mohammadi, Amber Xie, John So, Rohan Thakker, Sunggoo Jung|0.0|2
943|Stardust: Compiling Sparse Tensor Algebra to a Reconfigurable Dataflow Architecture|We introduce Stardust, a compiler that compiles sparse tensor algebra to reconfigurable dataflow architectures (RDAs). Stardust introduces new user-provided data representation and scheduling language constructs for mapping to resource-constrained accelerated architectures. Stardust uses the information provided by these constructs to determine on-chip memory placement and to lower to the Capstan RDA through a parallel-patterns rewrite system that targets the Spatial programming model. The Stardust compiler is implemented as a new compilation path inside the TACO open-source system. Using cycle-accurate simulation, we demonstrate that Stardust can generate more Capstan tensor operations than its authors had implemented and that it results in 138$\times$ better performance than generated CPU kernels and 41$\times$ better performance than generated GPU kernels.|arXiv.org|2022|10.48550/arXiv.2211.03251|Alexander Rucker, Tian Zhao, K. Olukotun, Fredrik Kjolstad, Olivia Hsu|0.0|2
945|Graphics processing unit-accelerated high-quality watercolor painting image generation|Stroke-based rendering is a rendering method that mimics the actual painting technique by drawing a stroke by stroke on a blank canvas image. In this paper, we propose a watercolor image generation method using stroke-based rendering. The proposed method generates an image that is a good approximation of the input image as well as having the characteristics of a watercolor painting by repeatedly painting strokes while referring to the input image. To generate a high-quality image, that is, an image thatcloselyresemblesanactualwatercolorpainting,varioustechniquesareemployed: modeling of watercolor paper, detailed physical simulation of the movement of water and pigment, strokes using a brush model, among others. The proposed method generates a large number of strokes and performs computationally intensive watercolor simulations for each stroke. Therefore, this paper also presents its parallel algorithm using a Graphics Processing Unit (GPU). We implemented this parallel algorithm on an NVIDIAA100GPU.TheexperimentalresultsshowthattheCPUimplementationswith sequential and parallel executions take 34,651 and 867 s to generate a 4K-watercolor image of size 3840 × 2144, respectively. In contrast, the GPU implementation with parallel execution succeeded in reducing the time to 44 s.|Concurrency and Computation|2022|10.1002/cpe.7471|Jiamian Huang, Yasuaki Ito, K. Nakano|0.0|2
946|Application of GPU-accelerated FDTD method to electromagnetic wave propagation in plasma using MATLAB Parallel Processing Toolbox|Since numerical computing with MATLAB offers a wide variety of advantages, such as easier developing and debugging of computational codes rather than lower-level languages, the popularity of this tool is significantly increased in the past decade. However, MATLAB is slower than other languages. Moreover, utilizing MATLAB parallel computing toolbox on the Graphics Processing Unit (GPU) face some limitations. The lack of attention to these limitations reduces the program execution speed. Even sometimes, parallel GPU codes are slower than serial. In this paper, some techniques in using MATLAB parallel computing toolbox are studied to improve the performance of solving complex electromagnetic problems by the Finite Difference Time Domain (FDTD) method. Implementing these techniques allows the GPU-Accelerated Parallel FDTD code to execute 20x faster than (basic) serial FDTD code. Eventually, GPU-Accelerated Parallel FDTD code is utilized to optimize the computational modeling of electromagnetic waves propagating in plasma. In this simulation, kinetic theory equations for plasma are used (excluding inelastic collisions), and temporal evolution is studied by the FDTD method (coupled FDTD with kinetic theory).||2022|10.31219/osf.io/h9d43|Shayan Dodge, M. Shafiee, B. Shokri|0.0|2
947|Fine‐Grained Memory Profiling of GPGPU Kernels|Memory performance is a crucial bottleneck in many GPGPU applications, making optimizations for hardware and software mandatory. While hardware vendors already use highly efficient caching architectures, software engineers usually have to organize their data accordingly in order to efficiently make use of these, requiring deep knowledge of the actual hardware. In this paper we present a novel technique for fine‐grained memory profiling that simulates the whole pipeline of memory flow and finally accumulates profiling values in a way that the user retains information about the potential region in the GPU program by showing these values separately for each allocation. Our memory simulator turns out to outperform state‐of‐the‐art memory models of NVIDIA architectures by a magnitude of 2.4 for the L1 cache and 1.3 for the L2 cache, in terms of accuracy. Additionally, we find our technique of fine grained memory profiling a useful tool for memory optimizations, which we successfully show in case of ray tracing and machine learning applications.|Computer graphics forum (Print)|2022|10.1111/cgf.14671|M. Buelow, S. Guthe, D. Fellner|0.0|2
953|BiPMAP: A Toolbox for Predictions of Perceived Motion Artifacts on Modern Displays|Presenting dynamic scenes without incurring motion artifacts visible to observers requires sustained effort from the display industry. A tool that predicts motion artifacts and simulates artifact elimination through optimizing the display configuration is highly desired to guide the design and manufacture of modern displays. Despite the popular demands, there is no such tool available in the market. In this study, we deliver an interactive toolkit, Binocular Perceived Motion Artifact Predictor (BiPMAP), as an executable file with GPU acceleration. BiPMAP accounts for an extensive collection of user-defined parameters and directly visualizes a variety of motion artifacts by presenting the perceived continuous and sampled moving stimuli side-by-side. For accurate artifact predictions, BiPMAP utilizes a novel model of the human contrast sensitivity function to effectively imitate the frequency modulation of the human visual system. In addition, BiPMAP is capable of deriving various in-plane motion artifacts for 2D displays and depth distortion in 3D stereoscopic displays.|arXiv.org|2022|10.48550/arXiv.2212.03854|M. Banks, Guanghan Meng, Dekel Galor, L. Waller|0.0|2
955|GPU-Accelerated infrared signature analysis model based on the Reverse Monte Carlo method|High-fidelity scene simulations are now possible thanks to advancements in computer technology. A fast analysis model for target infrared signature was developed by using the Graphics Processor Unit (GPU) device. The Malkmus statistical narrow band model is used to calculate the gas band-averaged transmittance. Mie scattering theory is used to calculate the particle scattering parameters, and based on the assumption that particles scatter light independently, the albedo and phase functions of the participating media are determined. The Reverse Monte Carlo method (RMCM) is used to complete the radiative transfer. Finally, a GPU device is used to accelerate the ray tracing process and achieve fast calculation of radiation. The calculation results show that the fast analysis model with high accuracy, and compared to Central Processing Unit (CPU) serial programs, the computational performance is greatly improved, with an acceleration ratio of more than 60. The thread block size has a significant impact on the occupancy and acceleration ratio, and the highest computational performance is obtained with the thread block size of 128. The scattering will reduce the acceleration ratio of the fast analysis model, and the performance will be reduced as the particle mass flow rate increases.|Journal of Physics: Conference Series|2022|10.1088/1742-6596/2403/1/012016|Xinyuan Liu, Y. Shi, Qingzhen Yang|0.0|2
965|Research on feature fusion of optical sensing database of power equipment based on canonical correlation analysis technology|"In the process of the construction of China's new power system, we will vigorously promote the research and development of UHV power equipment and the wide application of power electronic devices. UHV power equipment has complex insulation structure and huge volume, bear impact energy load of wind power and photovoltaic of new power system for a long time. It will cost a lot to carry out on-site operation and maintenance tests. Digital twin technology is becoming more and more perfect, and new power system construction is gradually introduced from automobile, aviation and other manufacturing industries. Based on this, this paper introduces the digital twin technology into the high-end power equipment of the new power system, and carries out on-site operation and maintenance simulation test and functional response analysis under high current, high voltage and multi harmonic loads according to its twin model. From the four sensing dimensions of mechanical vibration, gas composition, optical vision and electrical parameters, the improvement of intelligent sensing technology of new power system equipment is analyzed, and the interaction between on-site operating parameters and digital twin model data is realized. On the other hand, GPU computing power expansion technology supporting digital twin multi-source sensing technology is proposed, which can effectively support the dynamic behavior simulation monitoring of equipment from 10-5 seconds to 103 seconds, and the operation life evaluation strategy of high-end equipment is proposed. This paper focuses on the 3D construction of the digital twin model of the high-end equipment of the new power system, and its research method can be extended to the construction of the whole network digital twin model of the new power system. The research results can provide theoretical guidance and technical reference for the application of digital twin technology in high-end power equipment scenarios, and effectively support the safe and stable operation of the new power system with ""double high characteristics""."|Conference on Intelligent Computing and Human-Computer Interaction|2023|10.1117/12.2655868|Jianing Li, Shiling Zhang, Wei Song, Hanao Xia|0.0|2
966|Intrinsic Gaussian Process on Unknown Manifolds with Probabilistic Metrics|This article presents a novel approach to construct Intrinsic Gaussian Processes for regression on unknown manifolds with probabilistic metrics (GPUM) in point clouds. In many real world applications, one often encounters high dimensional data (e.g. point cloud data) centred around some lower dimensional unknown manifolds. The geometry of manifold is in general different from the usual Euclidean geometry. Naively applying traditional smoothing methods such as Euclidean Gaussian Processes (GPs) to manifold valued data and so ignoring the geometry of the space can potentially lead to highly misleading predictions and inferences. A manifold embedded in a high dimensional Euclidean space can be well described by a probabilistic mapping function and the corresponding latent space. We investigate the geometrical structure of the unknown manifolds using the Bayesian Gaussian Processes latent variable models(BGPLVM) and Riemannian geometry. The distribution of the metric tensor is learned using BGPLVM. The boundary of the resulting manifold is defined based on the uncertainty quantification of the mapping. We use the the probabilistic metric tensor to simulate Brownian Motion paths on the unknown manifold. The heat kernel is estimated as the transition density of Brownian Motion and used as the covariance functions of GPUM. The applications of GPUM are illustrated in the simulation studies on the Swiss roll, high dimensional real datasets of WiFi signals and image data examples. Its performance is compared with the Graph Laplacian GP, Graph Matern GP and Euclidean GP.|Journal of machine learning research|2023|10.48550/arXiv.2301.06533|Mu Niu, P. Cheung, Yizhu Wang, Zhenwen Dai|0.0|2
971|GPU libraries speed performance analysis for RCWA simulation matrix operations|Rigorous Coupled Wave Analysis (RCWA) method is highly efficient for the simulation of diffraction efficiency and field distribution patterns in periodic structures and textured optoelectronic devices. GPU has been increasingly used in complex scientific problems such as climate simulation and the latest Covid-19 spread model. In this paper, we break down the RCWA simulation problem to key computational steps (eigensystem solution, matrix inversion/multiplication) and investigate speed performance provided by optimized linear algebra GPU libraries in comparison to multithreaded Intel MKL CPU library running on IRIDIS 5 supercomputer (1 NVIDIA v100 GPU and 40 Intel Xeon Gold 6138 cores CPU). Our work shows that GPU outperforms CPU significantly for all required steps. Eigensystem solution becomes 60% faster, Matrix inversion improves with size achieving 8x faster for large matrixes. Most significantly, matrix multiplication becomes 40x faster for small and 5x faster for large matrix sizes.|OPTO|2023|10.1117/12.2650112|Jingxiao Xu, M. Charlton|0.0|2
975|GPU-Accelerated Sparse Matrix Vector Product based on Element-by-Element Method for Unstructured FEM using OpenACC|The development of directive based parallel programming models such as OpenACC has significantly reduced the cost in using accelerators such as GPUs. In this study, the sparse matrix vector product (SpMV), which was often the most computationally expensive part in physics-based simulations, was accelerated by GPU porting using OpenACC. Further speed-up was achieved by introducing the element-by-element (EBE) method in SpMV, an algorithm that is suitable for GPU architecture because it requires large amount of operations but small amount of memory access. In a comparison on one compute node of the supercomputer ABCI, using GPUs resulted in a 22- fold speedup over the CPU-only case, even when using the typical SpMV algorithm, and an additional 3.4-fold speedup when using the EBE method. The results on such analysis was applied to a seismic response analysis considering soil liquefaction, and using GPUs resulted in a 42-fold speedup compared to using only CPUs.|Workshop on Accelerator Programming using Directives|2022|10.1109/WACCPD56842.2022.00011|T. Ichimura, M. Hori, K. Fujita, M. Lalith, Ryota Kusakabe|0.0|2
977|Prospects for Ray-tracing Light Intensity and Polarization in Models of Accreting Compact Objects Using a GPU|The Event Horizon Telescope (EHT) has recently released high-resolution images of accretion flows onto two supermassive black holes. Our physical understanding of these images depends on the accuracy and precision of numerical models of plasma and radiation around compact objects. The goal of this work is to speed up radiative-transfer simulations used to create mock images of black holes for comparison with the EHT observations. A ray-tracing code for general relativistic and fully polarized radiative transfer through plasma in strong gravity is ported onto a graphics processing unit (GPU). We describe our GPU implementation and carry out speedup tests using models of optically thin advection-dominated accretion flow onto a black hole realized semianalytically and in 3D general relativistic magnetohydrodynamic simulations, low and very high image pixel resolutions, and two different sets of CPU+GPUs. We show that a GPU with high double precision computing capability can significantly reduce the image production computational time, with a speedup factor of up to approximately 1200. The significant speedup facilitates, e.g., dynamic model fitting to the EHT data, including polarimetric data. The method extension may enable studies of emission from plasma with nonthermal particle distribution functions for which accurate approximate synchrotron emissivities are not available. The significant speedup reduces the carbon footprint of the generation of the EHT image libraries by at least an order of magnitude.|Astrophysical Journal Supplement Series|2023|10.3847/1538-4365/acb6f9|M. Mościbrodzka, A. Yfantis|0.0|2
980|ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills|Generalizable manipulation skills, which can be composed to tackle long-horizon and complex daily chores, are one of the cornerstones of Embodied AI. However, existing benchmarks, mostly composed of a suite of simulatable environments, are insufficient to push cutting-edge research works because they lack object-level topological and geometric variations, are not based on fully dynamic simulation, or are short of native support for multiple types of manipulation tasks. To this end, we present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a regular workstation. It implements a render server infrastructure to allow sharing rendering resources across all environments, thereby significantly reducing memory usage. We open-source all codes of our benchmark (simulator, environments, and baselines) and host an online challenge open to interdisciplinary researchers.|International Conference on Learning Representations|2023|10.48550/arXiv.2302.04659|Xinyue Wei, Tongzhou Mu, Zhiao Huang, Xuanlin Li, Rui Chen, Xiao Yuan, Xiqiang Liu, P. Xie, Yihe Tang, Stone Tao, Z. Ling, Yuan Yao, Jiayuan Gu, Hao Su, Fanbo Xiang|0.0|2
988|FedML Parrot: A Scalable Federated Learning System via Heterogeneity-aware Scheduling on Sequential and Hierarchical Training|Federated Learning (FL) enables collaborations among clients for train machine learning models while protecting their data privacy. Existing FL simulation platforms that are designed from the perspectives of traditional distributed training, suffer from laborious code migration between simulation and production, low efficiency, low GPU utility, low scalability with high hardware requirements and difficulty of simulating stateful clients. In this work, we firstly demystify the challenges and bottlenecks of simulating FL, and design a new FL system named as FedML \texttt{Parrot}. It improves the training efficiency, remarkably relaxes the requirements on the hardware, and supports efficient large-scale FL experiments with stateful clients by: (1) sequential training clients on devices; (2) decomposing original aggregation into local and global aggregation on devices and server respectively; (3) scheduling tasks to mitigate straggler problems and enhance computing utility; (4) distributed client state manager to support various FL algorithms. Besides, built upon our generic APIs and communication interfaces, users can seamlessly transform the simulation into the real-world deployment without modifying codes. We evaluate \texttt{Parrot} through extensive experiments for training diverse models on various FL datasets to demonstrate that \texttt{Parrot} can achieve simulating over 1000 clients (stateful or stateless) with flexible GPU devices setting ($4 \sim 32$) and high GPU utility, 1.2 $\sim$ 4 times faster than FedScale, and 10 $\sim$ 100 times memory saving than FedML. And we verify that \texttt{Parrot} works well with homogeneous and heterogeneous devices in three different clusters. Two FL algorithms with stateful clients and four algorithms with stateless clients are simulated to verify the wide adaptability of \texttt{Parrot} to different algorithms.|arXiv.org|2023|10.48550/arXiv.2303.01778|Ryan Yide Ran, Chaoyang He, Zhenheng Tang, Yuxin Wang, Yonggang Zhang, Sunwoo Lee, A. Avestimehr, X. Chu, S. Shi, Alex Liang|0.0|2
989|FluidLab: A Differentiable Environment for Benchmarking Complex Fluid Manipulation|Humans manipulate various kinds of fluids in their everyday life: creating latte art, scooping floating objects from water, rolling an ice cream cone, etc. Using robots to augment or replace human labors in these daily settings remain as a challenging task due to the multifaceted complexities of fluids. Previous research in robotic fluid manipulation mostly consider fluids governed by an ideal, Newtonian model in simple task settings (e.g., pouring). However, the vast majority of real-world fluid systems manifest their complexities in terms of the fluid's complex material behaviors and multi-component interactions, both of which were well beyond the scope of the current literature. To evaluate robot learning algorithms on understanding and interacting with such complex fluid systems, a comprehensive virtual platform with versatile simulation capabilities and well-established tasks is needed. In this work, we introduce FluidLab, a simulation environment with a diverse set of manipulation tasks involving complex fluid dynamics. These tasks address interactions between solid and fluid as well as among multiple fluids. At the heart of our platform is a fully differentiable physics simulator, FluidEngine, providing GPU-accelerated simulations and gradient calculations for various material types and their couplings. We identify several challenges for fluid manipulation learning by evaluating a set of reinforcement learning and trajectory optimization methods on our platform. To address these challenges, we propose several domain-specific optimization schemes coupled with differentiable physics, which are empirically shown to be effective in tackling optimization problems featured by fluid system's non-convex and non-smooth properties. Furthermore, we demonstrate reasonable sim-to-real transfer by deploying optimized trajectories in real-world settings.|International Conference on Learning Representations|2023|10.48550/arXiv.2303.02346|H. Tung, A. Torralba, Zhou Xian, Zhenjia Xu, Bo Zhu, Chuang Gan, Katerina Fragkiadaki|0.0|2
992|Symmetric integration of the 1+1 Teukolsky equation on hyperboloidal foliations of Kerr spacetimes|This work outlines a fast, high-precision time-domain solver for scalar, electromagnetic and gravitational perturbations on hyperboloidal foliations of Kerr space-times. Time-domain Teukolsky equation solvers have typically used explicit methods, which numerically violate Noether symmetries and are Courant-limited. These restrictions can limit the performance of explicit schemes when simulating long-time extreme mass ratio inspirals, expected to appear in LISA band for 2-5 years. We thus explore symmetric (exponential, Pad\'e or Hermite) integrators, which are unconditionally stable and known to preserve certain Noether symmetries and phase-space volume. For linear hyperbolic equations, these implicit integrators can be cast in explicit form, making them well-suited for long-time evolution of black hole perturbations. The 1+1 modal Teukolsky equation is discretized in space using polynomial collocation methods and reduced to a linear system of ordinary differential equations, coupled via mode-coupling arrays and discretized (matrix) differential operators. We use a matricization technique to cast the mode-coupled system in a form amenable to a method-of-lines framework, which simplifies numerical implementation and enables efficient parallelization on CPU and GPU architectures. We test our numerical code by studying late-time tails of Kerr spacetime perturbations in the sub-extremal and extremal cases.|Social Science Research Network|2023|10.48550/arXiv.2303.08153|Anil Zenginoglu, C. Markakis, Sean Bray|0.0|2
993|Allegro-Legato: Scalable, Fast, and Robust Neural-Network Quantum Molecular Dynamics via Sharpness-Aware Minimization|"Neural-network quantum molecular dynamics (NNQMD) simulations based on machine learning are revolutionizing atomistic simulations of materials by providing quantum-mechanical accuracy but orders-of-magnitude faster, illustrated by ACM Gordon Bell prize (2020) and finalist (2021). State-of-the-art (SOTA) NNQMD model founded on group theory featuring rotational equivariance and local descriptors has provided much higher accuracy and speed than those models, thus named Allegro (meaning fast). On massively parallel supercomputers, however, it suffers a fidelity-scaling problem, where growing number of unphysical predictions of interatomic forces prohibits simulations involving larger numbers of atoms for longer times. Here, we solve this problem by combining the Allegro model with sharpness aware minimization (SAM) for enhancing the robustness of model through improved smoothness of the loss landscape. The resulting Allegro-Legato (meaning fast and""smooth"") model was shown to elongate the time-to-failure $t_\textrm{failure}$, without sacrificing computational speed or accuracy. Specifically, Allegro-Legato exhibits much weaker dependence of timei-to-failure on the problem size, $t_{\textrm{failure}} \propto N^{-0.14}$ ($N$ is the number of atoms) compared to the SOTA Allegro model $\left(t_{\textrm{failure}} \propto N^{-0.29}\right)$, i.e., systematically delayed time-to-failure, thus allowing much larger and longer NNQMD simulations without failure. The model also exhibits excellent computational scalability and GPU acceleration on the Polaris supercomputer at Argonne Leadership Computing Facility. Such scalable, accurate, fast and robust NNQMD models will likely find broad applications in NNQMD simulations on emerging exaflop/s computers, with a specific example of accounting for nuclear quantum effects in the dynamics of ammonia."|Information Security Conference|2023|10.48550/arXiv.2303.08169|K. Nomura, P. Vashishta, Shinnosuke Hattori, Hikaru Ibayashi, A. Nakano, Taufeq Mohammed Razakh, R. Kalia, M. Olguin, Liqiu Yang, Thomas M Linker, Ye Luo|0.0|2
996|Acceleration of Bi-Objective Optimization of Data-Parallel Applications for Performance and Energy on Heterogeneous Hybrid Platforms|"Accelerating the bi-objective optimization of applications for performance and energy is crucial to achieving energy efficiency objectives and meeting quality-of-service requirements in modern high-performance computing platforms and cloud computing infrastructures. In this work, we highlight the crucial challenges to accelerate model-based methods proposed for the bi-objective optimization of data-parallel applications for performance and energy that employ workload distribution between the executing processors as the decision variable. The methods solve unconstrained bi-objective optimization problems and take input, the processors’ performance and energy profiles in the form of discrete functions of workload size, and output Pareto-optimal solutions (workload distributions), minimizing the execution time and the total energy consumption of computations during the parallel execution of the application. One of the challenges is the fast computation of Pareto-optimal solutions. We then formulate the bi-objective optimization problem of data-parallel applications for performance and energy through workload distribution on a cluster of <inline-formula> <tex-math notation=""LaTeX"">$p$ </tex-math></inline-formula> identical hybrid nodes, each containing <inline-formula> <tex-math notation=""LaTeX"">$h$ </tex-math></inline-formula> heterogeneous processors. The state-of-the-art algorithm for solving the problem is sequential and takes exorbitant execution times to find Pareto-optimal solutions for even moderate numbers of processors. We propose two algorithms that address this shortcoming. The first algorithm is an exact sequential algorithm that is more efficient and amenable to parallelization and achieves a complexity reduction of <inline-formula> <tex-math notation=""LaTeX"">$\mathcal {O}(m \times h)$ </tex-math></inline-formula> over the state-of-the-art sequential algorithm where <inline-formula> <tex-math notation=""LaTeX"">$m$ </tex-math></inline-formula> is the cardinality of the input discrete execution time and dynamic energy functions. The second algorithm is a parallel algorithm executed by <inline-formula> <tex-math notation=""LaTeX"">$q$ </tex-math></inline-formula> identical parallel processes that reduces the complexity of our proposed sequential algorithm by <inline-formula> <tex-math notation=""LaTeX"">$\mathcal {O}(q)$ </tex-math></inline-formula>. It, therefore, achieves a complexity reduction of <inline-formula> <tex-math notation=""LaTeX"">$\mathcal {O}(m \times h \times q)$ </tex-math></inline-formula> over the state-of-the-art sequential algorithm. Finally, we experimentally analyze the practical efficacy of our proposed algorithms for two data-parallel applications, matrix multiplication and fast Fourier transform, on a heterogeneous hybrid node containing an Intel Haswell multicore CPU, an Nvidia k40c GPU, and an Nvidia P100 GPU and simulations of clusters of such hybrid nodes. The experiments demonstrate that our proposed algorithms provide tremendous speedups over state-of-the-art solutions."|IEEE Access|2023|10.1109/ACCESS.2023.3258684|Alexey L. Lastovetsky, Ravi Reddy Manumachu, Hamidreza Khaleghzadeh|0.0|2
998|Efficient and scalable hybrid fluid-particle simulations with geometrically resolved particles on heterogeneous CPU-GPU architectures|In recent years, it has become increasingly popular to accelerate numerical simulations using Graphics Processing Unit (GPU)s. In multiphysics simulations, the various combined methodologies may have distinctly different computational characteristics. Therefore, the best-suited hardware architecture can differ between the simulation components. Furthermore, not all coupled software frameworks may support all hardware. These issues predestinate or even force hybrid implementations, i.e., different simulation components running on different hardware. We introduce a hybrid coupled fluid-particle implementation with geometrically resolved particles. The simulation utilizes GPUs for the fluid dynamics, whereas the particle simulation runs on Central Processing Unit (CPU)s. We examine the performance of two contrasting cases of a fluidized bed simulation on a heterogeneous supercomputer. The hybrid overhead (i.e., the CPU-GPU communication) is negligible. The fluid simulation shows good performance utilizing nearly the entire memory bandwidth. Still, the GPU run time accounts for most of the total time. The parallel efficiency in a weak scaling benchmark for 1024 A100 GPUs is up to 71%. Frequent CPU-CPU communications occurring in the particle simulation are the leading cause of the decrease in parallel efficiency. The results show that hybrid implementations are promising for large-scale multiphysics simulations on heterogeneous supercomputers.|arXiv.org|2023|10.48550/arXiv.2303.11811|Samuel Kemmler, H. Köstler, C. Rettinger|0.0|2
999|Medical diffusion on a budget: textual inversion for medical image generation|Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible to perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access to large datasets and significant computational resources. In the case of medical image generation, the availability of large, publicly accessible datasets that include text reports is limited due to legal and ethical concerns. While training a diffusion model on a private dataset may address this issue, it is not always feasible for institutions lacking the necessary computational resources. This work demonstrates that pre-trained Stable Diffusion models, originally trained on natural images, can be adapted to various medical imaging modalities by training text embeddings with textual inversion. In this study, we conducted experiments using medical datasets comprising only 100 samples from three medical modalities. Embeddings were trained in a matter of hours, while still retaining diagnostic relevance in image generation. Experiments were designed to achieve several objectives. Firstly, we fine-tuned the training and inference processes of textual inversion, revealing that larger embeddings and more examples are required. Secondly, we validated our approach by demonstrating a 2\% increase in the diagnostic accuracy (AUC) for detecting prostate cancer on MRI, which is a challenging multi-modal imaging modality, from 0.78 to 0.80. Thirdly, we performed simulations by interpolating between healthy and diseased states, combining multiple pathologies, and inpainting to show embedding flexibility and control of disease appearance. Finally, the embeddings trained in this study are small (less than 1 MB), which facilitates easy sharing of medical data with reduced privacy concerns.|arXiv.org|2023|10.48550/arXiv.2303.13430|R. T. Broek, B. D. Wilde, H. Huisman, A. Saha|0.0|2
1000|4K-HAZE: A Dehazing Benchmark with 4K Resolution Hazy and Haze-Free Images|Currently, mobile and IoT devices are in dire need of a series of methods to enhance 4K images with limited resource expenditure. The absence of large-scale 4K benchmark datasets hampers progress in this area, especially for dehazing. The challenges in building ultra-high-definition (UHD) dehazing datasets are the absence of estimation methods for UHD depth maps, high-quality 4K depth estimation datasets, and migration strategies for UHD haze images from synthetic to real domains. To address these problems, we develop a novel synthetic method to simulate 4K hazy images (including nighttime and daytime scenes) from clear images, which first estimates the scene depth, simulates the light rays and object reflectance, then migrates the synthetic images to real domains by using a GAN, and finally yields the hazy effects on 4K resolution images. We wrap these synthesized images into a benchmark called the 4K-HAZE dataset. Specifically, we design the CS-Mixer (an MLP-based model that integrates \textbf{C}hannel domain and \textbf{S}patial domain) to estimate the depth map of 4K clear images, the GU-Net to migrate a 4K synthetic image to the real hazy domain. The most appealing aspect of our approach (depth estimation and domain migration) is the capability to run a 4K image on a single GPU with 24G RAM in real-time (33fps). Additionally, this work presents an objective assessment of several state-of-the-art single-image dehazing methods that are evaluated using the 4K-HAZE dataset. At the end of the paper, we discuss the limitations of the 4K-HAZE dataset and its social implications.|arXiv.org|2023|10.48550/arXiv.2303.15848|Zhuoran Zheng, Xiuyi Jia|0.0|2
1002|GPU Parallelization and Optimization of a Combustion Simulation Application|Graphics processing units (GPUs) are widely used in the area of scientific computing. While GPUs provide much higher peak performance, efficient implementation of real applications on the GPU architectures is still a non-trivial task. It is crucial to realize efficient solution algorithms that can better utilize GPU architectures. This paper presents our efforts in parallelizing and optimizing LESAP, a CFD application for scramjet combustion simulation, on NVIDIA GPUs. The GPU parallelization is realized based on the CUDA programming model, with a data-parallel implicit time-marching method that is efficient on the GPU architecture. Furthermore, shared memory and redundant calculation are proposed to reduce memory access overhead during GPU computation, and data transfer between CPU and GPU is optimized by packing the data to be transferred. The experimental results show that the GPU version, when runs on four V100 GPUs, achieves a speedup of 11.26 times compared to the CPU version that runs on two 24-core Intel Skylake Gold 6240R CPUs. Excellent parallel scalability across multiple GPUs is also observed.|2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)|2022|10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00043|Zhixiang Liao, Yongzhou Liu, Yonggang Che|0.0|2
1003|Pair Programming with Large Language Models for Sampling and Estimation of Copulas|Without writing a single line of code by a human, an example Monte Carlo simulation based application for stochastic dependence modeling with copulas is developed using a state-of-the-art large language model (LLM) fine-tuned for conversations. This includes interaction with ChatGPT in natural language and using mathematical formalism, which, under careful supervision by a human-expert, led to producing a working code in MATLAB, Python and R for sampling from a given copula model, evaluation of the model's density, performing maximum likelihood estimation, optimizing the code for parallel computing for CPUs as well as for GPUs, and visualization of the computed results. In contrast to other emerging studies that assess the accuracy of LLMs like ChatGPT on tasks from a selected area, this work rather investigates ways how to achieve a successful solution of a standard statistical task in a collaboration of a human-expert and artificial intelligence (AI). Particularly, through careful prompt engineering, we separate successful solutions generated by ChatGPT from unsuccessful ones, resulting in a comprehensive list of related pros and cons. It is demonstrated that if the typical pitfalls are avoided, we can substantially benefit from collaborating with an AI partner. For example, we show that if ChatGPT is not able to provide a correct solution due to a lack of or incorrect knowledge, the human-expert can feed it with the correct knowledge, e.g., in the form of mathematical theorems and formulas, and make it to apply the gained knowledge in order to provide a solution that is correct. Such ability presents an attractive opportunity to achieve a programmed solution even for users with rather limited knowledge of programming techniques.|arXiv.org|2023|10.48550/arXiv.2303.18116|Jan G'orecki|0.0|2
1004|Portable Programming Model Exploration for LArTPC Simulation in a Heterogeneous Computing Environment: OpenMP vs. SYCL|The evolution of the computing landscape has resulted in the proliferation of diverse hardware architectures, with different flavors of GPUs and other compute accelerators becoming more widely available. To facilitate the efficient use of these architectures in a heterogeneous computing environment, several programming models are available to enable portability and performance across different computing systems, such as Kokkos, SYCL, OpenMP and others. As part of the High Energy Physics Center for Computational Excellence (HEP-CCE) project, we investigate if and how these different programming models may be suitable for experimental HEP workflows through a few representative use cases. One of such use cases is the Liquid Argon Time Projection Chamber (LArTPC) simulation which is essential for LArTPC detector design, validation and data analysis. Following up on our previous investigations of using Kokkos to port LArTPC simulation in the Wire-Cell Toolkit (WCT) to GPUs, we have explored OpenMP and SYCL as potential portable programming models for WCT, with the goal to make diverse computing resources accessible to the LArTPC simulations. In this work, we describe how we utilize relevant features of OpenMP and SYCL for the LArTPC simulation module in WCT. We also show performance benchmark results on multi-core CPUs, NVIDIA and AMD GPUs for both the OpenMP and the SYCL implementations. Comparisons with different compilers will also be given where appropriate.|arXiv.org|2023|10.48550/arXiv.2304.01841|B. Viren, Meifeng Lin, Haiwang Yu, M. Atif, Meghna Battacharya, Zhihua Dong, Tianle Wang, K. Knoepfel, C. Leggett|0.0|2
1005|A Bayesian method to infer copy number clones from single-cell RNA and ATAC sequencing|Single-cell RNA and ATAC sequencing technologies allow one to probe expression and chromatin accessibility states as a proxy for cellular phenotypes at the resolution of individual cells. A key challenge of cancer research is to consistently map such states on genetic clones, within an evolutionary framework. To this end we introduce CONGAS+, a Bayesian model to map single-cell RNA and ATAC profiles generated from independent or multimodal assays on the latent space of copy numbers clones. CONGAS+ can detect tumour subclones associated with aneuploidy by clustering cells with the same ploidy profile. The framework is implemented in a probabilistic language that can scale to analyse thousands of cells thanks to GPU deployment. Our tool exhibits robust performance on simulations and real data, highlighting the advantage of detecting aneuploidy from two distinct molecules as opposed to other single-molecule models, and also leveraging real multi-omic data. In the application to prostate cancer, lymphoma and basal cell carcinoma, CONGAS+ did retrieve complex subclonal architectures while providing a coherent mapping among ATAC and RNA, facilitating the study of genotype-phenotype mapping, and their relation to tumour aneuploidy. Author summary Aneuploidy is a condition caused by copy number alterations (CNAs), which brings cells to acquire or lose chromosomes. In the context of cancer progression and treatment response, aneuploidy is a key factor driving cancer clonal dynamics, and measuring CNAs from modern sequencing assays is therefore important. In this framing, we approach this problem from new single-cell assays that measure both chromatin accessibility and RNA transcripts. We model the relation between single-cell data and CNAs and, thanks to a sophisticated Bayesian model, we are capable of determining tumour clones from clusters of cells with the same copy numbers. Our model works when input cells are sequenced independently for both assays, or even when modern multi-omics protocols are used. By linking aneuploidy to gene expression and chromatin conformation, our new approach provides a novel way to map complex genotypes with phenotype-level information, one of the missing factors to understand the molecular basis of cancer heterogeneity.|bioRxiv|2023|10.1101/2023.04.01.535197|M. Antoniotti, Lucrezia Patruno, R. Bergamin, Salvatore Milite, N. Calonaci, G. Caravagna, Fabio Anselmi, Alex Graudenzi, Alberto D’Onofrio|0.0|2
1008|Integrating Per-Stream Stat Tracking into Accel-Sim|Accel-Sim is a widely used computer architecture simulator that models the behavior of modern NVIDIA GPUs in great detail. However, although Accel-Sim and the underlying GPGPU-Sim model many of the features of real GPUs, thus far it has not been able to track statistics separately per stream. Instead, Accel-Sim combines statistics (e.g., cycles and cache hits/misses) across all simultaneously running streams. This can prevent users from properly identifying the behavior of specific kernels and streams and potentially lead to incorrect conclusions. Thus, in this work we extend Accel-Sim's and GPGPU-Sim's statistic tracking support to track per-stream statistics. To validate this support, we designed a series of multi-stream microbenchmarks and checked their reported per-kernel, per-stream counts.|arXiv.org|2023|10.48550/arXiv.2304.11136|Shichen Qiao, Matthew D. Sinclair, Xin Su|0.0|2
1012|Stable and low-precision training for large-scale vision-language models|We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) For stability, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.|arXiv.org|2023|10.48550/arXiv.2304.13013|Luke Zettlemoyer, Mitchell Wortsman, Ari S. Morcos, Tim Dettmers, Ali Farhadi, Ludwig Schmidt|0.0|2
1016|High‐performance graphics processing unit‐based strategy for tuning a unmanned aerial vehicle controller subject to time‐delay constraints|Recently, high‐performance computing strategies have been implemented to improve performance analysis and reduce the development time of new solutions in robotic applications, such as path planning, machine learning, and vision, which require massive matrix computations. In this sense, this work aims to study the aerial robots' behavior during their mission execution. Due to the large search space in the set of parameter combinations and the high computational cost required to perform such an analysis after sequentially executing thousands of simulations, this work proposes an open‐source graphics processing unit (GPU)‐based implementation to simulate the robot behavior. A GPU‐accelerated flight route analysis for multi‐unmanned aerial vehicle (UAV) systems is proposed for the tuning control problem in the parameters‘ space considering the problem of delay in sending information to a ground control station. Considering our implementation, the experimental results show a speedup up to 325 ×$$ \times $$ , 629 ×$$ \times $$ , and 5959 ×$$ \times $$ in comparison to the parallel version with 16 threads, C coder converter, and native Matlab code, respectively. The implementation is available in the Colab Google platform and it can easily be expanded for analyses involving larger amounts of different parameters, robot models, strategies, and controllers.|Concurrency and Computation|2023|10.1002/cpe.7767|R. Ferreira, Michael Canesche, Leonardo Fagundes-Junior, A. Brandão|0.0|2
1020|CMSSW Scaling Limits on Many-Core Machines|Today the LHC offline computing relies heavily on CPU resources, despite the interest in compute accelerators, such as GPUs, for the longer term future. The number of cores per CPU socket has continued to increase steadily, reaching the levels of 64 cores (128 threads) with recent AMD EPYC processors, and 128 cores on Ampere Altra Max ARM processors. Over the course of the past decade, the CMS data processing framework, CMSSW, has been transformed from a single-threaded framework into a highly concurrent one. The first multithreaded version was brought into production by the start of the LHC Run 2 in 2015. Since then, the framework's threading efficiency has gradually been improved by adding more levels of concurrency and reducing the amount of serial code paths. The latest addition was support for concurrent Runs. In this work we review the concurrency model of the CMSSW, and measure its scalability with real CMS applications, such as simulation and reconstruction, on mode rn many-core machines. We show metrics such as event processing throughput and application memory usage with and without the contribution of I/O, as I/O has been the major scaling limitation for the CMS applications.|CMSSW Scaling Limits on Many-Core Machines|2023|10.2172/1973605|Chris Jones, P. Gartung|0.0|2
1021|Learning Solutions in Large Economic Networks using Deep Multi-Agent Reinforcement Learning|Real-world economies can be modeled as a network with many heterogeneous and strategic agents. In this setting, it is very challenging to find optimal mechanisms, e.g., taxes, 1) when taking strategic best responses into account and 2) even when using re-strictive assumptions, e.g., that supply always meets demand. Deep multi-agent reinforcement learning (MARL) is a natural framework to learn mechanisms and model strategic best responses, but independent MARL often collapses to trivial solutions (e.g., where nobody works) as joint exploration severely distorts rewards and constraints. Here, we show how to use structured learning curricula and GPU-accelerated simulations to find non-trivial solutions in networks with many heterogeneous agents. We validate our approach in models with 100 worker-consumers, 10 firms, and a social planner who taxes and redistributes. We use empirical best-response analyses across agent types to show that it is difficult for agents to benefit by deviating from the learned solutions. In particular, we find income and corporate taxes that achieve 15% higher social welfare compared to baselines.|Adaptive Agents and Multi-Agent Systems|2023|10.5555/3545946.3599069|Michael J. Curry, Alexander R. Trott, Yunru Bai, Soham R. Phade, Stephan Zheng|0.0|2
1022|TornadoQSim: An Open-source High-Performance and Modular Quantum Circuit Simulation Framework|In this article, we present TornadoQSim, an open-source quantum circuit simulation framework implemented in Java. The proposed framework has been designed to be modular and easily expandable for accommodating different user-defined simulation backends, such as the unitary matrix simulation technique. Furthermore, TornadoQSim features the ability to interchange simulation backends that can simulate arbitrary quantum circuits. Another novel aspect of TornadoQSim over other quantum simulators is the transparent hardware acceleration of the simulation backends on heterogeneous devices. TornadoQSim employs TornadoVM to automatically compile parts of the simulation backends onto heterogeneous hardware, thereby addressing the fragmentation in development due to the low-level heterogeneous programming models. The evaluation of TornadoQSim has shown that the transparent utilization of GPU hardware can result in up to 506.5$x$ performance speedup when compared to the vanilla Java code for a fully entangled quantum circuit of 11 qubits. Other evaluated quantum algorithms have been the Deutsch-Jozsa algorithm (493.10$x$ speedup for a 11-qubit circuit) and the quantum Fourier transform algorithm (518.12$x$ speedup for a 11-qubit circuit). Finally, the best TornadoQSim implementation of unitary matrix has been evaluated against a semantically equivalent simulation via Qiskit. The comparative evaluation has shown that the simulation with TornadoQSim is faster for small circuits, while for large circuits Qiskit outperforms TornadoQSim by an order of magnitude.|arXiv.org|2023|10.48550/arXiv.2305.14398|J. Fumero, Christos Kotselidis, N. Foutris, Aleš Kubíček, Athanasios Stratikopoulos|0.0|2
1024|Study on the GPU-driven mesh generation algorithm for machining simulation|Machining simulation is often used to verify the rationality of NC (Numerical Control) machining toolpath and is the core function of CAM software. In order to improve computational efficiency, machining simulation algorithm is usually designed based on the discrete geometric model of workpiece, which is widely used in engineering. With the discrete geometric model as input, machining simulation algorithm is difficult to generate real-time changing 3D rendering visualization for workpiece. This paper proposed a GPU-driven mesh generation method for machining simulation. This method improves the traditional conversion method based on Marching Cube algorithm and uses the parallel computing capability of GPU to improve the efficiency of mesh generation. The method proposed in this paper supports real-time user interaction and may also be applied to virtual engraving modeling and digital twin for NC machining.|Other Conferences|2023|10.1117/12.2681295|Gang Zhao, Xian Cao, W. Xiao, Lianyu Zheng|0.0|2
1027|Automating GPU Scalability for Complex Scientific Models: Phonon Boltzman Transport Equation|Heterogeneous computing environments combining CPU and GPU resources provide a great boost to large-scale scientific computing applications. Code generation utilities that partition the work into CPU and GPU tasks while considering data movement costs allow researchers to more quickly and easily develop high-performance solutions, and make these resources accessible to a larger user base. We present developments for a domain-specific language (DSL) and code generation framework for solving partial differential equations (PDEs). These enhancements facilitate GPU-accelerated solution of the Boltzmann transport equation (BTE) for phonons, which is the governing equation for simulating thermal transport in semiconductor materials at sub-micron scales. The solution of the BTE involves thousands of coupled PDEs as well as complicated boundary conditions and nonlinear processing at each time step. These developments enable the DSL to generate configurable hybrid GPU/CPU code that couples accelerated kernels with user-defined code. We observed performance improvements of around 18X compared to a CPU-only version produced by this same DSL with minimal additional programming effort.|arXiv.org|2023|10.48550/arXiv.2305.19400|S. Mazumder, Aadesh Deshmukh, Siddharth Saurav, H. Sundar, E. Heisler, P. Sadayappan|0.0|2
1028|Parallel Directives Evaluation in Porous Media Application: A Case Study|High-performance computing provides the acceleration of scientific applications through the use of parallelism. Applications of this type usually demand a lot of computation time for a version with a single code execution stream. The adoption of different models of parallel programming enables the development of concurrent code. In this sense, this paper evaluates parallel interfaces and their programming models. Therefore, as a case study, we evaluate a porous media application that simulates grain drying using OpenMP (loop, sections, tasks, target, and teams approach) and OpenACC programming interfaces. The results show a reduction in processing time in all test cases. The total parallel simulation time for a multicore architecture using 16 physical cores was 5.61 times less using loops, 5.96 using targets, and 7.50 using teams. Task and section directives produce around 1.20 speedup due to the limitations of concurrent task executions of the application. The reduction using a single GPU was 7.54. We also contribute with some collected traces, identifying the parallel steps and synchronization time.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2023|10.1109/PDP59025.2023.00050|C. Schepke, Gabriel Tremarin, Natiele Lucca|0.0|2
1031|StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views|We present a novel multi-view implicit surface reconstruction technique, termed StreetSurf, that is readily applicable to street view images in widely-used autonomous driving datasets, such as Waymo-perception sequences, without necessarily requiring LiDAR data. As neural rendering research expands rapidly, its integration into street views has started to draw interests. Existing approaches on street views either mainly focus on novel view synthesis with little exploration of the scene geometry, or rely heavily on dense LiDAR data when investigating reconstruction. Neither of them investigates multi-view implicit surface reconstruction, especially under settings without LiDAR data. Our method extends prior object-centric neural surface reconstruction techniques to address the unique challenges posed by the unbounded street views that are captured with non-object-centric, long and narrow camera trajectories. We delimit the unbounded space into three parts, close-range, distant-view and sky, with aligned cuboid boundaries, and adapt cuboid/hyper-cuboid hash-grids along with road-surface initialization scheme for finer and disentangled representation. To further address the geometric errors arising from textureless regions and insufficient viewing angles, we adopt geometric priors that are estimated using general purpose monocular models. Coupled with our implementation of efficient and fine-grained multi-stage ray marching strategy, we achieve state of the art reconstruction quality in both geometry and appearance within only one to two hours of training time with a single RTX3090 GPU for each street view sequence. Furthermore, we demonstrate that the reconstructed implicit surfaces have rich potential for various downstream tasks, including ray tracing and LiDAR simulation.|arXiv.org|2023|10.48550/arXiv.2306.04988|Yikang Li, Chiyu Wang, Yeqi Bai, Xinyang Li, Jianfei Guo, Chenjing Ding, Nianchen Deng, Botian Shi, Dongliang Wang|0.0|2
1034|Efficient parallelization strategy for real-time FE simulations|This paper introduces an efficient and generic framework for finite-element simulations under an implicit time integration scheme. Being compatible with generic constitutive models, a fast matrix assembly method exploits the fact that system matrices are created in a deterministic way as long as the mesh topology remains constant. Using the sparsity pattern of the assembled system brings about significant optimizations on the assembly stage. As a result, developed techniques of GPU-based parallelization can be directly applied with the assembled system. Moreover, an asynchronous Cholesky precondition scheme is used to improve the convergence of the system solver. On this basis, a GPU-based Cholesky preconditioner is developed, significantly reducing the data transfer between the CPU/GPU during the solving stage. We evaluate the performance of our method with different mesh elements and hyperelastic models and compare it with typical approaches on the CPU and the GPU.|arXiv.org|2023|10.48550/arXiv.2306.05893|Z. Zeng, H. Courtecuisse|0.0|2
1035|Towards a Machine-Learned Poisson Solver for Low-Temperature Plasma Simulations in Complex Geometries|Poisson's equation plays an important role in modeling many physical systems. In electrostatic self-consistent low-temperature plasma (LTP) simulations, Poisson's equation is solved at each simulation time step, which can amount to a significant computational cost for the entire simulation. In this paper, we describe the development of a generic machine-learned Poisson solver specifically designed for the requirements of LTP simulations in complex 2D reactor geometries on structured Cartesian grids. Here, the reactor geometries can consist of inner electrodes and dielectric materials as often found in LTP simulations. The approach leverages a hybrid CNN-transformer network architecture in combination with a weighted multiterm loss function. We train the network using highly-randomized synthetic data to ensure the generalizability of the learned solver to unseen reactor geometries. The results demonstrate that the learned solver is able to produce quantitatively and qualitatively accurate solutions. Furthermore, it generalizes well on new reactor geometries such as reference geometries found in the literature. To increase the numerical accuracy of the solutions required in LTP simulations, we employ a conventional iterative solver to refine the raw predictions, especially to recover the high-frequency features not resolved by the initial prediction. With this, the proposed learned Poisson solver provides the required accuracy and is potentially faster than a pure GPU-based conventional iterative solver. This opens up new possibilities for developing a generic and high-performing learned Poisson solver for LTP systems in complex geometries.|arXiv.org|2023|10.48550/arXiv.2306.07604|M. Becker, K. Weltmann, Ihda Chaerony Siffa, J. Trieschmann|0.0|2
1040|Improved Water Sound Synthesis using Coupled Bubbles|We introduce a practical framework for synthesizing bubble-based water sounds that captures the rich inter-bubble coupling effects responsible for low-frequency acoustic emissions from bubble clouds. We propose coupled-bubble oscillator models with regularized singularities, and techniques to reduce the computational cost of time stepping with dense, time-varying mass matrices. Airborne acoustic emissions are estimated using finite-difference time-domain (FDTD) methods. We propose a simple, analytical surface-acceleration model, and a sample-and-hold GPU wavesolver that is simple and faster than prior CPU wavesolvers. Sound synthesis results are demonstrated using bubbly flows from incompressible, two-phase simulations, as well as procedurally generated examples using single-phase FLIP fluid animations. Our results demonstrate sound simulations with hundreds of thousands of bubbles, and perceptually significant frequency transformations with fuller low-frequency content.|ACM Transactions on Graphics|2023|10.1145/3592424|Kangrui Xue, Jui-Hsien Wang, Doug L. James, Ryan M. Aronson, Timothy R. Langlois|0.0|2
1043|OpenMP Offloading and OpenACC Programming Model Approach for Object-Oriented Plasma Device Algorithms|Plasma physics is becoming more important due to its applications in clean energy production (using fusion technology) and other fields, such as chemical and material science. Even recently, Lawrence Livermore National Laboratory (LLNL) has demonstrated the capability of producing more energy through fusion compared to laser energy. Therefore, in the future, we might need to do more computational simulations for further understanding and explore the advancement in plasma physics. Furthermore, this could be possible with the help of supercomputers. In this work, we parallelise a one-dimensional object-oriented plasma device algorithm, Object Oriented Plasma Device 1d (oopd1), on a multicore CPU and GPU. We use the OpenMP programming model for the CPU version, and for the GPU, we use OpenMP offloading and OpenACC offloading. All of these approaches are compared to each other. Thus, it provides further suitable programming models with parallel capabilities for the existing oopd1 to explore the available parallel architectures.|International Convention on Information and Communication Technology, Electronics and Microelectronics|2023|10.23919/MIPRO57284.2023.10159738|E. Krishnasamy, L. Kos, P. Bouvry, I. Vasileska|0.0|2
1044|High-throughput Simulation of Federated Learning via Resource-Aware Client Placement|Federated Learning (FL) is the privacy-preserving machine learning paradigm which collaboratively trains a model across millions of devices. Simulated environments are fundamental to large-scale FL research, allowing researchers to quickly test new ideas to solve system and statistical heterogeneity issues. This work proposes \emph{Pollen}, a novel resource-aware system capable of speeding up FL simulations by efficiently placing clients across distributed and heterogeneous hardware. We propose minimising server-GPU communication and using an efficient client placement policy based on the inherent trade-offs of FL client placement on heterogeneous GPUs. These trade-offs are explored experimentally. This exploration has been conducted via relevant baselines on three popular FL tasks: image classification, speech recognition and text generation. We compare \emph{Pollen} to existing ad-hoc FL frameworks, such as Flower, Flute and FedScale, and show performance gains of $50\%$ to $400\%$.|arXiv.org|2023|10.48550/arXiv.2306.17453|Wanru Zhao, Pedro Gusmão, Alexandru Iacob, Lorenzo Sani, J. Fernández-Marqués, Yan Gao, Xinchi Qiu, N. Lane|0.0|2
1046|Carbon-Efficient Neural Architecture Search|This work presents a novel approach to neural architecture search (NAS) that aims to reduce energy costs and increase carbon efficiency during the model design process. The proposed framework, called carbon-efficient NAS (CE-NAS), consists of NAS evaluation algorithms with different energy requirements, a multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS dynamically balances energy-efficient sampling and energy-consuming evaluation tasks based on current carbon emissions. Using a recent NAS benchmark dataset and two carbon traces, our trace-driven simulations demonstrate that CE-NAS achieves better carbon and search efficiency than the three baselines.|HotCarbon|2023|10.1145/3604930.3605708|Yiyang Zhao, Tian Guo|0.0|2
1048|Teaching optics with LightFlow: an intuitive framework for light propagation simulations|We introduce LightFlow, an open-source software package for simulating light wave propagation through custom optical components and systems. Built upon TensorFlow and Keras, it benefits from GPU acceleration and offers a user-friendly and modular architecture. Optical components are represented as layers, simplifying the design and modification of simulation models. Our approach also streamlines the addition of new custom components. LightFlow’s automatic gradient calculation is valuable for computational imaging applications involving optimization algorithms and inverse problems. With its intuitive interface, tested building blocks, and expandable design, LightFlow is well-suited for education and research, from undergraduate to advanced graduate levels. The GPU-accelerated processing enables efficient, real-time visualization of optical simulations, making LightFlow valuable across a broad range of user expertise and applications.|International Topical Meeting on Education and Training in Optics and Photonics|2023|10.1117/12.2672562|Nicolas C. Pégard, Vincent R. Curtis, Changjia Cai, M. H. Eybposh|0.0|2
1051|Accelerating 3-D Acoustic Full Waveform Inversion Using a Multi-GPU Cluster|Improving the computational efficiency of 3-D full waveform inversion (FWI) is a challenging task in seismic imaging. Using a multi-GPU cluster with an acceleration strategy to simulate wave propagation is an important means to improve its efficiency. We propose a multi-GPU acceleration 3-D acoustic FWI algorithm based on the finite-difference method in the time-domain (FDTD) method in this article. We improved the parallelism of the 3-D wavefield simulation algorithm based on a single GPU using a sliding 2-D thread block algorithm with three different 2-D shared memory stencils. For the multinode implementation, we achieved bidirectional parallel data transfer between GPUs and used multiple kernels to further overlap the calculation and transfer. Numerical tests verify the validity of our 3-D FWI algorithm accelerated with multi-GPU. The strategies used in our algorithm can significantly bring improvement in most cases. And the improvement is strongly related to the model size and the number of GPUs used. In our test, we achieve an acceleration of up to 19% in forward simulation and 25% in gradient calculation, compared with a typical multi-GPU implementation.|IEEE Transactions on Geoscience and Remote Sensing|2023|10.1109/TGRS.2023.3295377|Peimin Zhu, W. Wen, Jinpeng Jiang, Yanling Chen|0.0|2
1053|Web Programming Using the WebGPU API|Today's web-based programming environments has become more multifaceted for accomplishing tasks that go beyond 'browsing' web-pages. The process of developing efficient web-based programs for such a wide array of applications poses a number of challenges to the programming community. Applications possess a number of workload behaviors, ranging from control intensive (e.g., searching, sorting, and parsing) to data intensive (e.g., image processing, simulation and modeling, and data mining). Web-based applications can also be characterized as compute intensive (e.g., iterative methods, numerical methods, and financial modeling), where the overall throughput of the web application is heavily dependent on the computational efficiency of the underlying hardware. Of course, no single architecture is best for running all classes of workloads, and most applications possess a mix of the workload characteristics. For instance, control-intensive applications tend to run faster on the CPU, whereas data-intensive applications tend to run fast on massively parallel architectures (like the GPU), where the same operation is applied to multiple data items concurrently. To extend and support these various workload classes so that browser-based applications wouldn't be hindered, a new generation of API needed to be developed (open the door for developers so that they can access the power of new hardware/technologies). One example of this, is the WebGPU API which exposes the capabilities of GPU hardware for the Web. The course is intended to help you get started with the WebGPU API while understanding both the HOW and WHY behind it works, so you can create your own solutions. This course is designed to teach you the new WebGPU API for graphics and compute techniques without any prior knowledge. All you need is some JavaScript experience and preferably an understanding of basic trigonometry. Whether you're new to graphics and compute development or an old pro, everyone has to start somewhere. Generally, that means starting with the basics which is the focus of this course. You'll learn through simple, easy-to-learn hands-on exercises that help you master the subject. It does this by using multiple task-based activities and discussions which complement and build upon one another.|SIGGRAPH Courses|2023|10.1145/3587423.3595543|Benjamin Kenwright|0.0|2
1057|Adaptive Risk Sensitive Path Integral for Model Predictive Control via Reinforcement Learning|We propose a reinforcement learning framework where an agent uses an internal nominal model for stochastic model predictive control (MPC) while compensating for a disturbance. Our work builds on the existing risk-aware optimal control with stochastic differential equations (SDEs) that aims to deal with such disturbance. However, the risk sensitivity and the noise strength of the nominal SDE in the riskaware optimal control are often heuristically chosen. In the proposed framework, the risk-taking policy determines the behavior of the MPC to be risk-seeking (exploration) or risk-averse (exploitation). Specifically, we employ the risk-aware path integral control that can be implemented as a Monte-Carlo (MC) sampling with fast parallel simulations using a GPU. The MC sampling implementations of the MPC have been successful in robotic applications due to their real-time computation capability. The proposed framework that adapts the noise model and the risk sensitivity outperforms the standard model predictive path integral in simulation environments that have disturbances.|2023 31st Mediterranean Conference on Control and Automation (MED)|2023|10.1109/MED59994.2023.10185876|Chuyuan Tao, Hyung-Jin Yoon, P. Voulgaris, N. Hovakimyan, Hunmin Kim|0.0|2
1059|VASP porting and parallel optimization on GPU like accelerator|Vienna ab initio Simulation Package(VASP) is software for performing electronic structure calculations and quantum mechanical molecular dynamics simulations, and is widely used in materials simulation and computational material science research. Currently VASP is accelerated on NVIDIA GPUs via the OpenACC programming model, which cannot be directly compiled and used by domestic GPU like accelerator platform. We use the HIP programming model to port VASP to GPU like accelerator. Refer to the OpenACC port of VASP, we use the HIP API for device and data management, write kernel to implement cyclic accelerated computation, and use mathematical libraries such as hipBLAS and hipFFT to support the computation. After the porting was completed we compare the computed results of the HIP port version of VASP with the CPU version and the OpenACC version. We passed the validation of the correctness of the HIP ported version. We also analyze the principle of All-to-All parallel communication and propose optimization strategies such as aggregated communication and merged copy for the case of multiple bands communication, which are implemented on the HIP port version. We choose the B.hR105 testcase to test the All-to-All before and after optimization. The results demonstrate a 25.57% performance improvement after optimization.|International Conference on Applied Mathematics, Modelling and Intelligent Computing|2023|10.1117/12.2685955|P. Wu, Wudi Cao, Jiu Liu, Shibiao Hu, Xudong Tang|0.0|2
1061|Automatic Parallelization of Cellular Automata for Heterogeneous Platforms|Cellular Automata (CA) simulations model atomic interactions, diffusion of materials across membranes, and spread of fire or disease and allow scientists to make predictions in situations where performing physical experiments would otherwise be infeasible or impractical. Many parallel implementations of CAs exist; however, they either do not scale in terms of memory usage or are architecture-specific solutions. In this work, we present an automated parallel code-generation framework for deterministic CA applications, ACAC, that requires no programming effort from users to generate hybrid-memory parallel codes using MPI+OpenMP, MPI+CUDA, or MPI+HIP, for CPUs, NVIDIA, or AMD GPUs, respectively. We evaluate the ACAC framework for three different CA applications with four input sizes ranging from 2.36M to 236M cells. On Xeon Phi, NVIDIA, and AMD GPUs, respectively, the automated optimized parallel code from the ACAC framework for a CA with 2.36M cells yields up to 50.9×, 544.2×, and 544.2× speedup compared to their sequential baseline running on an Intel Kaby Lake processor; 1.6×, 16.8×, and 16.8× speedup compared to a state-of-the-art specialized FPGA implementation. To our knowledge, this work is the first to provide a general framework to scale CA simulations up to 10B cells.|Annual International Computer Software and Applications Conference|2023|10.1109/COMPSAC57700.2023.00055|Chase Phelps, T. Islam|0.0|2
1062|Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning|We propose Pgx, a suite of board game reinforcement learning (RL) environments written in JAX and optimized for GPU/TPU accelerators. By leveraging JAX's auto-vectorization and parallelization over accelerators, Pgx can efficiently scale to thousands of simultaneous simulations over accelerators. In our experiments on a DGX-A100 workstation, we discovered that Pgx can simulate RL environments 10-100x faster than existing implementations available in Python. Pgx includes RL environments commonly used as benchmarks in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx offers miniature game sets and baseline models to facilitate rapid research cycles. We demonstrate the efficient training of the Gumbel AlphaZero algorithm with Pgx environments. Overall, Pgx provides high-performance environment simulators for researchers to accelerate their RL experiments. Pgx is available at http://github.com/sotetsuk/pgx.||2023|10.1109/icdl55364.2023.10364471|Haruka Kita, Y. Murata, Keigo Habara, Shin Ishii, Shinri Okano, Soichiro Nishimori, Sotetsu Koyamada|0.0|2
1064|Enhancing Industrial Inspection with Efficient Edge Illumination X-Ray Phase Contrast Simulations|X-ray imaging is routinely used in non-destructive testing, where x-ray projections of an object are compared to a ground truth to detect anomalies. This ground truth can be simulated x-ray projections of a computer-aided design model of the object. While conventional x-ray imaging excels at distinguishing high from low absorbing materials, x-ray phase contrast imaging delivers higher contrast between different low absorbing materials. However, this requires efficient x-ray phase contrast imaging compatible computer-aided design projection simulation software, to generate the ground truth images. Currently available x-ray phase contrast imaging simulation tools are either notoriously slow Monte-Carlo simulators, or equally slow explicit wavefront propagation simulators. In this work, a recently developed computer-aided design projector toolbox is used to model the edge illumination x-ray phase contrast imaging setup within a GPU-based ray tracing framework, significantly speeding up simulations. Results for two industrial samples are shown. One has artificially introduced defects and the other is compared to a real edge illumination acquisition, demonstrating the potential to accurately and efficiently simulate phase contrast images, directly from a computer-aided design model. This paves the way for edge illumination to be applied in non-destructive testing.|EUROCON Conference|2023|10.1109/EUROCON56442.2023.10199074|P. Paramonov, Nicholas Francken, Jan Sijbers, J. De Beenhouwer|0.0|2
1065|Scientific modeling of optical 3D measuring devices based on GPU-accelerated ray tracing using the NVIDIA OptiX engine|Scientific optical 3D modeling requires the possibility to implement highly flexible and customizable mathematical models as well as high computing power. However, established ray tracing software for optical design and modeling purposes often has limitations in terms of access to underlying mathematical models and the possibility of accelerating the mostly CPU-based computation. To address these limitations, we propose the use of NVIDIA’s OptiX Ray Tracing Engine as a highly flexible and high-performing alternative. OptiX offers a highly customizable ray tracing framework with onboard GPU support for parallel computing, as well as access to optimized ray tracing algorithms for accelerated computation. To demonstrate the capabilities of our approach, a realistic focus variation instrument is modeled, describing optical instrument components (light sources, lenses, detector, etc.) as well as the measuring sample surface mathematically or as meshed files. Using this focus variation instrument model, exemplary virtual measurements of arbitrary and standardized sample surfaces are carried out, generating image stacks of more than 100 images and tracing more than 1E9 light rays per image. The performance and accuracy of the simulations are qualitatively evaluated, and virtually generated detector images are compared with images acquired by a respective physical measuring device.|Optical Metrology|2023|10.1117/12.2673777|Samuel Schmidt, Dietrich Beck, J. Seewig, A. Keksel|0.0|2
1067|IceCube experience using XRootD-based Origins with GPU workflows in PNRP|The IceCube Neutrino Observatory is a cubic kilometer neutrino telescope located at the geographic South Pole. Understanding detector systematic effects is a continuous process. This requires the Monte Carlo simulation to be updated periodically to quantify potential changes and improvements in science results with more detailed modeling of the systematic effects. IceCube's largest systematic effect comes from the optical properties of the ice the detector is embedded in. Over the last few years there have been considerable improvements in the understanding of the ice, which require a significant processing campaign to update the simulation. IceCube normally stores the results in a central storage system at the University of Wisconsin-Madison, but it ran out of disk space in 2022. The Prototype National Research Platform (PNRP) project thus offered to provide both GPU compute and storage capacity to IceCube in support of this activity. The storage access was provided via XRootD-based OSDF Origins, a first for IceCube computing. We report on the overall experience using PNRP resources, with both successes and pain points.|arXiv.org|2023|10.48550/arXiv.2308.07999|F. Würthwein, D. Weitzel, B. Riedel, D. Schultz, Fabio Andrijauskas, I. Sfiligoi|0.0|2
1070|Performant low-order matrix-free finite element kernels on GPU architectures|Numerical methods such as the Finite Element Method (FEM) have been successfully adapted to utilize the computational power of GPU accelerators. However, much of the effort around applying FEM to GPU's has been focused on high-order FEM due to higher arithmetic intensity and order of accuracy. For applications such as the simulation of subsurface processes, high levels of heterogeneity results in high-resolution grids characterized by highly discontinuous (cell-wise) material property fields. Moreover, due to the significant uncertainties in the characterization of the domain of interest, e.g. geologic reservoirs, the benefits of high order accuracy are reduced, and low-order methods are typically employed. In this study, we present a strategy for implementing highly performant low-order matrix-free FEM operator kernels in the context of the conjugate gradient (CG) method. Performance results of matrix-free Laplace and isotropic elasticity operator kernels are presented and are shown to compare favorably to matrix-based SpMV operators on V100, A100, and MI250X GPUs.|arXiv.org|2023|10.48550/arXiv.2308.09839|R. Settgast, Benjamin C. Corbett, William Tobin, N. Castelletto, Yohann Dudouit, S. Klevtsov|0.0|2
1072|POLCA: Power Oversubscription in LLM Cloud Providers|Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow. We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the average and peak power utilization in LLM clusters for inference should not be very high. Our deductions align with the data from production LLM clusters, revealing that inference workloads offer substantial headroom for power oversubscription. However, the stringent set of telemetry and controls that GPUs offer in a virtualized environment, makes it challenging to have a reliable and robust power oversubscription mechanism. We propose POLCA, our framework for power oversubscription that is robust, reliable, and readily deployable for GPU clusters. Using open-source models to replicate the power patterns observed in production, we simulate POLCA and demonstrate that we can deploy 30% more servers in the same GPU cluster for inference, with minimal performance loss|arXiv.org|2023|10.48550/arXiv.2308.12908|Brijesh Warrier, Nithish Mahalingam, R. Bianchini, Íñigo Goiri, Chaojie Zhang, Esha Choukse, Pratyush Patel|0.0|2
1073|Potato: A Data-Oriented Programming 3D Simulator for Large-Scale Heterogeneous Swarm Robotics|Large-scale simulation with realistic nonlinear dynamic models is crucial for algorithms development for swarm robotics. However, existing platforms are mainly developed based on Object-Oriented Programming (OOP) and either use simple kinematic models to pursue a large number of simulating nodes or implement realistic dynamic models with limited simulating nodes. In this paper, we develop a simulator based on Data-Oriented Programming (DOP) that utilizes GPU parallel computing to achieve large-scale swarm robotic simulations. Specifically, we use a multi-process approach to simulate heterogeneous agents and leverage PyTorch with GPU to simulate homogeneous agents with a large number. We test our approach using a nonlinear quadrotor model and demonstrate that this DOP approach can maintain almost the same computational speed when quadrotors are less than 5,000. We also provide two examples to present the functionality of the platform.|arXiv.org|2023|10.48550/arXiv.2308.12698|Ziwei Yan, Zhang Ren, Pengzhi Yang, Jinjie Li, Zhaotian Wang, Liang Han, Haoyang Yu|0.0|2
1074|CUDA-PIM: End-to-End Integration of Digital Processing-in-Memory from High-Level C++ to Microarchitectural Design|Digital processing-in-memory (PIM) architectures mitigate the memory wall problem by facilitating parallel bitwise operations directly within memory. Recent works have demonstrated their algorithmic potential for accelerating data-intensive applications; however, there remains a significant gap in the programming model and microarchitectural design. This is further exacerbated by the emerging model of partitions, which significantly complicates control and periphery. Therefore, inspired by NVIDIA CUDA, this paper provides an end-to-end architectural integration of digital memristive PIM from an abstract high-level C++ programming interface for vector operations to the low-level microarchitecture. We begin by proposing an efficient microarchitecture and instruction set architecture (ISA) that bridge the gap between the low-level control periphery and an abstraction of PIM parallelism into warps and threads. We subsequently propose a PIM compilation library that converts high-level C++ to ISA instructions, and a PIM driver that translates ISA instructions into PIM micro-operations. This drastically simplifies the development of PIM applications and enables PIM integration within larger existing C++ CPU/GPU programs for heterogeneous computing with significant ease. Lastly, we present an efficient GPU-accelerated simulator for the proposed PIM microarchitecture. Although slower than a theoretical PIM chip, this simulator provides an accessible platform for developers to start executing and debugging PIM algorithms. To validate our approach, we implement state-of-the-art matrix operations and FFT PIM-based algorithms as case studies. These examples demonstrate drastically simplified development without compromising performance, showing the potential and significance of CUDA-PIM.|arXiv.org|2023|10.48550/arXiv.2308.14007|R. Ronen, Shahar Kvatinsky, Orian Leitersdorf|0.0|2
1076|A Fine-Grained Scheduling Based on GPU Sharing for Deep Learning Jobs on the Cloud|Based on the suspend and resume mechanism, this paper proposes fine-grained scheduling with a fragmented resource-reusing approach to improve the utilization of a GPU sharing system. The HRRN and the best-fit schemes are also adopted to alleviate the starvation problem for large-scale machine learning jobs submitted to the cloud. Experiments are simulated based on the trace logs and benchmarking data collected from a real cluster and the TF-Slim tool. Various image classification models are applied to the experiments to simulate heterogeneous job submissions to the GPU sharing system. Experimental results show that our approach improves GPU resource utilization by around 4.3 times and reduces overall makespan by at least 3.9 times compared to the sequential execution without GPU sharing.|2023 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)|2023|10.1109/ICCE-Taiwan58799.2023.10226864|Jyun-Sen Tong, Wu-Chun Chung|0.0|2
1077|A Novel Asynchronous Network-On-Chip Based on Source Asynchronous Signaling|The communication subsystem of a system-on-chip(SoC) has been an exciting research area for more than three decades. With benefits ranging from increased power efficiency to better area utilization and scalability, networks-on-chip have seen an increased role in SoCs. Multi-core SoC architectures containing discrete functional units like GPUs and applicationspecific units are becoming increasingly popular, and in this, networks-on-chip play a critical role. Source Asynchronous Signaling (SAS) is a credit-based communication protocol that supports arbitrarily large but finite latency on communication links. The bandwidth and handshake frequency of this protocol are independent of wire latency. The work presented in this paper explores a new asynchronous network-on-chip (NoC) that utilizes SAS. First, we investigate the advantages of using SAS in NoCs. Then, we present a design method for building a deadlock-free NoC employing this protocol. Finally, we evaluate the power and performance numbers of this NoC using accurate simulation models in a torus architecture, and we compare it with a similar asynchronous NoC.|2023 28th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC)|2023|10.1109/ASYNC58294.2023.10239625|Kenneth S. Stevens, Venkata Nori, Mackenzie J. Wibbels, Baudouin Chauviere|0.0|2
1079|Oceananigans.jl: A model that achieves breakthrough resolution, memory and energy efficiency in global ocean simulations|Climate models must simulate hundreds of future scenarios for hundreds of years at coarse resolutions, and a handful of high-resolution decadal simulations to resolve localized extreme events. Using Oceananigans.jl, written from scratch in Julia, we report several achievements: First, a global ocean simulation with breakthrough horizontal resolution -- 488m -- reaching 15 simulated days per day (0.04 simulated years per day; SYPD). Second, Oceananigans simulates the global ocean at 488m with breakthrough memory efficiency on just 768 Nvidia A100 GPUs, a fraction of the resources available on current and upcoming exascale supercomputers. Third, and arguably most significant for climate modeling, Oceananigans achieves breakthrough energy efficiency reaching 0.95 SYPD at 1.7 km on 576 A100s and 9.9 SYPD at 10 km on 68 A100s -- the latter representing the highest horizontal resolutions employed by current IPCC-class ocean models. Routine climate simulations with 10 km ocean components are within reach.|arXiv.org|2023|10.48550/arXiv.2309.06662|Navid C Constantinou, Ali Ramadhan, Christopher Hill, Andre Souza, John Marshall, Raffaele Ferrari, Valentin Churavy, Matin Raayai Ardakani, Alan Edelman, J. Campin, Simone Silvestri, Johannes Blaschke, Gregory Wagner|0.0|2
1082|Matrix-based implementation and GPU acceleration of linearized ordinary state-based peridynamic models in MATLAB|Ordinary state-based peridynamic (OSB-PD) models have an unparalleled capability to simulate crack propagation phenomena in solids with arbitrary Poisson's ratio. However, their non-locality also leads to prohibitively high computational cost. In this paper, a fast solution scheme for OSB-PD models based on matrix operation is introduced, with which, the graphics processing units (GPUs) are used to accelerate the computation. For the purpose of comparison and verification, a commonly used solution scheme based on loop operation is also presented. An in-house software is developed in MATLAB. Firstly, the vibration of a cantilever beam is solved for validating the loop- and matrix-based schemes by comparing the numerical solutions to those produced by a FEM software. Subsequently, two typical dynamic crack propagation problems are simulated to illustrate the effectiveness of the proposed schemes in solving dynamic fracture problems. Finally, the simulation of the Brokenshire torsion experiment is carried out by using the matrix-based scheme, and the similarity in the shapes of the experimental and numerical broken specimens further demonstrates the ability of the proposed approach to deal with 3D non-planar fracture problems. In addition, the speed-up of the matrix-based scheme with respect to the loop-based scheme and the performance of the GPU acceleration are investigated. The results emphasize the high computational efficiency of the matrix-based implementation scheme.|arXiv.org|2023|10.48550/arXiv.2309.11273|M. Zaccariotto, Qizhi Zhu, T. Ni, U. Galvanetto|0.0|2
1083|Analog Content-Addressable Memory from Complementary FeFETs|To address the increasing computational demands of artificial intelligence (AI) and big data, compute-in-memory (CIM) integrates memory and processing units into the same physical location, reducing the time and energy overhead of the system. Despite advancements in non-volatile memory (NVM) for matrix multiplication, other critical data-intensive operations, like parallel search, have been overlooked. Current parallel search architectures, namely content-addressable memory (CAM), often use binary, which restricts density and functionality. We present an analog CAM (ACAM) cell, built on two complementary ferroelectric field-effect transistors (FeFETs), that performs parallel search in the analog domain with over 40 distinct match windows. We then deploy it to calculate similarity between vectors, a building block in the following two machine learning problems. ACAM outperforms ternary CAM (TCAM) when applied to similarity search for few-shot learning on the Omniglot dataset, yielding projected simulation results with improved inference accuracy by 5%, 3x denser memory architecture, and more than 100x faster speed compared to central processing unit (CPU) and graphics processing unit (GPU) per similarity search on scaled CMOS nodes. We also demonstrate 1-step inference on a kernel regression model by combining non-linear kernel computation and matrix multiplication in ACAM, with simulation estimates indicating 1,000x faster inference than CPU and GPU.|Device|2023|10.48550/arXiv.2309.09165|D. Jariwala, Paul Jacob, Santosh Kurinec, Pratik Chaudhari, Keshava Katti, Uwe Schroeder, Claudia Richter, Yunfei He, Xiwen Liu|0.0|2
1084|An Evaluation and Comparison of GPU Hardware and Solver Libraries for Accelerating the OPM Flow Reservoir Simulator|Realistic reservoir simulation is known to be prohibitively expensive in terms of computation time when increasing the accuracy of the simulation or by enlarging the model grid size. One method to address this issue is to parallelize the computation by dividing the model in several partitions and using multiple CPUs to compute the result using techniques such as MPI and multi-threading. Alternatively, GPUs are also a good candidate to accelerate the computation due to their massively parallel architecture that allows many floating point operations per second to be performed. The numerical iterative solver takes thus the most computational time and is challenging to solve efficiently due to the dependencies that exist in the model between cells. In this work, we evaluate the OPM Flow simulator and compare several state-of-the-art GPU solver libraries as well as custom developed solutions for a BiCGStab solver using an ILU0 preconditioner and benchmark their performance against the default DUNE library implementation running on multiple CPU processors using MPI. The evaluated GPU software libraries include a manual linear solver in OpenCL and the integration of several third party sparse linear algebra libraries, such as cuSparse, rocSparse, and amgcl. To perform our bench-marking, we use small, medium, and large use cases, starting with the public test case NORNE that includes approximately 50k active cells and ending with a large model that includes approximately 1 million active cells. We find that a GPU can accelerate a single dual-threaded MPI process up to 5.6 times, and that it can compare with around 8 dual-threaded MPI processes.|arXiv.org|2023|10.48550/arXiv.2309.11488|A. Thune, A. Rustad, R. Nane, Markus Blatt, Tong Dong Qiu|0.0|2
1085|OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control|In this work, we introduce OmniDrones, an efficient and flexible platform tailored for reinforcement learning in drone control, built on Nvidia's Omniverse Isaac Sim. It employs a bottom-up design approach that allows users to easily design and experiment with various application scenarios on top of GPU-parallelized simulations. It also offers a range of benchmark tasks, presenting challenges ranging from single-drone hovering to over-actuated system tracking. In summary, we propose an open-sourced drone simulation platform, equipped with an extensive suite of tools for drone learning. It includes 4 drone models, 5 sensor modalities, 4 control modes, over 10 benchmark tasks, and a selection of widely used RL baselines. To showcase the capabilities of OmniDrones and to support future research, we also provide preliminary results on these benchmark tasks. We hope this platform will encourage further studies on applying RL to practical drone systems.|arXiv.org|2023|10.48550/arXiv.2309.12825|Feng Gao, Yu Wang, Chao Yu, Botian Xu, Ruize Zhang, Yi Wu|0.0|2
1086|Slang - A Shader Compilation System for Extensible, Real-Time Shading|To produce compelling images, a real-time renderer is responsible for simulating many real-world visual effects. These effects range from modeling the material properties of surfaces to evaluating complex lighting conditions, to the animation of surfaces. Production real-time renderers contain hundreds of thousands of lines of code that define these shading effects. Like any complex software system, it is desirable for these code bases to be implemented in a flexible and extensible framework that embodies the key rendering concepts. In addition, authoring real-time renderers is particularly challenging because of the need to meet extreme performance goals, which involves efficiently using the massively parallel GPUs, and highly specializing code that executes in various GPU pipeline stages. In response to these challenges, we contribute a shading system design pattern called shader components, that serve as a guideline to authoring shading systems in a modular fashion that is consistent with developers' mental organization of rendering concepts, while retaining key optimizations required to achieve the desired performance goal. We present the design of the Slang shading language and its compilation system, which extends HLSL with a minimal set of general-purpose language features, including generics with interface constraints, associated types, and interface/structure extensions, to facilitate implementation of the shader components while providing an incremental path of adoption for current HLSL developers. We demonstrate how to rearchitect a large open source renderer to adopt shader components using Slang's compiler services, and show via a thorough evaluation that the resulting shading system is substantially simpler, easier to extend with new features, and achieves higher rendering performance than the original HLSL-based implementation.||2021|10.1184/r1/16826602.v1|Yong He|0.0|2
1088|Xsuite: an integrated beam physics simulation framework|Xsuite is a newly developed modular simulation package combining in a single flexible and modern framework the capabilities of different tools developed at CERN in the past decades, notably Sixtrack, Sixtracklib, COMBI and PyHEADTAIL. The suite consists of a set of Python modules (Xobjects, Xpart, Xtrack, Xcoll, Xfields, Xdeps) that can be flexibly combined together and with other accelerator-specific and general-purpose python tools to study complex simulation scenarios. The code allows for symplectic modeling of the particle dynamics, combined with the effect of synchrotron radiation, impedances, feedbacks, space charge, electron cloud, beam-beam, beamstrahlung, and electron lenses. For collimation studies, beam-matter interaction is simulated using the K2 scattering model or interfacing Xsuite with the BDSIM/Geant4 library. Tools are available to compute the accelerator optics functions from the tracking model and to generate particle distributions matched to the optics. Different computing platforms are supported, including conventional CPUs, as well as GPUs from different vendors.||2023|10.1109/indin51400.2023.10218255|A. Latina, D. Demetriadou, L. V. Riesen-Haupt, X. Buffat, T. Pieloni, P. Hermes, P. Kicsiny, A. Abramov, Sterbini, L. Mether, L. Deniau, D. Croce, R. D. Maria, P. Niedermayer, K. Paraschou, P. Belanger, G. Iadarola, F. V. D. Veken, P. Kruyt, S. Lopaciuk|0.0|2
1093|Active Learning with Dual Model Predictive Path-Integral Control for Interaction-Aware Autonomous Highway On-ramp Merging|Merging into dense highway traffic for an autonomous vehicle is a complex decision-making task, wherein the vehicle must identify a potential gap and coordinate with surrounding human drivers, each of whom may exhibit diverse driving behaviors. Many existing methods consider other drivers to be dynamic obstacles and, as a result, are incapable of capturing the full intent of the human drivers via this passive planning. In this paper, we propose a novel dual control framework based on Model Predictive Path-Integral control to generate interactive trajectories. This framework incorporates a Bayesian inference approach to actively learn the agents' parameters, i.e., other drivers' model parameters. The proposed framework employs a sampling-based approach that is suitable for real-time implementation through the utilization of GPUs. We illustrate the effectiveness of our proposed methodology through comprehensive numerical simulations conducted in both high and low-fidelity simulation scenarios focusing on autonomous on-ramp merging.|arXiv.org|2023|10.48550/arXiv.2310.07840|Ehsan Moradi-Pari, Tyler Naes, Hossein Nourkhiz Mahjoub, Jacob Knaup, P. Tsiotras, Behdad Chalaki, Jovin D'sa|0.0|2
1094|ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models|Alignment is crucial for training large language models. The predominant strategy is Reinforcement Learning from Human Feedback (RLHF), with Proximal Policy Optimization (PPO) as the de-facto algorithm. Yet, PPO is known to struggle with computational inefficiency, a challenge that this paper aims to address. We identify three important properties of RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on these properties, we develop ReMax, a new algorithm tailored for RLHF. The design of ReMax builds on the celebrated algorithm REINFORCE but is enhanced with a new variance-reduction technique. ReMax offers threefold advantages over PPO: first, it is simple to implement with just 6 lines of code. It further eliminates more than 4 hyper-parameters in PPO, which are laborious to tune. Second, ReMax reduces memory usage by about 50%. To illustrate, PPO runs out of memory when fine-tuning a Llama2-7B model on A100-80GB GPUs, whereas ReMax can support the training. Even though memory-efficient techniques (e.g., ZeRO and offload) are employed for PPO to afford training, ReMax can utilize a larger batch size to increase throughput. Third, in terms of wall-clock time, PPO is about twice as slow as ReMax per iteration. Importantly, these improvements do not sacrifice task performance. We hypothesize that these advantages can be maintained in larger-scale models.|arXiv.org|2023|10.48550/arXiv.2310.10505|Zhimin Luo, Ziniu Li, Yushun Zhang, Ruoyu Sun, Tian Xu, Yang Yu|0.0|2
1095|PIM-GPT: A Hybrid Process-in-Memory Accelerator for Autoregressive Transformers|Decoder-only Transformer models such as GPT have demonstrated superior performance in text generation, by autoregressively predicting the next token. However, the performance of GPT is bounded by low compute-to-memory-ratio and high memory access. Throughput-oriented architectures such as GPUs target parallel processing rather than sequential token generation, and are not efficient for GPT acceleration, particularly on-device inference applications. Process-in-memory (PIM) architectures can significantly reduce data movement and provide high computation parallelism, and are promising candidates to accelerate GPT inference. In this work, we propose PIM-GPT that aims to achieve high throughput, high energy efficiency and end-to-end acceleration of GPT inference. PIM-GPT leverages DRAM-based PIM solutions to perform multiply-accumulate (MAC) operations on the DRAM chips, greatly reducing data movement. A compact application-specific integrated chip (ASIC) is designed and synthesized to initiate instructions to PIM chips and support data communication along with necessary arithmetic computations. At the software level, the mapping scheme is designed to maximize data locality and computation parallelism by partitioning a matrix among DRAM channels and banks to utilize all in-bank computation resources concurrently. We develop an event-driven clock-cycle accurate simulator to validate the efficacy of the proposed PIM-GPT architecture. Overall, PIM-GPT achieves 41$-$137$\times$, 631$-$1074$\times$ speedup and 339$-$1085$\times$, 890$-$1632$\times$ energy efficiency over GPU and CPU baseline, respectively, on 8 GPT models with up to 1.4 billion parameters.|arXiv.org|2023|10.48550/arXiv.2310.09385|Yuting Wu, Ziyu Wang, Wei D. Lu|0.0|2
1097|OpenNucleome for high resolution nuclear structural and dynamical modeling|The intricate structural organization of the human nucleus is fundamental to cellular function and gene regulation. Recent advancements in experimental techniques, including high-throughput sequencing and microscopy, have provided valuable insights into nuclear organization. Computational modeling has played significant roles in interpreting experimental observations by reconstructing high-resolution structural ensembles and uncovering organization principles. However, the absence of standardized modeling tools poses challenges for furthering nuclear investigations. We present OpenNucleome—an open-source software designed for conducting GPU-accelerated molecular dynamics simulations of the human nucleus. OpenNucleome offers particle- based representations of chromosomes at a resolution of 100 KB, encompassing nuclear lamina, nucleoli, and speckles. This software furnishes highly accurate structural models of nuclear architecture, affording the means for dynamic simulations of condensate formation, fusion, and exploration of non-equilibrium effects. We applied OpenNucleome to uncover the mechanisms driving the emergence of “fixed points” within the nucleus—signifying genomic loci robustly anchored in proximity to specific nuclear bodies for functional purposes. This anchoring remains resilient even amidst significant fluctuations in chromosome radial positions and nuclear shapes within individual cells. Our findings lend support to a nuclear zoning model that elucidates genome functionality. We anticipate OpenNucleome to serve as a valuable tool for nuclear investigations, streamlining mechanistic explorations and enhancing the interpretation of experimental observations.|bioRxiv|2023|10.1101/2023.10.16.562451|Bin Zhang, Zhuohan Lao, Kartik Kamat, Zhongling Jiang|0.0|2
1098|Exploring the Potential of Generative AI for the World Wide Web|Generative Artificial Intelligence (AI) is a cutting-edge technology capable of producing text, images, and various media content leveraging generative models and user prompts. Between 2022 and 2023, generative AI surged in popularity with a plethora of applications spanning from AI-powered movies to chatbots. In this paper, we delve into the potential of generative AI within the realm of the World Wide Web, specifically focusing on image generation. Web developers already harness generative AI to help crafting text and images, while Web browsers might use it in the future to locally generate images for tasks like repairing broken webpages, conserving bandwidth, and enhancing privacy. To explore this research area, we have developed WebDiffusion, a tool that allows to simulate a Web powered by stable diffusion, a popular text-to-image model, from both a client and server perspective. WebDiffusion further supports crowdsourcing of user opinions, which we use to evaluate the quality and accuracy of 409 AI-generated images sourced from 60 webpages. Our findings suggest that generative AI is already capable of producing pertinent and high-quality Web images, even without requiring Web designers to manually input prompts, just by leveraging contextual information available within the webpages. However, we acknowledge that direct in-browser image generation remains a challenge, as only highly powerful GPUs, such as the A40 and A100, can (partially) compete with classic image downloads. Nevertheless, this approach could be valuable for a subset of the images, for example when fixing broken webpages or handling highly private content.|arXiv.org|2023|10.48550/arXiv.2310.17370|Nouar Aldahoul, Matteo Varvello, Yasir Zaki, Joseph Hong|0.0|2
1100|The Simple Cloud-Resolving E3SM Atmosphere Model Running on the Frontier Exascale System|We present an efficient and performance portable implementation of the Simple Cloud Resolving E3SM Atmosphere Model (SCREAM). SCREAM is a full featured atmospheric global circulation model with a nonhydrostatic dynamical core and state-of-the-art parameterizations for microphysics, moist turbulence and radiation. It has been written from scratch in C++ with the Kokkos library used to abstract the on-node execution model for both CPUs and GPUs. SCREAM is one of only a few global atmosphere models to be ported to GPUs. As far as we know, SCREAM is the first such model to run on both AMD GPUs and NVIDIA GPUs, as well as the first to run on nearly an entire Exascale system (Frontier). On Frontier, we obtained a record setting performance of 1.26 simulated years per day for a realistic cloud resolving simulation.|International Conference on Software Composition|2023|10.1145/3581784.3627044|Christopher Terai, Noel Keen, James B. White, L. R. Leung, Peter M. Caldwell, Luca Bertagna, A. Salinger, S. Sreepathi, Renata B McCoy, Benjamin Hillman, Danqing Wu, J. Foucar, Aaron Donahue, Conrad Clevenger, Matthew Norman, Jayesh Krishna, David C. Bader, O. Guba, Mark Taylor|0.0|2
1102|DEVELOPMENT OF MASSIVELY PARALLEL NEAR PEAK PERFORMANCE SOLVERS FOR THREE-DIMENSIONAL GEODYNAMIC MODELLING|Academia and industry have not yet been able to adapt software and numerical algorithms to the rapid, fundamental changes in hardware evolution since the beginning of the 21th century. Many of today's applications and algorithms are as a resuit not well suited for the available hardware, what implies performances far below hardware's peak. We address in this thesis the current need to design new parallel algorithms and tools that ease the development of applications that are suited for today's and tomorrow's hardware. We present (1) the MATLAB HPC compiler HPC.m, which greatly simplifies the building of parallel high performance applications and (2) parallel algorithms for the 3D simulation of strongly nonlinear processes as mechanical and reactive porosity waves.\nTo simulate mechanical porosity waves we employ a simple numerical algorithm that permits to resolve the deformation of fluid-filled viscoelastic porous media in 3D. The utilized mathematical model is based on Biot's poroelastic theory, extended to account for viscous deformation and plastic yielding. The algorithm is designed for massively parallel high performance Computing. It employs finite difference stencil calculations on a staggered grid to approximate spatial derivatives. Pseudo-translent iterations were utilized to formulate an explicit algorithm and Picard iterations to resolve the nonlinearities. The modelling results exhibit the impact of decompaction weakening on the formation of three-dimensional solitary-wave-like moving porosity channels. We evaluate the algorithm's suitability for the building of high performance massively parallel 3D solvers and compare the achievable performance and scalability to the ones of other state-of-the-art algorithms that were developed aiming at large-scale simulations. Our algorithm is found to be better suited for the building of high performance massively parallel 3D solvers than the other algorithms considered in this thesis. To simulate reactive porosity waves we use a solver for 3D deformation of fluid-filled reactive viscous porous media. The employed algorithm uses essentially the same numerical methods as the algorithm for the simulation of mechanical porosity waves. The Damkohler number (Da) of the simulations is varied in order to estimate the respective roles of viscous deformation (low Da) and reaction (high Da) on wave propagation. 3D waves are found to propagate independently of their source at constant speed by going through each other for ail the investigated Da. Soliton-like wave propagation as a resuit of metamorphic reaction provides an efficient mechanism for fluid flow in the Earth's crust. It is expected that this mechanism takes place at the meter-scale in the lower crust and at the kilometer-scale in the upper crust providing explanations for both metamorphic veins formation and fluid extraction in subduction zones.\nThe here developed tool HPC.m transforms simple MATLAB scripts into parallel high performance applications for GPU-, CPU- and MIC-supercomputers, clusters or workstations. It is designed forstencil-based applications, in particular for iterative PDE solvers that use finite differences and a regular Cartesian grld. Its core is a source- to-source translator. We illustrate in this thesis the great performance and versatility of HPC.m by deploying it to generate solvers for a variety of physics across multiple Earth Science disciplines, Ail solvers run close to hardware's peak performance and scale linearly on the 80 GPUs of the Octopus cluster, hosted by the Institute of Earth Sciences at University of Lausanne (Lausanne, Switzerland). They achieve moreover a speedup over the fully vectorised MATLAB input script of about 250x to 500x on one GPU, of lOOOx to 2000x on one workstation with 4 GPUs and of 17 OOOx to 35 OOOx on 80 GPUs. We show in addition that our nonlinear poroviscoelastic two-phase flow solver scales also linearly on the 5000 GPUs of the Piz Daint supercomputer at the Swiss National Supercomputing Centre (CSCS, Lugano, Switzerland), achieving a speedup over the fully vectorised MATLAB input script of over 500 OOOx. We expect a similar scaling for ail the ten solvers. The source-to-source translator contained in HPC.m is the first known to the authors that can perform automatically ail tasks that are required for the generation of a massively parallel near peak performance supercomputing application from a code developed in classical prototyping environment as MATLAB.\n--\nAcademie et industrie n'ont pas encore ete capables d'adapter les logiciels et les algorithmes numeriques aux changements rapides et fondamentaux dans l'evolution du hardware depuis le debut du 21eme siecle. Une grande partie des applications et algorithmes ne sont, par consequent, pas bien adapte au hardware disponible ; cela implique des performances bien au-dessous du maximum atteignable par le hardware. Nous repondons dans cette these au besoin actuel de developper des nouveaux algorithmes paralleles et des outils qui simplifient le developpement d'applications qui soient adaptees au hardware d'aujourd'hui et de demain. Nous presentons (1) le compilateur HPC MATLAB, HPCm, qui simplifie de maniere importante la construction des applications paralleles de haute performance et (2) des algorithmes pour la simulation 3D de processus non-lineaires tel que des ondes de porosite mecaniques et reactives. Pour simuler des ondes de porosite mecaniques, nous utilisons un algorithme numerique simple qui permet de resoudre la deformation 3D de matiere poreuse et viscoelastique, sature de fluide. Le modele mathematique utilise est base sur la theorie poroelastique de Biot, etendue pour tenir compte de deformation visqueuse et plastique. L'algorithme est concu pour le calcul massivement parallele de haute performance. Il utilise le calcul de differences finies pour approximer les derivees spatiales. Des iterations pseudo-transitoires ont ete utilisees pour formuler un algorithme explicit et des iterations de Picard pour resoudre des non-linearites. Les resultats de modelisation montrent l'impact de l'affaiblissement en decompaction sur la formation de canaux de porosite, qui se deplacent de maniere similaire a des solitons. Nous evaluons l'aptitude de l'algorithme pour la construction des solveurs massivement paralleles de haute performance et 3D ; de plus, nous comparons la performance et la scalabilite atteignable a ceux d'autres algorithmes qui ont ete developpes dans le but d'effectuer des simulations a grande echelle. Notre algorithme a ete trouve mieux adapte pour la construction des solveurs massivement paralleles de haute performance et 3D que les autres algorithmes consideres dans cette these.\nL'outil HPCm, developpe dans cette these, transforme des scripts MATLAB en applications paralleles de haute performance pour des super-ordinateurs, des clusters et des ordinateurs desktops avec GPU, CPU ou MICs. Il est concu pour des applications basees sur le calcul de stencil ; en particulier pour des solveurs PDE qui utilisent des differences finies et une grille Cartesien reguliere. Le cœur de HPCm est un traducteur source-source. Dans cette these, nous illustrons la grande performance et versatilite de HPC m en en faisant usage pour la generation de solveurs pour une variete de physiques a travers de multiples disciplines dans les Sciences de la Terre. Tous les solveurs atteignent une performance qui est proche du maximum que le hardware peut fournir et montrent une scalabilite lineaire sur les 80 GPUs du cluster de calcul Octopus a l'Institut des Sciences de la Terre a l'Universite de Lausanne (Lausanne, Suisse). De plus, ils atteignent un speedup sur le script MATLAB d'entree entierement vectorise d'environ 250x a 500x sur un GPU, lOOOx a 2000x sur un ordinateur desktop avec 4 GPUs et 17 000x a 35 OOOx sur 80 GPUs. Par ailleurs, nous montrons que notre solveur non-lineaire de flux deux-phases poroviscoelastique atteigne une scalabilite lineaire sur 5000 GPUs du super-ordinateur Piz Daint au Centre Suisse du Calcul Scientifique (CSCS, Lugano, Switzerland) et arrive ainsi a un speedup sur le script MATLAB d'entree entierement vectorise d'environ 500 OOOx. Nous nous attendons a une scalabilite similaire pour tous les dix solveurs. Le traducteur source- source qui est contenu dans HPCm est le premier connu aux auteurs qui peut effectuer tous les taches necessaires pour la generation d'une application massivement parallele de haute performance a partir d'un code developpe dans un environnement de prototypage classique tel que MATLAB.||2016|10.1016/j.jtcvs.2016.03.038|Samuel Omlin|0.0|2
1106|Porting and optimizing Meso-NH to AMD MI250X GPUs|"This paper presents the results of our efforts to port Meso-NH, an atmospheric non-hydrostatic research model, to AMD MI250X GPUs using OpenACC on the ADASTRA Machine, a technology similar to the Frontier system [1]. Meso-NH is a versatile model that covers a wide range of resolutions from synoptic to turbulent scales, and is designed for studies of physics and chemistry. It can be used in 3D for storms and thunderstorms simulation, 2D or 1D for the simulation of academic studies. Numerical simulation of the atmosphere is crucial for understanding and predicting weather and climate extremes. Current numerical weather prediction codes are limited to specibic resolutions on global and regional scales. The Meso- NH code, however, tackles scales and complexities beyond what is typically used in operational forecasting. We collaborated with GENCI, CINES, HPE, and AMD on the ""progress contract,"" for ADASTRA machine aiming to achieve simulations at hectometric resolution for recent storms in the Atlantic and Mediterranean regions, characterized by extreme wind gusts. This higher resolution allows us to explicitly represent a cascade of scales, from the storm core (>100 km) to the deep and shallow convective circulations behind the gusts (<1 km). The successful porting of Meso-NH to AMD MI250X GPUs made this numerical achievement possible. The paper focuses on the challenges faced during the porting process, optimization strategies, and the lessons learned throughout the project. Additionally, we share the current performance results from relevant benchmark problems. A comparative study highlights the performance and energy consumption aspects of a fraction of the code that dominates the computation time. Notably, we achieved a speed-up of 3.5 times compared to computation on AMD- Genoa processors. The results of this porting effort open up new possibilities for atmospheric simulations at hectometric resolutions, enhancing the accuracy and sophistication of weather phenomena representation. The collaboration between different institutions has paved the way for advanced meteorological research, contributing to the ongoing progress in the bield."|SC Workshops|2023|10.1145/3624062.3624603|Juan Escobar Munoz, Naima Alaoui Ismaili, Gabriel Hautreux, Philippe Wautelet|0.0|2
1107|Moment Representation of Regularized Lattice Boltzmann Methods on NVIDIA and AMD GPUs|The lattice Boltzmann method is a highly scalable Navier-Stokes solver that has been applied to flow problems in a wide array of domains. However, the method is bandwidth-bound on modern GPU accelerators and has a large memory footprint. In this paper, we present new 2D and 3D GPU implementations of two different regularized lattice Boltzmann methods, which are not only able to achieve an acceleration of ∼ 1.4 × w.r.t. reference lattice Boltzmann implementations but also reduce the memory requirements by up to 35% and 47% in 2D and 3D simulations respectively. These new approaches are evaluated on NVIDIA and AMD GPU architectures.|SC Workshops|2023|10.1145/3624062.3624250|Jeffrey S. Vetter, John P. Gounley, A. Randles, Pedro Valero-Lara|0.0|2
1108|Accurate Measurement of Application-level Energy Consumption for Energy-Aware Large-Scale Simulations|Sustainability in high performance computing (HPC) is a major challenge not only for HPC centers and their users, but also for society as the climate goals become stricter. A lot of effort went into reducing the energy consumption of systems in general. Even though certain efforts to optimize the energy-efficiency of HPC workloads exist, most such efforts propose solutions targeting CPUs. As HPC systems shift more and more to GPU-centric architectures, simulation codes increasingly adopt GPU-programming models. This leads to an urgent need to increase the energy-efficiency of GPU-enabled codes. However, studies for reducing the energy consumption of large-scale simulations executing on CPUs and GPUs have received insufficient attention. In this work, we enable accurate power and energy measurements using an open-source toolkit across a range of CPU+GPU node architectures. We use this approach in SPH-EXA, an open-source GPU-centric astrophysical and cosmological simulation framework. We show that with simple code instrumentation, users can accurately measure power and energy related data about their application, beyond data provided by HPC systems alone. The accurate power and energy data provide significant insight to users for conducting energy-aware computational experiments and future energy-aware code development.|SC Workshops|2023|10.1145/3624062.3624272|O. Simsek, F. Ciorba, J. Piccinali|0.0|2
1111|Radio Propagation Digital Twin Aided Multi-Point Transmission With In-Network Dynamic On-Off Switching|GPUs and programmable data planes have gone through an enormous evolution in the past years. GPUs can be used for modeling the real-world environment accurately, while programmable data planes can monitor the network in real-time and implement novel packet processing and decision logic. In this paper, we investigate how these two distant technologies can be combined to implement efficient coordinated multi-point (CoMP) transmission in an indoor environment. The proposed system implements the concept of dynamic on-off switching (DOOS) among various radio transmitters, relying on two information sources: 1) radio propagation digital twin based on real-time simulations of radio channels in the accurate 3D digital representation of the real industrial environment and 2) traffic load collected by the data plane for each transmitter. The proposed DOOS method implemented as a data plane algorithm dynamically selects a set of radio transmitters for each receiver by mixing information provided by the digital twin and the in-network traffic load measurements. The proposed method has low computational complexity and reduces the number of actively used radio transmitters. The proof-of-concept implementation of the proposed system has been validated in simulations as well as with measurements. The method achieves 69% energy saving for the radio compared to the default CoMP transmission and reception.|IEEE Access|2023|10.1109/ACCESS.2023.3333525|Csaba Györgyi, József Peto, Géza Szabó, S. Laki, Péter Vörös|0.0|2
1112|p-adaptive discontinuous Galerkin method for the shallow water equations on heterogeneous computing architectures|Heterogeneous computing and exploiting integrated CPU-GPU architectures has become a clear current trend since the flattening of Moore's Law. In this work, we propose a numerical and algorithmic re-design of a p-adaptive quadrature-free discontinuous Galerkin method (DG) for the shallow water equations (SWE). Our new approach separates the computations of the non-adaptive (lower-order) and adaptive (higher-order) parts of the discretization form each other. Thereby, we can overlap computations of the lower-order and the higher-order DG solution components. Furthermore, we investigate execution times of main computational kernels and use automatic code generation to optimize their distribution between the CPU and GPU. Several setups, including a prototype of a tsunami simulation in a tide-driven flow scenario, are investigated, and the results show that significant performance improvements can be achieved in suitable setups.|arXiv.org|2023|10.48550/arXiv.2311.11348|S. Kuckuk, V. Aizinger, Harald Köstler, Richard Angersbach, S. Faghih-Naini|0.0|2
1113|Sampling-based Model Predictive Control Leveraging Parallelizable Physics Simulations|We present a method for sampling-based model predictive control that makes use of a generic physics simulator as the dynamical model. In particular, we propose a Model Predictive Path Integral controller (MPPI), that uses the GPU-parallelizable IsaacGym simulator to compute the forward dynamics of a problem. By doing so, we eliminate the need for explicit encoding of robot dynamics and contacts with objects for MPPI. Since no explicit dynamic modeling is required, our method is easily extendable to different objects and robots and allows one to solve complex navigation and contact-rich tasks. We demonstrate the effectiveness of this method in several simulated and real-world settings, among which mobile navigation with collision avoidance, non-prehensile manipulation, and whole-body control for high-dimensional configuration spaces. This method is a powerful and accessible open-source tool to solve a large variety of contact-rich motion planning tasks.|arXiv.org|2023|10.48550/arXiv.2307.09105|Javier Alonso-Mora, Max Spahn, C. Salmi, C. H. Corbato, Corrado Pezzato, Elia Trevisan|0.0|2
1114|IDESS: a toolbox for identification and automated design of stochastic gene circuits|Abstract Motivation One of the main causes hampering predictability during the model identification and automated design of gene circuits in synthetic biology is the effect of molecular noise. Stochasticity may significantly impact the dynamics and function of gene circuits, specially in bacteria and yeast due to low mRNA copy numbers. Standard stochastic simulation methods are too computationally costly in realistic scenarios to be applied to optimization-based design or parameter estimation. Results In this work, we present IDESS (Identification and automated Design of Stochastic gene circuitS), a software toolbox for optimization-based design and model identification of gene regulatory circuits in the stochastic regime. This software incorporates an efficient approximation of the Chemical Master Equation as well as a stochastic simulation algorithm—both with GPU and CPU implementations—combined with global optimization algorithms capable of solving Mixed Integer Nonlinear Programming problems. The toolbox efficiently addresses two types of problems relevant in systems and synthetic biology: the automated design of stochastic synthetic gene circuits, and the parameter estimation for model identification of stochastic gene regulatory networks. Availability and implementation IDESS runs under the MATLAB environment and it is available under GPLv3 license at https://doi.org/10.5281/zenodo.7788692.|Bioinformatics|2023|10.1093/bioinformatics/btad682|Irene Otero-Muras, M. Pájaro, Carlos Vázquez, Carlos Sequeiros, J. Banga|0.0|2
1115|LiFi: Learn to Incentivize Federated Learning in Automotive Edge Computing|Federated learning (FL) is the promising privacy-preserve approach to continually update the central machine learning (ML) model (e.g., object detectors in edge servers) by aggregating the gradients obtained from local observation data in distributed connected and automated vehicles (CAVs). The incentive mechanism is to incentivize individual selfish CAVs to participate in FL towards the improvement of overall model accuracy. It is, however, challenging to design the incentive mechanism, due to the complex correlation between the overall model accuracy and unknown incentive sensitivity of CAVs, especially under the non-independent and identically distributed (Non-IID) data of individual CAVs. In this paper, we propose a new learn-to-incentivize algorithm to adaptively allocate rewards to individual CAVs under unknown sensitivity functions. First, we gradually learn the unknown sensitivity function of individual CAVs with accumulative observations, by using compute-efficient Gaussian process regression (GPR). Second, we iteratively update the reward allocation to individual CAVs with new sampled gradients, derived from GPR. Third, we project the updated reward allocations to comply with the total budget. We evaluate the performance of extensive simulations, where the simulation parameters are obtained from realistic profiling of the CIFAR-10 dataset and NVIDIA RTX 3080 GPU. The results show that our proposed algorithm substantially outperforms existing solutions, in terms of accuracy, scalability, and adaptability.|arXiv.org|2023|10.48550/arXiv.2311.12720|Yuru Zhang, Ming Zhao, Qiang Liu|0.0|2
1116|Vast TVB parameter space exploration: A Modular Framework for Accelerating the Multi-Scale Simulation of Human Brain Dynamics|Global neural dynamics emerge from multi-scale brain structures, with neurons communicating through synapses to form transiently communicating networks. Network activity arises from intercellular communication that depends on the structure of connectome tracts and local connection, intracellular signalling cascades, and the extracellular molecular milieu that regulate cellular properties. Multi-scale models of brain function have begun to directly link the emergence of global brain dynamics in conscious and unconscious brain states to microscopic changes at the level of cells. In particular, AdEx mean-field models representing statistical properties of local populations of neurons have been connected following human tractography data to represent multi-scale neural phenomena in simulations using The Virtual Brain (TVB). While mean-field models can be run on personal computers for short simulations, or in parallel on high-performance computing (HPC) architectures for longer simulations and parameter scans, the computational burden remains high and vast areas of the parameter space remain unexplored. In this work, we report that our TVB-HPC framework, a modular set of methods used here to implement the TVB-AdEx model for GPU and analyze emergent dynamics, notably accelerates simulations and substantially reduces computational resource requirements. The framework preserves the stability and robustness of the TVB-AdEx model, thus facilitating finer resolution exploration of vast parameter spaces as well as longer simulations previously near impossible to perform. Given that simulation and analysis toolkits are made public as open-source packages, our framework serves as a template onto which other models can be easily scripted and personalized datasets can be used for studies of inter-individual variability of parameters related to functional brain dynamics.|arXiv.org|2023|10.48550/arXiv.2311.13337|A. Destexhe, Sandra Díaz-Pier, L. Kusch, V. Jirsa, Jennifer S. Goldman, Michiel A. van der Vlag|0.0|2
1117|Accelerating Hyperdimensional Classifier with SYCL|Hyperdimensional (HD) computing is based on mathematical properties of high-dimensional spaces which show remarkable agreement with brain-controlled behaviors [1] . Rahimi et al. describe an HD-based classifier for the task of recognizing the languages of text samples [2] . It consists of an encoding module that generates a hypervector for each text sample and a search module that compares the generated vector with a set of trained hypervectors. One of the challenges of the HD computing research is that hardware simulation of the classifier is extremely time-consuming with many text samples. To address the challenge, the classifier may be modelled as a compute routine in Open Computing Language (OpenCL) and executed on graphics processing units (GPUs) for acceleration [3] . While OpenCL allows for writing parallel and portable programs targeting vendors’ computing platforms, writing an OpenCL program tends to be error-prone and time-consuming. Built on the underlying concepts, portability, and efficiency of OpenCL, SYCL defines a single-source abstract layer in C++ [4] . In this work, we adopt the SYCL abstraction for productivity and performance. Compared to the OpenCL application, the SYCL application approximately reduces the lines of code by 24% and increases the performance by 2.13X on four GPUs. In addition, the speedups of executing the application in parallel over the fastest serial execution on the four heterogeneous computing systems are approximately 2.11X, 1.23X, 1.56X, and 1.03X, respectively.|2023 IEEE International Conference on Cluster Computing Workshops (CLUSTER Workshops)|2023|10.1109/CLUSTERWorkshops61457.2023.00025|Zheming Jin, Jeffrey S. Vetter|0.0|2
1118|Efficient Construction of Voxel Models for Ore Bodies Using an Improved Winding Number Algorithm and CUDA Parallel Computing|The three-dimensional (3D) geological voxel model is essential for numerical simulation and resource calculation. However, it can be challenging due to the point in polygon test in 3D voxel modeling. The commonly used Winding number algorithm requires the manual setting of observation points and uses their relative positions to restrict the positive and negative solid angles. Therefore, we proposed the Winding number with triangle network coding (WNTC) algorithm and applied it to automatically construct a 3D voxel model of the ore body. The proposed WNTC algorithm encodes the stratum model by using the Delaunay triangulation network to constrain the index order of each vertex of the triangular plane unit. GPU parallel computing was used to optimize its computational speed. Our results demonstrated that the WNTC algorithm can greatly improve the efficiency and automation of 3D ore body modeling. Compared to the Ray casting method, it can compensate for a voxel loss of about 0.7%. We found the GPU to be 99.96% faster than the CPU, significantly improving voxel model construction speed. Additionally, this method is less affected by the complexity of the stratum model. Our study has substantial potential for similar work in 3D geological modeling and other relevant fields.|ISPRS Int. J. Geo Inf.|2023|10.3390/ijgi12120473|Min Ji, Lei Liu, Huimeng Wang, Yong Sun, Jiantao Liu|0.0|2
1119|NDARTS: A Differentiable Architecture Search Based on the Neumann Series|Neural architecture search (NAS) has shown great potential in discovering powerful and flexible network models, becoming an important branch of automatic machine learning (AutoML). Although search methods based on reinforcement learning and evolutionary algorithms can find high-performance architectures, these search methods typically require hundreds of GPU days. Unlike searching in a discrete search space based on reinforcement learning and evolutionary algorithms, the differentiable neural architecture search (DARTS) continuously relaxes the search space, allowing for optimization using gradient-based methods. Based on DARTS, we propose NDARTS in this article. The new algorithm uses the Implicit Function Theorem and the Neumann series to approximate the hyper-gradient, which obtains better results than DARTS. In the simulation experiment, an ablation experiment was carried out to study the influence of the different parameters on the NDARTS algorithm and to determine the optimal weight, then the best performance of the NDARTS algorithm was searched for in the DARTS search space and the NAS-BENCH-201 search space. Compared with other NAS algorithms, the results showed that NDARTS achieved excellent results on the CIFAR-10, CIFAR-100, and ImageNet datasets, and was an effective neural architecture search algorithm.|Algorithms|2023|10.3390/a16120536|Zifan Wang, Guohua Liu, Xiaoyu Han, Chenyu Li|0.0|2
1120|Hiperwalk: Simulation of Quantum Walks with Heterogeneous High-Performance Computing|The Hiperwalk package is designed to facilitate the simulation of quantum walks using heterogeneous high-performance computing, taking advantage of the parallel processing power of diverse processors such as CPUs, GPUs, and acceleration cards. This package enables the simulation of both the continuous-time and discrete-time quantum walk models, effectively modeling the behavior of quantum systems on large graphs. Hiperwalk features a user-friendly Python package frontend with comprehensive documentation, as well as a high-performance C-based inner core that leverages parallel computing for efficient linear algebra calculations. This versatile tool empowers researchers to better understand quantum walk behavior, optimize implementation, and explore a wide range of potential applications, including spatial search algorithms.|International Conference on Quantum Computing and Engineering|2023|10.1109/QCE57702.2023.00055|Anderson F.P. Santos, Gustavo A. Bezerra, Renato Portugal, Paulo Motta|0.0|2
1122|A Parametric Kinetic Solver for Simulating Boundary-Dominated Turbulent Flow Phenomena|Boundary layer flow plays a very important role in shaping the entire flow feature near and behind obstacles inside fluids. Thus, boundary treatment methods are crucial for a physically consistent fluid simulation, especially when turbulence occurs at a high Reynolds number, in which accurately handling thin boundary layer becomes quite challenging. Traditional Navier-Stokes solvers usually construct multi-resolution body-fitted meshes to achieve high accuracy, often together with near-wall and sub-grid turbulence modeling. However, this could be time-consuming and computationally intensive even with GPU accelerations. An alternative and much faster approach is to switch to a kinetic solver, such as the lattice Boltzmann model, but boundary treatment has to be done in a cut-cell manner, sacrificing accuracy unless grid resolution is much increased. In this paper, we focus on simulating the boundary-dominated turbulent flow phenomena with an efficient kinetic solver. In order to significantly improve the cut-cell-based boundary treatment for higher accuracy without excessively increasing the simulation resolution, we propose a novel parametric boundary treatment model, including a semi-Lagrangian scheme at the wall for non-equilibrium distribution functions, together with a purely link-based near-wall analytical mesoscopic model by analogy with the macroscopic wall modeling approach, which is yet simple to compute. Such a new method is further extended to handle moving boundaries, showing increased accuracy. Comprehensive analyses are conducted, with a variety of simulation results that are both qualitatively and quantitatively validated with experiments and real life scenarios, and compared to existing methods, to indicate superiority of our method. We highlight that our method not only provides a more accurate way for boundary treatment, but also a valuable tool to control boundary layer behaviors. This has not been achieved and demonstrated before in computer graphics, which we believe will be very useful in practical engineering.|ACM Transactions on Graphics|2023|10.1145/3618313|Mengyun Liu, Xiaopei Liu|0.0|2
1123|A Hardware Evaluation Framework for Large Language Model Inference|The past year has witnessed the increasing popularity of Large Language Models (LLMs). Their unprecedented scale and associated high hardware cost have impeded their broader adoption, calling for efficient hardware designs. With the large hardware needed to simply run LLM inference, evaluating different hardware designs becomes a new bottleneck. This work introduces LLMCompass, a hardware evaluation framework for LLM inference workloads. LLMCompass is fast, accurate, versatile, and able to describe and evaluate different hardware designs. LLMCompass includes a mapper to automatically find performance-optimal mapping and scheduling. It also incorporates an area-based cost model to help architects reason about their design choices. Compared to real-world hardware, LLMCompass' estimated latency achieves an average 10.4% error rate across various operators with various input sizes and an average 4.1% error rate for LLM inference. With LLMCompass, simulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference can be done within 16 minutes on commodity hardware, including 26,400 rounds of the mapper's parameter search. With the aid of LLMCompass, this work draws architectural implications and explores new cost-effective hardware designs. By reducing the compute capability or replacing High Bandwidth Memory (HBM) with traditional DRAM, these new designs can achieve as much as 3.41x improvement in performance/cost compared to an NVIDIA A100, making them promising choices for democratizing LLMs. LLMCompass is planned to be fully open-source.|arXiv.org|2023|10.48550/arXiv.2312.03134|August Ning, D. Wentzlaff, Hengrui Zhang, Rohan Prabhakar|0.0|2
1126|Real-time Height-field Simulation of Sand and Water Mixtures|We propose a height-field-based real-time simulation method for sand and water mixtures. Inspired by the shallow-water assumption, our approach extends the governing equations to handle two-phase flows of sand and water using height fields. Our depth-integrated governing equations can model the elastoplastic behavior of sand, as well as sand-water-mixing phenomena such as friction, diffusion, saturation, and momentum exchange. We further propose an operator-splitting time integrator that is both GPU-friendly and stable under moderate time step sizes. We have evaluated our method on a set of benchmark scenarios involving large bodies of heterogeneous materials, where our GPU-based algorithm runs at real-time frame rates. Our method achieves a desirable trade-off between fidelity and performance, bringing an unprecedentedly immersive experience for real-time applications.|ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia|2023|10.1145/3610548.3618159|Zherong Pan, Kui Wu, Mridul Aanjaneya, Haozhe Su, Siyu Zhang, Xifeng Gao|0.0|2
1127|GPU Accelerated Modelling and Real-time Rendering of Fluid Motion|This paper proposes a fluid rendering pipeline that uses OpenGL-4 shaders to employ the parallel processing capabilities of the GPU. The fluid’s surface mesh is produced using tessellation shader stages where the input patches are assigned tessellation levels based on the fluid heightmap’s curvature. The curvature is stored using a texture buffer object which allows access by shaders, thus allowing the tessellation calculations to be carried out in parallel. Use of this adaptive tessellation method increases both the simulation’s framerate as well as its capacity to handle a greater number of primitives. Furthermore, it more optimally distributes the mesh’s vertices to effectively increase the level of detail without using more primitives. Polygon culling using the geometry shader further optimises the number of primitives used to define the fluid surface. The Phong-Blinn model is used for surface lighting. We propose two GPU-based fluid surface flow visualisation methods. Texture buffer objects can be used to store and update a surface texture. Alternatively, particle positions are updated each frame using the geometry shader and stored in a buffer object using transform feedback. These flow visualisation techniques are particularly effective for communicating the swirling motion of vortices.|Image and Vision Computing New Zealand|2023|10.1109/IVCNZ61134.2023.10343884|R. Mukundan, William Valentine|0.0|2
1128|Efficient Online Learning of Contact Force Models for Connector Insertion|Contact-rich manipulation tasks with stiff frictional elements like connector insertion are difficult to model with rigid-body simulators. In this work, we propose a new approach for modeling these environments by learning a quasi-static contact force model instead of a full simulator. Using a feature vector that contains information about the configuration and control, we find a linear mapping adequately captures the relationship between this feature vector and the sensed contact forces. A novel Linear Model Learning (LML) algorithm is used to solve for the globally optimal mapping in real time without any matrix inversions, resulting in an algorithm that runs in nearly constant time on a GPU as the model size increases. We validate the proposed approach for connector insertion both in simulation and hardware experiments, where the learned model is combined with an optimization-based controller to achieve smooth insertions in the presence of misalignments and uncertainty. Our website featuring videos, code, and more materials is available at https://model-based-plugging.github.io/.|arXiv.org|2023|10.48550/arXiv.2312.09190|Yuval Tassa, Tom Erez, Keegan Go, Stefan Schaal, K. Tracy, Ajinkya Jain, Zachary Manchester|0.0|2
1130|Dual Connectivity in Heterogeneous Cellular Networks: Analysis of Optimal Splitting of Elastic File Transfers Using Flow-Level Performance Models|The dual connectivity feature in heterogeneous cellular networks can be used to improve the download performance for elastic applications by splitting a file transfer over two connections. We employ two parallel processor sharing queues along with a heavy-traffic approximation to develop an extended framework that allows for analysis of file download performance in dual connectivity enabled networks from a relatively less-investigated yet more tractable flow-level perspective rather than a packet-level perspective. Unlike existing models, the framework developed jointly accounts for different transmission capacities and utilizations of base stations, thus enabling a proper and comprehensive assessment of user-perceived file transfer delays. We analyze the optimum file splitting ratio for reducing download delays using convex optimization and validate our findings via both queueing network and flow-level wireless simulations. Our in-house flow-level wireless simulator takes into account user locations and macroscopic propagation characteristics of wireless channels in order to create a realistic evaluation environment; we observe that optimal splitting under heavy-traffic conditions can result in up to 60% reduction in download delays for commonly encountered wireless system specifications when the macrocell and small cell base stations operating with different transmission capacities both have high utilizations. We further illustrate that our flow-level model can successfully incorporate interfering sources and different transmit powers which can easily be subsumed into the transmission capacities used in the model. Overall, the results presented show that it is indeed crucial to consider different transmission capacities as well as utilizations of base stations when determining the optimum splitting ratio.|IEEE Access|2023|10.1109/ACCESS.2023.3342073|D. Arifler, J. O. Olaifa|0.0|2
1132|FCBench: Cross-Domain Benchmarking of Lossless Compression for Floating-Point Data|While both the database and high-performance computing (HPC) communities utilize lossless compression methods to minimize floating-point data size, a disconnect persists between them. Each community designs and assesses methods in a domain-specific manner, making it unclear if HPC compression techniques can benefit database applications or vice versa. With the HPC community increasingly leaning towards in-situ analysis and visualization, more floating-point data from scientific simulations are being stored in databases like Key-Value Stores and queried using in-memory retrieval paradigms. This trend underscores the urgent need for a collective study of these compression methods' strengths and limitations, not only based on their performance in compressing data from various domains but also on their runtime characteristics. Our study extensively evaluates the performance of eight CPU-based and five GPU-based compression methods developed by both communities, using 33 real-world datasets assembled in the Floating-point Compressor Benchmark (FCBench). Additionally, we utilize the roofline model to profile their runtime bottlenecks. Our goal is to offer insights into these compression methods that could assist researchers in selecting existing methods or developing new ones for integrated database and HPC applications.|arXiv.org|2023|10.48550/arXiv.2312.10301|Xinyu Chen, Yan Yan, Cynthia Freeman, Ian Beaver, Jianguo Wang, Jiannan Tian, Dingwen Tao|0.0|2
1133|Enhancing predictive capabilities in fusion burning plasmas through surrogate-based optimization in core transport solvers|This work presents the PORTALS framework, which leverages surrogate modeling and optimization techniques to enable the prediction of core plasma profiles and performance with nonlinear gyrokinetic simulations at significantly reduced cost, with no loss of accuracy. The efficiency of PORTALS is benchmarked against standard methods, and its full potential is demonstrated on a unique, simultaneous 5-channel (electron temperature, ion temperature, electron density, impurity density and angular rotation) prediction of steady-state profiles in a DIII-D ITER Similar Shape plasma with GPU-accelerated, nonlinear CGYRO. This paper also provides general guidelines for accurate performance predictions in burning plasmas and the impact of transport modeling in fusion pilot plants studies.|arXiv.org|2023|10.48550/arXiv.2312.12610|A. E. White, C. Holland, A. Saltzman, M. Balandat, S. Kantamneni, J. Candy, P. Rodriguez-Fernandez, S. Ament, N. T. Howard|0.0|2
1134|ZO-AdaMU Optimizer: Adapting Perturbation by the Momentum and Uncertainty in Zeroth-order Optimization|Lowering the memory requirement in full-parameter training on large models has become a hot research area. MeZO fine-tunes the large language models (LLMs) by just forward passes in a zeroth-order SGD optimizer (ZO-SGD), demonstrating excellent performance with the same GPU memory usage as inference. However, the simulated perturbation stochastic approximation for gradient estimate in MeZO leads to severe oscillations and incurs a substantial time overhead. Moreover, without momentum regularization, MeZO shows severe over-fitting problems. Lastly, the perturbation-irrelevant momentum on ZO-SGD does not improve the convergence rate. This study proposes ZO-AdaMU to resolve the above problems by adapting the simulated perturbation with momentum in its stochastic approximation. Unlike existing adaptive momentum methods, we relocate momentum on simulated perturbation in stochastic gradient approximation. Our convergence analysis and experiments prove this is a better way to improve convergence stability and rate in ZO-SGD. Extensive experiments demonstrate that ZO-AdaMU yields better generalization for LLMs fine-tuning across various NLP tasks than MeZO and its momentum variants.|arXiv.org|2023|10.48550/arXiv.2312.15184|Qingcai Chen, Xiangping Wu, Chuanyi Liu, Yukang Lin, Youcheng Pan, Shuoran Jiang, Xiaobao Song, Yang Xiang|0.0|2
1136|The Implementation of Gas-liquid Two-phase Flow Simulations with Surfactant Transport Based on GPU Computing and Adaptive Mesh Refinement|We proposed an implementation for surfactant transport simulations in gas-liquid two-phase flows. This implementation employs a tree-based interface-adapted adaptive mesh refinement (AMR) method, assigning a high-resolution mesh around the interface region, significantly reducing computational resources, such as memory and execution time. We developed GPU code by CUDA programming language for the AMR method to further enhance performance through GPU parallel computing. The piece-wise linear interface calculation (PLIC) method, an interface-capturing approach for two-phase flows, is implemented based on the tree-based AMR method and GPU computing. We adopted the height function (HF) method to calculate interface curvature for surface tension assessment to suppress the spurious currents, and implemented it on the AMR mesh as well. We incorporated the Langmuir model to describe surfactant transport, as well as surfactant adsorption and desorption at the gas-liquid interface. Our implementation was applied to simulate a two-dimensional process where a bubble freely rises to the liquid surface, forms a thin liquid film, and eventually results in the film’s rupture. This simulation confirmed a reduction in the number of mesh grids required with our proposed implementations.|HPC Asia Workshops|2024|10.1145/3636480.3636485|Takayuki Aoki, Shintaro Matsushita, Tongda Lian|0.0|2
1137|Development of a central-moment phase-field lattice Boltzmann model for thermocapillary flows: Droplet capture and computational performance|This study develops a computationally efficient phase-field lattice Boltzmann model with the capability to simulate thermocapillary flows. The model was implemented into the open-source simulation framework, waLBerla, and extended to conduct the collision stage using central moments. The multiphase model was coupled with both a passive-scalar thermal LB, and a RK solution to the energy equation in order to resolve temperature-dependent surface tension phenomena. Various lattice stencils (D3Q7, D3Q15, D3Q19, D3Q27) were tested for the passive-scalar LB and both the second- and fourth-order RK methods were investigated. There was no significant difference observed in the accuracy of the LB or RK schemes. The passive scalar D3Q7 LB discretisation tended to provide computational benefits, while the second order RK scheme is superior in memory usage. This paper makes contributions relating to the modelling of thermocapillary flows and to understanding the behaviour of droplet capture with thermal sources analogous to thermal tweezers. Four primary contributions to the literature are identified. First, a new 3D thermocapillary, central-moment phase-field LB model is presented and implemented in the open-source software, waLBerla. Second, the accuracy and computational performance of various techniques to resolve the energy equation for multiphase, incompressible fluids is investigated. Third, the dynamic droplet transport behaviour in the presence of thermal sources is studied and insight is provided on the potential ability to manipulate droplets based on local domain heating. Finally, a concise analysis of the computational performance together with near-perfect scaling results on NVIDIA and AMD GPU-clusters is shown. This research enables the detailed study of droplet manipulation and control in thermocapillary devices.||2024|10.2139/ssrn.4696417|T. Mitchell, Markus Holzer, Ulrich Ruede, Christopher R. Leonardi|0.0|2
1139|Fast generation of quantum dynamics data using a GPU implementation of the time-dependent Schrodinger equation|Efficient methods for generating samples of wave packet trajectories are needed to build machine learning models for quantum dynamics. However, simulating such data by direct integration of the time-dependent Schrodinger equation can be demanding, especially when multiple spatial dimensions and realistic potentials are involved. In this paper, we present a graphics processor unit (GPU) implementation of the finite-difference time-domain (FDTD) method for simulating the time-dependent Schrodinger equation. The performance of our implementation is characterized in detail by simulating electron diffraction from realistic material surfaces. On our hardware, our GPU implementation achieves a roughly 350 times performance increase compared to a serial CPU implementation. The suitability of our implementation for generating samples of quantum dynamics data is also demonstrated by performing electron diffraction simulations from multiple configurations of an organic thin film. By studying how the structure of the data converges with sample sizes, we acquire insights into the sample sizes required for machine learning purposes.||2024|10.1109/qce60285.2024.10313|Daniel M. Packwood, Rei Nagaya, Haruki Omatsu|0.0|2
1143|Feasibility Study of Multi-Agent Simulation at the Cellular Level with FLAME GPU|Multi-Agent Systems (MAS) are a common approach to simulating biological systems. Multi-agent modelling provides a natural method for describing individual level behaviours of cells. However, the computation cost of simulating behaviours at an individual level is considerably larger than top down equation based modelling approaches. A recent possibility to improve computational performance is the use of Graphics Processing Units (GPUs) to provide the necessary parallel computing power. In this paper we show that multi-agent models describing biological systems at cellular level are well suited to GPU acceleration. Cellular level systems are characterised by vast numbers of agents that intensively communicate, indirectly through diffusion of chemical substances, or directly, through connection of chemical receptors. We present a study which utilises the FLAME GPU software to target a MAS model of a generic pathogen induced infection to validate the suitability of the GPU for simulation of a broader class of cellular level systems.|The Florida AI Research Society|2016|10.1007/978-3-319-33509-4_34|A. P. Oliveira, P. Richmond|0.0|2
1150|SU-C-BRB-05: Determining the Adequacy of Auto-Contouring Via Probabilistic Assessment of Ensuing Treatment Plan Metrics in Comparison with Manual Contours.|PURPOSE\nTo determine if auto-contour and manual-contour-based plans differ when evaluated with respect to probabilistic coverage metrics and biological model endpoints for prostate IMRT.\n\n\nMETHODS\nManual and auto-contours were created for 149 CT image sets acquired from 16 unique prostate patients. A single physician manually contoured all images. Auto-contouring was completed utilizing Pinnacle's Smart Probabilistic Image Contouring Engine (SPICE). For each CT, three different 78 Gy/39 fraction 7-beam IMRT plans are created; PD with drawn ROIs, PAS with auto-contoured ROIs, and PM with auto-contoured OARs with the manually drawn target. For each plan, 1000 virtual treatment simulations with different sampled systematic errors for each simulation and a different sampled random error for each fraction were performed using our in-house GPU-accelerated robustness analyzer tool which reports the statistical probability of achieving dose-volume metrics, NTCP, TCP, and the probability of achieving the optimization criteria for both auto-contoured (AS) and manually drawn (D) ROIs. Metrics are reported for all possible cross-evaluation pairs of ROI types (AS,D) and planning scenarios (PD,PAS,PM). Bhattacharyya coefficient (BC) is calculated to measure the PDF similarities for the dose-volume metric, NTCP, TCP, and objectives with respect to the manually drawn contour evaluated on base plan (D-PD).\n\n\nRESULTS\nWe observe high BC values (BC≥0.94) for all OAR objectives. BC values of max dose objective on CTV also signify high resemblance (BC≥0.93) between the distributions. On the other hand, BC values for CTV's D95 and Dmin objectives are small for AS-PM, AS-PD. NTCP distributions are similar across all evaluation pairs, while TCP distributions of AS-PM, AS-PD sustain variations up to %6 compared to other evaluated pairs.\n\n\nCONCLUSION\nNo significant probabilistic differences are observed in the metrics when auto-contoured OARs are used. The prostate auto-contour needs improvement to achieve clinically equivalent plans.|Medical Physics (Lancaster)|2016|10.1118/1.4955559|J. Siebers, M. Ahmad, H. Nourzadeh, W. Watkins|0.0|2
1151|Real-time Inextensible Hair with Volume and Shape|Hair simulation is a common topic extensively studied in computer graphics. One of the many challenges in this field is simulating realistic hair in a real-time environment. In this paper, we propose a unified simulation scheme to consider three of the key features in hair simulation; inextensibility, shape preservation and hair-hair interaction. We use an extension to the Dynamic Follow the Leader (DFTL) method to include shape preservation. Our implementation is also coupled with a Lagrangian approach to address the hair-hair interaction dynamics. A GPU-friendly scheme is proposed that is able to exploit the massive parallelism these devices offer, being able to simulate thousands of strands in real-time. The method has been integrated in a game development platform with a shading model for rendering and several test applications have been developed using this implementation.|Spanish Computer Graphics Conference|2015|10.2312/ceig.20151194|I. García-Fernández, R. M. Sánchez-Banderas, H. Barreiro, Mariano Pérez|0.0|2
1153|Comparing parallel simulation of single and multi-compartmental spiking neuron models using gpgpu|Characterizing neural responses and behavior require large scale simulation of brain circuits. Spatio-temporal information processing in large scale neural simulations often require compromises between computing resources and realistic details to be represented. In this work, we compared the implementations of point neuron models and biophysically detailed neuron models on serial and parallel hardware. GPGPU like architectures provide improved run time performance for multi compartmental Hodgkin-Huxley (HH) type neurons in a computationally cost effective manner. Single compartmental Adaptive Exponential Integrate and Fire (AdEx) model implementations, both in CPU and GPU outperformed embarrassingly parallel implementation of multi compartmental HH neurons. Run time gain of CPU implementation of AdEx cluster was approximately 10 fold compared to the GPU implementation of 10-compartmental HH neurons. GPU run time gain for Adex against GPU run time gain for HH was around 35 fold. The results suggested that careful selection of the neural model, capable enough to represent the level of details expected, is a significant parameter for large scale neural simulations.|International Conference on Advances in Computing, Communications and Informatics|2017|10.1109/ICACCI.2017.8125894|Krishnapriya Ushakumari, Manjusha Nair, Athira Ramakrishnan, B. Nair|0.0|2
1154|GPGPU Computing for Microscopic Simulations of Crowd Dynamics|We compare GPGPU implementations of two popular models of crowd dynamics. Specifically, we consider a continuous social force model, based on differential equations (molecular dynamics) and a discrete social distances model based on non-homogeneous cellular automata. For comparative purposes both models have been implemented in two versions: on the one hand using GPGPU technology, on the other hand using CPU only. We compare some significant characteristics of each model, for example: performance, memory consumption and issues of visualization. We also propose and test some possibilities for tuning the proposed algorithms for efficient GPU computations.|Computing and informatics|2016|10.1007/978-3-319-33482-0_39|P. Topa, Jarosław Wąs, Hubert Mróz|0.0|2
1155|LiteMaker: Interactive Luminaire Development using Progressive Photon Tracing and Multi-Resolution Upsampling|Industrial applications like luminaire development (the creation of a luminaire in terms of geometry and material) or lighting design (the efficient and aesthetic placement of luminaires in a virtual scene) rely heavily on high realism and physically correct simulations. Using typical approaches like CAD modeling and offline rendering, this requirement induces long processing times and therefore inflexible workflows. In this paper, we combine a GPU-based progressive photon-tracing algorithm to accurately simulate the light distribution of a luminaire with a novel multi-resolution image-filtering approach that produces visually meaningful intermediate results of the simulation process. By using this method in a 3D modeling environment, luminaire development is turned into an interactive process, allowing for real-time modifications and immediate feedback on the light distribution. Since the simulation results converge to a physically plausible solution that can be imported as a representation of a luminaire into a light-planning software, our work contributes to combining the two former decoupled workflows of luminaire development and lighting design, reducing the overall production time and cost for luminaire manufacturers. CCS Concepts •Computing methodologies → Ray tracing; Image processing; Mesh geometry models;|International Symposium on Vision, Modeling, and Visualization|2017|10.2312/vmv.20171253|Christian Luksch, M. Schwärzler, M. Wimmer, Katharina Krösl|0.0|2
1161|Accurate 3D mapping and immersive visualization for Search and Rescue|This paper concentrates on the topic of gathering, processing and presenting 3D data for use in Search and Rescue operations. The data are gathered by unmanned ground platforms, in form of 3D point clouds. The clouds are matched and transformed into a consistent, highly accurate 3D model. The paper describes the pipeline for such matching based on Iterative Closest Point algorithm supported by loop closing done with LUM method. The pipeline was implemented for parallel computation with Nvidia CUDA, which leads to higher matching accuracy and lower computation time. An analysis of performance for multiple GPUs is presented. The second problem discussed in the paper is immersive visualization of 3d data for search and rescue personnel. Five strategies are discussed: plain 3D point cloud, hypsometry, normal vectors, space descriptors and an approach based on light simulation through the use of NVIDIA OptiX Ray Tracing Engine. The results from each strategy were shown to end users for validation. The paper discusses the feedback given. The results of the research are used in the development of a support module for ICARUS project.|International Workshop on Robot Motion and Control|2015|10.1109/RoMoCo.2015.7219728|J. Będkowski, Piotr Majek, M. Pelka, P. Musialik, A. Typiak, K. Majek, A. Maslowski|0.0|2
1165|New geometric algorithms and data structures for collision detection of dynamically deforming objects|Any virtual environment that supports interactions between virtual objects and/or a user and objects, \nneeds a collision detection system to handle all interactions in a physically correct or plausible way. A \ncollision detection system is needed to determine if objects are in contact or interpenetrates. These \ninterpenetrations are resolved by a collision handling system. Because of the fact, that in nearly all \nsimulations objects can interact with each other, collision detection is a fundamental technology, that \nis needed in all these simulations, like physically based simulation, robotic path and motion planning, \nvirtual prototyping, and many more. Most virtual environments aim to represent the real-world as \nrealistic as possible and therefore, virtual environments getting more and more complex. Furthermore, \nall models in a virtual environment should interact like real objects do, if forces are applied to the \nobjects. Nearly all real-world objects will deform or break down in its individual parts if forces are \nacted upon the objects. Thus deformable objects are becoming more and more common in virtual \nenvironments, which want to be as realistic as possible and thus, will present new challenges to the \ncollision detection system. The necessary collision detection computations can be very complex and this \nhas the effect, that the collision detection process is the performance bottleneck in most simulations. \nMost rigid body collision detection approaches use a BVH as acceleration data structure. This \ntechnique is perfectly suitable if the object does not change its shape. For a soft body an update step \nis necessary to ensure that the underlying acceleration data structure is still valid after performing a \nsimulation step. This update step can be very time consuming, is often hard to implement and in most \ncases will produce a degenerated BVH after some simulation steps, if the objects generally deform. \nTherefore, the here presented collision detection approach works entirely without an acceleration data \nstructure and supports rigid and soft bodies. Furthermore, we can compute inter-object and intraobject \ncollisions of rigid and deformable objects consisting of many tens of thousands of triangles in a \nfew milliseconds. To realize this, a subdivision of the scene into parts using a fuzzy clustering approach \nis applied. Based on that all further steps for each cluster can be performed in parallel and if desired, \ndistributed to different GPUs. Tests have been performed to judge the performance of our approach \nagainst other state-of-the-art collision detection algorithms. Additionally, we integrated our approach \ninto Bullet, a commonly used physics engine, to evaluate our algorithm. \nIn order to make a fair comparison of different rigid body collision detection algorithms, we propose \na new collision detection Benchmarking Suite. Our Benchmarking Suite can evaluate both the performance \nas well as the quality of the collision response. Therefore, the Benchmarking Suite is subdivided \ninto a Performance Benchmark and a Quality Benchmark. This approach needs to be extended to \nsupport soft body collision detection algorithms in the future.||2015|10.21268/20151117-122639|David Mainzer|0.0|2
1166|Enabling simulation of high‐dimensional micro‐macro biophysical models through hybrid CPU and multi‐GPU parallelism|Micro‐macro models provide a powerful tool to study the relationship between microscale mechanisms and emergent macroscopic behavior. However, the detailed microscopic modeling requires tracking and evolving a potentially high‐dimensional configuration space at high computational cost. In this work, we present a novel parallel algorithm for simulating a high‐dimensional micro‐macro model of a gliding motility assay. We utilize a holistic approach aligning the data residency and simulation scales with the hybrid CPU and multi‐GPU hardware. Our novel approach achieves a speedup factor of 9.25× over previous GPU‐accelerated micro‐macro methods on the same hardware. Furthermore, by decoupling dependencies in the microstructure update, we are able to efficiently distribute the microstructure over multiple GPUs with minimal overhead. We test on up to four GPUs and observe excellent scaling, suggesting that significant further speedups are achievable with additional GPUs. Our approach enables micro‐macro simulations of higher complexity and resolution than would otherwise be feasible.|Concurrency and Computation|2019|10.1002/cpe.6305|Tamar Shinar, Steven C. Cook|0.0|2
1167|Special issue: IV ECCOMAS thematic conference on computational vision and medical image processing (VipIMAGE)|et al. based on a fourth-order total variation flow; Lachiondo et al. investigate the use of Legendre moments as biomarkers for an efficient and accurate classification of bone tissue on images taking into account computational methods and GPU acceleration; methods proposed for the identification of fractured bone tissue from CT images are reviewed by Paulano et al.; Ricordeau and Mellouli explore the potential ability of a 3D-skeleton coupled with a statistical tensor analysis to locally describe the trabecular structure for binary images towards the simulation of bone remodelling process; Maheshwari et al. propose a robust and accurate method for the reconstruction of standard 12-lead system from Frank vectorcardiographic system; a comparative experiment vs. modelling analysis of the strain-line pattern in the human left ventricle is presented by Evangelista et al.; the automatic tracking and deformation analysis of red blood cells flowing through a microchannel with a microstenosis is studied by Taboada et al.; Tichý et al. suggest a model to extract input and organ functions in dynamic scintigraphic imaging; a new imaging biomarker of prostate cancer aggressiveness based on MR-T2-weighted signal intensity is proposed by Giannini et al.; and finally, Binaghi et al. propose a novel behavioural comparison strategy specifically oriented to accuracy assessment in MRI glial tumour segmentation studies. The Guest Editors wish to thank all the VipIMAGE 2013 authors and members of the Programme Committee for sharing their expertise, the International European Community on Computational Methods in Applied Sciences (ECCOMAS), the Portuguese Association of Theoretical, Applied and Computational Mechanics (APTMAC), the “Universidade do Porto” (U.Porto) and the “Instituto de Ciência e Inovação em Engenharia Mecânica e Engenharia Industrial” (INEGI) for supporting VipIMAGE, the authors of the selected manuscripts and also to the CMBBE: Imaging & Visualization Editors and Reviewers for helping improve the manuscripts accepted.|Comput. methods Biomech. Biomed. Eng. Imaging Vis.|2016|10.1080/21681163.2016.1175759|R. Jorge, J. Tavares|0.0|2
1169|Real-time optical flow estimation on a GPU for a skied-steered mobile robot|Accurate egomotion estimation is required for mobile robot navigation. Often the egomotion is estimated using optical flow algorithms. For an accurate estimation of optical flow most of modern algorithms require high memory resources and processor speed. However simple single-board computers that control the motion of the robot usually do not provide such resources. On the other hand, most of modern single-board computers are equipped with an embedded GPU that could be used in parallel with a CPU to improve the performance of the optical flow estimation algorithm. This paper presents a new Z-flow algorithm for efficient computation of an optical flow using an embedded GPU. The algorithm is based on the phase correlation optical flow estimation and provide a real-time performance on a low cost embedded GPU. The layered optical flow model is used. Layer segmentation is performed using graph-cut algorithm with a time derivative based energy function. Such approach makes the algorithm both fast and robust in low light and low texture conditions. The algorithm implementation for a Raspberry Pi Model B computer is discussed. For evaluation of the algorithm the computer was mounted on a Hercules mobile skied-steered robot equipped with a monocular camera. The evaluation was performed using a hardware-in-the-loop simulation and experiments with Hercules mobile robot. Also the algorithm was evaluated using KITTY Optical Flow 2015 dataset. The resulting endpoint error of the optical flow calculated with the developed algorithm was low enough for navigation of the robot along the desired trajectory.|Photonics Europe|2016|10.1117/12.2227556|V. Kniaz|0.0|2
1176|A Particle-Based Real-Time CG Rendering of Carbonated Water with Automatic Release of Bubbles|For virtual reality (VR) environment with interactivity, this paper proposes a real-time computer graphics (CG) rendering method, which can represent bubbles released in carbonated water. In the method, two types of bubbles are considered, which are, generated on the liquid surface at the water pouring, and ones generated in a container. The proposed method consists of 1) simulation of bubbles' behavior and 2) real time rendering. In the first step, we introduce the smoothed particle hydrodynamics (SPH) method into the simulation of the bubbles' behavior, which can calculate particles' movement in the deformable water at relatively low computational cost. Also the SPH method is executed under the parallel computing on a graphics processing unit (GPU). In addition, in this method we propose bubbles generation model using state transition of particles. This model consider that keeping total mass and volume constant. In the second step, we also use the GPU to render CG images in real-time. The proposed method is evaluated by three aspects, reality, real-time and interactivity. By comparing with photographs/movies taken in the real world, we confirm that the proposed method has necessary reality. Also we confirm real-time computation with interactive manipulation. Therefore we conclude that the proposed method can be applied in a VR environment where some glasses/cups filled with poured carbonated water.|International Conference on Cyberworlds|2015|10.1109/CW.2015.62|M. Makino, K. Yanai|0.0|2
1178|Exploiting Hierarchical Parallelism in an Astrophysical Equation of State Using OpenACC and OpenMP|Modeling thermonuclear supernovae is a premier application for leadership-class supercomputers and requires multi-physics simulation codes to capture hydrodynamics, nuclear burning, gravitational forces, etc. As a nuclear detonation burns through the stellar material, it also increases the temperature. An equation of state (EOS) is then required to determine, for example, the new pressure associated with this temperature increase. In fact, an EOS is needed after the thermodynamic conditions are changed by any physics routines. This means it is called many times throughout a simulation, requiring the need for a fast EOS implementation. Fortunately, these calculations can be performed independently during each time step, so the work can be offloaded to GPUs. Using results from the IBM/NVIDIA early test system (Summitdev, a precursor to the upcoming Summit supercomputer) at Oak Ridge National Laboratory, we describe a hybrid OpenMP implementation with offloaded work to GPUs. We compare performance results between the two implementations, with a discussion of some of the currently available features of OpenACC and OpenMP 4.5.|International Conference on Parallel Computing|2017|10.3233/978-1-61499-843-3-517|O. E. Bronson Messer, T. Papatheodore|0.0|2
1180|SU-G-TeP1-06: Fast GPU Framework for Four-Dimensional Monte Carlo in Adaptive Intensity Modulated Proton Therapy (IMPT) for Mobile Tumors.|PURPOSE\nTo demonstrate the feasibility of fast Monte Carlo (MC) treatment planning and verification using four-dimensional CT (4DCT) for adaptive IMPT for lung cancer patients.\n\n\nMETHODS\nA validated GPU MC code, gPMC, has been linked to the patient database at our institution and employed to compute the dose-influence matrices (Dij) on the planning CT (pCT). The pCT is an average of the respiratory motion of the patient. The Dijs and patient structures were fed to the optimizer to calculate a treatment plan. To validate the plan against motion, a 4D dose distribution averaged over the possible starting phases is calculated using the 4DCT and a model of the time structure of the delivered spot map. The dose is accumulated using vector maps created by a GPU-accelerated deformable image registration program (DIR) from each phase of the 4DCT to the reference phase using the B-spline method. Calculation of the Dij matrices and the DIR are performed on a cluster, with each field and vector map calculated in parallel.\n\n\nRESULTS\nThe Dij production takes ∼3.5s per beamlet for 10e6 protons, depending on the energy and the CT size. Generating a plan with 4D simulation of 1000 spots in 4 fields takes approximately 1h. To test the framework, IMPT plans for 10 lung cancer patients were generated for validation. Differences between the planned and the delivered dose of 19% in dose to some organs at risk and 1.4/21.1% in target mean dose/homogeneity with respect to the plan were observed, suggesting potential for improvement if adaptation is considered.\n\n\nCONCLUSION\nA fast MC treatment planning framework has been developed that allows reliable plan design and verification for mobile targets and adaptation of treatment plans. This will significantly impact treatments for lung tumors, as 4D-MC dose calculations can now become part of planning strategies.|Medical Physics (Lancaster)|2016|10.1118/1.4956996|S. Jiang, C. Grassberger, P. Botas, H. Paganetti, N. Qin, G. Sharp, X. Jia|0.0|2
1182|CUDA Deformers for Model Reduction|Real-time deformable object simulation is important in interactive applications such as games and virtual reality. One common approach to achieve speed is to employ model reduction, a technique whereby the equations of motion of a deformable object are projected to a suitable low-dimensional space. Improving the real-time performance of model-reduced systems has been the subject of much research. While modern GPUs play an important role in real-time simulation and parallel computing, existing model reduction systems typically utilize CPUs and seldom employ GPUs. We give a method to efficiently employ GPUs for vertex position computation in model-reduced simulations. Our CUDA-based algorithm gives a substantial speedup compared to a CPU implementation, thanks to our system architecture that employs a memory layout friendly to GPU memory, reduces the communication between the CPU and GPU, and enables the CPU and GPU to work in parallel.|Motion in Games|2020|10.1145/3424636.3426895|J. Barbič, Bohan Wang|0.0|2
1186|Efficient numerical methods for the simulation of particulate and liquid-solid flows|In this work a set of efficient numerical methods for the simulation of particulate flows and virtual prototyping applications are proposed. These methods are implemented as modular components in the FEATFLOW software package which is used as the fluid flow solver. In direct particulate flow simulations the calculation of the hydrodynamic forces acting on the particles is of central importance. For this task acceleration techniques are proposed based on hierarchical spatial partitioning. For arbitrary shaped particles the usage of distance maps to rapidly process the needed geometric information is employed and analyzed. In case of collisions between the particles it is shown how these same structures can be used to efficiently handle the collision broad phase and narrow phase. The computation of collision forces in the proposed particulate flow solving scheme can be handled by several collision models. The used models are based on a constrained-based formulation which leads to a linear complementarity problem (LCP). Another approach is added into the particulate flow solver that is based on the discrete element method (DEM). This approach is suited very well to an Implementation on graphic processing units (GPU) as the particles can be handled independently and thus excellent use of the massive parallel computing capabilities of the GPU can be made. In order to extend the DEM to handle non-spherical particles or rigid bodies, an inner sphere representation of such shapes is used. Furthermore, a mesh adaptation technique to increase the numerical efficiency of the CFD-simulations is shown which is based on Laplacian smoothing with special weights. The proposed techniques are validated in various benchmark configurations or comparisons to experimental data.||2016|10.17877/de290r-17216|R. Münster|0.0|2
1193|Markerless Eye-Hand Kinematic Calibration on the iCub Humanoid Robot|Humanoid robots are resourceful platforms and can be used in diverse application scenarios. However, their high number of degrees of freedom (i.e., moving arms, head and eyes) deteriorates the precision of eye-hand coordination. A good kinematic calibration is often difficult to achieve, due to several factors, e.g., unmodeled deformations of the structure or backlash in the actuators. This is particularly challenging for very complex robots such as the iCub humanoid robot, which has 12 degrees of freedom and cable-driven actuation in the serial chain from the eyes to the hand. The exploitation of real-time robot sensing is of paramount importance to increase the accuracy of the coordination, for example, to realize precise grasping and manipulation tasks. In this code paper, we propose an online and markerless solution to the eye-hand kinematic calibration of the iCub humanoid robot. We have implemented a sequential Monte Carlo algorithm estimating kinematic calibration parameters (joint offsets) which improve the eye-hand coordination based on the proprioception and vision sensing of the robot. We have shown the usefulness of the developed code and its accuracy on simulation and real-world scenarios. The code is written in C++ and CUDA, where we exploit the GPU to increase the speed of the method. The code is made available online along with a Dataset for testing purposes.|Frontiers in Robotics and AI|2018|10.3389/frobt.2018.00046|L. Jamone, Alexandre Bernardino, Pedro Vicente|0.0|2
1195|Large scale circuit simulation exploiting combinatorial multigrid on massively parallel architectures|The complexity of modern very large scale integrated circuits renders circuit simulation very essential in the design process, as it is the only feasible way to verify circuit's behaviour prior to manufacturing. The heart of circuit simulation relies on the solution of huge systems resulting after the modelling using Modified Nodal Analysis. Matrices arising in those systems are sparse Symmetric Diagonally Dominant (SDD) matrices, and as a result, iterative methods for efficiently manipulating them are very crucial to the performance of the simulation. In recent years, the emphasis has been placed on preconditioning methods which reduce the number of iterations for solving such systems, while significant advancements have been accomplished in efficient implementations on massively parallel architectures like GPUs. This paper presents a GPU-accelerated simulator, based on the Preconditioned Conjugate Gradient method, which exploits the Combinatorial Multigrid, a reliably efficient SDD solver based on support theory principles, for fast DC and transient analysis of large-scale circuits. Experimental results on IBM power grids, demonstrate speedups up to 4.56x and 5.10x for the PCG method and the CMG preconditioning algorithm, respectively, compared to the optimized CPU implementations.|International Conference on Modern Circuits and Systems Technologies|2018|10.1109/MOCAST.2018.8376654|Dimitrios Garyfallou, N. Evmorfopoulos, G. Stamoulis|0.0|2
1198|MICHELLE for high-level optimization, large scale problems and HPC environments|New capability has been implemented in the MICHELLE code and its environment over the past two years in support of rapid device design and optimization. Central to this is the adoption of High Performance Computing (HPC) tools and the porting of the code to HPC clusters. The solver now supports distributed- memory, MPI-based domain decomposition. The development also supports per-node accelerators such as GPUs and multicore CPUs. The code can handle large- scale models supporting the ability to simulate multiple beam devices, such as MBK guns and MB-IOTs, and at adequate resolution to properly resolve the dynamics. Significant to this development has been the integration of MICHELLE with the CAPSTONE mesher, the ParaView data visualizer, and the GSB/DAKOTA execution environment and optimization library, which enables the rapid device design and optimization process. With the current development program in its final stage, this presentation provides an overview of the new capability that has been achieved.|International Vacuum Electronics Conference|2018|10.1109/IVEC.2018.8391477|J. Petillo, A. Burke, S. Cooke, A. Jensen, Ben Held Alan Nichols, E. Nelson, S. Ovtchinnikov, G. Stantchev|0.0|2
1200|Framework for the hybrid parallelisation of simulation codes|Therefore, we present a framework for hybrid parallelisation which is based on a job model that allows the user to incorporate sequential code with manageable effort and code modifications in order to be executed in parallel. The primary application domain of this framework are simulation codes from engineering disciplines as those are in many cases still sequential and as a result of their memory and runtime demands prominent candidates for parallelisation. While a solely multithreaded approach is quite easy to achieve, it usually does not scale with larger numbers of threads, lack of sufficient support in complex task design [1] and according to [2] discards properties such as predictability and determinism. In the case of hybrid parallelisation, i.e. the interplay of distributed and shared memory programming it becomes even worse, for instance as a result of insufficient thread safety within MPI calls [3]. Hence, multithread code within MPI programs needs special treatment in order to run properly on parallel and distributed environments, again, something the user does not need to take care of because these problems are addressed by our framework. The framework is based on a strict job scheduling, where such a job can be anything from a complete program up to a single instruction. Those jobs, together with their dependencies on the results of other jobs, are defined by the user on any desired level of granularity. The difference and main advantage compared with classical parallelisation is that the user does not need to care about communication and synchronisation of the single jobs as well as data distribution and load balancing which is all inherently carried out by the framework. This enables advancing from sequential to parallel codes with less effort as the complexity of the parallel program is (mostly) hidden from the user. Comparing the framework to an efficient and `pure' MPI implementation of a Jacobi solver for linear equation systems shows already excellent results, especially when considering that the parallel implementation using our framework has been derived from a sequential version with almost no code changes. As the framework offers a plenitude of further possibilities, future steps could comprise basic monitoring and fault tolerance properties as well as its application using different hardware such as GPUs or the Cell Broadband Engine.|arXiv.org|2018|10.4203/ccp.95.53|Marko Ljucovic, E. Rank, R. Mundani|0.0|2
1207|NVIDIA OptiX ray-tracing engine as a new tool for modelling medical imaging systems|The most accurate technique to model the X- and gamma radiation path through a numerically defined object is the Monte Carlo simulation which follows single photons according to their interaction probabilities. A simplified and much faster approach, which just integrates total interaction probabilities along selected paths, is known as ray tracing. Both techniques are used in medical imaging for simulating real imaging systems and as projectors required in iterative tomographic reconstruction algorithms. These approaches are ready for massive parallel implementation e.g. on Graphics Processing Units (GPU), which can greatly accelerate the computation time at a relatively low cost. In this paper we describe the application of the NVIDIA OptiX ray-tracing engine, popular in professional graphics and rendering applications, as a new powerful tool for X- and gamma ray-tracing in medical imaging. It allows the implementation of a variety of physical interactions of rays with pixel-, mesh- or nurbs-based objects, and recording any required quantities, like path integrals, interaction sites, deposited energies, and others. Using the OptiX engine we have implemented a code for rapid Monte Carlo simulations of Single Photon Emission Computed Tomography (SPECT) imaging, as well as the ray-tracing projector, which can be used in reconstruction algorithms. The engine generates efficient, scalable and optimized GPU code, ready to run on multi GPU heterogeneous systems. We have compared the results our simulations with the GATE package. With the OptiX engine the computation time of a Monte Carlo simulation can be reduced from days to minutes.|Medical Imaging|2015|10.1117/12.2082349|M. Cieślar, K. Kacperski, Jakub Pietrzak|0.0|2
1209|Geometry and energy constrained projection extension.|BACKGROUND\nIn clinical computed tomography (CT) applications, when a patient is obese or improperly positioned, the final tomographic scan is often partially truncated. Images directly reconstructed by the conventional reconstruction algorithms suffer from severe cupping and direct current bias artifacts. Moreover, the current methods for projection extension have limitations that preclude incorporation from clinical workflows, such as prohibitive computational time for iterative reconstruction, extra radiation dose, hardware modification, etc.METHOD:In this study, we first established a geometrical constraint and estimated the patient habitus using a modified scout configuration. Then, we established an energy constraint using the integral invariance of fan-beam projections. Two constraints were extracted from the existing CT scan process with minimal modification to the clinical workflows. Finally, we developed a novel dual-constraint based optimization model that can be rapidly solved for projection extrapolation and accurate local reconstruction.\n\n\nRESULTS\nBoth numerical phantom and realistic patient image simulations were performed, and the results confirmed the effectiveness of our proposed approach.\n\n\nCONCLUSION\nWe establish a dual-constraint-based optimization model and correspondingly develop an accurate extrapolation method for partially truncated projections. The proposed method can be readily integrated into the clinical workflow and efficiently solved by using a one-dimensional optimization algorithm. Moreover, it is robust for noisy cases with various truncations and can be further accelerated by GPU based parallel computing.|Journal of X-Ray Science and Technology|2018|10.3233/XST-18383|Qian Wang, K. Sen Sharma, Hengyong Yu|0.0|2
1217|Special issue on Biomechanical and Parametric Modeling of Human Anatomy (PMHA-2015)|In our ongoing Parametric Modeling of Human Anatomy (PMHA) workshop series, the third workshop (PMHA-2015) was held from 29 to 30 August 2015 in Montreal, Canada. The international workshop brought together computer scientists, dentists, engineers, linguists, anatomists, oro-facial surgeons, among others, to share techniques and approaches to modelling the biomechanics of the human body and applying these models to new biomedical and human-factor applications. Continuing the tradition of the annual PMHA workshop series, this special issue of the journal Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization (CMBBE: Imaging & Visualization) consists of 62 papers drawn from research introduced at PMHA-2015. These articles were reviewed according to the CMBBE: Imaging & Visualization policy. Biomechanical modelling of the human body covers a wide range of research activities as was evident in the topics at the PMHA-2015 workshop. As in the previous year, discussion topics included: measuring structural anatomy through magnetic resonance imaging (MRI) and X-ray computed tomography; measuring functional movements through dynamic MRI, video fluoroscopy, optical motion analysis, simulating fluid flow for swallowing and speech production; and simulating musculoskeletal dynamics of the neck, spine and lower limb; simulating tissue mechanics for the tongue, soft-palate and upper extremity; and visualising anatomy with 3D rendering, 3D atlases and virtual reality. Sessions were divided into anatomy, upper airway function, imaging data to models, speech and biomechanical modelling. An afternoon workshop on advanced biomechanical modelling with the ArtiSynth toolkit (www.artisynth.org) was also held for participants. While the research that appeared at PHMA-2015 covered a wide range of topics, for this special issue, advances in musculoskeletal simulation and visualisation feature prominently. Starting with the work of Malakoutian et al., a new musculoskeletal model of the lumbar spine was developed in ArtiSynth that integrates lumbar vertebrae, non-linear inter-vertebral stiffness, muscles and abdominal pressure. Simulations were performed to estimate and visualise the effect of muscle forces on lumbar flexion and extension movements. Smith et al., report a new approach to efficiently compute cartilage contact pressure for multi-body models of joint contact. The paper proposes and evaluates an efficient collision detection algorithm that uses bounding-box hierarchies and a GPU implementation to gain an order of magnitude speed-up for simulating knee contact pressures. The last simulation-based paper in the special issue, by Shabani and Stavness, proposes a novel approach to include muscle co-contraction within musculoskeletal simulations. Rather than an arbitrary parameter to directly specific co-contraction of agonist and antagonist muscle groups, the authors use a target level of joint stiffness to encourage co-contraction during simulations, which has the advantage of being a measurable biomechanical parameter. The co-contraction scheme is demonstrated in the context of controlling postural stability in simulated stance with a full-body musculoskeletal model. The remaining papers in the special issue focus on new techniques to visualise and analyse movements of the upper airway, which is a challenging region of anatomy to measure and characterise. Starting with static anatomical structure, Stone et al. provide a comprehensive analysis of tongue musculature based on high-resolution 3D magnetic resonance images. The study analyses images from 14 participants, making it one of the largest digital anatomical analyses of the tongue reported to date. These data are presented with a number of visualisations to help illustrate the complex 3D structure and volume of each tongue muscle region. Incorporating tongue movements through cineMRI, Woo et al., report a spatio-temporal atlas of tongue movement. Similar ‘4D’ atlases have been developed for the heart, but this a first-of-its-kind atlas for the tongue, which enables new types of visualisation and statistical analysis. Tran et al., addressed an important clinical application in dysphagia research in their paper. They used annotated video-fluoroscopic image to visualise the changes to swallowing mechanics that take place after a specific dysphagia intervention: respiratory–swallow phase training. They were able to show that laryngeal vestibular closure was the main positive change brought about by the training. The final special issue paper, by Ho et al., takes swallowing visualisation|Comput. methods Biomech. Biomed. Eng. Imaging Vis.|2018|10.1080/21681163.2018.1443026|S. Fels, I. Stavness|0.0|2
1218|Estimating Gaussian Mixture Autoregressive model with Sequential Monte Carlo algorithm: A parallel GPU implementation|In this paper, we propose using Bayesian sequential Monte Carlo (SMC) algorithm to estimate the univariate Gaussian mixture autoregressive (GMAR) model. The prominent benefit of the Bayesian approach is that the stationarity restriction required by the GAMR model can be straightforwardly imposed via prior distribution. In addition, compared to MCMC (Markov Chain Monte Carlo) and other simulation based algorithms, the SMC is robust to multimodal posteriors, and capable of providing fast on-line estimation when new data is available. Furthermore, it has a linear computational complexity and is ready for parallelism. To demostrate the SMC, an empirical application with US GDP growth data is considered. After estimation, we conduct the Bayesian model selection to evaluate the empirical evidence for different GMAR models. To facilitate the realization of this compute-intensive estimation, we parallelize the SMC algorithm on a nVidia CUDA compatible Graphical Process Unit (GPU) card.||2015|10.4236/oalib.1102002|M. Yin|0.0|2
1228|GPU accelerated atmospheric chemical kinetics in the ECHAM/MESSy (EMAC) Earth system model|This paper presents an application of GPU accelerators in Earth system modeling. We focus on atmospheric chemical kinetics, one of the most computationally intensive tasks in climate–chemistry model simulations. We developed a software package that automatically generates CUDA kernels to numerically integrate atmospheric chemical kinetics in the global climate model ECHAM/MESSy Atmospheric Chemistry (EMAC), used to study climate change and air quality scenarios. A source-to-source compiler outputs a CUDA-compatible kernel by parsing the FORTRAN code generated by the Kinetic PreProcessor (KPP) general analysis tool. All Rosenbrock methods that are available in the KPP numerical library are supported. Performance evaluation, using Fermi and Pascal CUDAenabled GPU accelerators, shows achieved speed-ups of 4.5× and 20.4×, respectively, of the kernel execution time. A node-to-node real-world production performance comparison shows a 1.75× speed-up over the non-accelerated application using the KPP three-stage Rosenbrock solver. We provide a detailed description of the code optimizations used to improve the performance including memory optimizations, control code simplification, and reduction of idle time. The accuracy and correctness of the accelerated implementation are evaluated by comparing to the CPU-only code of the application. The median relative difference is found to be less than 0.000000001 % when comparing the output of the accelerated kernel the CPU-only code. The approach followed, including the computational workload division, and the developed GPU solver code can potentially be used as the basis for hardware acceleration of numerous geoscientific models that rely on KPP for atmospheric chemical kinetics applications.||2017|10.5194/gmd-2017-63|Michail Alvanos, Theodoros Christoudias|0.0|2
1238|Scattering Model for Ray Launching Tool, and Validation in 5.4GHz Indoor|In this paper, we show how RL techniques are extended to include the directive diffuse scattering model based on game engines in conjunction with the GPUs, and then compare channel characteristics using both simulations and measurements in 5.4 GHz in indoor environment, to validate our proposed scattering model.|Topical Conference on Antennas and Propagation in Wireless Communications|2018|10.1109/APWC.2018.8503795|A. Navarro, Byron Medina, D. Guevara|0.0|2
1241|Editorial Issue 29.5|This issue contains six papers. In the first paper, Christos Kyrlitsias and Despina Michael-Grigoriou, from the Cyprus University of Technology in Limassol, Cyprus, investigate the conformity to virtual humans in an immersive virtual environment using two experiments. In the first experiment, they study whether agents have social influence on the participants by conducting the Asch conformity experiment. In the second experiment, they use a similar method to study how the factors “agency” and “behavioral realism” affect social conformity. The results of the experiment show that conformity can be caused by virtual humans in immersive virtual environments. In the second paper, Yu Zhu, Shiying Li, Xi Luo, Kang Zhu, Qiang Fu, Huixing Gong, Xilin Chen, and Jingyi Yu, from the Shanghai Institute of Microsystem and Information Technology, ShanghaiTech University, in Shanghai, and the Chinese Academy of Sciences, in Beijing, China, propose SAVE (shared augmented virtual environment), a mixed-reality system that overlays the virtual world with real objects captured by a Kinect depth camera. They connect the virtual and real worlds with a bridge controller mounted on the Kinect and only need to calibrate the whole system once before use. Subsequently, they refine the depth map and exploit a GPU-based natural image matting method to obtain the real objects from cluttered scenes. In the synthetic mixed-reality world, they can render real and virtual objects in real time and handle the depth from both worlds properly. In the third paper, Umut Agil and Ugur Gudukbay, from Bilkent University, Ankara, Turkey, propose a saliency model that enables virtual agents to produce plausible gaze behavior. The model measures the effects of distinct saliency features implemented by examining the state-of-the-art perception studies. When predicting an agent's interest point, they compute the saliency scores by using a weighted sum function for other agents and environment objects in the field of view of the agent for each frame. Then, they determine the most salient entity for each agent in the scene; thus, agents gain a visual understanding of their environment. Besides, their model introduces new aspects to crowd perception, such as perceiving characters as groups of people, applying social norms on crowd gaze behavior, effects of agent personality on gaze, gaze copy phenomena, and effects of agent velocity on attention. In the fourth paper, Yao Lu, Shang Zhao, Naji Younes, and James K. Hahn, from George Washington University, Washington, United States, present a cost-effective and easy-to-use 3D body reconstruction system using consumer-grade depth sensors, which provides reconstructed body shapes with a high degree of accuracy and reliability appropriate for medical applications. Their surface registration framework integrates the articulated motion assumption, global loop closure constraint, and a general as-rigid-as-possible deformation model. To enhance the reconstruction quality, they propose a novel approach to accurately infer skeletal joints from anatomic data using multimodality registration. They further propose a supervised predictive model to infer the skeletal joints for arbitrary subjects independent from anatomic data reference. A rigorous validation test has been conducted on real subjects to evaluate reconstruction accuracy and repeatability. In the fifth paper, Wanrong Huang, Xiaodong Yi, and Xue-Jun Yang, from the National University of Defense Technology in Changsha and the National Innovation Institute of Defense Technology (NIIDT) in Beijing, China, have designed a three-layer framework for multirobot coordination. Furthermore, a novel-distributed algorithm is proposed to achieve the navigation objective while satisfying connectivity maintenance and collision avoidance constraints. The algorithm is a hybrid of an RRT-based planner and an extended DNF-based controller. The coordination framework and the distributed algorithm are demonstrated to be effective through a series of illustrative simulations. They outperform the current state-of-the-art method in terms of efficiency and applicability. In the last paper, Kai Wang and Shiguang Liu, from Tianjin University, in China, present an automatic approach for the semantic modeling of indoor scenes based on a single photograph, instead of relying on depth sensors. Without using handcrafted features, they guide indoor scene modeling with feature maps extracted by fully convolutional networks. Three parallel fully convolutional networks are adopted to generate object instance masks, a depth map, and an edge map of the room layout. Based on these high-level features, support relationships between indoor objects can be efficiently inferred in a data-driven manner. Constrained by the support context, a global-to-local model matching strategy is|Comput. Animat. Virtual Worlds|2018|10.1002/cav.1862|D. Thalmann, N. Magnenat-Thalmann|0.0|2
1242|Crowd simulation and visualization|This paper presents a methodology to simulate and visualize crowds. Our goal is to represent the most realistic possible scenarios in a city. Due to the high demand of resources a GPU Cluster is used. We use real data from which we identify the behavior of the masses applying statistical and artificial intelligence techniques. In order to take advantge of the processing power of the GPU cluster we use the following programming models during the characters simulation: MPI, OmpSs and CUDA. We developed different visualization schemes: a) In situ, b) Streaming, c) Web. The web scheme is the most flexible, allowing to interact in real time with the simulation through a web browser. For this scheme we use WebGL and Cesium.||2015|10.1007/978-3-319-15862-4_10|Javier-Alfonso Espinosa-Oviedo, Benjamín Hernández, Hugo Perez, E. Parra, Isaac Rudomín, Genoveva Vargas-Solar|0.0|2
1246|An Intelligent Image Processing System for Real-Time Detection of Surface Flaws|This paper presents an intelligent parallel image processing system, PVCS (Parallel Visual Computing System), to handle in-line surface defect detection for large objects. PVCS is a parallel expansion of a previously-reported non-destructive Compute Unified Device Architecture (CUDA)-enabled optical inspection system to accommodate large test objects. PVCS is heterogeneous both in terms of hardware and software. From the hardware perspective, PVCS consists of multiple CPUs and GPUs; from the software perspective, PVCS adopts the Message Passing Interface (MPI) and CUDA programming models. A parallel prototype system, consisting of three CPUs and two GPUs, is used to inspect a simulated object with an area eight times greater than that of the real object in our previous work. Given the same resolution requirements, the simulation results show that PVCS can obtain the correct number of defects within a large-size image at a satisfactory processing rate.||2016|10.5875/AUSMT.V6I1.1006|W. Liao, Y. Chou, Ming Chang, P. Lin|0.0|2
1248|Long-Term Simulations of Beam-Beam Dynamics on GPUs|Future machines such as the electron-ion colliders (JLEIC), linac-ring machines (eRHIC) or LHeC are particularly sensitive to beam-beam effects. This is the limiting factor for long-term stability and high luminosity reach. The complexity of the non-linear dynamics makes it challenging to perform such simulations which require millions of turns. Until recently, most of the methods used linear approximations and/or tracking for a limited number of turns. We have developed a framework which exploits a massively parallel Graphical Processing Units (GPU) architecture to allow for tracking millions of turns in a symplectic way up to an arbitrary order and colliding them at each turn. The code is called GHOST for GPU-accelerated High-Order Symplectic Tracking. As of now, there is no other code in existence that can accurately model the single-particle non-linear dynamics and the beam-beam effect at the same time for a large enough number of turns required to verify the long-term stability of a collider. Our approach relies on a matrix-based, arbitrary-order, symplectic particle tracking for beam transport and the Bassetti-Erskine approximation for the beam-beam interaction.||2017|10.18429/JACOW-IPAC2017-THPAB086|F. Lin, A. Godunov, He Zhang, E. Nissen, R. Majeti, Y. Roblin, M. Zubair, B. Terzić, V. Morozov, D. Ranjan, M. Stefani, K. Arumugam, C. Cotnoir, T. Satogata|0.0|2
1249|Bugaroo: Exposing Memory Model Bugs in Many-Core Systems|Modern many-core architectures such as GPUs aggressively reorder and buffer memory accesses. Updates to shared and global data are not guaranteed to be visible to concurrent threads immediately. Such updates can be made visible to other threads by using some fence instructions. Therefore, missing the required fences can introduce subtle bugs, called Memory Model Bugs. We propose Bugaroo to expose memory model bugs in any arbitrary GPU program. It works by statically instrumenting the code to buffer some shared and global data for as long as possible without violating the semantics of any fence or synchronization instruction. Any program failure that results from such buffering indicates the presence of subtle memory model bugs in the program. Bugaroo later provides detailed debugging information regarding the failure. Bugaroo is the first proposal to expose memory model bugs of GPU programs by simulating memory buffers. We present a detailed design and implementation of Bugaroo. We evaluated it using seven programs. Our approach uncovers new findings about missing and redundant fences in two of the programs. This makes Bugaroo an effective and useful tool for GPU programmers.|IEEE International Symposium on Software Reliability Engineering|2018|10.1109/ISSRE.2018.00028|A. Muzahid, M. M. Islam|0.0|2
1259|High fidelity sky models|Light sources are an important part of physically-based rendering when accurate imagery is required. High-fidelity models of sky illumination are essential when virtual environments are illuminated via the sky as is commonplace in most outdoor scenarios. The complex nature of sky lighting makes it difficult to accurately model real life skies. The current solutions to sky illumination can be analytically based and are computationally expensive for complex models, or based on captured data. Such captured data is impractical to capture and difficult to use due to temporally inconsistencies in the captured content. This thesis enhances the state-of-the-art in sky lighting by addressing these problems via novel sky illumination methods that are accurate, practical and flexible. This thesis presents two novel sky illumination methods where; the first of which focuses on clear sky lighting and the second one deals with illumination from cloudy skies. \n \nThe first approach compactly and efficiently represents sky illumination from both existing analytic sky models and from captured environment maps. For analytic models, the approach leads to a low, constant runtime cost for evaluating lighting. When applied to environment maps, this approach approximates the captured lighting at a significantly reduced memory cost, and enables smooth transitions of sky lighting to be created from a small set of environment maps captured at discrete times of day. This makes capture and rendering of real world sky illumination a practical proposition. Results demonstrate less than 4% loss of accuracy compared to ground truth data. The straightforward implementation makes it possible to compute skies at sub milliseconds times on modest GPUs. \n \nThe second approach focuses on modelling of clouds from whole sky HDR images by using classification and optimisation techniques. This method pre-classifies the input image according to the cloud types of the pixels which improves both the duration and accuracy of the optimisation. The classification process itself compares well with similar processes from meteorological science and classifies whole images with 97% accuracy and individual pixels with an 80% accuracy. The method can be applied to any cloud type as soon as the optical properties are known. When combined with artificial sky lighting models consisting of arbitrary sun position to relight the extracted cloud model any day time simulations can be obtained based on the original single capture. Results for this method demonstrate a performance of 90% accuracy for fully digitally generated environment maps constructed from a single captured environment map when compared with the original capture.||2016|10.1615/rad-16.290|Pinar Satilmis|0.0|2
1265|Enabling Grid Computing resources within the KM3NeT computing model|KM3NeT is a future European deep-sea research infrastructure hosting a new generation neutrino detectors that – located at the bottom of the Mediterranean Sea – will open a new window on the universe and answer fundamental questions both in particle physics and astrophysics. International collaborative scientific experiments, like KM3NeT, are generating datasets which are increasing exponentially in both complexity and volume, making their analysis, archival, and sharing one of the grand challenges of the 21st century. These experiments, in their majority, adopt computing models consisting of different Tiers with several computing centres and providing a specific set of services for the different steps of data processing such as detector calibration, simulation and data filtering, reconstruction and analysis. The computing requirements are extremely demanding and, usually, span from serial to multi-parallel or GPU-optimized jobs. The collaborative nature of these experiments demands very frequent WAN data transfers and data sharing among individuals and groups. In order to support the aforementioned demanding computing requirements we enabled Grid Computing resources, operated by EGI, within the KM3NeT computing model. In this study we describe our first advances in this field and the method for the KM3NeT users to utilize the EGI computing resources in a simulation-driven use-case.||2016|10.1051/EPJCONF/201611607002|Christos Filippidis|0.0|2
1266|3D Monte Carlo localization using a depth camera and terrestrial laser scanner|Effective and accurate localization method in three-dimensional indoor environments is a key requirement for indoor navigation and lifelong robotic assistance. So far, Monte Carlo Localization (MCL) has given one of the promising solutions for the indoor localization methods. Previous work of MCL has been mostly limited to 2D motion estimation in a planar map, and a few 3D MCL approaches have been recently proposed. However, their localization accuracy and efficiency still remain at an unsatisfactory level (a few hundreds millimetre error at up to a few FPS) or is not fully verified with the precise ground truth. Therefore, the purpose of this study is to improve an accuracy and efficiency of 6DOF motion estimation in 3D MCL for indoor localization. Firstly, a terrestrial laser scanner is used for creating a precise 3D mesh model as an environment map, and a professional-level depth camera is installed as an outer sensor. GPU scene simulation is also introduced to upgrade the speed of prediction phase in MCL. Moreover, for further improvement, GPGPU programming is implemented to realize further speed up of the likelihood estimation phase, and anisotropic particle propagation is introduced into MCL based on the observations from an inertia sensor. Improvements in the localization accuracy and efficiency are verified by the comparison with a previous MCL method. As a result, it was confirmed that GPGPUbased algorithm was effective in increasing the computational efficiency to 10-50 FPS when the number of particles remain below a few hundreds. On the other hand, inertia sensor-based algorithm reduced the localization error to a median of 47mm even with less number of particles. The results showed that our proposed 3D MCL method outperforms the previous one in accuracy and efficiency.||2016|10.4287/JSPRS.55.235|R. Hatakeyama, H. Date, S. Kanai|0.0|2
1268|ACCTuner: OpenACC Auto-Tuner For Accelerated Scientific Applications|ACCTuner: OpenACC Auto-Tuner for Accelerated Scientific Applications Fatemah Ramzy AlZAyer We optimize parameters in OpenACC clauses for a stencil evaluation kernel executed on Graphical Processing Units (GPUs) using a variety of machine learning and optimization search algorithms, individually and in hybrid combinations, and compare execution time performance to the best possible obtained from brute force search. Several auto-tuning techniques – historic learning, random walk, simulated annealing, Nelder-Mead, and genetic algorithms – are evaluated over a large two-dimensional parameter space not satisfactorily addressed to date by OpenACC compilers, consisting of gang size and vector length. A hybrid of historic learning and Nelder-Mead delivers the best balance of high performance and low tuning e↵ort. GPUs are employed over an increasing range of applications due to the performance available from their large number of cores, as well as their energy e ciency. However, writing code that takes advantage of their massive fine-grained parallelism requires deep knowledge of the hardware, and is generally a complex task involving program transformation and the selection of many parameters. To improve programmer productivity, the directive-based programming model OpenACC was announced as an industry standard in 2011. Various compilers have been developed to support this model, the most notable being those by Cray, CAPS, and PGI. While the archi5 tecture and number of cores have evolved rapidly, the compilers have failed to keep up at configuring the parallel program to run most e ciently on the hardware. Following successful approaches to obtain high performance in kernels for cachebased processors using auto-tuning, we approach this compiler-hardware gap in GPUs by employing auto-tuning for the key parameters “gang” and “vector” in OpenACC clauses. We demonstrate results for a stencil evaluation kernel typical of seismic imaging over a variety of realistically sized three-dimensional grid configurations, with di↵erent truncation error orders in the spatial dimensions. Apart from random walk and historic learning based on nearest neighbor in grid size, most of our heuristics, including the one that proves best, appear to be applied in this context for the first time. This work is a stepping-stone towards an OpenACC auto-tuning framework for more general high-performance numerical kernels optimized for GPU computations.||2015|10.25781/KAUST-K7EKW|Fatemah AlZayer|0.0|2
1269|On Cartesian grids in some adaptive algorithms of aerodynamics|Introduction. Hydro and gas dynamics have critical scientific and technical applications. Moreover, along with experimental and rigorous mathematical results, mathematical modeling, based on the use of parallel supercomputer systems, plays an increasing role. Often, the problems under investigation are characterized by a strong difference in space-time scales, for the solution of which not only high-precision numerical methods are required, but also very detailed spatial grids in geometrically complex domains whose boundaries can depend on time. For such tasks, up to half the time necessary to simulate and perform the calculation is spent on generating the initial grid and then modifying it (usually by hand) in order to obtain a solution of adequate quality. Therefore, one of the urgent problems is the development of methods for automatic adaptation of grids. There is no strict mathematical theory for mathematical models describing these phenomena and processes that distinguishes suitable functional compacts in which solutions of the corresponding initial-boundary value problems lie. Therefore, the considerations underlying the approximation of solutions and the construction of discrete models and algorithms should serve to achieve a compromise between physical representations about the nature of the solutions (first of all, smoothness and multi-scale), the optimality in the number of operations, the possibility of massive parallelization, and also the specific architecture and element base (CPU, GPU) of the supercomputer used for calculations. In the numerical solution of hydrodynamic problems, discretization is used when the domain of the solution is divided into countable cells by means of an appropriately selected grid, and the continuous problem reduces to a discrete approximation approximating the differential operators and boundary conditions. Currently, there are several approaches to this task. The first is the standard method of connected grids, the second is the method of chimera grids, where the main grid is built throughout the calculation area, and the second-level grid is constructed separately around each solid object over the main grid. The third approach is the method of Cartesian grids, where the design area is chosen in the simplest form, for example, in the form of a cube, in which a regular Cartesian grid is constructed [2]-[4]. Surfaces of bodies in the current region cross||2017|10.23947/2587-8999-2017-2-180-184|Nikitin Vyacheslav Sergeyevich, Afendikov Andrei Leonidovich|0.0|2
1270|LEVERAGING HIGH-FIDELITY SIMULATION TO EVALUATE AUTONOMY ALGORITHM SAFETY|The age of large autonomous ground vehicles has arrived. Wherever vehicles are used, autonomy is desired and, in most cases, being studied and developed. The last barrier is to prove to decision makers (and the general public) that these autonomous systems are safe. This paper describes a rigorous safety testing environment for large autonomous vehicles. Our approach to this borrows elements from game theory, where multiple competing players each attempt to maximize their payout. With this construct, we can model an environment that as an agent that seeks poor performance in an effort to find the rare corner cases that can lead to automation failure. INTRODUCTION The age of large autonomous ground vehicles has arrived. More autonomy is desired wherever vehicles are used in repetitive, dirty, and dangerous tasks. Vehicle autonomy is actively being researched and developed by many large commercial and government entities. John Deere makes automated harvesters, Caterpillar makes automated mining trucks, Google makes selfdriving passenger vehicles, and the Army makes automated transport vehicles. In all this effort, arguably the greatest near-term value of this technology lies with the military, as automating military logistics vehicles will both increase productivity and keep Americans away from combat risks. The military’s needs correspond to what is likely to be the first transformative fielding of large-vehicle autonomy. PROBLEM DESCRIPTION Now, autonomous large vehicles are carefully watched and guided. John Deere’s harvesters have monitors in the cab, Caterpillar restricts application, Google has passive drivers, and the Army tests vehicles in cleared and walled areas. The disappointing lack of fielding in many domains comes from concerns over safety. The autonomy algorithms are not sufficiently robust, and the consequences of failure—a runaway multi-ton vehicle—too high. Researchers are trying to address the worry over safety through hardware testing. Systems are run through exercises and studies in the field. But these are often seen as inadequate. Funding is (always) limited, and it is impossible to reproduce the great variety of events that a fielded autonomous system will encounter over its life. The main obstacle to acceptance is uncertainty in how the systems will react to untested conditions. As a result, the tests do not convince safety Proceedings of the 2017 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) Leveraging High-Fidelity Simulation to Evaluate Autonomy Algorithm Safety, Penning, et al. Page 2 of 18 regulatory boards and the autonomy technology languishes unfielded. The best and only answer to this problem lies in digital simulation. Simulation can support many more hours of testing in a greater variety of situations at a lower cost. It scales to more and larger computers, runs day and night, and is 100% safe. Digital simulation can be combined with hardware components (a hardware-in-the-loop approach) to add realism and incorporate proprietary algorithms (such as those in sensors and autopilots). A large body of research supports sensor simulation, including for cameras, lidar, GPS, and radar, as well as simulation of vehicles and terrain. Many new sensor modeling techniques leverage GPU programming to speed calculation and quality. And parallel implementation across multiple computers, each with multiple graphics cards, has created a faster computing environment for simulation. Digital simulation, though, carries two pressing issues. It has in the past offered poorer fidelity than hardware testing, with results that did not carry the engineering or psychological weight of field tests. Simulation also has not, through parametric and Monte Carlo studies, offered adequate coverage of the situations and environments that lead to failure. The corner cases that fail, in the real world, appear as vanishing probability events in most simulation models. This is due significantly to inaccurate probability distributions being applied to Monte Carlo simulation parameters. The problem has deep roots, as many probability distributions are not even knowable. They may depend on unstudied phenomena, human activities, or one-time events. Variables not even having probability distributions are regarded as profoundly unknown. It is the neglect of profoundly unknown variables that drives many simulations to discredit and disuse—they do not provide results that are accurate enough, based on wise assessment from subject matter experts, to support mission planning and execution. Yet the problem runs even deeper than this. Often in real-world situations there are multiple agents involved, each attempting to maximize their own set of metrics and produce the outcome most desirable to them. The optimal behavior of each agent is dependent on its competitors’ behavior. The resulting problem can be staggeringly complex: multiple independent systems, each adapting to the others’ behavior and to the environment, with system dynamics, sensors, control algorithms and other factors— some profoundly unknown—all determining the resulting system performance and safety. These effects compound to render the problem intractable (and prohibitively expensive) to solve via traditional hardware testing. A better way must be found. PREVIOUS WORK Basic Autonomy Validation Currently, there are a number of different approaches to validating autonomy algorithms. Traditionally, and most simplistically, there is the standard test matrix approach. This approach sets up and tests combinations of conditions and behaviors in an attempt to determine what produces a system failure. This approach is reasonable for systems with a small number of discrete variables. However, as the number of variables grows, the number of possible combinations grows exponentially, and the problem becomes intractable. This is exacerbated by the introduction of continuous variables that can take any value. Many researchers have recognized these shortcomings, and have devised alternative means of validation that seek out failures. Schultz et al. proposed using genetic algorithms to attempt to find failure modes [1]. This transformed evaluation into an optimization problem that attempts to identify failure modes by maximizing a usefulness metric. Similarly, Wegener and Bühler used an evolutionary algorithm to evaluate an autonomous parking system [2]. Proceedings of the 2017 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) Leveraging High-Fidelity Simulation to Evaluate Autonomy Algorithm Safety, Penning, et al. Page 3 of 18 Game Theory in Robotics A related body of work also currently exists that uses game theoretic constructs in the control of cooperative robot teams. Emery-Montemerlo et al. posed a robotic soccer team as a partially observable stochastic game to determine the best actions for each individual player to take, given the uncertainty surrounding the roles of the opposing players. They used the partial observability approach to logically reason about uncertainty in the world state, in determining these actions [3]. Vidal et al. utilized game theory in a two team pursuit problem, in which a team attempts to evade pursuit by an opposing force of unmanned ground and aerial vehicles [4]. In this, the pursuing team has no knowledge of the pursuit area, and must simultaneously reason about the environment and construct a model of it as the game evolves. By casting the game as a probabilistic search problem, they established both local and global maxima search routines, and demonstrated success in tracking and pursuit. Lygeros et al. demonstrated the successful use of a game theoretic construction of an automated highway system [5]. In their approach, a set of autonomous vehicles is controlled, and multiple metrics are optimized on a simulated fully autonomous highway. The focus is on when it is optimal for individual vehicles to join/leave a “platoon” of other vehicles traveling on a similar trajectory, and how best to safely change lanes. Pareto Optimality The science of Multi-Objective Optimization (MOO), where it is desired to extremize multiple objective functions (such as autonomy metrics) is relevant. A good overview is given in [6]. This is closely related to set-based design, as described in [7]. Using the nomenclature of [8], given a set of functions of vector x, { ) (x i F }, MOO concerns the following )] ( ),... ( ), ( [ ) ( minimize 2 1 x x x x F x k F F F  (1) i.e., minimize a vector of independent functions, subject to m j g j ,..., 2 , 1 , 0 ) (   x (2)||2017|10.4271/2017-01-1141|R. Penning, J. English, Paul L. Muench, L. LimoneBrett, Daniel Melanz, D. Bednarz|0.0|2
1271|MPI- and CUDA- implementations of modal finite difference method for P-SV wave propagation modeling|Among different discretization approaches, Finite Difference Method (FDM) is widely used for acoustic and elastic full-wave form modeling. An inevitable deficit of the technique, however, is its sever requirement to computational resources. A promising solution is parallelization, where the problem is broken into several segments, and the calculations are distributed over different processors. For the present FD routines, however, such parallelization technique inevitably needs domain-decomposition and inter-core data exchange, due to the coupling of the governing equations. In this study, a new FD-based procedure for seismic wave modeling, named as ‘Modal Finite Difference Method (MFDM)” is introduced, which deals with the simulation in the decoupled modal space; thus, neither domain-decomposition nor inter-core data exchange is anymore required, which greatly simplifies parallelization for both MPI- and CUDA implementations over CPUs and GPUs. With MFDM, it is also possible to simply cut off less-significant modes and run the routine for just the important ones, which will effectively reduce computation and storage costs. The efficiency of the proposed MFDM is shown by some numerical examples.||2016|10.22064/TAVA.2016.45442.1052|H. Samadiyeh, R. Khajavi|0.0|2
1273|Performance Study of Monte Carlo Codes on Xeon Phi Coprocessors — Testing MCNP 6.1 and Profiling ARCHER Geometry Module on the FS7ONNi Problem|This paper contains two parts revolving around Monte Carlo transport simulation on Intel Many Integrated Core coprocessors (MIC, also known as Xeon Phi). (1) MCNP 6.1 was recompiled into multithreading (OpenMP) and multiprocessing (MPI) forms respectively without modification to the source code. The new codes were tested on a 60-core 5110P MIC. The test case was FS7ONNi, a radiation shielding problem used in MCNP’s verification and validation suite. It was observed that both codes became slower on the MIC than on a 6-core X5650 CPU, by a factor of ~4 for the MPI code and, abnormally, ~20 for the OpenMP code, and both exhibited limited capability of strong scaling. (2) We have recently added a Constructive Solid Geometry (CSG) module to our ARCHER code to provide better support for geometry modelling in radiation shielding simulation. The functions of this module are frequently called in the particle random walk process. To identify the performance bottleneck we developed a CSG proxy application and profiled the code using the geometry data from FS7ONNi. The profiling data showed that the code was primarily memory latency bound on the MIC. This study suggests that despite low initial porting e_ort, Monte Carlo codes do not naturally lend themselves to the MIC platform — just like to the GPUs, and that the memory latency problem needs to be addressed in order to achieve decent performance gain.||2017|10.1051/EPJCONF/201715306022|P. Caracappa, Tianyu Liu, X. Xu, C. Carothers, W. Ji, Noah Wolfe, K. Zieb, Hui Lin|0.0|2
1274|dCUDA: GPU Cluster Programming using IB Verbs|Over the last decade, the usage of GPU hardware to accelerate high performance computing tasks increased due to the parallel computing nature of GPUs and their floating point performance. Despite the popularity of GPU accelerated compute clusters for high performance applications like atmospheric simulations, no unified programming model for GPU cluster programming is established in the community. dCUDA, a unified programming model for GPU cluster programming designed at ETH Zürich, is a candidate to fill this gap. dCUDA provides a device-side library for Remote Memory Access (RMA) of a global address space and supports fine-grained communication and synchronisation on the level of CUDA thread blocks. The dCUDA programming model enables native overlap of communication and computation for latency hiding and better utilisation of GPU resources by oversubscribing the system. dCUDA provides a well designed communication library and outperforms traditional GPU cluster programming approaches. We extend the dCUDA library and improve the performance of the dCUDA programming model. New functionality enables new low latency synchronisation mechanisms and several optimisations increase the overall performance of dCUDA. The replacement of MPI based communication by a newly designed InfiniBand Verbs based network manager decreases dCUDA’s remote communication latency by a factor of 2 to 3. Device local communication latency was improved by a factor of almost 3 thanks to a redesign of the dCUDA notifications system. Performance benchmarks demonstrate that the optimised framework using the InfiniBand network manager outperforms MPI based dCUDA implementations. Depending on communication patterns, it shows about 40% to 90% improved performance to traditional state of the art GPU cluster programming approaches.||2017|10.3929/ethz-a-010889936|Lukas Kuster|0.0|2
1277|FPGA Based Hybrid Computing Platform for ESS Linac Simulator|This paper presents a scalable and high-throughput hybrid computing platform for the real-time multi-particle based Linac (Linear accelerator) simulation model to be used at the European Spallation Source (ESS). The multi-particle simulation model with non-linear modeling is needed to provide a realistic behavior of the particle beam for reducing the losses at the superconducting structures. The computation complexity of the simulations can reach 1012 matrix multiplication operations for a test case of 106 beam particles simulated over 106 cells. An OpenCL (Open Computing Language) based framework is used to map the processing intensive parts of the simulation model efficiently to any configuration of a CPU-, GPU- and FPGA-based platform. Optimizations using data precision strategies have also been explored to further improve the throughput after reaching memory access saturation. We are able to achieve up to $89 \times$ speed up compared to a C++ benchmark of the same system.|2018 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium of System-on-Chip (SoC)|2018|10.1109/NORCHIP.2018.8573518|Liang Liu, E. Laface, Maurizio Donna, A. Jeevaraj, F. Edman|0.0|2
1278|A Fast Particle Tracking Tool for the Simulation of Dielectric Laser Accelerators|In order to simulate the beam dynamics in grating based Dielectric Laser Accelerators (DLA) fully self-consistent PIC codes are usually employed. These codes model the evolution of both the electromagnetic fields inside a laser-driven DLA and the beam phase space very accurately. The main drawback of these codes is that they are computationally very expensive. While the simulation of a single DLA period is feasible with these codes, long multi-period structures cannot be studied without access to HPC clusters. We present a fast particle tracking tool for the simulation of long DLA structures. DLATracker is a parallelized code based on the analytical reconstruction of the in-channel electromagnetic fields and a Boris/Vay-type particle pusher. Its computational kernel is written in OpenCL and can run on both CPUs and GPUs. The main code is following a modular approach and is written in Python 2.7. This way the code can be easiliy extended for different use cases. In order to benchmark the code, simulation results are compared to results obtained with the PIC code VSim 7.2. INTRODUCTION The concept of dielectric laser accelerators (DLA) has gained increasing attention in accelerator research, because of the high achievable acceleration gradients (∼GeV/m) [1]. This is due to the high damage threshold of dielectrics at optical frequencies. In order to simulate the interaction of the incoming electron bunch with the electromagnetic fields inside the laser illuminated dielectric structure self-consistent Particle-In-Cell (PIC) codes are usually employed. This way both the evolution of the fields inside the acceleration channel and the beam dynamics can be simulated very accurately. The main drawback of PIC codes is that they can be computationally very expensive and thus are usually used on HPC clusters. In this work we focus on so called grating type DLAs (see Fig. 1). In the context of the Accelerator on a CHip International Program (ACHIP) the typical period length of a grating DLA is 2 micron and the channel width <1 micron. The in-channel fields can be decomposed into an infinite sum of so called spatial harmonics (see section Theoretical Background) [2]. In order to resolve higher harmonic contributions to the field, a sufficiently high spatial grid resolution has to be used. Together with the well-known Courant-Friedrichs-Lewy stability condition [3] for time domain algorithms||2017|10.18429/JACoW-IPAC2017-THPAB013|F. Mayet, W. Kuropka, R. Assmann, U. Dorda|0.0|2
1279|Annealing evolutionary parallel algorithm analysis of optimization arrangement on mistuned blades with non-linear friction|This paper sets up a lumped parameter model of engine bladed disk system when considering the nonlinear friction damping based on mistuned parameters which is obtained from the blade modal experiment. A bladed arrangement optimization method, namely annealing evolutionary algorithm with tabu list is presented which combines the local search ability of SA (simulated annealing) and the global searching ability of GA (genetic algorithm) introducing tabu list as the search memory list. Parallel TAEA (tabu annealing evolutionary algorithm) is presented based on CUDA (Compute Unified Device Architecture) combining GPU (Graphics Processing Unit) and its performance is analyzed. The results show that optimization based on CUDA framework can improve computing speed. At the same time using optimization results can reduce the amplitude of forced vibration response of bladed disk system and make it in the range of allowable engineering.||2015|10.2991/icmmcce-15.2015.285|Honggang Pan, Wenjun Yang, Huiqun Yuan, Tianyu Zhao|0.0|2
1281|GPU-Accelerated Tools for Medical Image Registration and Biomechanical Modeling|GPU-Accelerated Tools for Medical Image Registration and Biomechanical Modeling Grant Lloyd Schunemann Marchelli Co-Chairs of the Supervisory Committee: Professor Duane W. Storti and Professor Mark A. Ganter Department of Mechanical Engineering This dissertation explores the implications of GPU-accelerated computing (i.e., harnessing the parallel computing power of modern graphics processing units) for applications in image registration and biomechanical modeling. The work described herein includes the development of two software toolkits: one that implements functions for anatomical modeling based on distance fields, and one for registration of 3D (volumetric) medical imaging with 2D imaging (including stereo pairs). In both cases, the general aim of the research effort was to provide practical support for creating and simulating anatomical models with greater accuracy and efficiency; the specific goals involved applications of interest of a research group focused on understanding the biomechanics of the human foot. Thus, the distance-based tools were employed for purposes such as enhancing an existing finite-element model of the foot by identifying appropriate locations for cartilage iv elements that cannot be reliably resolved from typical imaging studies. The primary application described for the 2D-3D registration toolkit involves high-accuracy markerless tracking of the kinematics of the bones in the foot during walking gait. In both cases, GPU-based parallelism was found to have a significant impact on software execution time for data-intense applications and succeeded in realizing gains in computational efficiency of 10to 150-fold over traditional CPU-based computing. The ramifications of such a radical increase in raw computing power are many, but possibly one of the most important outcomes for the group is the ability to rapidly and accurately quantify bone motion in the human foot during gait. Moreover, this research will deepen our understanding of foot bone motion as it relates to subjects exhibiting both normal and abnormal (i.e., deformities) foot conditions. Previous attempts at engaging in studies involving a large subject pool were stifled by prohibitively long software execution times; however, the GPU-based image processing tools developed in this dissertation will enable our group to revisit the once impractical investigation. While the human foot is the anatomical region of choice for studies described in this dissertation, the tools presented can be adapted to other anatomical localities in the body, such as the knee, hip, hand or shoulder. Additionally, this toolbox need not be limited to the human anatomy, opening the door to efficient exploration of animal models. Simulation of anatomically correct models can help investigators to more accurately predict physiological function, diagnose disease, and analyze environmental impact, which may lead to higher success rates during treatment and recovery. The GPU-based tools presented in this dissertation will provide the foundation for future work in the field of computational biomechanics by arming investigators with resources that encompass particularly relevant||2015||G. Marchelli|0.0|2
1282|Fine-Grained Network Decomposition for Massively Parallel Electromagnetic Transient Simulation of Large Power Systems|Electromagnetic transient (EMT) simulation is one of the most complex power system studies that requires detailed modeling of the study system including all frequency-dependent and nonlinear effects. Large-scale EMT simulation is becoming commonplace due to the increasing growth and interconnection of power grids, and the need to study the impact of system events of the wide area network. To cope with enormous computational burden, the massively parallel architecture of the graphics processing unit (GPU) is exploited in this paper for large-scale EMT simulation. A fine-grained network decomposition, called shattering network decomposition, is proposed to divide the power system network exploiting its topological and physical characteristics into linear and nonlinear networks, which adapt to the unique features of the GPU-based massive thread computing system. Large-scale systems, up to 240 000 nodes, with typical components, including synchronous machines, transformers, transmission lines, and nonlinear elements, and multiple levels modular multilevel converter with up to 6144 submodules, are tested and compared with mainstream simulation software to verify the accuracy and demonstrate the speed-up improvement with respect to sequential computation.|IEEE Power & Energy Society General Meeting|2018|10.7939/R3B56DK7X|V. Dinavahi, Zhiyin Zhou|0.0|2
1283|Accelerated iterative image reconstruction in three-dimensional optoacoustic tomography|Iterative image reconstruction algorithms can model complicated imaging physics, compensate for imperfect data acquisition systems, and exploit prior information regarding the object. Hence, they produce higher quality images than do analytical image reconstruction algorithms. However, three-dimensional (3D) iterative image reconstruction is computationally burdensome, which greatly hinders its use with applications requiring a large field-of-view (FOV), such as breast imaging. In this study, an improved GPU-based implementation of a numerical imaging model and its adjoint have been developed for use with general gradient-based iterative image reconstruction algorithms. Both computer simulations and experimental studies are conducted to investigate the efficiency and accuracy of the proposed implementation for optoacoustic tomography (OAT). The results suggest that the proposed implementation is more than five times faster than the previous implementation.|Biomedical optics|2015|10.1117/12.2084087|R. Su, A. Oraevsky, Kun Wang, M. Anastasio, F. Anis, S. Ermilov|0.0|2
1285|Radix sort for massively parallel hardware: a multi-GPU implementation|Computers, servers and supercomputers with more than one GPU become more and more common. To fully utilize such systems the application needs to be aware of the additional GPUs. E.g., computational workload needs to be distributed across the GPUs. Along with that goes data movement as well as communication. Recently, NVIDIA introduced new features, Unified Memory and Cooperative Groups, that enable programmers to extend their applications with multi-GPU capabilities. In this thesis the aforementioned features are presented in detail and evaluated in the context of the GPU molecular dynamics simulation HAL's MD package on the latest NVIDIA GPU architecture Pascal. A hotspot analysis with the NVIDIA Visual Profiler identified the radix sort as a major hotspot within HAL's MD package. This thesis presents an approach towards a multi-GPU implementation of radix sort. The individual kernels are analysed and an implementation is discussed. Then, the radix sort is transformed into a multi-GPU implementation kernel per kernel. For each step the individual runtime measurements and bottlenecks are presented to analyse the approach. A general sequence diagram of the implementation shows how the individual parts interact with each other. The outlook discusses the further development of HAL's MD package, especially the force computation for multi-GPU which has not been covered in this thesis.||2018||A. Kammeyer|0.0|2
1286|Real-time house-collapsing simulations based on performance evaluation of physics engines|House-collapsing simulation using a physics engine is an effective method for acquiring structural data regarding collapsed houses with the aim of understanding the properties of destroyed or disordered structures for designing and operating rescue-robots. However, the simulation needs a lot of time since a house model consists of a large number of rigid bodies and joints. In order to find an appropriate configuration of computer hardware and software for accelerating house-collapsing simulations, this study evaluates the performances of four major physics engines, namely Open Dynamics Engine, Bullet Physics Library, PhysX 2.8.1 and PhysX 3.4, by comparing the processing time about two sample structures including only rigid bodies or rigid bodies constrained by joints. Results of the experiments show that the use of multi-core CPU and GPU, especially high-speed GPU, on PhysX 3.4 has the best performance since it can process a large number of rigid bodies and joints in parallel. Based on the results, an existing simulation system has been improved and the collapsing process of one-house, which consists of about 7,500 rigid bodies and 15,000 joints, can be simulated in real-time using multi-core CPU and GPU. Moreover, this study estimates the size of GPU memory which is required for simulating the large-scale field on PhysX 3.4 to enlarge the scale of simulation. Consequently, the collapsing process of thirty-houses which includes interactions between collapsed houses can be simulated.||2018|10.1299/TRANSJSME.17-00480|Takuto Hamano, F. Tanaka, M. Onosato|0.0|2
1287|Simulating High Detail Brush Painting on Mobile Devices : Using OpenGL, Data-Driven Modeling and GPU Computation|This report presents FastBrush, an advanced implementation for real time brush simulation, which achieves high detail with a large amount of bristles, and is lightweight enough to be implemented fo ...||2016|10.1007/978-1-4842-1993-5_36|Adrian Blanco Paananen|0.0|2
1295|Real Time auralization using ray tracing on a GPU|Auralization is of interest in several disciplines - virtual reality, architecture and video games. Current trends in the eld of computuation have led to dramatic increases in performance in recent years. In this project, a prototype of an auralization-engine is developed. The program is capable of plausible room acoustic simulation in real time for a moving receiver and arbitrary room models. This is achieved by use of the Nvidia Optix GPU raytracing framework, implementing specular sound reections. The auralization engine is coupled to a graphical user interface, allowing for concurrent visualization of the auralized environment, and interactive control. The modular architecture of the project facilitates simple implementation of further features - for example more complicated reection schemes (Less)||2016|10.1117/12.2227219|Erik Molin|0.0|2
1296|The application of GPU to molecular communication studies|This thesis applies the recent trends in parallel processing, via graphics processing unit (GPU), to the field of molecular communications (MC), an investigation into communication possibilities of futuristic in vivo nanomachines. Existing MC simulations have not fully accounted for structural boundaries and the associated simulation of a massive number of messenger molecule paths for stochastic evaluation. These molecules are influenced by a Brownian motion as well as the flow of the blood, which is modeled using numerical methods based on the Fokker-Planck stochastic differential equation. By using a GPU these paths can be calculated on a massive scale, both in the number of simulated paths, and the number of time steps. The use of a GPU also allows for other obstacles and complications to be added to the path of those molecules in future works. This study should enable and expedite existing as well as future study in the MC field.||2018|10.4324/9781351055987-3|Tobias Cain|0.0|2
1297|Selection of optimal multispectral imaging system parameters for small joint arthritis detection|Early detection and treatment of arthritis is essential for a successful outcome of the treatment, but it has proven to be very challenging with existing diagnostic methods. Novel methods based on the optical imaging of the affected joints are becoming an attractive alternative. A non-contact multispectral imaging (MSI) system for imaging of small joints of human hands and feet is being developed. In this work, a numerical simulation of the MSI system is presented. The purpose of the simulation is to determine the optimal design parameters. Inflamed and unaffected human joint models were constructed with a realistic geometry and tissue distributions, based on a MRI scan of a human finger with a spatial resolution of 0.2 mm. The light transport simulation is based on a weighted-photon 3D Monte Carlo method utilizing CUDA GPU acceleration. An uniform illumination of the finger within the 400-1100 nm spectral range was simulated and the photons exiting the joint were recorded using different acceptance angles. From the obtained reflectance and transmittance images the spectral and spatial features most indicative of inflammation were identified. Optimal acceptance angle and spectral bands were determined. This study demonstrates that proper selection of MSI system parameters critically affects ability of a MSI system to discriminate the unaffected and inflamed joints. The presented system design optimization approach could be applied to other pathologies.|BiOS|2018|10.1117/12.2288192|M. Milanič, R. Dolenec, E. Laistler, J. Stergar|0.0|2
1298|Geometric Modeling and Shape Analysis for Biomolecular Complexes Based on Eigenfunctions|Geometric modeling of biomolecules plays an important role in the study of biochemical processes. Many simulation methods depend heavily on the geometric models of biomolecules. Among various studies, shape analysis is one of the most important topics, which reveals the functionalities of biomolecules. To enable the geometric modeling and shape analysis for various biomolecular complexes, we develop: (a) an efficient multi-scale modeling method for the biomolecular complexes accelerated using the CPUand GPUbased parallel computation; (b) a structure-aligned surface parameterization method; (c) an adaptive and anisotropic T-mesh generation algorithm; (d) a shape correspondence analysis method for biomolecules based on volumetric eigenfunctions; and (e) a shape analysis approach based on geometric operators from the second fundamental form of the surface. Various algorithms for the analysis of biomoleucles have been developed based on their surface or volumetric meshes. We introduce an efficient computational framework to construct multi-scale models, which reflect the geometries of the biomolecular complexes represented by atomic resolution data in the Protein Data Bank (PDB). A multilevel summation of Gaussian kernel functions is employed to generate implicit models for biomolecules. The coefficients in the summation are designed as functions of the structure indices, which specify the structures at a certain level and enable a local resolution control on the biomolecular surface. To improve the efficiency of Gaussian density map construction, an error-bounded atom elimination method is introduced to reduce the atom number. Moreover, a method called neighboring search is adopted to locate the grid points close to the expected biomolecular surface, and reduce the number of grids to be analyzed. For a specific grid point, a KD-tree or bounding volume hierarchy is applied to search for the atoms contributing to its density computation, and faraway atoms are ignored due to the decay of Gaussian kernel functions. In addition to density map construction, three modes are also employed and compared during mesh generation and quality improvement: CPU||2015|10.1515/mlbmb-2015-0007|Tao Liao|0.0|2
1300|Real-Time GPU Based Video Segmentation with Depth Information|In the context of video segmentation with depth sensor, prior work maps the Metropolis algorithm, a simulated annealing based key routine during segmentation, onto an Nvidia Graphics Processing Unit (GPU) and achieves real-time performance for 320×256 video sequences. However that work utilizes depth information in a very limited manner. This paper presents a new GPU-based method that expands the use of depth information during segmentation and shows the improved segmentation quality over the prior work. In particular, we discuss various ways to restructure the segmentation flow, and evaluate the impact of several design choices on throughput and quality. We introduce a scaling factor for amplifying the interaction strength between two spatially neighboring pixels and increasing the clarity of borderlines. This allows us to reduce the number of required Metropolis iterations by over 50% with the drawback of over-segmentation. We evaluate two design choices to overcome this problem. First, we incorporate depth information into the perceived color difference calculations between two pixels, and show that the interaction strengths between neighboring pixels can be more accurately modeled by incorporating depth information. Second, we pre-process the frames with Bilateral filter instead of Gaussian filter, and show its effectiveness in terms of reducing the difference between similar colors. Both approaches help improve the quality of the segmentation, and the reduction in Metropolis iterations helps improve the throughout from 29 fps to 34 fps for 320×256 video sequences.|ACS/IEEE International Conference on Computer Systems and Applications|2018|10.1109/AICCSA.2018.8612854|Nilangshu Bidyanta, A. Akoglu|0.0|2
1301|Proposta de implementação em GPU do modelo de particulas auto-propelentes para segregação celular|Cellular movement is the basis of several biologic processes, for example: cell sorting, wound healing, cancer metastases, morphogenesis, and others. In some of them, like cell sorting, the dynamics mechanism are physical or chemical, so this study is essentially multidisciplinary. Cell studies are in vivo or in vitro, however, it is very hard to isolate the analysis methods and the results interpretation. Computational simulations are another way to study cells. They can be classified in two groups: i) the lattice based models (cells are a pixels set and they only move to valid net positions); ii) the off-lattice models(the cells are dots and they are free to move to all the continuous space). In this work, we analyse a classic cell sorting simulation model: the Vicsek’s Model. We propose two serial implementation ways based on the molecular dynamics problem. Aditionally we treat the massive parallel possible algorithms, relative to each serial one, speacially for the GPUs executions. The aim is to find out which version has the smaller execution time.||2016||C. Beatrici|0.0|2
1310|Specification and Visualization of Interconnection Networks of Mobile GPUs|The number of components that can be fit into a single chip is increasing over \n \ntime, because of improved manufacturing technology. An efficient way to con- \nnect the components is through a packet switched network, a Network on Chip. \n \nSuch a network can be designed in many different ways. An effective way to \n \nexplore the large design space is by simulation. In this project, we have devel- \noped a network description language to define the network of a mobile GPU. \n \nAs part of this process, we have surveyed how NoCs are specified in several \n \nsimulators. We have implemented support for the language in a GPU simula- \ntor. We have also implemented a script that outputs a visual representation of \n \nnetworks defined in the language. \nThe purpose of the project has been to develop tools to make Network on \nChip modeling faster, easier, and less error prone. The result was evaluated \nqualitatively and quantitatively.||2017|10.1007/978-3-658-11783-2_1|Björn Wictorin|0.0|2
1311|The computational performance and power consumption of the parallel FDTD on a smartphone platform|─ The use of the FDTD in Android applications heralds the use of mobile phone platforms for performing electromagnetic modeling tasks. The Samsung S4 and Alpha smartphones computations are powered by a pair of multi-core Advanced RISC Machines (ARM) processors, supported by the Android operating system, which comprises a self-contained platform, which can be exploited for numerical simulation applications. In this paper, the parallelized two dimensional FDTD is implemented on the Samsung Smartphone using threading and SIMD techniques. The computational efficiency and power consumption of the parallelized FDTD on this platform are compared to that for other systems, such as Intel’s i5 processor, and Nvidia’s GTX 480 GPU. A comparison is made of the power consumption of the different techniques that can be used to parallelize the FDTD on a conventional multicore processor. In addition to parallelizing the FDTD using threading, the feasibility of accelerating the FDTD with the SIMD registers inherent in the phone’s ARM processor is also examined. Index Terms ─ ARM, EXYNOS, FDTD, NEON.||2015|10.1002/pmj.21486|D. Davidson, R. G. Ilgner|0.0|2
1317|Towards RealTime 3D Coronary Hemodynamics Simulations During Cardiac Catheterisation|Virtual Fractional Flow Reserve (vFFR) is an emerging technology that assesses the severity of coronary stenosis by means of patient specific of Computational Fluid Dynamics simulations. To be of practical clinical utility within a catheter laboratory, FFR results must be obtainable within minutes to guide intervention. We present the design of a novel Lattice-Boltzmann method code specifically tailored for fully automatic near real-time 3D coronary blood flow simulations. The key contributions of the work include a hybrid multicore-GPU accelerated sparse lattice generation algorithm and specialized 3D-0D coupled hemodynamics solver. We present results on state of the art GPU hardware, simulating hemodynamics within multi segment coronary tree. The results demonstrate that vFFR simulations can be performed in the order of minutes, making the replacement of pressure wire based FFR in a catheter laboratory setting with vFFR simulations feasible, without the need to reduce the fidelity of the hemodynamics modelling.|International Conference on Computing in Cardiology|2018|10.22489/CinC.2018.237|Kerry J. Halupka, S. Zhuk, Stephen Moore|0.0|2
1319|Implementation of Cloth Simulation Using Parallel Computing on Mobile Device|Physically based modeling and simulation is an important technique for deformable object simulation, which is widely used to represent the realistic shape change and movement of objects for mobile game or 3D simulation. However, they require the high computational cost for representing the physical phenomenon on deformable objects when it applied on mobile device. In this paper, we designed and implemented the cloth simulation for deformable object simulation using the parallel technique on mobile device to optimize the computational burden. We especially applied GPU parallel technique for the integration solving process such as Euler, Midpoint, 4th-order Runge-Kutta method to estimate the particles' next status using positions and velocities. Also we applied multi-thread parallel technique for calculating the spring force. Then we compared the performance of each integration methods between under only CPU and CPU with GPU on mobile device. Also we compared the computing time of spring calculation between only CPU and using CPU multi-thread.||2015|10.11591/IJECE.V5I3.PP562-568|S. Min, Min Hong, JaeHong Jeon|0.0|2
1321|Fast parallel algorithm for three-dimensional distance-driven model in iterative computed tomography reconstruction|The projection matrix model is used to describe the physical relationship between reconstructed object and projection.Such a model has a strong influence on projection and backprojection,two vital operations in iterative computed tomographic reconstruction.The distance-driven model(DDM) is a state-of-the-art technology that simulates forward and back projections.This model has a low computational complexity and a relatively high spatial resolution;however,it includes only a few methods in a parallel operation with a matched model scheme.This study introduces a fast and parallelizable algorithm to improve the traditional DDM for computing the parallel projection and backprojection operations.Our proposed model has been implemented on a GPU(graphic processing unit) platform and has achieved satisfactory computational efficiency with no approximation.The runtime for the projection and backprojection operations with our model is approximately 4.5 s and 10.5 s per loop,respectively,with an image size of 256×256×256 and 360 projections with a size of 512×512.We compare several general algorithms that have been proposed for maximizing GPU efficiency by using the unmatched projection/backprojection models in a parallel computation.The imaging resolution is not sacrificed and remains accurate during computed tomographic reconstruction.||2015|10.3788/gzxb20154405.0517002|李磊, 蔡爱龙, 陈建林, 李建新, 闫镔, 席晓琦, 王林元, 张瀚铭|0.0|2
1322|Parallel Approaches to Shortest-Path Problems for Multilevel Heterogeneous Computing|Many graph algorithms have given solution to the problem of finding shortest paths between nodes in a graph. These problems are considered among the fundamental combinatorial optimization problems. They have many applications, such as car/robot navigation systems, traffic simulations, tramp steamer problem, courier-scheduling optimization, Internet route planners, web searching, or exploiting arbitrage opportunities in currency exchange, among others. During the last decades, the interest of the scientific community in these problems has significantly increased not only due to this wide-applicability, but also thanks to the currently popular and efficient parallel computing. Additionally, the advent of new parallel programming models together with modern powerful hardware accelerators, such as the Graphics Processing Units or the many-core XeonPhis boards, may highly improve the performance of previous parallel algorithms, and also has open the possibility to study new and more efficient parallel approaches to exploit these specific architectures. Furthermore, the emerging of heterogeneous parallel computing combining these powerful hardware accelerators with the classical and increasingly powerful CPUs, provides a perfect environment to face the most costly shortest-path problems in the context of High Performance Computing (HPC). However, the programming of hardware accelerators, the optimization of their running times, and also, the coordination of these devices with other computational units of different nature, are still very complex tasks for non-expert programmers. One important indicator of the added complexity found in these environments is the lack of studies to guide the programmer into the correct use of proper values for GPU runtime configuration parameters. Regarding the coordination of different computational devices, there are also few models or frameworks that simplifies the programming when different parallel computing layers are used, to combine the use of many-cores and classical CPU cores present in a shared-memory system, or even from different systems. This Ph.D. thesis addresses both mentioned problems, the algorithmic GPU programming and the heterogeneous parallel coordination in the context of: Developing new GPUbased approaches to the shortest path problem; the study of the tuning of the GPU configuration parameters; and also, designing solutions where both sequential and parallel algorithms are deployed concurrently in heterogeneous environments.||2016|10.35376/10324/16003|Héctor Ortega Arranz|0.0|2
1324|Effiziente SPH-basierte Flüssigkeitssimulation mit Visualisierung auf einem GPU-Cluster|Three dimensional liquid simulations are very important in many fields of research and design. A very popular method for a three dimensional liquid simulation is the so called smoothed particles hydrodynamics method, which is also abbreviated as the SPH method. Such a SPH based liquid simulation is very computationally expensive. But it is also very parallelizable and has a mediocre data parallelism. In addition modern GPUs are parallel computers, which process their instruction data parallel according to the single instruction multiple data model (SIMD model). That is why they are very eligible for such a SPH simulation. Furthermore a SPH simulation also performs very local memory accesses. Thus a SPH simulation is eligible for a GPU cluster, too. That is why this master thesis aims to implement a SPH based liquid simulation in a GPU cluster. Also the thesis aims to optimize and examine the simulation regarding the scalability and the utilization of the GPUs. Additionally a visualization of a liquid simulation is advantageous for the evaluation of its results. But a liquid visualization is also computationally expensive. Therefore this thesis will implement a visualization of the SPH simulation, which is performed by the GPU cluster. For this purpose the thesis first explains OpenGL and CUDA basics. First the hardware structure of a GPU will be explained. Furthermore the execution of GPU programs, so called kernels, is explained. Therefore the SIMD properties or precisely the SIMT properties of a GPU and the related warp execution efficiency are illustrated. Also this thesis explains the latency hiding behavior of the GPUs and the related concept of the occupancy. Next the different memory types of CUDA are presented. Finally the thesis demonstrates the basic OpenGL functionality based on a simple OpenGL pipeline. Subsequently the thesis explains the physical basics of a SPH based liquid simulation. The SPH method discretizes the liquid by particles. Each particle carries properties of the liquid within the volume of the particle, namely the mass, the density and the velocity. Those particles and their properties are used for the short ranged and spatial interpolation of the scalar, vector and gradient fields of the partial differential equations, which define the liquid. This thesis uses the continuity equation of the density and the momentum equation for the definition of the liquid. The interpolation performs a spatial discretization of the partial differential equations. This discretization results in the temporal changes for the variables of each particle. Since the interpolation is short ranged, it is algorithmically considered as a fixed radius near neighbors problem. Such a problem always consists of finding all neighboring particles for each particle within a given cutoff distance. The so found neighbors are used for performing problem specific computations. The solutions of the fixed radius near neighbors problems always need most of the run time of a SPH simulation. After the spatial discretization the liquid simulation performs the temporal discretization by using the temporal changes. Therefore this thesis uses a second order predictor corrector method. Thus the particles are moved. After that the thesis illustrates, how the physical basics are implemented within a SPH simulation, which is initially only computed by a single GPU. An efficient solution of a fixed radius near neighbors problem requires a spatial partitioning data structure. For that this thesis uses a special grid data structure, into which the particles are sorted. Furthermore it uses an atomic version of the counting sort algorithm for the construction of the grid. For the fixed radius near neighbors computation itself this thesis uses the linked cell approach. This approach computes||2015|10.1055/s-0035-1550502|T. Werner|0.0|2
1325|Технология суперкомпьютерного 3D моделирования сейсмических волновых полей в сложно построенных средах|The paper considered computing technology solving problems related to the modeling of seismic wave propagation in inhomogeneous media typical of volcanic structures using supercom-puter simulations in order to create systems of vibroseis monitoring for quake-prone objects. The physico-mathematical model of the magmatic volcano is constructed and software implementation on the basis of the known numerical method that effectively using the architecture of modern su-percomputers equipped with GPU is developed. The parallel 2D and 3D algorithms and software for simulation of elastic wave propagation in a complicated medium (2D model is separation of original 3D model using various angles and planes) on basis of the explicit finite-difference scheme for the shifted grids and CFS-PML method of absorbing boundaries is developed. Scalability of algorithms is investigated. The application of the developed technology allows for much more effi-cient to carry out studies of the structure of the wave field due to the geometry of the internal boundaries and refinement of its kinematic and dynamic characteristics.||2015|10.14529/CMSE150406|Глинский Борис Михайлович, Мартынов Валерий Николаевич, Сапетина Анна Федоровна|0.0|2
1327|Simulating Multiple Realizations of Very Large Reservoir Models using MPI and GPU Acceleration|Demands for higher fidelity and predictive capability from reservoir simulation have resulted in the development of reservoir simulation models in the hundreds of millions of cells. When combined with the need to quickly assess many realizations of such models, the cluster size and power requirements can become impractical. GPUs provide an extremely dense and efficient computational platform that can help reduce the required hardware footprint and power envelope. We discuss our attempt to efficiently scale reservoir simulation to the GPU cluster using a combination of CUDA and MPI. We describe several potential bottlenecks on performance and our strategies to them. We give examples on synthetic and real-field models, assessing both performance and accuracy. Finally, we discuss the surrounding workflow challenges which must be met to make best use of an extremely high-performance simulator.|International Conference on High Performance Computing|2015|10.3997/2214-4609.201414022|K. Mukundakrishnan, J. Gilman, J. Shumway, B. Suchoski, H. Meng, D. Dembeck, Yongpeng Zhang, K. Esler, V. Natoli|0.0|2
1329|Performance comparison of Lattice Boltzmann fluid flow simulation using OpenCL and CUDA frameworks|This paper presents performance comparison, of the lid-driven cavity flow simulation, with Lattice Boltzmann method, example, between CUDA and OpenCL parallel programming frameworks. CUDA is parallel programming model developed by NVIDIA for leveraging computing capabilities of their products. OpenCL is an open, royalty free, standard developed by Khronos group for parallel programming of heterogeneous devices (CPU’s, GPU’s, ... ) from different vendors. OpenCL promises portability of the developed code between heterogeneous devices, but portability has performance penalty. We investigate performance downside of portable OpenCL code comparing to similar CUDA code run on the NVIDIA graphic cards. Lid-driven cavity flow benchmark code, for both examples, has been written in Java programming language, and uses open source libraries to communicate with OpenCL and CUDA. Results of simulations for different grid sizes (from 128 to 896) have been presented and analyzed. Simulations have been carried out on an NVIDIA GeForce GT 220 GPU.||2015|10.2298/apt1546157l|P. Tekic, J. Tekic, M. Rackovic|0.0|2
1332|GPU-based ray tracing algorithm for high-speed propagation prediction in typical indoor environments|A fast 3-D ray tracing propagation prediction model based on virtual source tree is presented in this paper, whose theoretical foundations are geometrical optics(GO) and the uniform theory of diffraction(UTD). In terms of typical single room indoor scene, taking the geometrical and electromagnetic information into account, some acceleration techniques are adopted to raise the efficiency of the ray tracing algorithm. The simulation results indicate that the runtime of the ray tracing algorithm will sharply increase when the number of the objects in the single room is large enough. Therefore, GPU acceleration technology is used to solve that problem. As is known to all, GPU is good at calculation operation rather than logical judgment, so that tens of thousands of threads in CUDA programs are able to calculate at the same time, in order to achieve massively parallel acceleration. Finally, a typical single room with several objects is simulated by using the serial ray tracing algorithm and the parallel one respectively. It can be found easily from the results that compared with the serial algorithm, the GPU-based one can achieve greater efficiency.|SPIE Remote Sensing|2015|10.1117/12.2197384|Zhongyu Liu, Li-xin Guo, Xiaowei Guan|0.0|2
1335|Computational efficiency of high-performance shallow water flow model|A shallow water flow model is parallelized with OpenMP and OpenACC to improve the computational efficiency.The Fujiangsha reach is used to validate the validity of the present model.Results of the numerical simulation indicate that the calculated results are in good agreement with the measured ones and there's no obvious differences compared with the results of sequential program.The two modes of parallel processing can reduce largely the calculating time without losing accuracy.Moreover,the speed-up ratio of GPU parallel program reaches more than 6.7 times.||2015|10.1109/scivis.2015.7429515|B. Jiang|0.0|2
1337|Running Parallel Discrete Event Simulators on Sierra|In this proposal we consider porting the ROSS/Charm++ simulator and the discrete event models that run under its control so that they run on the Sierra architecture and make efficient use of the Volta GPUs.||2015|10.2172/1235388|D. Jefferson, P. Barnes|0.0|2
1340|3D Character Animation: A Brief Review|The Augmented Reality (AR) and Virtual Reality (VR) tech -nologies provide many contents that we can barely experience in reality. Further, the developments in performance of comput-er and GPU are based on putting virtual environment into our reality efficiently. The core technologies in developing contents of AR and VR are 3D character modeling and 3D character ani -mation. These are used in various areas such as 3D film, game industry, Physics, Medical simulation and Educational contents. In this review, we discuss the method of animating contents in the virtual reality. There are two topics of 3D character animation, namely, Fa-cial animation and Body animation. Facial animation implies the representation of natural facial expressions. There are two methods of producing this type of animation. One is the Geom-etry-based approach which is based on control point in 3D char-acter face mesh and the other is Blend shape model-based ap-proach which utilizes 3D facial expression template blending. The tree-structured skeleton is usually used for animating body motion. In addition, to achieve more realistic character move-ments, not only physics but also non-rigid deformation of skin or clothes are adopted. In this paper, we discuss Geometry- and Blend shape model-based of facial animation as well as Skeleton- and Physics-based of body animation.||2015|10.18204/JISSIS.2015.2.2.052|Jiwoo Kang, Hyewon Song, Sanghoon Lee, Suwoong Heo|0.0|2
1343|Modele symulacyjne do szacowania wskaźników niezawodności strukturalnej systemów elektroenergetycznych tworzone na bazie kart graficznych|An algorithmic simulation model of operation for two elements of a power system with the usage of the graphics processing unit (GPU) based on the C++ AMP is implemented. It allows to increase the performance of calculations by the parallel use of several hundred GPU cores compared with several cores of central processing unit (CPU). The test results for the simulation model are presented. (Modelling of the electric power system elements operation in terms of reliability with the usage of graphics processing unit). Słowa kluczowe: GPGPU, symulacja równoległa, C++ AMP, model Markowa.||2017|10.15199/48.2017.03.11|A. Grishkevich|0.0|2
1345|From Atoms to Cells: Interactive and Illustrative Visualization of Digitally Reproduced Lifeforms|Macromolecules, such as proteins, are the building blocks of the machinery of life, and therefore are essential to the comprehension of physiological processes. In physiology, illustrations and animations are often utilized as a mean of communication because they can easily be understood with little background knowledge. However, their realization requires numerous months of manual work, which is both expensive and time consuming. Computational biology experts produce everyday large amount of data that is publicly available and that contains valuable information about the structure and also the function of these macromolecules. Instead of relying on manual work to generate illustrative visualizations of the cell biology, we envision a solution that would utilize all the data already available in order to streamline the creation process. In this thesis are presented several contributions that aim at enabling our vision. First, a novel GPU-based rendering pipeline that allows interactive visualization of realistic molecular datasets comprising up to hundreds of millions of macromolecules. The rendering pipeline is embedded into a popular game engine and well known computer graphics optimizations were adapted to support this type of data, such as level-of-detail, instancing and occlusion queries. Secondly, a new method for authoring cutaway views and improving spatial exploration of crowded molecular landscapes. The system relies on the use of clipping objects that are manually placed in the scene and on visibility equalizers that allows fine tuning of the visibility of each species present in the scene. Agent-based modeling produces trajectory data that can also be combined with structural information in order to animate these landscapes. The snapshots of the trajectories are often played in fast-forward to shorten the length of the visualized sequences, which also renders potentially interesting events occurring at a higher temporal resolution invisible. The third contribution is a solution to visualize time-lapse of agent-based simulations that also reveals hidden information that is only observable at higher temporal resolutions. And finally, a new type of particle-system that utilize quantitative models as input and generate missing spatial information to enable the visualization of molecular trajectories and interactions. The particle-system produces a similar visual output as traditional agent-based modeling tools for a much lower computational footprint and allows interactive changing of the simulation parameters, which was not achievable with previous methods.||2016|10.2312/2631125|Mathieu Le Muzic|0.0|2
1347|Executing Multiple Simulations in the MERPSYS Environment|The work was performed within grant “Modeling efficiency, reliability and power con- sumption of multilevel parallel HPC systems using CPUs and GPUs” sponsored by and covered by funds from the National Science Center in Poland based on decision no DEC- 2012/07/B/ST6/01516||2016||P. Rosciszewski|0.0|2
1352|Fast Perturbation Monte Carlo simulation for heterogeneous medium and its utilization in functional near-infrared spectroscopy|In near-infrared spectroscopy, fiber optic probe is usually applied to incident light into the bio-sample and detect the spatial and temporal resolved optical signal re-emitted from the turbid medium. In this point-source-point-detector measurement system, seed Perturbation Monte Carlo (Pmc) method is an effective model to perform the forward simulation. In our study, the integration of parallel computing with graphics processing units(GPU) into the existing seed Pmc method substantially accelerate the speed of the original simulation. The GPU based seed Pmc provide an excellent solution for the application of fiber optic probe in both homogeneous a heterogeneous turbid medium.||2016|10.1088/1742-6596/680/1/012019|F. Cai, Y. Song, J. W. Li|0.0|2
1356|Plasma physics computations on emerging hardware architectures|This thesis explores the potential of emerging hardware architectures to increase the impact of high performance computing in fusion plasma physics research. For next generation tokamaks like ITER, realistic simulations and data-processing tasks will become significantly more demanding of computational resources than current facilities. It is therefore essential to investigate how emerging hardware such as the graphics processing unit (GPU) and field-programmable gate array (FPGA) can provide the required computing power for large data-processing tasks and large scale simulations in plasma physics specific computations. \n \nThe use of emerging technology is investigated in three areas relevant to nuclear fusion: (i) a GPU is used to process the large amount of raw data produced by the synthetic aperture microwave imaging (SAMI) plasma diagnostic, (ii) the use of a GPU to accelerate the solution of the Bateman equations which model the evolution of nuclide number densities when subjected to neutron irradiation in tokamaks, and (iii) an FPGA-based dataflow engine is applied to compute massive matrix multiplications, a feature of many computational problems in fusion and more generally in scientific computing. The GPU data processing code for SAMI provides a 60x acceleration over the previous IDL-based code, enabling inter-shot analysis in future campaigns and the data-mining (and therefore analysis) of stored raw data from previous MAST campaigns. The feasibility of porting the whole Bateman solver to a GPU system is demonstrated and verified against the industry standard FISPACT code. Finally a dataflow approach to matrix multiplication is shown to provide a substantial acceleration compared to CPU-based approaches and, whilst not performing as well as a GPU for this particular problem, is shown to be much more energy efficient. \n \nEmerging hardware technologies will no doubt continue to provide a positive contribution in terms of performance to many areas of fusion research and several exciting new developments are on the horizon with tighter integration of GPUs and FPGAs with their host central processor units. This should not only improve performance and reduce data transfer bottlenecks, but also allow more user-friendly programming tools to be developed. All of this has implications for ITER and beyond where emerging hardware technologies will no doubt provide the key to delivering the computing power required to handle the large amounts of data and more realistic simulations demanded by these complex systems.||2016||J. Chorley|0.0|2
1358|Research on Parallel Discrete Event Simulator based on a CPU+MIC Platform|The widespread of many-core processors brings new opportunities to enhance performance of PDES applications. MIC is a relative new many-core architecture compared with the widely-used GPU. In our previous paper, experiments for evaluating performance of a PDES simulator ROSS are designed and tested on MIC and CPU respectively. However, no related works have been done on evaluation PDES simulators on CPU+MIC collaborative platform. We observe that the MPI based ROSS performs poorly, while the multi-thread ROSS is not able to work on CPU+MIC platform. We propose a PDES simulator based on a MPI+OpenMP hybrid parallel modeled ROSS which can be called as ROSS-OMPI. We use MPI to handle the interactive events between CPU and MIC, while several OpenMP threads are issued to process events in parallel inside CPU and MIC respectively. The experiment results show that the hybrid model of ROSS brings better performance.||2016|10.2991/AMEII-16.2016.172|Tianlin Li, Yiping Yao, Jin Li, Huilong Chen|0.0|2
1359|Electromagnetic Physics Models for Parallel Computing Architectures|The recent emergence of hardware architectures characterized by many-core or accelerated processors has opened new opportunities for concurrent programming models taking advantage of both SIMD and SIMT architectures. GeantV, a next generation detector simulation, has been designed to exploit both the vector capability of mainstream CPUs and multi-threading capabilities of coprocessors including NVidia GPUs and Intel Xeon Phi. The characteristics of these architectures are very different in terms of the vectorization depth and type of parallelization needed to achieve optimal performance. In this paper we describe implementation of electromagnetic physics models developed for parallel computing architectures as a part of the GeantV project. Results of preliminary performance evaluation and physics validation are presented as well.||2016|10.1088/1742-6596/762/1/012014|S. Vallecorsa, R. Seghal, A. Gheata, C. Bianchini, T. Nikitina, R. Brun, A. Bhattacharyya, A. Aurora, M. Novak, S. Wenzel, R. Iope, W. Pokorski, M. Gheata, A. Ananya, P. Canal, D. Elvira, G. Amadio, A. Ribon, J. Apostolakis, Y. Zhang, O. Shadura, I. Goulas, A. Mohanty, S. Jun, F. Carminati, L. Duhem, M. Bandieramonte, G. Lima|0.0|2
1360|Graphics processing unit accelerated numerical model for collinear holographic data storage system|Collinear holographic data storage system is a promising candidate for next-generation storage technique. Numerical simulation plays a vital role in the process of revealing physical insight into the effectiveness of proposed methods and providing guidance for further system optimization. In this work, we demonstrated a GPU accelerated numerical model for image formation in collinear holographic data storage system. An average 125 times speedup with 99.8% accuracy was achieved with our accelerated model compared to conventional CPU based simulation. Applications of our model for collinear holographic data storage system such as wavelength drift compensation and noise study were demonstrated.|OPTO|2017|10.1117/12.2251280|Yong Huang, Xiao Lin, X. Tan|0.0|2
1361|Numerical Simulation of Cutting Face Parallelization of Computing Research|For the calculation of high precision of discrete model and the cutting tool volume,because of the great amount of 3d image reconstruction calculation,nc machining simulation system which is based on the traditional CPU implementation can 't satisfy the real-time 3d rendering. Based on characteristics of parallelism of GPU,parallel numerical control cutting face simulation show algorithm was proposed: parallel process model body element based on MC algorithm and tool cutting volume by the entity structure of thread,and then look up configuration index table and node number index table according to voxel corner and edge information,so as to accelerate the nc simulation of cutting face extraction and improve the rendering speed of three-dimensional images,and satisfy the requirement of real-time display.||2015|10.1109/iciea.2015.7334294|Yuheng Sha|0.0|2
1362|Paralelização do Método Lattice Boltzmann 2D em CUDA|Computational fluid dynamics has been requesting an increasingly computing capacity for simulating real world scenarios involving fluid flows in aerospace engineering. Graphics processors have gained ground on the acceleration of numerical methods that are employed in several applications, for its use becomes more efficient the higher the arithmetic intensity and the degree of parallelism of the algorithm. The Lattice-Boltzmann Method is known for embracing such characteristics in the context of numerical methods in CFD and its processing in parallel platforms such as GPUs has been shown to be promising. The current work performs a parallel implementation in C language of the LBGK D2Q9 model utilizing CUDA TM for the processing in NVIDIA R © graphics cards. A function for imposing periodic boundary conditions has been developed over a widespread parallelization model within the literature in order to simulate a flow between infinite parallel plates. The latter, for once, is used for verifying the code, demonstrating it is in good agreement with the analytic solution. The verified program was used to simulate a benchmark of lid-driven cavity flow and presented satisfactory qualitative results.||2016|10.33233/fb.v17i1.26|Denis Leite Gomes|0.0|2
1363|GPU - Accelerated Finite Element Forward Solver in Diffuse Optical Tomography|Diffuse Optical Tomography (DOT) is a recently emerging imaging modality that provides optical properties of a tissue. One of main challenges in DOT is the fact that it requires modeling of light propagation in tissue, which is a time consuming and computationally intensive task. However, this process can be accelerated by parallelizing the application. A graphics processing unit (GPU) has the suitable architecture for this task. Therefore, the main goal of this work is to implement a GPU-based solver for the forward problem of DOT. A Finite Element method is utilized to solve the diffusion approximation of the photon transport model in the project. CUDA's parallel architecture and MATLAB software are combined for the implementation of GPU-based forward solver. Several simulations are performed to test computational accuracy and efficiency of the solver. The results show that GPU-based implementation provides a significant speed-up with high accuracy.||2016||Turkay Kart|0.0|2
1364|Evolutionary Inference of Biological Systems Accelerated on Graphics Processing Units|In silico analysis of biological systems represents a valuable alternative and complementary approach to experimental research. Computational methodologies, indeed, allow to mimic some conditions of cellular processes that might be difficult to dissect by exploiting traditional laboratory techniques, therefore potentially achieving a thorough comprehension of the molecular mechanisms that rule the functioning of cells and organisms. In spite of the benefits that it can bring about in biology, the computational approach still has two main limitations: first, there is often a lack of adequate knowledge on the biological system of interest, which prevents the creation of a proper mathematical model able to produce faithful and quantitative predictions; second, the analysis of the model can require a massive number of simulations and calculations, which are computationally burdensome. The goal of the present thesis is to develop novel computational methodologies to efficiently tackle these two issues, at multiple scales of biological complexity (from single molecular structures to networks of biochemical reactions). The inference of the missing data — related to the three-dimensional structures of proteins, the number and type of chemical species and their mutual interactions, the kinetic parameters — is performed by means of novel methods based on Evolutionary Computation and Swarm Intelligence techniques. General purpose GPU computing has been adopted to reduce the computational time, achieving a relevant speedup with respect to the sequential execution of the same algorithms. The results presented in this thesis show that these novel evolutionary-based and GPU-accelerated methodologies are indeed feasible and advantageous from both the points of view of inference quality and computational performances.||2015|10.1016/j.compeleceng.2015.04.006|Marco S. Nobile|0.0|2
1365|Parallel, Data-Driven Simulation and Visualization of the Heart|This thesis focuses on the Lagrangian approach to fluid simulation, its parallelization, and its application in the medical imaging and simulation contexts. The fundamentals of Smoothed Particle Hydrodynamics (SPH) are analyzed, and common implementation techniques are shown. We describe our SPH implementation and show a novel approach to particle-mesh collision resolution. We also focus on the data pre-processing step, so that captured time-varying volumetric heart scans can be directly used to drive the simulation, rather than hand-crafted models. Our new mesh interpolation approach generates intermediate steps to allow stable, higher resolution simulations. Multithreading and GPU parallelism are analyzed, and a multi-CPU approach is shown, which allows the simulation to be highly scalable. We present a visualization framework, VSim, and its application to heart simulations, especially for training, education and collaboration purposes. Additionally, we show the relation between Lagrangian fluids and our previously published work on particle-based hair simulation, and we explore ultrasound volume registration methods with the purpose of enabling blood flow simulations in large volumes.||2016||E. Poyart|0.0|2
1366|A study on computation optimization method for three-dimension scene light field radiation simulation in visible light band|The simulation of high accuracy three-dimension (3D) scene optical field radiation distribution can provide the input for camera design, optimization of key parameters and testing of various imaging models. It can benefit for reducing the strong coupling between the imaging models and scene simulation. However, the simulation computation is extremely large and the non-optimization computing method can’t performed efficiently. Therefore, a study was carried out from the algorithm optimization and using high-performance platform to accelerate the operation speed. On the one hand, the visibility of scene was pre-computed which include the visibility from the light source to each facet in scene and the visibility between facets. The bounding box accelerate algorithm was adopted which can avoid a lot of time-consuming computation of occlusion in the light field radiation simulation process. On the other hand, since the 3D scene light field radiation simulation was obtained by a large number of light approximation, the algorithms can be divided blocks and processed parallelly. The GPU parallel framework was adopted to realize the simulation model of light field radiation in visible band. Finally, experiments were performed. The result shown the proposed method was more efficient and effective compared with the non-optimization method.|Remote Sensing|2016|10.1117/12.2240631|Xin Meng, Wei Ni, Ligang Li, Feifei Shen, Zhen Yang, Xiaoshan Ma|0.0|2
1368|Parallel Methods for Protein Coordinate Conversion|of the Thesis Parallel Methods for Protein Coordinate Conversion by Mahsa Bayati Master of Science in Electrical and Computer Engineering Northeastern University, April 2015 Dr. Miriam Leeser, Adviser Proteins contain thousands to millions of atoms. Their positions can be represented using one of two methods: Cartesian or internal coordinates (bond lengths, angles, etc.). In molecular dynamics and modeling of proteins in different conformational states, it is often necessary to transform one coordinate system to another. In addition, since proteins change over time, any computation must be done over successive time frames, increasing the computational load. To lessen this computational load we have applied different parallel techniques to the protein conversion problem. The Cartesian to internal coordinate translation computes bond distances, bond angles, and torsion angles for each time frame by using the protein chemical structure and atomic trajectories as inputs. This direction is easily parallelizable and we realized several orders of magnitude speed up using various parallel techniques including a GPU implementation. The reverse direction, is used in molecular simulations for such tasks as fitting atomic structures to experimental data and protein engineering. This computation has inherent dependency in the data structures because bond lengths and angles are relative to neighboring atoms. Existing implementations walk over a protein structure in a serial fashion. This thesis presents the first fast parallel implementation of internal to Cartesian coordinates, in which substructures of the protein backbone are converted into their own local Cartesian coordinate spaces, and then combined using a reduction technique to find global Cartesian coordinates. We observed orders of magnitude speedup using parallel processing.||2015|10.1109/hpec.2015.7322478|Mahsa Bayati|0.0|2
1375|GPU-accelerated CFD Simulations for Turbomachinery Design Optimization|Design optimization relies heavily on time-consuming simulations, especially when using gradient-free optimization methods. These methods require a large number of simulations in order to get a remarkable improvement over reference designs, which are nowadays based on the accumulated engineering knowledge already quite optimal. High-Performance Computing (HPC) is essential to reduce the execution time of the simulations. While parallel programming using the CPU is established since more than two decades, the use of accelerators, such as the Graphics Processing Unit (GPU), is relatively recent in design optimization. The GPU has actually a huge computational power comparable to a many-core cluster but concentrated in one device. This raw power is not easy to utilize as entire code parts have to be rewritten using a GPU programming language. Even though high-level standards (e.g. openACC) are able to bring a basic acceleration with a low development effort, it is not simple to get large speedups with these methods. Low-level programming languages are more efficient but different speedups are reported and there is a need for a deep analysis to make the GPU potential more transparent to scientists especially non-experts in HPC. In order to study the GPU acceleration for CFD steady simulations, two in-house CFD solvers have been ported to the GPU; one with explicit and the second with implicit time-stepping. After the porting and the validation of the GPU solvers, the GPU code optimization leads to the identification of a set of key parameters affecting the GPU efficiency. At the same time, both methods have been compared resulting into a performance model and a classification of the GPU acceleration of some CFD operations. The purpose is to enable scientists to take an educated decision concerning the GPU porting of their CPU applications by providing an expected GPU speedup. In addition to the two GPU CFD solvers that are now integrated into the in- house design optimization software package, this research provided key elements to reduce the ambiguity about the GPU potential, namely a qualitative analysis and a classification. These tools can help selecting the best candidate for a breakthrough in CFD acceleration. At the same time, this work identified serious limitations in the preconditioning of a linear system of equations and the limit of today iterative matrix factorization methods in terms of stability and convergence. There is a need for a paradigm shift toward inherently parallel preconditioners. The developed tools have been used for the optimization of a compressor and a turbine cascade resulting into a faster optimization process on the GPU.||2018|10.4233/UUID:1FCC6AB4-DAF5-416D-819A-2A7B0594C369|M. Aissa|0.0|2
1379|High-performance computing on GPUs for resistivity logging of oil and gas wells|We developed and implemented into software an algorithm for high-performance simulation of electrical logs from oil and gas wells using high-performance heterogeneous computing. The numerical solution of the 2D forward problem is based on the finite-element method and the Cholesky decomposition for solving a system of linear algebraic equations (SLAE). Software implementations of the algorithm used the NVIDIA CUDA technology and computing libraries are made, allowing us to perform decomposition of SLAE and find its solution on central processor unit (CPU) and graphics processor unit (GPU). The calculation time is analyzed depending on the matrix size and number of its non-zero elements. We estimated the computing speed on CPU and GPU, including high-performance heterogeneous CPU-GPU computing. Using the developed algorithm, we simulated resistivity data in realistic models.We developed and implemented into software an algorithm for high-performance simulation of electrical logs from oil and gas wells using high-performance heterogeneous computing. The numerical solution of the 2D forward problem is based on the finite-element method and the Cholesky decomposition for solving a system of linear algebraic equations (SLAE). Software implementations of the algorithm used the NVIDIA CUDA technology and computing libraries are made, allowing us to perform decomposition of SLAE and find its solution on central processor unit (CPU) and graphics processor unit (GPU). The calculation time is analyzed depending on the matrix size and number of its non-zero elements. We estimated the computing speed on CPU and GPU, including high-performance heterogeneous CPU-GPU computing. Using the developed algorithm, we simulated resistivity data in realistic models.||2017|10.1063/1.5007422|V. Glinskikh, A. Dudaev, I. Surodina, O. Nechaev|0.0|2
1383|AUTOMATIC VISUALIZATION AND CONTROL OF ARBITRARY NUMERICAL SIMULATIONS|. Visualization of numerical simulation data has become a cornerstone for many in-dustries and research areas today. There exists a large amount of software support, which is usually tied to speciﬁc problem domains or simulation platforms. However, numerical simulations have commonalities in the building blocks of their descriptions (e. g., dimensionality, range constraints, sample frequency). Instead of encoding these descriptions and their meaning into software architecures we propose to base their interpretation and evaluation on a data-centric model. This approach draws much inspiration from work of the IEEE Simulation Interoperability Standards Group as currently applied in distributed (military) training and simulation sce-narios and seeks to extend those ideas. By using an extensible self-describing protocol format, simulation users as well as simulation-code providers would be able to express the meaning of their data even if no access to the underlying source code was available or if new and unforseen use cases emerge. A protocol deﬁnition will allow simulation-domain experts to describe constraints that can be used for automatically creating appropriate visualizations of simulation data and control interfaces. Potentially, this will enable leveraging innovations on both the simulation and visualization side of the problem continuum. We envision the design and development of algorithms and software tools for the automatic visualization of complex data from numerical simulations executed on a wide variety of plat-forms (e. g., remote HPC systems, local many-core or GPU-based systems). We also envisage using this automatically gathered information to control (or steer) the simulation while it is running, as well as providing the ability for ﬁne-tuning representational aspects of the visualizations produced.||2016|10.7712/100016.1894.7269|J. P. Springer, H. Wright|0.0|2
1384|Scale Out vs. Scale Up for Ultra-Scale Reservoir Simulation|It is an undisputed truth that demand for computational performance for simulating very large models in upstream applications is ever increasing. This demand can be met conceptually in one of two ways. “Scale-out”, implies exploiting additional computational nodes, while “scale-up” implies increasing the computational power, particularly floating point throughput and memory bandwidth of each node. In practice, these two approaches provide opposite bounds on a spectrum of cluster designs, from the use of many relatively weak, “thin” nodes, to a smaller number of powerful, “fat” nodes. The scale-out approach gained increasing dominance in HPC as scalability was prefered over absolute efficiency. \n\nOver the past decade, however, energy efficiency has become the key performance limiter. For applications with significant communication requirements, including reservoir simulation, the use of scale-up fat nodes provides an opportunity to localize communications and minimize interconnect traffic, thereby increasing energy efficiency. However, harnessing fat fat nodes comprising of several extremely high-performance GPUs to achieve performance for implicit simulations requires careful software design and novel algorithmic approaches. \n\nWe will first present the algorithmic and computational challenges faced and the approaches needed to efficiently utilize the massive parallelism offered by such scaled-up nodes.||2017|10.3997/2214-4609.201702313|R. Gandham, D. Dembeck, K. Esler, V. Natoli, K. Mukundakrishnan, J. Shumway|0.0|2
1386|GPU-accelerated Immersed Boundary Method for the efficient simulation of biomedical fluid-structure interactions|Immersed boundary methods have become the most usable and useful tools for simulation of \nbiomedical fluid-structure interaction, e.g., in the aortic valve of human heart. In such problems, \ncomplex geometry and motion of the soft tissue impose significant computational cost for bodyfitted- \nmesh methods. Resorting to a fixed Eulerian grid for the flow simulation along with the \nimmersed boundary method to model the interaction with the soft tissue eliminates the expensive \nmesh generation and updating costs. Nevertheless, the computational cost for the geometry \noperations including adaptive search algorithms are still significant. Herein, we implemented the \nimmersed boundary kernels with CUDA to be transferred and executed on thousands of parallel \nthreads on the general purpose GPU. Host-device memory optimisation along with optimal usage \nof GPU multiprocessors results in a boosted performance in fluid-structure interaction simulation.||2016|10.7892/boris.97099|B. Becsek, Hadi Zolfaghari, M. Nestola, D. Obrist, R. Krause|0.0|2
1392|Memory Constrained Iterative Phase Retrieval using GPGPU|Prediction of a scattering experiment for the new generation of coherent X-ray sources, such as X-Ray FreeElectron Laser (XFEL) requires a significant advance in both precision and numerical effectivity of radiation damage modeling. The preferred method of analyzing data from such experiment to obtain structure information relies on an iterative approach of refining an object estimate by repeatedly simulating the beam propagation between exiting the object and arriving at the detector. Currently, the most cost-effective way is to delegate work to General-Purpose computing on Graphics Processing Units (GPGPU), working around its limitations given by a reduced instruction set, limited memory size and increased latency when accessing the rest of the system. In this paper, we present managing fast GPU memory when performing reconstruction of an object consisting of up to 128 million voxels on currently available devices with less than 6GB of onboard memory. Keywords–Radiation scattering; Inverse problem; Reconstruction; GPGPU.||2015||L. Mikes|0.0|2
1399|Acceleration of Large-Scale Electronic Structure Simulations with Heterogeneous Parallel Computing|Large-scale electronic structure simulations coupled to an empirical modeling approach are critical as they present a robust way to predict various quantum phe-nomena in realistically sized nanoscale structures that are hard to be handled with density functional theory. For tight-binding (TB) simulations of electronic structures that normally involve multimillion atomic systems for a direct comparison to experimentally realizable nanoscale materials and devices, we show that graphical processing unit (GPU) devices help in saving computing costs in terms of time and energy consumption. With a short introduction of the major numerical method adopted for TB simulations of electronic structures, this work presents a detailed description for the strategies to drive performance enhancement with GPU devices against traditional clusters of multicore processors. While this work only uses TB electronic structure simulations for benchmark tests, it can be also utilized as a practical guideline to enhance performance of numerical operations that involve large-scale sparse matrices.|High Performance Parallel Computing|2018|10.5772/INTECHOPEN.80997|Oh-Kyoung Kwon, H. Ryu|0.0|2
1401|Virtual Mining Simulation of Cultural Relics Based on Virtual Reality|The virtual mining of cultural relics is an important part of the simulation of interactive cultural relics. This paper proposes a virtual mining simulation of cultural relics based on virtual reality technology. In order to efficiently perform real-time simulation of virtual mining, the entire regular model is used to replace the discrete pit-based model composed of a large number of clay particles, and the dynamic cutting clay pit-based model using a limited number of archaeological shovel sets and the particle system are used for real-time rendering. Firstly, the mesh model and the bump map are used to construct the clay pit-based model. The calculation process is completed by the GPU, and the real-time performance is obviously improved. Then, the posture and inclination of the shovel are continuously adjusted, and obtain a set of different three-dimensional modules that intersect the shovel and pit-based models to determine the mining area. Secondly, combined with the blanking algorithm, the set of intersecting modules is eliminated, and a series of visual effects excavated are displayed to improve the three-dimensional display effect. Finally, the particle system is used for modeling to realize the special effect of soil dust around the excavation. The effectiveness of the system is proved in the experimental simulation.|International Conference on Computer Science and Artificial Intelligence|2018|10.1145/3297156.3297166|Hongan Zhao, Sijia Li, Xinyan Liu|0.0|2
1404|Simplified Ocean Models on GPUs|This paper describes the implementation of three different simplified ocean models on a GPU (graphics processing unit) using Python and PyOpenCL. The three models are all based on the solving the shallow water equations on Cartesian grids, and our work is motivated by the aim of running very large ensembles of forecast models for fully nonlinear data assimilation. The models are the linearized shallow water equations, the non-linear shallow water equations, and the two-layer non-linear shallow water equations, respectively, and they contain progressively more physical properties of the ocean dynamics. We show how these models are discretized to run efficiently on a GPU, discuss how to implement them, and show some simulation results. The implementation is available online under an open source license, and may serve as a starting point for others to implement similar oceanographic models.||2018||A. Brodtkorb|0.0|2
1410|Contribution of Artificial Intelligence and Machine Learning in U.S. DOE's Efforts During the Aftermath of Deepwater Horizon|\n Petroleum Engineering and Analytics Research Laboratory at West Virginia University (PEARL) played a key role in estimating the amount of oil being discharged into the Gulf of Mexico during the Deepwater Horizon disaster in 2010. PEARL's calculation of the amount of oil that was being discharges into the Gulf of Mexico were based on the Smart Proxy technology. Smart Proxy is a unique an innovative implementation of Artificial Intelligence and Machine Learning to develop fast and accurate proxy models of high fidelity numerical simulations that do not compromise the complex physics and the high resolution of the original numerical simulation models. Smart Proxy modeling was implemented by many of PEARL's scholars for their Ph.D. dissertations. Execution of the Smart Proxy models takes from fractions of a second to few minutes on a PC workstation (laptop) to accurately replicate detail results of high fidelity numerical simulation models that take hours or days to run on High Performance Computational facilities (HPC - clusters of large number of CPUs or GPUs).\n As a subcontractor to the U.S. Department of Energy, National Energy Technology Laboratory (NETL), PEARL was asked to join a team of scientists and engineers from across the United States from multiple DOE national Labs in order to start an effort to estimate the amount of oil that was being discharged in the Gulf of Mexico. Since the initial BP's estimate of 1,000 barrels per day seemed an under-estimation (this estimate was later increased to 5,000 barrels per day after the initial estimate were challenged by the U.S. government), the Obama administration asked the secretary of Energy (the Noble Laureate, Steven Chu) to set up a team for providing a realistic estimate of the oil discharge rate into the GOM.\n This article presents a summary the efforts by PEARL in estimating the oil discharge rate into the GOM during the Deepwater Horizon disaster. PEARL's use of Smart Proxy modeling of numerical reservoir simulation models coupled with detail numerical flow in pipe and network models allowed the team to generate millions of scenarios in order to exhaustively examine and quantify the uncertainties associated with all that was unknown and were being estimated at the time of analyses.\n The final estimate of the oil discharge rate into the GOM calculated by PEARL proved to be highly accurate once the well was contained and the flow was actually measure providing a valuable use-case of the application of the application of Artificial Intelligence and Machine Learning in the upstream oil and gas industry.|Day 2 Tue, September 25, 2018|2018|10.2118/191613-MS|S. Mohaghegh|0.0|2
1412|A Large-Scale 3D Simulation of Continuous Social Dynamics using Social Particle Swarm Model on Parallel Architecture|Cooperative relationships are important to maintain high adaptivity in our societies while their structures undergo constant change. The development of information technology made a great impact on our lives and the way we interact; Social Networking Services brought about high continuity in our social interactions in two senses: One is temporal continuity by keeping us constantly connected in time in a non-discrete way, as well as in parallel; through hand-held devices for example, numerous people interact within an extended time frame and mostly are connected at the same time. The second is continuity in the closeness of social relationships. We use various kinds of SNS with different topologies of interactions and communicate with others at different frequencies, which presents continuous variations in the degree of social closeness with others. In addition, the exponential expansion of these networks in the world and consequently, the scale of interactions is another indicator of the major effect of these networks on our social relationships, allowing for larger-scale communication by alleviating the physical boundaries and increasing the rate of interactions over the globe. Based on the concept of self-driven particle models, Nishimoto et al. constructed a simple computational model for investigating such continuously changing social relationships termed social particle swarm (SPS) model (Nishimoto et al., 2013). They assumed that individuals were in a twodimensional social or psychological space, and the proximity between two individuals reflects their social or psychological closeness. Interactions between individuals in the space is regarded as changes in social or psychological relationships based on game-theoretical relationship (e.g., prisoners dilemma) between strategies of individuals. Each particle has a strategy for the prisoners dilemma game, and gets closer to (away from) neighbors from which each particle obtains positive (negative) payoffs. They observed repeated occurrences of explosive dynamics that consisted of a formation of an altruistic cluster followed by its collapse with explosive dispersal of defective particles. The similar dynamics were observed both computationally and experimentally (see (Suzuki et al., 2018)). However, it is still not clear whether and how the significant increase in the scale of social networks, as discussed above, can bring about more diversity and demonstrate emerging patterns for higher-level interactions. Such hierarchical structures and interactions between communities are ubiquitous in real societies, and we expect that larger population sizes enable us to see novel emerging patterns both structural and behavioral. Our purpose is to construct and analyze a large-scale 3D simulation of continuous social dynamics using an extended SPS model on parallel architecture. We use a 3D space as a minimal approximation of a complex of social relationships that have much larger dimensions and comprise many channels of interaction, keeping sufficient spatial locality even when there is a large number of agents. Due to the demanding computational power of large multi-agent systems’ simulations, parallel architecture as a simulation platform best fits our goal. We devised an extension of the original SPS model to 3D space and implemented it using an open source framework for multi-agent simulation on GPGPU, called Flame GPU (Richmond and Chimeh, 2011). We conduct a preliminary analysis on the behavior of the population while varying the population size.|IEEE Symposium on Artificial Life|2018|10.1162/ISAL_A_00092|Zineb Elhamer, Takaya Arita, Reiji Suzuki|0.0|2
1413|Parallel computation of Doppler spectrum from dynamic sea surfaces at microwave bands|The Doppler spectrum of the electromagnetic (EM) scattering field from the two-dimensional dynamic sea surface is calculated based on the composite scattering model. The two-dimensional dynamic sea surfaces are generally simulated as a superposition of large-scale gravity waves and small-scale capillary ripples. On this basis, the Doppler spectrum of the EM scattering field from the two-dimensional dynamic sea surface can be calculated based on the composite scattering model, which takes both the quasi-specular scattering and Bragg scattering mechanism into account. However, due to the high resolution and real-time dynamic complexity of the dynamic sea surfaces, the calculation of the Doppler spectrum will be computationally expensive and very time-consuming. In this paper, a GPU-based algorithm of Doppler spectrum was proposed by utilizing the Tesla K80 GPUs with diverse CUDA optimization techniques. The GPU-based Doppler spectrum implementation includes five optimization strategies: first, the temporary arrays are utilized to reduce the repeat float-points operations in the loop; then the device memory was effectively exploited to reduce the data transfer time between the CPU and GPU; the fast math compiler option was also utilized to further improve the computation performance of the Doppler spectrum calculation; finally the data transfer time between the device and host memories can be effectively hide by using the asynchronous data transfer (ADT). Compared to the CPU serial program executed on Intel(R) Core(TM) i5-3450 CPU, the GPU-based Doppler spectrum implementation can achieve a significant speedup of1200× .|Remote Sensing|2018|10.1117/12.2326957|Jiaji Wu, Zhensen Wu, Jinpeng Zhang, Longxiang Linghu|0.0|2
1420|High accuracy protein structures from minimal sparse paramagnetic solid-state NMR restraints|There is a pressing need for new computational tools to integrate data from diverse experimental approaches in structural biology. We present a strategy that combines sparse paramagnetic solid-state NMR restraints with physics-based atomistic simulations. Our approach explicitly accounts for uncertainty in the interpretation of experimental data through the use of a semi-quantitative mapping between the data and the restraint energy that is calibrated by extensive simulations. We apply our approach to solid-state NMR data for the model protein GB1 labeled with Cu2+-EDTA at six different sites. We are able to determine the structure to ca. 1 Å accuracy within a single day of computation on a modest GPU cluster. We further show that in 4 of 6 cases, the data from only a single paramagnetic tag are sufficient to fold the protein to high accuracy.|bioRxiv|2018|10.1101/463158|C. Jaroniec, Kari Gaalswyk, J. MacCallum, Alberto Pérez|0.0|2
1424|INTERACTIVE AND IMMERSIVE COASTAL HYDRODYNAMIC SIMULATION|In this presentation, we will discuss the development and application of a GPU-based Boussinesq-type wave model. The novelty of this approach is that it is meant to serve the primary purpose of being interactive â€“ allowing the user to modify the boundary conditions and model parameters as the model is running, and to see the effect of these changes immediately. To accomplish this, the model is coded in a shader language environment, and our physical variables (e.g. ocean surface elevation, water velocity) are represented in the model as textures, which can be rapidly rendered and visualized via a GPU. This software can help scientists better understand nearshore wave dynamics as it allows them to observe wave interactions in real-time and modify the boundary conditions and model parameters as the model is running to see the effect of these changes immediately. The model is named â€œCelerisâ€, and is released under the GNU (open-source, open-access) license.|Coastal Engineering Proceedings|2018|10.9753/ICCE.V36.WAVES.63|P. Lynett, S. Tavakkol|0.0|2
1427|Efficient GPU Parallelization of the Agent-Based Models Using MASS CUDA Library|Efficient GPU Parallelization of the Agent-Based Models Using MASS CUDA Library Elizaveta Kosiachenko Chair of the Supervisory Committee: Professor Munehiro Fukuda Computing & Software Systems Agent-based models (ABMs) simulate the actions and interactions of autonomous agents and their effects on the system as a whole. Many disciplines benefit from using ABMs, such as biological systems modeling or traffic simulations. However, ABMs need computational scalability for practical simulation and thus consume a lot of time. Multi-Agent Spatial Simulation (MASS) CUDA is a library, which allows using CUDA-enabled GPUs to perform multi-agent and spatial simulations efficiently while maintaining user-friendly and easily extensible API, which does not require the knowledge of CUDA on the user part. This thesis describes the optimization techniques for the spatial simulation, which allowed us to achieve up to 3.9 times speed-up compared to the sequential CPU execution of the same applications. We also propose solutions to challenges of implementing the support for dynamic agents as part of MASS CUDA library, including agent instantiation and mapping to the places, agent migration, agent replication and agent termination.||2018||E. Kosiachenko|0.0|2
1429|Real-Time Simulation and Prognosis of Smoke Propagation in Compartments Using a GPU|The evaluation of life safety in buildings in case of fire is often based on \nsmoke spread calculations. However, recent simulation models – in general, \nbased on computational fluid dynamics – often require long execution \ntimes or high-performance computers to achieve simulation results in or \nfaster than real-time. \n \n Therefore, the objective of this study is the development of a concept \nfor the real-time and prognosis simulation of smoke propagation in compartments \nusing a graphics processing unit (GPU). The developed concept \nis summarized in an expandable open source software basis, called \nJuROr (Julich's Real-time simulation within ORPHEUS). JuROr simulates \nbuoyancy-driven, turbulent smoke spread based on a reduced modeling \napproach using finite differences and a Large Eddy Simulation turbulence \nmodel to solve the incompressible Navier-Stokes and energy equations. \nThis reduced model is fully adapted to match the target hardware \nof highly parallel computer architectures. Thereby, the code is written \nin the object-oriented programming language C++ and the pragma-based \nprogramming model OpenACC. This model ensures to maintain a single \nsource code, which can be executed in serial and parallel on various \narchitectures. \n \n Further, the study provides a proof of JuROr's concept to balance sufficient \naccuracy and practicality. First, the code was successfully verified \nusing unit and (semi-) analytical tests. Then, the underlying model was \nvalidated by comparing the numerical results to the experimental results \nof scenarios relevant for fire protection. Thereby, verification and validation \nshowed acceptable accuracy for JuROr's application. Lastly, the \nperformance criteria of JuROr – being real-time and prognosis capable \nwith comparable performance across various architectures – was successfully \nevaluated. Here, JuROr also showed high speedup results on a GPU \nand faster time-to-solution compared to the established Fire Dynamics \nSimulator. These results show JuROr's practicality.||2018|10.1136/bmj.k72|Anne Küsters|0.0|2
1430|Mobile phone camera-based SpO2 measurements using broadband light and colored paper filters (Conference Presentation)|The mobile health field has given rise to a surge of point-of-care diagnostic attachments for mobile phones. These attachments, however, are limited in adoption in low-resource settings due to initial acquisition and subsequent maintenance cost challenges. Point-of-care devices that require no or minimum attachment can make a great impact to the accessibility of such devices in resource-poor regions. In this abstract, we report a simulation study to demonstrate the feasibility of using an ultra-low-cost color-paper filter and a mobile phone to perform broadband pulse oximetry. We run a series of GPU-based Monte Carlo simulations using a previously segmented 7T MRI scan of a finger 3D model. We sweep the optical properties of the finger tissues between the wavelengh band of 400-800 nm with a 1 nm increment, with intensity based on the measured spectrum of an iPhone 8’s LED. We also measured the transmission spectra from paper filters of various colors, which we used to further alter the light source spectrum. Using a discretized photoplethysmogram (PPG) signal, we simulate a 60 bpm oscillation optical measurements due to an up to 15% volume changes of the finger arterioles. Simulations were repeated for various peripheral blood oxygen levels (SpO2). Finally, we estimate the SpO2 using the simulated PPG signals using the Ratio of Ratios (RR) method. We evaluate the performance of different color paper filters by comparing 1) total optical signal intensity, 2) maximum magnitude of the RR signal variations and 3) the correlation of the computed and assumed SpO2 values. We found that the purple-colored filter produced the highest RR signal variations and the cyan-colored paper resulted in the largest SpO2 changes in the tested range.|Optics and Biophotonics in Low-Resource Settings V|2019|10.1117/12.2510896|E. Laistler, Anh Phong Tran, Q. Fang, Morris Vanegas|0.0|2
1434|Abstract 235: Integration of Patient-specific Computational Hemodynamics and Vessel Wall Shear Stress Into MRI Diagnosis of Vascular Diseases|The research objective is to expand the capability of current MRI imaging technique in assessing the overall risk and predicted outcomes of atherosclerotic diseases through the quantification of individual patient-specific hemodynamics, including flow, pressure, and wall-shear stress. A unique computational modeling technique, named InVascular, is integrated directly into clinical MRI scanners as the extension of the image reconstruction and post-processing pipeline so that velocity, pressure, vorticity, and WSS can be available immediately with other diagnostic images. InVascular is a unified and GPU accelerated computation platform to model and simulate patient-specific hemodynamics and flow-vessel interaction based on MRI imaging data. In this study, we validate the efficiency and accuracy of InVascular through quantitative hemodynamics in vertebral and carotid arteries. A group of five volunteers participated in the scanning of high resolution time-of-flight (TOF) and low resolution electrocardiogram (ECG) gated phase contrast (PC) MR angiogram (MRA) images. For each case, InVascular successively processes the images to get vessel geometry from TOF MRA and velocity slices from PC MRA and solve the fluid dynamics inside the carotid arteries with PC MRA measured velocity at the inlet and outlet (Fig. 1 a-c). The velocity profiles from Invascular and PC MRA are compared at the same location (Fig. 1 d-g ). We conclude that integration of MRAs and InVascular can well captured the velocity fields as MRI measures. InVascular can provide quantitative pressure and WSS (Fig. 1h ) information as well.||2016|10.1161/res.119.suppl_1.235|S. Kralik, Zhiqiang Wang, Rou Chen, Chen Lin, R. Patil, Xi Chen, Huidan Yu|0.0|2
1435|Simulation of whole mammalian kidneys using complex networks|Modelling of kidney physiology can contribute to understanding of kidney function by formalising existing knowledge into mathematical equations and computational procedures. Modelling in this way can suggest further research or stimulate theoretical development. The quantitative description provided by the model can then be used to make predictions and identify further areas for experimental or theoretical research, which can then be carried out, focusing on areas where the model and reality are different, creating an iterative process of improved understanding. Better understanding of organ function can contribute to the prevention and treatment of disease, as well as to efforts to engineer artificial organs. Existing research in the area of kidney modelling generally falls into one of three categories: • Morphological and anatomical models that describe the form and structure of the kidney • Tubule and nephron physiological models that describe the function of small internal parts of the kidney • Whole kidney physiological models that describe aggregate function but without any internal detail There is little overlap or connection between these categories of kidney models as they currently exist. This thesis brings together these three types of kidney models by computer generating an anatomical model using data from rat kidneys, simulating dynamics and interactions using the resulting whole rat kidney model with explicit representation of each nephron, and comparing the simulation results against physiological data from rats. This thesis also describes methods for simulation and analysis of the physiological model using high performance computer hardware. In unifying the three types of models above, this thesis makes the following contributions: • Development of methods for automated construction of anatomical models of arteries, nephrons and capillaries based on rat kidneys. These methods produce a combined network and three-dimensional euclidean space model of kidney anatomy. • Extension of complex network kidney models to include modelling of blood flow in an arterial network and modelling of vascular coupling communication between nephrons using the same arterial network. • Development of methods for simulation of kidney models on high performance computer hardware, and storage and analysis of the resulting data. The methods used include multithreaded parallel computation and GPU hardware acceleration.||2016|10.54337/nlc.v10.8928|T. Gale|0.0|2
1443|New Binding Mode of SLURP Protein to a7 Nicotinic Acetylcholine Receptor Revealed by Computer Simulations|SLURP-1 is a member of three-finger toxin-like proteins. Their characteristic feature is a set of three beta strands extruding from hydrophobic core stabilized by disulfide bonds. Each beta-strand carries a flexible loop, which is responsible for recognition. SLURP-1 was recently shown to act as an endogenous growth regulator of keratinocytes and tumor suppressor by reducing cell migration and invasion by antagonizing the pro-malignant effects of nicotine. This effect is achieved through allosteric interaction with alpha7 nicotinic acetylcholine receptors (alpha-7 nAChRs) in an antagonist-like manner. Moreover, this interaction is unaffected by several well-known agents specifically alpha-bungarotoxin. In this work, we carry out the conformational analysis of the SLURP-1 by a microsecond-long full-atom explicit solvent molecular dynamics simulations followed by clustering, to identify representative states. To achieve this timescale we employed a GPU-accelerated version of GROMACS modeling package. To avoid human bias in clustering we used a non-parametric clustering algorithm Affinity Propagation adapted for biomolecules and HPC environments. Then, we applied protein-protein molecular docking of the ten most massive clusters to alpha7-nAChRs in order to test if structural variability can affect binding. Docking simulations revealed the unusual binding mode of one of the minor SLURP-1 conformations.|Supercomputing Frontiers and Innovations|2018|10.14529/JSFI180407|V. Tsetlin, D. Kudryavtsev, A. Golovin, A. Zalevsky, Igor Diankin|0.0|2
1454|A2e High Fidelity Modeling: Strategic Planning Meetings|ions will also be a key element in enabling the integration of the full workflow, including pre-processing, post-processing, uncertainty quantification and optimization. In addition to integration, there will be a need for computational steering, i.e., changing the fundamental parameters of a simulation based on information and analytics computed while a simulation is in process. This cost can be mitigated somewhat by the development of toolkits and libraries that implement commonly used functionality efficiently for each architecture. Examples of functionality that might be captured in a library include the following: 1. Linear and nonlinear solvers with common preconditioners 2. Time integrators 3. Input and output 4. Mesh operations, including refinement 5. Discretization templates and element libraries 6. Dynamic load balancing 7. Checkpointing and restart 8. Optimization and uncertainty quantification Even within a library, it may be possible to provide an abstraction for many of the architectural details through the use of templates and interface layers instantiated by libraries such as Kokkos. Another goal is to provide as much flexibility as possible, both in the programming and in the selection of algorithms, deferring many decisions to compile time (in the case of a machine architecture), or even to run time (in the case of heterogeneous node architectures or dynamic selection of discretizations or solvers). Certainly there is a lot of existing work across DOE and DOD laboratories and many academic institutions to develop abstractions and libraries targeted at exascale computers. It will be important to develop collaborations with these institutions and to leverage as much of the existing work as possible. The ultimate goal is to leverage existing research and to achieve portability of the codes, including the portability of “good” performance, across a wide range of architectures, without putting the burden of performance on the wind plant modeler. Performance characterization and optimization of simulation codes It has always been challenging to extract code and workflow performance from large-scale computers. For the past two decades, the dominant HPC architecture has been relatively large, cache-based nodes with distributed memories and message passing based on MPI. However, this basic architecture will continue to evolve to include many-core nodes, vector and GPU||2015|10.2172/1225833|D. Womble, S. Hammond, Michael Sprague, M. Barone|0.0|2
1456|GPU-based parallel method of temperature field analysis in a floor heater with a controller|Abstract A parallel method enabling acceleration of the numerical analysis of the transient temperature field in an air floor heating system is presented in this paper. An initial-boundary value problem of the heater regulated by an on/off controller is formulated. The analogue model is discretized using the implicit finite difference method. The BiCGStab method is used to compute the obtained system of equations. A computer program implementing simultaneous computations on CPUand GPU(GPGPUtechnology) was developed. CUDA environment and linear algebra libraries (CUBLAS and CUSPARSE) are used by this program. The time of computations was reduced eight times in comparison with a program executed on the CPU only. Results of computations are presented in the form of time profiles and temperature field distributions. An influence of a model of the heat transfer coefficient on the simulation of the system operation was examined. The physical interpretation of obtained results is also presented.Results of computations were verified by comparing them with solutions obtained with the use of a commercial program - COMSOL Mutiphysics.||2016|10.1515/eng-2016-0019|J. Forenc|0.0|2
1460|Component-Based Application Code Development Part 2: Demonstration on a Land-Ice Model and Proposed Extension to Other Climate Components.|This paper illustrates the success of the components-based code development strategy for developing a next-generation world-class climate code in the specific case of the Albany/FELIX land-ice solver created as a part of the PISCEES SciDAC3 project. The proposed idea is to find opportunities to apply this approach in other climate areas (e.g., atmosphere, sea-ice, ocean) to make these models more scalable, robust and portable to emerging architectures, to endow these models with improved analysis capabilities (e.g., adjoint-based optimization, embedded uncertainty quantification), all towards a more integrated climate modeling framework that shares software, data standards and tools, and model components. This submission is a companion paper to a submission by A. Salinger on the Agile Components Strategy and Albany code. Background/Research to Date According to a 2012 report by the National Research Council [1], there is a critical need for a next generation of advanced climate models. More specifically, the report calls for climate models to take a more integrated path and use a common software infrastructure while adding regional detail, new simulation capabilities, and new approaches for collaborating with their user community. Although climate models have improved in recent years, much work is needed to make these models reliable and efficient on continental scales, to quantify uncertainties in the models’ outputs, and to port the models to a new generation of high performance computer (HPC) architectures. Many legacy climate models lack advanced analysis capabilities, like sensitivity and adjoint calculations. Moreover, legacy codes need to be rewritten substantially in order to run accurately and efficiently on new architecture machines (e.g., GPUs), as they are based on algorithms optimized for existing architectures (e.g., CPUs). A promising approach for developing next-generation performance-portable solvers with advanced analysis capabilities is the so-called components-based code development strategy1 to building application codes. In this approach, mature, modular libraries are combined using abstract interfaces and template-based generic programming, resulting in a final code that is verified, scalable, fast and robust, and has access to dozens of algorithmic and advanced analysis capabilities. One recent success story for the components-based code development approach in the area of climate modeling is Albany/FELIX 2 [4, 5], an unstructured grid (Fig. 1, left) finite element land-ice solver written using the Trilinos [3] libraries and the Albany [2] code base as a part of the SciDAC3 PISCEES3 project for integration into earth system models (ESMs). The component-based code development approach has led to the rapid development (≈3 FTEs) of this world-class land-ice model with many sophisticated capabilities. The integration of automatic differentiation into the code has enabled robust nonlinear solves, sensitivity analysis, Detailed in a companion idea paper submitted by A. Salinger entitled “Component-Based Application Code Development, Part 1: The Agile Components Strategy and Albany Code”. Finite Elements for Land Ice eXperiments. Predicting Ice Sheet and Climate Evolution at Extreme Scales.||2015|10.2172/1222925|R. Tuminaro, A. Salinger, M. Perego, I. Tezaur|0.0|2
1461|Solver of Multi-GPU Compressible Turbulence Parallel Simulations Used in Aerodynamic Teaching|The focus of the current paper is the development of a Solver named multi-GPU compressible turbulence parallel simulations, and its application in aerodynamic teaching. Test of a space shuttle is chosen to demonstrate the accuracy and acceleration performance of the solver. Some teaching examples in aerodynamics which students may have difficulties in understanding are presented to show the advantages of this solver on helping the teachers and students in aerodynamics. Introduction Computational fluid dynamics (CFD) is playing a more and more important role in science research and engineering technology [1-2], if we import CFD in classroom teaching, the teaching efficiency must be doubled. We have already established aerodynamic lab which is operating well, but only with low speed wind tunnel, we cannot teach and research efficiently on high speed aerodynamics. Now we have developed a CFD solver to well extend the speed range which can used in sub-sonic, trans-sonic and super-sonic fields, and is considerable accuracy, efficient and low-cost. The solver will strongly promote aerodynamic teaching efficiency, not only in science research. The Solver We developed a finite volume computational fluid dynamics solver on the Graphical Processing Unites (GPUs) using CUDA Fortran [3] for compressible turbulence simulations. The solver is implemented with an AUSMPW+ scheme for the spatial discretization, the k-ω SST model for turbulence model, and MPI communication for parallel computing. We calculate a space shuttle model [4] with Mach number 4.95, AOA-5 degree, and Reynolds number 5.26x107. The length of the space shuttle is 0.29m, and highly accurate experimental results were presented. Considering a steady turbulent state, the wall surface adopted the condition of heat-insulating. The grid size is 0.134 billion with 26 zones, the height of first wall surface grid is 1x10 m. 4GPUs has been used in the parallel computing, each of which has a 33.50 million grid size load. Figure 1 shows the pressure distribution along the upper surface of the body symmetric central line, we can see that the computational results revealing the variation of pressure were basically coincide with those got from experiments, which confirm the accuracy and flexibility of the solver. When implementing parallel computing with 4GPUs, every iteration costs 0.667s, and every single GPU costs 0.611s, the parallel efficiency is 91.6%, so the computational efficiency is considerably high. 4th International Conference on Electrical & Electronics Engineering and Computer Science (ICEEECS 2016) Copyright © 2016, the Authors. Published by Atlantis Press. This is an open access article under the CC BY-NC license (http://creativecommons.org/licenses/by-nc/4.0/). Advances in Computer Science Research, volume 50||2016|10.2991/ICEEECS-16.2016.140|Wenbi Cao, L. Song, Song Li, Kai Luo|0.0|2
1463|Simulation-based MDP verification for leading-edge masks|For IC design starts below the 20nm technology node, the assist features on photomasks shrink well below 60nm and the printed patterns of those features on masks written by VSB eBeam writers start to show a large deviation from the mask designs. Traditional geometry-based fracturing starts to show large errors for those small features. As a result, other mask data preparation (MDP) methods have become available and adopted, such as rule-based Mask Process Correction (MPC), model-based MPC and eventually model-based MDP. The new MDP methods may place shot edges slightly differently from target to compensate for mask process effects, so that the final patterns on a mask are much closer to the design (which can be viewed as the ideal mask), especially for those assist features. Such an alteration generally produces better masks that are closer to the intended mask design. Traditional XOR-based MDP verification cannot detect problems caused by eBeam effects. Much like model-based OPC verification which became a necessity for OPC a decade ago, we see the same trend in MDP today. Simulation-based MDP verification solution requires a GPU-accelerated computational geometry engine with simulation capabilities. To have a meaningful simulation-based mask check, a good mask process model is needed. The TrueModel® system is a field tested physical mask model developed by D2S. The GPU-accelerated D2S Computational Design Platform (CDP) is used to run simulation-based mask check, as well as model-based MDP. In addition to simulation-based checks such as mask EPE or dose margin, geometry-based rules are also available to detect quality issues such as slivers or CD splits. Dose margin related hotspots can also be detected by setting a correct detection threshold. In this paper, we will demonstrate GPU-acceleration for geometry processing, and give examples of mask check results and performance data. GPU-acceleration is necessary to make simulation-based mask MDP verification acceptable.|Photomask Japan|2017|10.1117/12.2280841|R. Pearman, Aki Fujimara, Oleg Syrel, B. Su, Leo Pang, M. Pomerantsev, K. Hagiwara|0.0|2
1464|Cutter Engagement Feature Extraction Using Triple-Dexel Representation Workpiece Model and GPU Parallel Processing Function|For an accurate analysis of the cutting force, the cutter engagement feature (CEF) representing the contact area between the cutter and workpiece must be extracted for each small feed motion of the cutter. We previously proposed a method for accelerating the CEF computation using the parallel processing function of a graphics processing unit. To improve the stability in the CEF computation, we propose a novel CEF extraction algorithm using an intersection analysis between the cutter and the workpiece shape. A triple-dexel model is adopted to represent the workpiece shape to realize a highly accurate computation. The algorithm is extended to compute the CEF, not only for the cylindrical surface portion of a cutter but also for the flat area of a flat-end cutter and the spherical surface area of a ball-end cutter. Our cutting simulation system based on this algorithm can compute a single CEF in 0.12 ms to 0.25 ms.|Computer-Aided Design and Applications|2018|10.14733/CADCONFP.2018.189-193|Noboyuki Umezu, Masayoshi Kobayashi, M. Inui|0.0|2
1465|Simulating the cardinal movements of childbirth using finite element analysis on the graphics processing unit|Many problems can occur during childbirth which may lead to instant or future \nmorbidity and even mortality. Therefore the computer-based simulation of the \nmechanisms and biomechanics of human childbirth is becoming an increasingly \nimportant area of study, to avoid potential trauma to the baby and the mother \nthroughout, and immediately following, the childbirth process. Computer-based \nnumerical methods, such as the Finite Element Method, have become more \nwidespread to simulate biological phenomena over the last two decades or so. \nOne of the important aspects of such methods is them being able to accurately \nmodel the underlying physics and biomechanics of biological processes. In the \ncase of the childbirth process, an important role is played by the fetal head and \nits motion as it is being born. The most important manifestations of the head’s \nmotion are described as the cardinal movements. Being able to model the cardinal \nmovements in a computer-based model of the human childbirth process is compulsory as they occur in almost every normal delivery. Many existing simulations \nuse reverse-engineered approaches to model the cardinal movements by imposing \npre-defined trajectories that approximate a real childbirth. These approaches lack \nphysical accuracy and are unable to extend the simulation to unseen scenarios \nwhere for example the childbirth process does not develop normally. To create \na simulation software capable of simulating realistic, including unseen, scenarios, \nand which does not make any pre-simulation assumptions about the cardinal \nmovements, a physical and forward-engineered approach in which the motions of \nthe head are determined by the underlying physics, is required. \nThis thesis presents a simulation system where the physical behaviour of the \nfetal head is modelled during the second stage of childbirth. Accurate models \nof the fetal head and trunk, the maternal uterine cervix, bony pelvis and pelvic \nfloor muscles, were created. Some of these are considered to be rigid bodies in \nthe simulation (fetal head, trunk and maternal bony pelvis), whereas others are \nconsidered to be deformable (maternal uterine cervix and pelvic floor muscles. \nThe Finite Element Method (FEM) is used to model the deformable tissues by \nimplementing the Total Lagrangian Explicit Dynamics (TLED) approach on the \nGPU. The combined TLED/contact mechanics method was first tested on simple hyperelastic objects. Following successful validation, the interaction between the \nfetal head and the deformable tissues was evaluated using a projection based \ncontact method in conjunction with TLED. \nSeveral experiments had to be done to find the required set of factors contributing \nto the occurrence of the cardinal movements. These steps are reported \nin the thesis as well as the results of the final experiments which do exhibit the \nkey cardinal movements of a normal childbirth process, marking the successful, \nand key, contribution of this thesis. \nThe GPU based acceleration allows running the simulation in near real-time, \nwhich makes it possible to create interactive simulations for training purposes \nof trainee obstetricians and midwives. The simulation system presented in this \nwork is also the first step towards a fully patient specific system that would allow \nclinicians to diagnose and/or predict adverse scenarios of childbirth based on the \nMRI scans of real pregnancies prior to labour.||2017||Zelimkhan Gerikhanov|0.0|2
1466|Potential Ideas for an El Capitan Center of Excellence (COE) Around Intelligent Simulation|Proposing a Center of Excellence under the CORAL-2 NRE is a mandatory requirement. The current Sierra COE (CORAL-1) is largely focused on helping LLNL applications make the disruptive transition to a heterogeneous GPU-based system by 2018. We expect a continuation of COE activities around optimizing our broad and diverse application base (of so-called “traditional” simulation codes) optimized for the El Capitan architecture, as well as supporting the underlying software stack (compilers, tools, programming models, etc.) – but do not expect this to require as much effort in the El Capitan COE (assuming a heterogeneous node architecture).||2018|10.2172/1438616|B. Spears, C. Clouse, B. V. Essen, M. McCoy, R. Neely|0.0|2
1467|Memory efficient atmospheric effects modeling for infrared scene generators|The infrared (IR) energy radiated from any source passes through the atmosphere before reaching the sensor. As a result, the total signature captured by the IR sensor is significantly modified by the atmospheric effects. The dominant physical quantities that constitute the mentioned atmospheric effects are the atmospheric transmittance and the atmospheric path radiance. The incoming IR radiation is attenuated by the transmittance and path radiance is added on top of the attenuated radiation. In IR scene simulations OpenGL is widely used for rendering purposes. In the literature there are studies, which model the atmospheric effects in an IR band using OpenGLs exponential fog model as suggested by Beers law. In the standard pipeline of OpenGL, the related fog model needs single equivalent OpenGL variables for the transmittance and path radiance, which actually depend on both the distance between the source and the sensor and also on the wavelength of interest. However, in the conditions where the range dependency cannot be modeled as an exponential function, it is not accurate to replace the atmospheric quantities with a single parameter. The introduction of OpenGL Shading Language (GLSL) has enabled the developers to use the GPU more flexible. In this paper, a novel method is proposed for the atmospheric effects modeling using the least squares estimation with polynomial fitting by programmable OpenGL shader programs built with GLSL. In this context, a radiative transfer model code is used to obtain the transmittance and path radiance data. Then, polynomial fits are computed for the range dependency of these variables. Hence, the atmospheric effects model data that will be uploaded in the GPU memory is significantly reduced. Moreover, the error because of fitting is negligible as long as narrow IR bands are used.|Defense + Security Symposium|2015|10.1117/12.2176852|Seçkin Özsaraç, Caglar Kavak|0.0|2
1469|Particle-in-cell simulations Of highly collisional plasmas on the GPU in 1 and 2 dimensions|During 20th century few branches of science have proved themselves to be more industrially applicable than Plasma science and processing. Across a vast range of discharge types and regimes, and through industries spanning semiconductor manufacture, surface sterilisation, food packaging and medicinal treatment, industry continues to find new usefulness in this physical phenomenon well into 21st century. To better cater to this diverse motley \nof industries there is a need for more detailed and accurate understanding of plasma chemistry and kinetics, which drive the plasma processes central to manufacturing. Extensive efforts have been made to characterise plasma \ndischarges numerically and mathematically leading to the development a number of different approaches. \n \nIn our work we concentrate on the Particle-In-Cell (PIC) - Monte Carlo Collision (MCC) approach to plasma modelling. This method has for a long time been considered computationally prohibitive by its long run times \nand high computational resource expense. However, with modern advances in computing, particularly in the form of relatively cheap accelerator devices such as GPUs and co-processors, we have developed a massively parallel \nsimulation in 1 and 2 dimensions to take advantage of this large increase in computing power. Furthermore, we have implemented some changes to the traditional PIC-MCC implementation to provide a more generalised simulation, with greater scalability and smooth transition between low and high (atmospheric) pressure discharge regimes. We also present some preliminary physical and computational benchmarks for our PIC-MCC implementation providing a strong case for validation of our results.||2015||N. Hanzlikova|0.0|2
1470|SU‐E‐T‐673: Recent Developments and Comprehensive Validations of a GPU‐Based Proton Monte Carlo Simulation Package, GPMC|Purpose: A GPU-based Monte Carlo (MC) simulation package gPMC has been previously developed and high computational efficiency was achieved. This abstract reports our recent improvements on this package in terms of accuracy, functionality, and code portability. Methods: In the latest version of gPMC, nuclear interaction cross section database was updated to include data from TOPAS/Geant4. Inelastic interaction model, particularly the proton scattering angle distribution, was updated to improve overall simulation accuracy. Calculation of dose averaged LET (LETd) was implemented. gPMC was ported onto an OpenCL environment to enable portability across different computing devices (GPUs from different vendors and CPUs). We also performed comprehensive tests of the code accuracy. Dose from electro-magnetic (EM) interaction channel, primary and secondary proton doses and fluences were scored and compared with those computed by TOPAS. Results: In a homogeneous water phantom with 100 and 200 MeV beams, mean dose differences in EM channel computed by gPMC and by TOPAS were 0.28% and 0.65% of the corresponding maximum dose, respectively. With the Geant4 nuclear interaction cross section data, mean difference of primary proton dose was 0.84% for the 200 MeV case and 0.78% for the 100 MeV case. After updating inelastic interaction model, maximum difference of secondary proton fluence and dose were 0.08% and 0.5% for the 200 MeV beam, and 0.04% and 0.2% for the 100 MeV beam. In a test case with a 150MeV proton beam, the mean difference between LETd computed by gPMC and TOPAS was 0.96% within the proton range. With the OpenCL implementation, gPMC is executable on AMD and Nvidia GPUs, as well as on Intel CPU in single or multiple threads. Results on different platforms agreed within statistical uncertainty. Conclusion: Several improvements have been implemented in the latest version of gPMC, which enhanced its accuracy, functionality, and code portability.||2015|10.1118/1.4925036|D. Giantsoudi, Z. Tian, H. Paganetti, N. Qin, X. Jia, A. Pompoš, J. Schuemann, Steve B. Jiang|0.0|2
1471|SU-E-T-112: An OpenCL-Based Cross-Platform Monte Carlo Dose Engine (oclMC) for Coupled Photon-Electron Transport|Purpose: Low computational efficiency of Monte Carlo (MC) dose calculation impedes its clinical applications. Although a number of MC dose packages have been developed over the past few years, enabling fast MC dose calculations, most of these packages were developed under NVidia’s CUDA environment. This limited their code portability to other platforms, hindering the introduction of GPU-based MC dose engines to clinical practice. To solve this problem, we developed a cross-platform fast MC dose engine named oclMC under OpenCL environment for external photon and electron radiotherapy. Methods: Coupled photon-electron simulation was implemented with standard analogue simulation scheme for photon transport and Class II condensed history scheme for electron transport. We tested the accuracy and efficiency of oclMC by comparing the doses calculated using oclMC and gDPM, a previously developed GPU-based MC code on NVidia GPU platform, for a 15MeV electron beam and a 6MV photon beam in a homogenous water phantom, a water-bone-lung-water slab phantom and a half-slab phantom. We also tested code portability of oclMC on different devices, including an NVidia GPU, two AMD GPUs and an Intel CPU. Results: Satisfactory agreements were observed in all photon and electron cases, with ∼0.48%–0.53% average dose differences at regions within 10% isodose line for electron beam cases and ∼0.15%–0.17% for photon beam cases. It took oclMC 3–4 sec to perform transport simulation for electron beam on NVidia Titan GPU and 35–51 sec for photon beam, both with ∼0.5% statistical uncertainty. The computation was 6%–17% slower than gDPM due to the differences in both physics model and development environment, which is considered not significant for clinical applications. In terms of code portability, gDPM only runs on NVidia GPUs, while oclMC successfully runs on all the tested devices. Conclusion: oclMC is an accurate and fast MC dose engine. Its high cross-platform portability makes it clinically attractive.||2015|10.1118/1.4924473|M. Folkerts, Z. Tian, N. Qin, X. Jia, F. Shi, Steve B. Jiang|0.0|2
1476|Analyzing Molecular Simulations Trajectories by Utilizing CUDA on GPU Architecture|With the advent of high-performance computing techniques, the data for analysis has grown significantly. Here, graphic processing unit (GPU) based program kernels are discussed to exploit parallelism in the analysis codes specific to molecular simulations trajectories and data, hence reducing the time consumption. Commonly computed properties of systems which utilize static and dynamic correlations are considered. In static properties, order metrics based on two-particle correlations are shown to exhibit 10-50x speedups relative to conventional serial CPU codes. Efficiency in finding three-particle correlations, which are relatively more time consuming calculations, are also shown to be 10-45x faster depending on system size. In the case of dynamic properties, the viscosity is computed using Green-Kubo formalism at a 10-25x faster rate depending on the correlation time and total trajectory length. Similarly, for mean square displacement calculations, the speedup of approximately 40-80x is achieved for all system sizes and simulation times. Particularly for these embarrassingly parallel computationally intensive problems, the compute unified device architecture application programming interface affords maximum scale up with minimum implementation time. It is shown that all the GPU codes render 1-2 order of magnitude reduction in the analyses times. Hence these codes, in conjunction with GPU accelerated molecular simulations, can lead to an overall improved and efficient performance.||2017|10.1007/978-981-10-4035-1_16|G. Shrivastav, Manish Agarwal|0.0|2
1478|QCDGPU: an Open-Source OpenCL Tool for Monte Carlo Lattice Simulations on Heterogeneous GPU Cluster|QCDGPU is an open-source tool for Monte Carlo lattice simulations of the SU(N) gluodynamics and O(N) models. In particular, the package allows to study vacuum thermodynamics in external chromomagnetic fields, spontaneous vacuum magnetization at high temperature in the SU(N) gluodynamics and other new phenomena. The QCDGPU code is implemented in the OpenCL environment and tested on different OpenCL-compatible devices. It supports singleand multi-GPU modes as well as GPU clusters. Also, the QCDGPU has a client-server part for distributed simulations over VPN. The core of Monte Carlo procedure is based on the PRNGCL library, which contains implementations of the most popular pseudorandom number generators. The package supports single-, mixedand full double-precision including pseudorandom number generation. The current version of the QCDGPU is available online.||2015|10.3204/DESY-PROC-2014-05/29|N. Kolomoyets, V. Demchik|0.0|2
1479|SU‐C‐BRA‐05: Fast Generation of Respiratory Gated CT Images at User Selected Breathing Phases On a Graphics Processing Unit|Purpose: The previously published 5DCT respiratory gated image acquisition and analysis technique enables generation of images at any user selected breathing phase. This work describes acceleration of the image generation process using a graphics processing unit (GPU) and its application to internal target volume (ITV) definition and the creation of simulated cine scans. Methods: 25 fast helical, free breathing CT scans of 7 lung cancer patients were acquired using a low dose protocol with simultaneous breathing surrogate monitoring. For each patient, the first scan was deformably registered to the following 24. Deformation vectors were used to determine voxel-specific parameters of a motion model. A single, low noise reference image in the geometry of the first scan was created using image averaging. The motion model was used to predict the deformation from the reference image to selected breathing phases. Internal target volumes were generated by deforming a single contour of the gross tumor volume (GTV) to the most common breathing phases accounting for 90% of observed respiration. Simulated cines were created by generating volumetric images at 0.25 second intervals along the measured breathing trace and taking slices at desired positions. Computations were performed on an NVIDIA Tesla K40. Results: Calculation of motion model parameters took approximately 3 seconds per dataset. Image generation took approximately 0.25 seconds total for a 450 x 450 x 300 image with isotropic 1 mm3 resolution. Conclusion: GPU acceleration enabled rapid generation of breathing gated CT images using the 5DCT technique and facilitated use of a novel method for defining customized lung tumor ITVs that account for a specified percentage of observed respiration, and the creation of simulated cine images in a clinically acceptable time frame. Investigation of the differences between ITVs generated using the technique described here and ITVs defined on conventional 4DCT datasets is ongoing.||2015|10.1118/1.4923815|L. Yang, D. O'Connell, T. Dou, D. Low, David H. Thomas, J. Lamb|0.0|2
1480|Exploring Heterogeneous Task-Level Parallelism in a BMA Video Coding Application using System-Level Simulation|High abstraction level models can be used within the system-level simulation to allow rapid evaluations of architectural aspects in early Design Space Exploration (DSE) and direct the development decisions. Further, early DSE is of paramount importance in the specification of future Embedded Systems (ES) and its evaluation for applications with high computing demands and energy restrictions. This paper presents the exploration of Heterogeneous Task-Level Parallelism (HTLP) in a Block-Matching Algorithm (BMA) video coding application. HTLP means the creation and execution of simultaneous threads of kernels defined for different types of Processing Elements (PE) - e.g., CPU and GPU - but all for an equal purpose. We employ a BMA implementation as a case study, and its characteristics are used to explore the HTLP - in particular, its kernels for data preparation, SAD (sum of absolute differences) criteria calculation, and SAD values grouping. For the exploration, a system-level simulation framework (SAVE-htlp) is augmented, being able to support the HTLP. In the performed experiments, SAVE-htlp simulates workload and architecture models and explores 22 settings varying the PE type employed during the tasks' execution and the number of concurrent threads for each kernel. Execution time, performance, energy, and power results show HTLP settings overcoming CPU-only ones as well as those with solely GPUs to process its tasks.|Brazilian Symposium on Computing System Engineering|2018|10.1109/SBESC.2018.00020|A. Miele, Mateus Melo, A. Rahmani, Carlos Michel Betemps, N. Dutt, B. Zatt|0.0|2
1484|Simulations and real-time control of adaptive optics systems with GPUs|We present strategies to use GPU as efficient compute engines for AO, both for high performance numerical simulations through end-to-end and analytical models and for real-time control through the use of dedicated hardware to feed the GPU with sensors data. Challenges for ELTs are outlined.||2015|10.1364/AOMS.2015.AOM3F.2|D. Gratadour, A. Sevin, D. Perret, F. Ferreira, J. Brulé, M. Lainé|0.0|2
1485|Large scale simulation of synthetic markets|High-frequency trading has been experiencing an increase of interest both for practical purposes within financial institutions and within academic research; recently, the UK Government Office for Science reviewed the state of the art and gave an outlook analysis. Therefore, models for tick-by-tick financial time series are becoming more and more important. Together with high-frequency trading comes the need for fast simulations of full synthetic markets for several purposes including scenario analyses for risk evaluation. These simulations are very suitable to be run on massively parallel architectures. Aside more traditional large-scale parallel computers, high-end personal computers equipped with several multi-core CPUs and general-purpose GPU programming are gaining importance as cheap and easily available alternatives. A further option are FPGAs. In all cases, development can be done in a unified framework with standard C or C++ code and calls to appropriate libraries like MPI (for CPUs) or CUDA for (GPGPUs). Here we present such a prototype simulation of a synthetic regulated equity market. The basic ingredients to build a synthetic share are two sequences of random variables, one for the inter-trade durations and one for the tick-by-tick logarithmic returns. Our extensive simulations are based on several distributional choices for the above random variables, including Mittag-Leffler distributed inter-trade durations and alpha-stable tick-by-tick logarithmic returns.||2015|10.1685/JOURNAL.CAIM.535|E. Scalas, G. Germano, L. Gerardo-Giorda|0.0|2
1490|A GPU Based Explicit Solid-Shell Finite Element Solver|In this work we present a co-rotational/updated Lagrangian, strain-rate based explicit finite element code which uses hexahedral solid-shell tri-linear elements, intended for simulation of the incremental sheet forming (ISF) process. This element is based heavily on the elements described in [1, 2, 3]: it is under-integrated with a single stack of stress integration points in the thickness direction passing through the elements center; it uses Assumed Natural Strain (ANS) interpolates for the thickness and transverse shear strains; it uses a single parameter Enhanced Assumed Strain (EAS) for the thickness strain; and it selectively scales the mass in the through thickness direction to increase the stable time-step. We combine these methods with a hypo-elastic constitutive model to simulate the ISF process. Initial results obtained with a GPU implementation of the element are presented.|Journal of Physics: Conference Series|2018|10.1088/1742-6596/1063/1/012191|M. Elford, Andrew J. E. Stephan, W. Daniel|0.0|2
1494|Performance of GeantV EM Physics Models|The recent progress in parallel hardware architectures with deeper vector pipelines or many-cores technologies brings opportunities for HEP experiments to take advantage of SIMD and SIMT computing models. Launched in 2013, the GeantV project studies performance gains in propagating multiple particles in parallel, improving instruction throughput and data locality in HEP event simulation on modern parallel hardware architecture. Due to the complexity of geometry description and physics algorithms of a typical HEP application, performance analysis is indispensable in identifying factors limiting parallel execution. In this report, we will present design considerations and preliminary computing performance of GeantV physics models on coprocessors (Intel Xeon Phi and NVidia GPUs) as well as on mainstream CPUs.||2017|10.1088/1742-6596/898/7/072017|S. Vallecorsa, R. Seghal, A. Gheata, T. Nikitina, R. Brun, A. Bhattacharyya, A. Aurora, M. Novak, R. Iope, S. Wenzel, W. Pokorski, Y. Zhang, G. Amadio, M. Gheata, A. Ananya, P. Canal, G. Cosmo, D. Elvira, Calebe P. Bianchini, A. Ribon, J. Apostolakis, O. Shadura, I. Goulas, A. Mohanty, S. Jun, F. Carminati, L. Duhem, G. Folger, M. Bandieramonte, G. Lima|0.0|2
1495|CUDA-Accelerated Simulation of Brownian Dynamics|Molecular simulations are increasingly used to study nanoscale biological and synthetic systems. To model interactions between molecules, we look to Brownian Dynamics. An n-body problem, the calculation of pairwise forces between molecules is naively an O(n2) algorithm. In our work, we begin with a single-threaded C++ code, simulating toluene (C6H5CH3) molecules suspended in water and their interaction with a graphene sheet. Our goal was to study algorithmic improvements for n-body problems and benchmark implementations along the way. We reduce computational cost with Verlet pairlists and cell decomposition and explored strategies to optimize cache utilization. Finally, we implement and benchmark a CUDA-accelerated code targeting the NVIDIA GeForce GTX 1080 Ti GPU.|Practice and Experience in Advanced Research Computing|2018|10.1145/3219104.3229260|George Walker, Daniel Andresen, C. Baldwin, Cortez Gray, Kevin Dice|0.0|2
1496|Monte Carlo Simulations of the Ising Model on GPU|Monte Carlo simulations of twoand three-dimensional Ising model on graphic cards (GPU) are described. The standard Metropolis algorithm has been employed. In the framework of the implementation developed by us, simulations were up to 100 times faster than their sequential CPU analogons. It is possible to perform simulations for systems containing up to 10 spins on Tesla C2050 GPU. As a physical application, higher cumulants for the 3d Ising model have been calculated.||2015|10.12921/CMST.2015.21.02.003|K. Kalinowski, J. Wojtkiewicz|0.0|2
1497|Using laboratory measurement data to improve acoustic simulations and evaluate performance|There are many different uses for the data that comes from a measurement. With the advent of new testing methodologies, data has become finely granular—allowing for precise analysis of the properties and performance of materials, geometries, and even simulations. Raw measurement data can be retained and compared directly to the output of complex acoustic models for development, improvement, evaluation, and eventually a form of calibration. Advances in technology are rapidly removing the limitations of computational power needed to create accurate models of acoustic phenomena in a timely and efficient manner. This allows a progression from limited particle simulations on single workstations, to large scale wave-based simulations using cloud-based GPU computing clusters. Performance will be evaluated, comparing their output to laboratory measurements, with the goal of creating better tools for acoustic design, prediction, experimentation, and development.There are many different uses for the data that comes from a measurement. With the advent of new testing methodologies, data has become finely granular—allowing for precise analysis of the properties and performance of materials, geometries, and even simulations. Raw measurement data can be retained and compared directly to the output of complex acoustic models for development, improvement, evaluation, and eventually a form of calibration. Advances in technology are rapidly removing the limitations of computational power needed to create accurate models of acoustic phenomena in a timely and efficient manner. This allows a progression from limited particle simulations on single workstations, to large scale wave-based simulations using cloud-based GPU computing clusters. Performance will be evaluated, comparing their output to laboratory measurements, with the goal of creating better tools for acoustic design, prediction, experimentation, and development.|Journal of the Acoustical Society of America|2018|10.1121/1.5068008|H. Azad, James J. DeGrandis, Ronald Sauro|0.0|2
1503|A graphics processing unit-accelerated meshless method for two-dimensional compressible flows|ABSTRACT A graphics processing unit (GPU) -accelerated meshless method is presented for solving two-dimensional compressible flows over aerodynamic bodies. The Compute Unified Device Architecture (CUDA) Fortran programming model is employed to port the meshless method from central processing unit to GPU as a way of achieving efficiency, which involves implementation of CUDA kernels and management of data storage structure and thread hierarchy. The CUDA kernel subroutines are designed to meet with the point-based computing of the meshless method. The corresponding point-based data structure and thread hierarchy are constructed or manipulated in the paper by presenting two specific GPU implementations of the meshless method, which are developed for solving Navier–Stokes equations. The Jameson–Schmidt–Turkel scheme is used to estimate the flux terms of the Navier–Stokes equations and an explicit four-stage Runge–Kutta scheme is applied to update the solution at time level. After tuning the performances of the resulting two GPU-accelerated meshless solvers by changing the number of threads in a block, a set of typical flows over aerodynamic bodies are simulated for validation. Numerical results are shown in a comparison with available experimental data or computational values that appear in extant literature with an analysis of code performance. This reveals that the cost of computing time of the presented test cases is significantly reduced for both solvers without losing accuracy, while impressive speedups up to 64 times are achieved due to careful management of memory access.||2017|10.1080/19942060.2017.1317027|Hongquan Chen, Cheng Cao, Jia-Le Zhang|0.0|2
1504|Nuclear Reactor Simulation|A summary is described about nuclear power reactors analyses and simulations in the last decades with emphasis in recent developments for full 3D reactor core simulations using highly advanced computing techniques. The development of the computer code AZKIND is presented as a practical exercise. AZKIND is based on multi-group time dependent neutron diffusion theory. A space discretization is applied using the nodal finite element method RTN-0; for time discretization the θ-method is used. A high-performance computing (HPC) methodology was implemented to solve the linear algebraic system. The numerical solution of large matrix-vector systems for full 3D reactor cores is achieved with acceleration tools from the open-source PARALUTION library. This acceleration consists of threading thousands of arithmetic operations into GPUs. The acceleration is demonstrated for different nuclear fuel arrays giving extremely large matrices. To consider the thermal-hydraulic (TH) feedback, several strategies are nowadays implemented and under development. In AZKIND, a simplified coupling between the neutron kinetics (NK) model and TH model is implemented for reactor core simulations, for which the TH variables are used to update nuclear data (cross sections). Test cases have been documented in the literature and demonstrate the HPC capabilities in the field of nuclear reactors analysis.||2018|10.5772/INTECHOPEN.79723|A. Gómez-Torres, Andres Hernandez, E. Valle-Gallegos|0.0|2
1506|The Investigation of Efficiency of Physical Phenomena Modelling Using Differential Equations on Distributed Systems|This work is dedicated to development of mathematical modelling software. In this dissertation numerical methods and algorithms are investigated in software making context. While applying a numerical method it is important to take into account the limited computer resources, the architecture of these resources and how do methods affect software robustness. Three main aspects of this investigation are that software implementation must be efficient, robust and be able to utilize specific hardware resources. The hardware specificity in this work is related to distributed computations of different types: single CPU with multiple cores, multiple CPUs with multiple cores and highly parallel multithreaded GPU device. The investigation is done in three directions: GPU usage for 3D FDTD calculations, FVM method usage to implement efficient calculations of a very specific heat transferring problem, and development of special techniques for software for specific bacteria self organization problem when the results are sensitive to numerical methods, initial data and even computer round-off errors. All these directions are dedicated to create correct technological components that make a software implementation robust and efficient. The time prediction model for 3D FDTD calculations is proposed, which lets to evaluate the efficiency of different GPUs. A reasonable speedup with GPU comparing to CPU is obtained. For FVM implementation the OpenFOAM open source software is selected as a basis for implementation of calculations and a few algorithms and their modifications to solve efficiency issues are proposed. The FVM parallel solver is implemented and analyzed, it is adapted to heterogeneous cluster Vilkas. To create robust software for simulation of bacteria self organization mathematically robust methods are applied and results are analyzed, the algorithm is modified for parallel computations.||2015|10.20334/2341-M|Andrej Bugajev|0.0|2
1508|Fast and Accurate 3D X ray Image Reconstruction for Non Destructive Test Industrial Applications|2D and 3D X-ray Computed Tomography (CT) is widely used in medical imaging as well as in Non Destructive Testing (NDT) for industrial applications. In both domains, there is a need to reduce the number of projections. In some cases we may also be limited in angles. The measured data are always with errors (measurement and modelling errors). We are consequently almost always in the situation of ill-posed inverse problems. The role of the probabilistic methods and the prior modelling become crucial. For prior modelling, in particular in NDT applications, the object under examination is composed with several homogeneous materials, with several continuous blocs separated by some discontinuities and contours. This type of object is called the piecewise-continuous object. The focus of this thesis on the reconstruction of the picewise continuous or constant, or more generally piecewise homogeneous objects. In summary two main methods are proposed in the context of the Bayesian inference. The first method consists in reconstructing the object while enforcing the sparsity of the discrete Haar transformation coefficients of the object. A hierarchical Bayesian model is proposed. In this method, the unknown variables and parameters are estimated and the hyper-parameters are initialized according to the definition of prior models. The second method reconstruct objects while the contours are estimated simultaneously. The piecewise continuous object is modeled by a non-homogeneous Markovian model, which depends on the gradient of the object, while the gradient also depends on the estimation of the object. In this methods, the semi-supervised system model is also achieved, with the parameters estimated automatically. Both methods are adapted to the 3D big data size reconstructions, in which the GPU processor is used to accelerate the computation. The methods are validated with both simulated and real data, and are compared with several conventional state-of-the-art methods.||2017|10.2174/2352096510666161228124346|Li Wang|0.0|2
1513|Simulation of Newtonian Flows on Sudden Contraction Geometries: GPU Implementation|In this paper, a GPU-based simulation of a Newtonian fluid flow on sudden contraction geometries is presented. The fluid is modeled with the Navier-Stokes equations and solved by the projection method with first order in time and second order in space discretizations. A semi-implicit scheme of finite differences is used in the dicretization process. The solution of the resulting system of linear equations is considered as an optimization problem and is solved by the preconditioned biconjugate gradient stabilized method (BiCGSTAB) implemented on a graphic processor using the CUDA libraries cuSPARSE and cuBLAS.|Research on computing science|2018|10.13053/RCS-147-12-29|Héctor D. Ceniceros, Rigo Alvarado, Juan J. Tapia|0.0|2
1514|Parallel algorithms and data structures for interactive data problems. (Algorithmes et structures de données parallèles pour applications interactives)|The quest for performance has been a constant through the history of computing systems. It has been more than a decade now since the sequential processing model had shown its first signs of exhaustion to keep performance improvements.Walls to the sequential computation pushed a paradigm shift and established the parallel processing as the standard in modern computing systems. With the widespread adoption of parallel computers, many algorithms and applications have been ported to fit these new architectures. However, in unconventional applications, with interactivity and real-time requirements, achieving efficient parallelizations is still a major challenge.Real-time performance requirement shows-up, for instance, in user-interactive simulations where the system must be able to react to the user's input within a computation time-step of the simulation loop. The same kind of constraint appears in streaming data monitoring applications. For instance, when an external source of data, such as traffic sensors or social media posts, provides a continuous flow of information to be consumed by an on-line analysis system. The consumer system has to keep a controlled memory budget and delivery fast processed information about the stream.Common optimizations relying on pre-computed models or static index of data are not possible in these highly dynamic scenarios. The dynamic nature of the data brings up several performance issues originated from the problem decomposition for parallel processing and from the data locality maintenance for efficient cache utilization.In this thesis we address data-dependent problems on two different application: one in physics-based simulation and other on streaming data analysis. To the simulation problem, we present a parallel GPU algorithm for computing multiple shortest paths and Voronoi diagrams on a grid-like graph. To the streaming data analysis problem we present a parallelizable data structure, based on packed memory arrays, for indexing dynamic geo-located data while keeping good memory locality.||2017|10.5151/2594-3626-30539|J. Toss|0.0|2
1515|Acceleration of Stochastic Seismic Inversion in Open CL-based Heterogeneous Platforms|The recently proposed Geostatistical seismic AVO inversion algorithm uses direct sequential simulation and co-simulation as the model parameter space perturbation technique. To reduce the execution time of this iterative geostatistical inversion procedure, a simplified version of the sequential simulation algorithm was parallelized to exploit multi-core Central Processing Units (CPUs). By applying a straightforward functional decomposition of the algorithm, an acceleration of up to 3.5x was observed for a quad-core CPU. This solution is limited not only in scalability but also in the capacity to exploit modern heterogeneous computing systems composed of multiple processors. An efficient parallelization approach of the geostatistical seismic AVO inversion algorithm is here proposed, by considering highly heterogeneous platforms composed by several devices with different computational capabilities. Such a flexible solution is achieved by using the OpenCL API, allowing each part of the algorithm to be easily migrated among the several coexisting CPUs and GPUs.||2015|10.3997/2214-4609.201413611|Rúben Nunes, P. Tomás, A. Soares, N. Roma, F. Pratas, L. Azevedo, Tomás Ferreirinha|0.0|2
1517|GPU-based optimization of pilgrim simulation for hajj and umrah rituals|Tawaf ritual performed during Hajj and Umrah is one of the most unique, large-scale multi-cultural events in this modern day and age. Pilgrims from all over the world circumambulate around a stone cube structure called Ka'aba. Disasters at these types of events are inevitable due to erratic behaviours of pilgrims. This has prompted researchers to present several solutions to avoid such incidents. Agent-based simulations of a large number of pilgrims performing different the ritual can provide the solution to obviate such disasters that are either caused by mismanagement or because of irregular event plans. However, the problem arises due to limited parallelisation capabilities in existing models for concurrent execution of the agent-based simulation. This limitation decreases the efficiency by producing insufficient frames for simulating a large number of autonomous agents during Tawaf ritual. Therefore, it has become very necessary to provide a parallel simulation model that will improve the performance of pilgrims performing the crucial ritual of Tawaf in large numbers. To fill in this gap between large-scale agent-based simulation and navigational behaviours for pilgrim movement, an optimised parallel simulation software of agent-based crowd movement during the ritual of Tawaf is proposed here. The software comprises parallel behaviours for autonomous agents that utilise the inherent parallelism of Graphics Processing Units (GPU). In order to implement the simulation software, an optimized parallel model is proposed. This model is based on the agent-based architecture which comprises agents having a reactive design that responds to a fixed set of stimuli. An advantage of using agents is to provide artificial anomaly to generate heterogeneous movement of the crowd as opposed to a singular movement which is unrealistic. The purpose is to decrease the execution time of complex behaviour computation for each agent while simulating a large crowd of pilgrims at increased frames per second (fps). The implementation utilises CUDA (Compute Unified Device Architecture) platform for general purpose computing over GPU. It exploits the underlying data parallel capability of an existing library for steering behaviours, called OpenSteer. It has simpler behaviours that when combined together, produces more complex realistic behaviours. The data-independent nature of these agent-based behaviours makes it a very suitable candidate to be parallelised. After an in-depth review of previous studies on the simulation of Tawaf ritual, two key behaviours associated with pilgrim movement are considered for the new model. The parallel simulation is executed on three different high-performance configurations to determine the variation in different performance metrics. The parallel implementation achieved a considerable speedup in comparison to its sequential counterpart running on a single-threaded CPU. With the use of parallel behaviours, 100,000 pilgrims at 10 fps were simulated.||2018|10.2298/fil1806173e|A. Majid, N. A. W. Hamid, A. R. Rahiman, Basim J. Zafar|0.0|2
1519|Analysis and numerical methods for stochastic volatility models in valuation of financial derivatives|espanolEl objetivo principal de la tesis se centra en el estudio del modelo de volatilidad estocastica SABR para los subyacentes (activos o tipos de interes) con vista a la valoracion de diferentes productos derivados. En el caso de los derivados de tipos de interes, el modelo SABR se combina con el modelo de mercado de tipos de interes mas popular en estos momentos, el LIBOR market model (LMM). Los metodos numericos de valoracion son fundamentalmente de tipo Monte Carlo y la resolucion numerica de los modelos de ecuaciones en derivadas parciales (EDPs) correspondientes. Las EDPs asociadas a modelos SABR/LIBOR tienen alta dimension en espacio, por lo que se estudian tecnicas de sparse grid para vencer la maldicion de la dimension. Ademas, se discute detalladamente como calibrar los parametros de los modelos a las cotizaciones de mercado, para lo cual se propone el uso del algoritmo de optimizacion global estocastica Simulated Annealing. Los algoritmos citados tienen un alto coste computacional. Con el objetivo de que tanto las valoraciones como las calibraciones se hagan en el menor tiempo posible se emplean diferentes tecnicas de computacion de altas prestaciones (multicomputadores, multiprocesadores y GPUs.) Finalmente se dise~na un nuevo algoritmo basado en Least-Squares Monte Carlo (LSMC) para aproximar la solucion de Backward Stochastic Differential Equations (BSDEs). EnglishThe main objective of this thesis concerns to the study of the SABR stochastic volatility model for the underlyings (equity or interest rates) in order to price several market derivatives. When dealing with interest rate derivatives the SABR model is joined with the LIBOR market model (LMM) which is the most popular interest rate model in our days. In order to price derivatives we take advantage not only of Monte Carlo algorithms but also of the numerical resolution of the partial di erential equations (PDEs) associated with these models. The PDEs related to SABR/LIBOR market models are high dimensional in space. In order to cope with the curse of dimensionality we will take advantage of sparse grids. Furthermore, a detailed discussion about the calibration of the parameters of these models to market prices is included. To this end the Simulated Annealing global stochastic minimization algorithm is proposed. The above mentioned algorithms involve a high computational cost. In order to price derivatives and calibrate the models as soon as possible we will make use of high performance computing (HPC) techniques (multicomputers, multiprocessors and GPUs). Finally, we design a novel algorithm based on Least-Squares Monte Carlo (LSMC) in order to approximate the solution of Backward Stochastic Di erential Equations (BSDEs).||2016|10.1007/978-3-319-23413-7_11|L. Salas, J. German|0.0|2
1520|Geometry segregated CFD model solving framework for complex geometry calculation|Abstract The computational fluid dynamics (CFD) simulators emerged as an excellent tool to support engineering problem-solving in the past decades. There are limits to the application of a CFD simulator, for example, the validation need of the model, and the computation demand of the simulation. With multiple core computers, the computation time can be lowered, and GPU computing can also be a way to decrease the computation demand. But in some cases, for example in catalyst beds, the number of individual particles is too high to calculate with a CFD simulator. In this study, we show a way to segregate the geometry of the device into smaller parts (decomposition of the geometry) and calculate only the parts of the simulation at a time, instead of whole. In this way, the computation need can be significantly lowered, without losing the crucial information, which can be stored between the geometrical steps. In this article, a framework was developed for the segregation of the geometry. The operation of the framework is shown using different case studies.|Chemical Engineering Communications|2020|10.1080/00986445.2019.1596898|A. Egedy|0.0|2
1521|Workload-aware Scheduling Techniques for General Purpose Applications on Graphics Processing Units|In the last decade, there has been a wide scale adoption of Graphics Processing Units (GPUs) as a co-processor for accelerating data-parallel general purpose applications. A primary driver of this adoption is that GPUs offer orders of magnitude higher floating point arithmetic throughput and memory bandwidth compared to their CPU counterparts. As GPU architectures are designed as throughput processors, they adopt a manycore architecture with 10 to 100s of cores, each with multiple vector processing pipelines. A significant amount of the die area is dedicated to floating point units, at the expense of not having hardware units used for memory latency hiding in conventional CPU architectures. The quintessential technique used for memory latency tolerance is exploiting data-level parallelism in the workload, and interleaving execution of multiple SIMD threads, to overlap the latency of threads waiting on data from memory with computation from other threads. With each architecture generation, GPU architectures are providing an increasing amount of floating point throughput and memory bandwidth. Alongside, the architectures support an increasing number of simultaneously active threads. We envision that to continue making advancements in GPU computing, workload-aware scheduling techniques are required. In the GPU computing work flow, scheduling is performed at three levels the system or chip level, the core level and the thread level. The work proposed in the research aims at designing novel workload aware scheduling techniques at each of the three levels of scheduling. We show that GPU computing workloads have significantly varying characteristics, and design techniques that monitor the hardware state to aide at each of the three levels of scheduling. Each technique is implemented in a cycle level GPU architecture simulator, and their effect on performance is analyzed against state of the art scheduling techniques used in GPU architectures.||2017|10.1007/978-981-10-4765-7_58|Mihir Awatramani|0.0|2
1524|Visualization Methods for Simulation-based Natural Disaster Risk Assessments of Road Networks|Infrastructure managers are in charge of ensuring that infrastructure provides an acceptable level of service to society. To evaluate whether such a level of service can be maintained in the case of natural disasters, a generic risk assessment process was developed in the INFRARISK project. This process accounts for the complex interactions between a multitude of spatio-temporal systems, which may ultimately lead to negative impacts on society due to impairment of the infrastructure networks. Implementations of this process rely heavily on computer support to perform a large number of natural disaster simulations. The vast volume of resulting data additionally undergoes several aggregation steps, which leads to a high variety of different data classes, each with unique characteristics. The overall goal of this thesis is the development of visualization methods that enable an efficient analysis of the data generated during such risk assessments. For this purpose, a simulation environment was engineered to allow the execution of a multitude of simulations to investigate changes in the evolution of the modeled systems due to different input parameters. The simulation results are automatically postprocessed to yield several risk measures. Visualization methods described in the literature and suitable for the analysis of each of these data classes were determined. In two cases, no appropriate visualization method could be found. To fill these gaps, the state dependency graph (SDG) and the rendering-based analysis techniques were developed. The SDG facilitates the understanding of single simulations by representing each generated state of a system as a node and the dependencies between these states as edges. Encoding state information in the visual variables of the nodes and enriching the edges with impact information, such as maps depicting the location of a landslide, allow to easily comprehend the evolution of the represented systems, determine how impacts propagate through them, and compare the outcomes of different simulations. This technique is particularly useful when used with an interactive map in which the geospatial representation of a state of interest is displayed. Including maps in a visual analysis tool requires a powerful rendering engine that allows to navigate along time-series of data in real-time. Hence, the concept of rendering-based geospatial analysis is investigated. This approach allows to undertake typical geospatial operations as part of the GPUaccelerated rendering pipeline interactively, which is advantageous when dealing with a large volume of geospatial datasets. Because of the high speed at which these operations are conducted, the resulting datasets do not need to be physically stored, but can be immediately recomputed when required. This is useful when comparing different states by generating difference maps, for example, or when computing distance fields and buffers to obtain proximity information. In addition, the effects of parameter changes for many methods such as kernel density estimation and inverse distance weighting can immediately be investigated for multiple states. This simplifies the validation of the chosen parameters. The developed methods were integrated into a prototypical visualization tool that was successfully applied to risk assessment results of the INFRARISK project for the region of Chur, Switzerland.||2017|10.3929/ethz-b-000248952|M. Heitzler|0.0|2
1530|Steel Structure Prediction for Continuous Steel Casting by Means of a Parallel GPU-Based Heat Transfer and Solidification Model|Continuous casting of steel is currently a predominant production method of steel, which is used for more than 95% of the total world steel production. An effort of steelmakers is to cast high-quality steel with a desired structure and with a minimum number of defects, which reduce the productivity. The paper presents our developed GPU-based heat transfer and solidification model for continuous casting, which is coupled with a submodel used for the prediction of the steel micro-structure. The model is implemented in CUDA/C++, which allows for rapid computing on NVIDIA GPUs. The time-dependent temperature distribution calculated by the thermal model is iteratively passed to the submodel for the steel micro-structure prediction. The structural submodel determines the spatially-dependent rates of temperature change in the strand, for which the interdendritic solidification model IDS predicts the micro-structure of steel. The paper presents preliminary simulation results for the steel grade used for pressure vessel plates, which is sensitive to rapid cooling rates.Copyright © 2015 by ASME||2015|10.1115/IMECE2015-51425|L. Klimeš, T. Mauder, J. Stetina|0.0|2
1536|Waterblock Modelling for GPU Liquid Cooling|Nowadays, cooling of electronic chips is one of the most serious challenges due to the exponential growth in the demand of increasingly powerful computer systems; the overheating of these components has become a problem of high importance. For this reason, the new cooling technologies such as liquid cooling systems replace the conventional air-cooling systems to avoid the effect of hotspots generated on a chip. To make matters worse, the current use of video games is requiring a tremendous amount of energy dissipation, over passing the cooling requirements of CPUs. Therefore, in this paper a new geometry is proposed to keep cool the graphic processor unit (GPU) in a CPU, using water as the working fluid. The main aim of the design is to enhance the heat dissipation in the GPU, decrease the pressure drop during the cooling process and reduce the amount of material used to build the waterblock. A numerical simulation solves the energy and momentum equations. The thermal performance of the proposed geometry is compared with a commercial heat sink geometry previously characterized. The results for the new geometry show that greater heat dissipation is not reached (results are about the same as the results for the commercial geometry) but due to the modification made, there is less pressure drop, while reducing the size of the waterblock. These results make this new geometry quite a good candidate for the new state of the art of cooling waterblocks.|Volume 8B: Heat Transfer and Thermal Engineering|2018|10.1115/IMECE2018-88567|A. Hernandez-Guerrero, J. Zúñiga-Cerroblanco, J. Gutierrez-Garcia, L. Luviano-Ortiz, Jose-Carlos Vargas-Vazquez|0.0|2
1539|TUFLOW classic and HPC comparison using a coupled 1D/2D model of Toowoomba|The use of Graphics Processing Units (GPUs) in engineering computing and more specifically in hydraulic modelling has become increasingly prevalent in the last few years. GPUs provide a cost-effective alternative to traditional Computational Processing Units (CPUs) by significantly reducing model run times at a relatively low cost. The TUFLOW GPU solver has been used in the water industry for several years now and delivers reductions in model run times by factors of up to 100 in comparison with TUFLOW Classic. Until recently, a major limitation of the TUFLOW GPU solver was its inability to be coupled to TUFLOW's 1D ESTRY module. With the release of the TUFLOW HPC (Heavily Parallelised Compute) solver in September 2017, this limitation no longer exists. TUFLOW HPC now includes 1D pit, pipe, culvert and open channel functionality and hence fully supports 1D/2D coupling. This means hydraulic modellers are now able to achieve the significant reductions in model run times even when carrying out studies requiring modelling of sub-grid scale features in 1D. This study examines the application of the finite volume HPC scheme and GPU technology to flood modelling, using a coupled 1D/2D TUFLOW model of the upper Gowrie Creek catchment in Toowoomba as a test case. The finite volume TUFLOW HPC model was benchmarked against a corresponding finite difference TUFLOW Classic model calibrated to the January 2011 flood event. To allow robust model benchmarking, both models were based on the exact same data inputs and used the same grid resolution and model parameters. The differences in modelling results between the two numerical approaches are analysed and possible reasons for these differences are investigated. Following model benchmarking, the TUFLOW HPC model was recalibrated to the January 2011 flood event with hydraulic roughness used as the only calibration parameter. The main differences between the HPC and Classic schemes were observed around bridge structures, areas with steep terrain and areas with shallow water and are further discussed in this paper. The study demonstrates that the computational speed of coupled 1D/2D simulations can be significantly reduced by using the HPC solver. The paper addresses the current knowledge gap in regards to HPC modelling and the techniques that can be implemented to achieve an accurate result.||2018|10.29322/ijsrp.8.8.2018.p80103|Mitchell Redenbach, Monika Balicki, W. Carlisle, K. Bashar|0.0|2
1542|Acoustic Vibration of a Fluid in a Three-Dimensional Cavity: Finite Element Method Simulation using CUDA and MATLAB|This paper describes an implementation of a FEM acoustic application on a GPU using C/C++ and CUDA libraries. The acoustic model is a rigid-walled cavity with enclosed fluid and rectangular faces. Natural frequencies were computed using inertia and stiffness matrices in a general eigenvalue problem. These matrices are symmetric, dense and grow in a cubic ratio from the number of divisions in the grid. The model was implemented using cuSOLVER libraries to solve the eigenvalue problem. The MATLAB implementation was performed for CPU in order to compare the results of GPU implementation. The GPU-based Jacobi method in single precision gives the best results, this method is five times faster than the MATLAB implementation. The divide and conquer method in double precision for GPU is the most accurate implementation when comparing with the exact solution of the model. Lastly, the sound pressure distribution in the cavity was graphed using eigenvectors.|International Conference of the Chilean Computer Science Society|2018|10.1109/SCCC.2018.8705226|C. Navarro, Juan F. Chango, Mario A. Gonzalez-Montenegro|0.0|2
1544|Human brain deformation simulation method based on multiple GPUs|The invention relates to a human brain deformation simulation method based on multiple GPUs. A total Lagrangian Method, a central difference method and a nonlinear finite element Neo-Hookean model are used for carrying out explicit iterative simulation on brain deformation, the speed of simulation calculation is quickened through parallel algorithm at GPU ends, after the parallel ability is further improved through introducing the multiple GPUs, a breadth first search method is adopted to carry out data clustering on original data and renumber and minimize a data correlation between nodes, an extra data structure is used to enable a one-to-one correspondence relationship to exist between input data in each node and the calculation process, a stream transmission is further used to realize parallelism between calculation and data transmission, and finally, a multi-core calculation architecture mixing CPUs/GPUs is used for accelerating, and precision and speed requirements in human brain deformation simulation are met. Subtlety and spatio-temporal continuity features in human brain deformation are made full use of, and the brain deformation process can be vividly simulated in a virtual environment constructed by a computer.||2015|10.12792/jjiiae.003.01.01|沈旭昆, 田野, 胡勇|0.0|2
1545|GPU-based acceleration of Monte Carlo simulations for migration-coalescence evolution of gas bubbles in materials|Monte Carlo (MC) simulations based on the migration-coalescence model are important tools for studying the evolution of gas bubbles in materials, a subject of concern across numerous fields. The acceleration of MC simulations is always desired especially when a large simulation system is required to extract highly accurate statistics. The present work focuses on the algorithmic study. A graphics processing unit (GPU)-based implementation of an MC simulation of the migration-coalescence evolution of bubbles is described. The acceleration of simulations involves two factors. In addition to acceleration due to the parallel computing features of GPUs, algorithmic strategies are proposed to improve performance. Considering that bubbles have a finite radius and grow with time, this paper proposes generating a nearest-neighbor-list (NNL) instead of a cutoff-radius-neighbor-list. An evaluation demonstrated that an NNL strategy can improve computational efficiency and, more importantly, conserve valuable GPU memory. This paper also proposes a strategy using many small boxes in one run instead of one large box. An evaluation of this strategy shows that significant promotion of the computational efficiency for generating neighbor-lists can be achieved. Regarding the migration and coalescence process, this paper proposes a method in which a merging table is generated and CUDA atomic operations are applied to bubbles in a bi-directional procedure that handles coalescence events. This method avoids asynchronous reading and writing conflicts that may occur in the GPU implementation of the migration and coalescence process without losing numerical accuracy. The algorithms and implementations presented in this paper would be instructive to the development of simulation codes, for an example the code of object kinetic Monte Carlo, in which the defects have finite sizes.|Modelling and Simulation in Materials Science and Engineering|2019|10.1088/1361-651X/ab1d14|Q. Hou, L. Zhai, Chaoqiong Ma, Jiechao Cui|0.0|2
1549|Closedâ€“loop Optical Integrated Modeling|Adaptive optics has long been an add-on instrument attached to telescopes. Nowadays, with the development of adaptive secondary mirrors, AO is increasingly becoming a part of the telescope itself. As such, AO must now be included at a very early stage in the design of telescopes as they will dictate some stringent requirements like focal plane residual jitter. This requires new modeling tools able to combine finite element models, an optical propagation model, and complex control systems. For the Giant Magellan Telescope, new tools have been developed which combine, in a control system modeling environment, finite element models with optical propagation models. The modeling framework reproduces the telescope dynamics subject to the effects of wind buffeting, for example. It includes the structural coupling of M1 and M2, the mechanical deformations of the mirror surfaces, the dynamic response of the mirror segment actuators and positionners and the feedback of the wavefront sensors of the active and adaptive optics systems. This modeling environment is called DOS for Dynamic Optical Simulation. It relies on commercial software (Matlab, Simulink, Nastran, and Femap) and on custom software for the optical propagation: CEO. CEO (Cuda– Engined–Optics) is a GPU-based and cloud-based geometric and Fourier propagation software. CEO exchanges data with the other simulation components through Simulink using a custom-made Simulink toolbox SimCEO. The paper describes the processes that convert the finite element model of the telescope, the telescope optical prescription and the control algorithms into a format that allows these components to interact with each other. The paper also shows how DOS allows us to evaluate the dynamic performance of the telescope in its different modes of operations.||2017|10.26698/AO4ELT5.0144|D. Schwartz, A. Bouchez, R. Conan, G. Angeli, B. McLeod, F. Quirós-Pacheco, Ben Irrarazaval|0.0|2
1555|GPU optimized parallel implementation of NaSch traffic model|Traffic simulation has a practical interest for modern society, mainly in minimizing the problems of congestion, which is the main cause of road accidents. In this way, several traffic models were developed based on Cellular Automata (CA). Moreover, the simulation of traffic system requires a high computing ability. Therefore, this paper explores the ways to use the graphics-processing unit (GPU) for reducing time simulation of such applications. We propose several software implementations to maximize GPU performance for NaSch model parallelization. Compared to CPU, the simulation results show that we need optimization effort to get better performance results for GPU.|Proceedings of the Third International Conference on Computing and Wireless Communication Systems, ICCWCS 2019, April 24-25, 2019, Faculty of Sciences, Ibn Tofaïl University -Kénitra- Morocco|2019|10.4108/EAI.24-4-2019.2284233|Abdessamad El Rharras, M. Wahbi, Rachid Saadane, Yassine El Hafid|0.0|2
1561|Heart-function modeling for the masses|Computer Science\nModeling cardiac dynamics allows scientists to understand individual heart behaviors, such as arrhythmia. These models typically require supercomputers to solve an individualized network of differential equations that capture the fluid dynamics within a heart. Kaboudian et al. translated popular cardiac models to run on graphics processing units, or GPUs, that normally handle image and video processing. The result is a massively parallel simulation that can run quickly inside a web browser on a standard mobile phone. This technology may be broadly applicable to many other computationally expensive biomedical calculations.\n\nSci. Adv. 10.1126/sciadv.aav6019 (2019).|Science|2019|10.1126/SCIENCE.363.6434.1411-I|A. Clauset, Kollen Post|0.0|2
1564|GPU-based distortion correction for CMOS positioning camera using star point measurement|Due to machining errors and assembly errors, the optical system of the camera is somewhat different from the ideal model through the pinholes. That means, there are different degrees of non-linear optical distortion between the ideal image and the actual image formed by the camera. In order to improve the positioning accuracy of the camera, distortion correction should be done. This paper puts forward a GPU-Based Star Calibration and Proofreading Algorithm (GBSC-PA). This algorithm is completed by CUDA programmable graphics processor technologies, and a number of lens distortion coefficients are solved using star point images. Moreover, the optimal distortion correction matrix can be used to improve the accuracy and speed of the algorithm greatly. Through the experimental measurements and simulation, the feasibility and efficiency of the new algorithm are verified. Compared with the conventional correction algorithm based on CPU, the speed-up ratio of GPU-based parallel program can be reached to 43.094.|2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)|2018|10.1109/ITOEC.2018.8740375|Weicheng Cao, Fan Bu, Yuehong Qiu, Dalei Yao, Li Wang|0.0|2
1567|An Advanced Wildfire Simulator: vFirelib.v2|Wildfires can cause severe amounts of damage to wildlife habitat as well as commercial and residential properties. They put at risk the well-being of the environment and the firefighters who combat them to minimize damage and ensure the safety of the public. A fire simulator that is accurate, real-time, and convenient to use can be of great assistance in developing the most efficient and safest plan to stop a wildfire’s spread. This thesis presents an advanced wildfire simulator. The application has two components: a web-based application and a 3D virtual reality application. The web-based application offers portability, speed, and ease of use. This has the added advantage in supporting many different platforms and devices. The 3D virtual reality application provides a real-time on the ground scene perspective. Using a GPGPU framework we can calculate faster than real-time results of an entire wildfire simulation employing the fire model developed by Richard C. Rothermel. This is one of the most widely used fire spread models among wildfire simulators. Taking advantage of GPU’s massively parallel computational power the simulator can compute a large wildfire simulation in a matter of seconds. The wildfire simulator has been advanced with features such as dynamic wind, moisture, and vegetation fuel which allow users to input changes to the environment that can include rain, water drops by helicopter and or plane, curing or saturation of vegetation, firebreaks created by bulldozers or firefighters, and even controlled burns. On top of those features the ability to enable spotting fire effects due to crown fires has been implemented as well as moving a majority of the sequential preprocessing necessary for the fire simulator onto the GPU thus decreasing overall run time.||2018|10.1016/b978-0-12-811151-2.00006-7|Andy García|0.0|2
1568|Development of a simulator for 3D pattern recognition scanners|Shape reconstruction using coded structured light is considered one of the most reliable techniques to recover object surfaces. The aim of the thesis was to develop a simulator capable of emulating such a model. Different types of objects were used for testing the algorithms in order to compare the results obtained with real scanners. The implementation recurs to GPU computing to take advantage of the high computational power. The thesis was carried out in collaboration with Euclid Labs.||2018|10.5121/ijite.2018.7102|A. Rossi|0.0|2
1570|Acceleration of a Computational Simulation Application for Radiofrequency Ablation Procedure Using GPU|Computational simulation is a technique used in several research areas. In the medicine area, the RAFEM program (Radiofrequency Ablation Finite Element Method) was developed, which is used to simulate the RFA (RadioFrequency Ablation) process, that is a medical procedure to treat hepatic cancer. This program presents a high computational time to perform a simulation, taking up to 20 hours for a simulation while the RFA procedure itself lasts from four to six minutes. Some efforts have already been carried out to obtain a better performance for the program [2], however, none of them considered the use of GPUs for acceleration of the application, which is something quite studied for finite element method programs as reported in the work of [3]. The aim of this work is to propose a parallelization approach to explore ways to obtain better performance for the application through the use of GPUs using the CUDA parallel programming interface. Thus CUDA kernels were developed to parallelize the assembly step on the program, which is one of its most costly steps. In the assembly stage of the matrix of each element of the finite element mesh there are no race conditions, however, the same is not true for the grouping of these matrices into a single matrix, the global matrix. This matrix represents the problem as a system of linear equations that models the behavior of the characteristics to be simulated. To perform the assembly of this matrix the technique of graph colouring was used, thus avoiding the race conditions generated by neighboring elements that write in a same position of the matrix. The computational environment used for experiments consists of two Intel Xeon E5-2650 processors and two Nvidia GPUs: one Tesla C2075 and one Quadro M5000. This approach was applied in the parallel version of the application assembly step, improving performance up to 17 times against the sequential code.|Symposium on High Performance Computing Systems|2018|10.1109/WSCAD.2018.00054|C. Schepke, M. Miletto|0.0|2
1573|Разработка параллельного алгоритма для информационно-исследовательской системы «MD-Slag-Melt» на основе технологии CUDA|Development of new materials with predetermined properties is a priority trend in modern science. Here computer modeling (CM), including molecular dynamics method allowing us to determine a set of properties (structural, thermodynamic and transport) and study the relationship of nanostructure and physical-chemical properties, are widely applied. Computer modeling is ensured through the development of automated information systems (AIS) aimed at expanding research boundaries, optimizing and accelerating scientific work. Research and Information System of «Slag Melt» (RIS MD-SLAG-MELT) is a perfect example of such automated systems [1]. RIS’s data domain supposes that without using distributed computing it is currently possible to calculate group behavior of the systems containing tens of thousands of particles at most. However, certain tasks of describing nanoscale three-dimensional clusters require increasing the dimensions of the model system to millions of particles. It is almost impossible to simulate such system on a local computer in a sequential manner due to time input, as the experiment may take several months, hence distributed computing is required. The authors have developed a parallel algorithm adapting the existing linear algorithm (legacy application) of calculating the forces of intermolecular interaction for distributed computing at CPU and GPU computing device and implemented in RIS MD-SLAG-MELT. The algorithm is based on CUDA distributed parallel programming technology.||2015|10.7868/s0424857015040143|В. И. Воронов, А. С. Трунов, Л. И. Воронова|0.0|2
1577|Potential for interactive design simulations in discrete element modelling|Discrete element modelling gained momentum from the mid 90s when small industrial scale simulations of a couple of thousand spherical particles in two dimensions were analysed on a more regular basis. A decade later discrete element modelling simulations allowed for moderate industrial scale simulations of a few hundred thousand spherical particles [1]. Over the last five years the landscape of large scale industrial simulations started to emerge with the utilization of the graphical processing unit (GPU) [2]. It is now common to simulate a couple of million polyhedral shaped particles and tens of millions of spherical particles [2] within a couple of days. In addition, moderate scale industrial simulations can now be modelled within hours on a workstation instead of weeks or months on a cluster merely a decade ago. Combining the advances in high performance GPU based discrete element modelling with sound computational complexity reduction techniques such as reduced order modelling [3] allows for the possibility of design optimization or design modification for industrial problems that involve granular flow. However, conventional design optimization remains characterised by either the analyse-wait-modify-analyse cycle or more recently the batch analysewait-modify-batch analyse cycle. High performance GPU based DEM modelling is enabling a new and alternative paradigm denoted interactive simulation and design (ISD) as demonstrated by BlazeDEM-GPU. BlazeDEM-GPU [2] allows for changes to be made during a simulation in real time utilizing multiple GPUs to conduct a simulation. This allows for an engineer to interactively engage with a simulation to study the effect of various model parameters, geometrical changes of the environment with which the particles interact or changes in boundary conditions in steady state processes or continuous bulk material handling processes e.g. changes in inter-particle cohesion in a feeder system, the effect of lifters height in a ball design or changes in flow rate in a continuous bulk materials handling application. This would allow engineers to interact with the simulation to both quantitatively and qualitatively engage with a design problem or granular material process, allowing for formalized and intuitive understanding of the factors that influence granular flow to ultimately drive towards an enhanced understanding of optimal design solutions. The role that this paradigm will play in education is invaluable as an in-house corporate training tool for young engineers to actively train and develop understanding for specific industrial processes. This would also allow engineers to conduct just-in-time (JIT) simulation based assessment of processes before commencing on actual site visits, allowing for shorter and more focussed site excursions.||2017|10.1007/978-3-319-67988-4_67|D. Wilke, P. Pizette, R. Rajamani, N. Govender|0.0|2
1578|Enabling Large Scale DFT Simulation with GPU Acceleration and Machine Learning|The rapid increase in computational power and the development of linear scaling methods now allow for single-energy DFT calculations of systems with 10.000 1.000.000 atoms. However, the approach is computationally demanding for routine application, especially if first principles molecular dynamics or relaxation is required. In this work, two complementary approaches were developed to boost the performance of large scale DFT calculations. First, the predominant operation, sparse matrix-matrix multiplication, was ported to GPU accelerators. Afterwards, the performance was further increased employing geometry adapted basis sets obtained with machine learning. For an increasing number of supercomputers, the majority of computing power is provided by accelerator devices, including CSCS’s current flagship Piz Daint. To take full advantage of the GPU’s high throughput parallelism within the established and highly MPI+OpenMP-parallel CP2K code base required significant algorithmic development. In particular, to accommodate for the GPU’s asynchronous task based programming model, the matrix multiplication code in CP2K’s sparse matrix library DBCSR was redesigned from the ground up. It was transformed from a lock-step scheme into a barrier free algorithm that relies solely on Cuda streams and events, as wells as asynchronous MPI calls. This software architecture allows for fully overlapping the double-buffered Cannon’s algorithm with host-to-device copies, CPU book-keeping work, and Cuda kernel execution. Additionally, Cuda kernels for representative matrix sizes were developed together with Nvidia engineers as part of a co-design effort. These kernels served as templates for the auto-tuning of over 2350 kernels for domain specific matrix-sizes. For example, the single-node performance for the important 23 matrix size, comparing GPU+CPU vs CPU-only, was improved by 450%. As a consequence, sparse matrix-matrix multiplication has become an important primitive also for other methods in CP2K.||2016|10.3929/ethz-a-010819495|O. Schütt|0.0|2
1579|OPPORTUNITIES FOR INTERACTIVE, PHYSICS-DRIVEN WAVE SIMULATION USING THE BOUSSINESQ-TYPE MODEL, CELERIS|In this paper, we discuss the recent developments of our GPU-based Boussinesq-type wave simulation software, Celeris. This software is meant to serve the primary purpose of being interactive – i.e. allowing the user to modify the boundary conditions and model parameters as the model is running, and to see the effect of these changes immediately. To accomplish this, the model is coded in a shader language environment, and our physical variables (e.g. ocean surface elevation, water velocity) are represented in the model as graphical textures, which can therefore be rapidly rendered and visualized via a GPU. The model may run faster than real-time for problems with practical setups. Following a description of the numerical development of the wave model, we elaborate on the recent features that are added to the software such as irregular waves and uniform time series boundary conditions. Since the model is previously validated for breaking and non-breaking wave, in this paper, we compare the numerical results of the model with experimental results of a current benchmark and show its good agreement.||2017|10.9753/ICCE.V35.WAVES.11|P. Lynett, S. Tavakkol|0.0|2
1582|Development of Massively Parallel Near Peak Performance Solvers for Three-Dimensional Geodynamic Modelling|"We address in this thesis the current need to design new parallel algorithms and tools that ease the development of geodynamic modelling applications that are suited for today's and tomorrow's hardware. We present (1) the MATLAB HPC compiler HPC.m, which greatly simplifies the building of parallel high performance applications and (2) parallel algorithms for the 3D simulation of strongly nonlinear processes as mechanical and reactive porosity waves. To simulate mechanical porosity waves we employ a massively parallel algorithm that permits to resolve the deformation of fluid-filled viscoelastic porous media in 3D. The utilized mathematical model is based on Biot's poroelastic theory, extended to account for viscous deformation and plastic yielding. The modelling results exhibit the impact of decompaction weakening on the formation of three-dimensional solitary-wave-like moving porosity channels. To simulate reactive porosity waves we use a solver for 3D deformation of fluid-filled reactive viscous porous media. The Damk\""ohler number (Da) of the simulations is varied in order to estimate the respective roles of viscous deformation (low Da) and reaction (high Da) on wave propagation. 3D waves are found to propagate independently of their source at constant speed by going through each other for all the investigated Da. Soliton-like wave propagation as a result of metamorphic reaction provides an efficient mechanism for fluid flow in the Earth's crust. We illustrate the great performance and versatility of HPC.m by deploying it to generate solvers for a variety of physics across multiple Earth Science disciplines. All solvers run close to hardware's peak performance and were shown to scale linearly on a institute cluster with 80 GPUs. Moreover, our nonlinear poroviscoelastic two-phase flow solver scales close to ideally on Piz Daint's 5000 GPUs at the Swiss National Supercomputing Centre."|arXiv.org|2022|10.48550/arXiv.2207.08716|L. Räss, Y. Podladchikov, B. Malvoisin, Samuel Omlin|0.0|2
1594|Numerical Assessment for Accuracy and Efficiency of Time Evolution Schemes in TD-DMRG algorithm: CPU vs GPU.|Time dependent density matrix renormalization group (TD-DMRG) has become one of the cutting edge methods of quantum dynamics for complex systems. In this paper, we comparatively study the accuracy and efficiency for three time evolution schemes in TD-DMRG method, the direct propagation and compression with 4th order Runge-Kutta propagator (P&C-RK4), time dependent variational principle with matrix unfolding (TDVP-MU) and projector splitting (PS) by performing benchmarks on the exciton dynamics of Fenna-Matthews-Olson (FMO) complex. We show that the PS scheme is the most accurate and the fastest method because the propagation in each step is unitary by combining Krylov subspace algorithm and there is no error caused by regularization or instability of matrix inversion. We also compare the parallelization efficiency of multi-core central processing units (CPU) with graphical processing units (GPU) and we find that although 28 cores of CPU can merely double the speed, GPU is able to speed up the TDVP-MU and PS scheme by up to 57 times through efficient acceleration of matrix multiplication. Using the optimal strategy, we are able to simulate the full quantum exciton dynamics of 7-site FMO model with 252 degrees of freedom to a timescale of 1 ps at fairly high accuracy within only 4 minutes.||2019|10.1109/ipdpsw.2019.00091|Z. Shuai, Jiajun Ren, Weitang Li|0.0|2
1595|Level ex: tracing all kinds of rays... on mobile|Presented are new methods to raytrace and simulate raytracing of different types of rays through a range of different participating media in real-time, including: *SDF ray-tracing to recreated refractive physics-based viscous fluids that interface with a surrounding soft-body environment and other physics objects in the scene. SDFs are blended seamlessly with each other and environment objects. Special considerations are made to maximize cache and memory-coherence on tile-based mobile GPU architectures. *X-Rays - Which follow logarithmic attenuation functions and fresnel-like behaviors as they are absorbed and scattered through different materials on their way from the emitter to the detector plate. X-ray tracing behaves more like a transparent shadowing technique than anything else. * Ultrasonic Sound waves - Used in real-time ultrasound imaging, these rays break all the rules - you can't even rely on their propagation speed to stay constant. Dozens of different artifact types (shadows, ringing, etc.) must be simulated through the tracing behavior. For example - highly reflective objects outside of the ultrasound beam 'slice' may bounce back into the frame, creating the appearance of 'ghost' objects that aren't actually there. Various mobile-friendly thread bundling approaches are taken to cast and bounce rays in the scene. Worth noting The SDF ray-marching technique in Pulm Ex was shown last year but has been drastically improved with refractions, performance optimizations, and new SDF shapes, blends, and techniques.|ACM SIGGRAPH 2019 Real-Time Live!|2019|10.1145/3306305.3338471|S. Glassenberg, Matthew Yaeger|0.0|2
1596|HPC challenges for the next years: the rising of heterogeneity and its impact on simulations|Computing systems with a large number of processing units are increasingly common, both in the form of processors employing multiple execution cores (e.g., multi-core CPUs, GPUs, accelerators), or computing clusters with a large number of nodes. These many- core architectures bring up new capabilities, opportunities, as well as challenges. Adapting applications effectively use these architectures has involved a significant amount of effort, not only in code modifications, but also in developing the programming environment. However, several industry trends seem to indicate that large-scale homogeneous platforms, which contain a single type of compute engine with a relatively straightforward memory hierarchy, will soon be a thing of the past. The expansion of heterogeneity in multiple dimensions - compute, memory, storage, systems, software, applications, and usage models – leverages an extreme heterogeneity scenario that will affect both the processor industry but also the software industry, with strong repercussions on programming models, scientific workflows and data management systems. In this talk we examine how high-performance computing has changed over the last 10 years, what are the current projects towards exaflop computing, and look toward the future in terms of trends.||2019||L. Steffenel|0.0|2
1598|High-efficiency Numerical Simulation of Electromagnetic Response of Complex Geoelectric Structures Based on GPU Parallel Symplectic Algorithm|For forward modeling of large size and fine structures, the numerical accuracy and computational efficiency are not high due to the stability conditions and dense grid number. In this paper, the symplectic Euler algorithm, surface conformal technique and GPU acceleration technique were combined to establish a precise and high-efficiency numerical model of electromagnetic wave propagation in complex geoelectric structures. And we realized the refined and efficient calculation of the electromagnetic response of arbitrary shape underground target. The results show that the accuracy and efficiency of ground penetrating radar (GPR) forward modeling are greatly improved, which provides a theoretical basis for accurately interpreting GPR detection data, and provides an accurate and efficient forward modeling for the inversion imaging.|IEEE International Conference on Consumer Electronics|2019|10.1109/COMPEM.2019.8779147|H. Fang, Jianwei Lei|0.0|2
1601|Max-plus matrix multiplication library for GPUs - MPMML|MAX-PLUS MATRIX MULTIPLICATION LIBRARY FOR GPUS MPMML Max-Plus algebra finds its applications in discrete event simulations, dynamic programming, biological sequence comparisons etc. Although there exist highly tuned libraries like CUDA Linear Algebra Subprograms (CuBLAS) [1] for matrix operations, they implement the standard matrix-multiplication (multiply-add) for floating points. We found no standard library for MaxPlus-Matrix-Multiplication (MPMM) on integers. Hence,we developed a highly tuned parallelized MPMM library kernel. We chose GPUs as hardware platform for this work because of their significantly more parallelism and arithmetic functional units as compared to CPUs. We designed this kernel to be portable across three successive Nvidia GPU architectures and it achieves performance in the range 3065 GOPs/S 3631 GOPs/S on all of these architectures. We closely followed the benchmarking approach described by Volkov et al. [2] when they contributed to cuBLAS. This MPMM kernel can be part of a max-plus algebra library for GPUs and can help speed up Biological Sequence comparison applications like BPMax.||2019||P. Ghalsasi|0.0|2
1606|Efficient Simulation of Fluid Flow and Transport in Heterogeneous Media Using Graphics Processing Units (GPUs)|Networks of interconnected resistors, springs and beams, or pores are standard models of studying scalar and vector transport processes in heterogeneous materials and media, such as fluid flow in porous media, and conduction, deformations, and electric and dielectric breakdown in heterogeneous solids. The computation time and required memory are two limiting factors that hinder the scalability of the computations to very large sizes. We present a dual approach, based on the use of a combination of the central processing units (CPUs) and graphics processing units (GPUs), to simulation of flow, transport, and similar problems using the network models. A mixed-precision algorithm, together with the conjugate-gradient method is implemented on a single GPU solver. The efficiency of the method is tested with a variety of cases, including pore- and random-resistor network models in which the conductances are long-range correlated, and also contain percolation disorder. Both isotropic and anisotropic networks are considered. To put the method to a stringent test, the long-range correlations are generated by a fractional Brownian motion (FBM), which we generate by a message-passing interface method. For all the cases studied an overall speed-up factor of about one order of magnitude or better is obtained, which increases with the size of the network. Even the critical slow-down in networks near the percolation threshold does not decrease the speed-up significantly. We also obtain approximate but accurate bounds for the permeability anisotropy $K_x/K_y$ for stratified porous media.|arXiv.org|2019|10.1201/9780429020261-5|Hassan Dashtian, M. Sahimi|0.0|2
1607|Simulations of Core Collapse Supernova Explosion on PEZY-SC Processors and GPUs|: The core collapse supernovae are one of key phenomena to understand the history of the Universe and the origin of heavy elements. To understand their explosion mechanism, large scale numerical simulations are essential that require to solve a multi-physics system described by coupled equations of hydrodynamics and neutrino-radiation transfer in multidimensions. Since the neutrino transfer is governed by the Boltzmann equation in six-dimensional space, necessary computational resource rapidly increases as the number of grids in simulations grows. So far numerical studies have been performed mostly on massively parallel computers and only a few studies have been made using accelerator architectures, such as GPUs, despite their large potential. The PEZY-SC processors are novel many-core architecture that have tremendous potential for scientific high-performance computing. While they share typical features with GPUs, one needs to port and optimize an application considering the difference in their multi-level structure of cores and caches to fully make use of their computational performance. In this work, we apply the PEZY-SC processors to the numerical computations of neutrino radiation hydrodynamics under spherically symmetry as a prototype of the multi-dimensional supernova simulations. Adopting the implicit scheme for discretized time evolution, the bottlenecks of the simulations are as follows: (a) An iterative linear equation solver for the coefficient matrix in the evolution equation, (b) Computation of collision term in the Boltzmann equation, and (c) Inversion of a block diagonal matrix used for preconditioning. The most time-consuming part is typically (a), while (b) and (c) are non-negligible. The steps (a) and (c) concern a block tridiagonal matrix composed of O(1000) block matrices each having O(500) ranks. We offload these hot spots to PEZY-SC processors employing PEZY-CL (a variant of OpenCL) in our simulation code based on our previous studies to exploit the GPUs using NVIDIA CUDA framework. We measure the performance on Suiren Blue (wth PEZY-SC) and Suiren2 (PEZY-SC2) systems at KEK and compared with the result on a system with NVIDIA P100 processors. The achieved performance is sufficient for simulations with improved resolutions than the previous model size, which are not sufficient for recent observational progress. We also discuss the similarity and difference of these architectures and how to ease the porting effort as a prospect for application to the multi-dimension simulations.|International Conference on Computational & Experimental Engineering and Sciences|2019|10.32604/icces.2019.05390|H. Matsufuru, K. Sumiyoshi|0.0|2
1611|GPU-Accelerated Implementation of Continuous Constant pH Molecular Dynamics in Amber: pKa Predictions with Single-pH Simulations|We present a GPU implementation of the continuous constant pH molecular dynamics (CpHMD) based on the most recent generalized Born implicit-solvent model in the pmemd engine of the Amber molecular dynamics package. To test the accuracy of the tool for rapid pKa predictions, a series of 2-ns single-pH simulations were performed for over 120 titratable residues in 10 benchmark proteins that were previously used to test the various continuous CpHMD methods. The calculated pKa's showed a root-mean-square deviation of 0.80 and correlation coefficient of 0.83 with respect to experiment. 90% of the pKa's were converged with estimated errors below 0.1 pH units. Surprisingly, this level of accuracy is similar to our previous replica-exchange simulations with 2 ns per replica and an exchange attempt frequency of 2 ps-1 (Huang, Harris and Shen, J Chem Info Model, 2018). Interestingly, for the linked titration sites in two enzymes, although residue-specific protonation state sampling in the single-pH simulations was not converged within 2 ns, the protonation fraction of the linked residues appeared to be largely converged, and the experimental macroscopic {\pka} values were reproduced to within 1 pH unit. Comparison with replica-exchange simulations with different exchange attempt frequencies showed that the splitting between the two macroscopic pKa's is underestimated with frequent exchange attempts such as 2 ps$^{-1}$, while single-pH simulations overestimate the splitting. The same trend is seen for the single-pH vs. replica-exchange simulations of a hydrogen-bonded aspartyl dyad in a much larger protein. A 2-ns single-pH simulation of a 400-residue protein takes about one hour on a single NVIDIA GeForce RTX 2080 graphics card, which is over 1000 times faster than a CpHMD run on a single CPU core of a high-performance computing cluster node. Thus, we envision that GPU-accelerated continuous CpHMD may be used in routine pKa predictions for a variety of applications, from assisting MD simulations with protonation state assignment to offering pH-dependent corrections of binding free energies and identifying reactive hot spots for covalent drug design.|Journal of Chemical Information and Modeling|2019|10.26434/chemrxiv.9642071.v1|Jana K. Shen, Robert C. Harris|0.0|2
1614|GPU-Assisted Generation of System Matrices for High-Resolution Imaging Systems|Traditional methods of point spread function (PSF) modeling of pinhole SPECT systems, such as fitting the PSF with a 2D Gaussian, are generally sufficient in characterizing imaging systems in which the detector is the main source of PSF blur. However, when modeling the PSF of a high-resolution imaging system, these methods are too simplistic and fail to capture important PSF features, resulting in errors in later simulation studies. In this work, we present a method for parameterizing point spread functions that is then used to rapidly generate system matrices of gamma-ray imaging systems using high-resolution detectors with sub-millimeter spatial resolution. Our algorithm, which utilizes a GPU-based ray tracer to simulate a system’s true PSF, accounts for the PSF blur due to not only the detector’s depth of interaction, but also the penetration through the pinhole and the finite size of of the source. By considering all three blurring sources, we are able to better model the PSF, capturing features like the flat-top in the center and the tilt and drop-off towards the edges. Comparisons to the 2D Gaussian fitting model are examined and we report that as the ratio between the source size and pinhole radius increases, the two models converge to one another.|Nuclear Science Symposium and Medical Imaging Conference|2018|10.1109/NSSMIC.2018.8824756|L. Furenlid, Xin Li, A. Lin, M. Kupinski|0.0|2
1621|Coal pyrolysis simulation by GPU-enabled ReaxFF MD and cheminformatics analysis|This paper overviews the new methodology that combines GPU-based high performance computing and cheminformatics analysis for ReaxFF MD simulation. The methodology was proposed to overcome the obstacles of the expensive computational costs and reaction analysis capability lacking in exploration of chemical reaction mechanisms of coal pyrolysis. With the creation of GMD-Reax and VARxMD, the first GPU-enabled ReaxFF MD codes with significant improved computational performances and the first tool allowing for chemical reaction analysis and visualization of ReaxFF MD trajectory, large scale coal models with about 10000 atoms can be simulated. The observed evolution of pyrolyzates with temperature and time, and the detailed chemical reactions obtained in the simulations can provide additional insights on coal pyrolysis, which is hardly accessible experimentally or with other computational methods. We believe that the methodology is promising for a deep and comprehensive understanding of the complex reaction mechanism in coal pyrolysis.||2015|10.1360/n032014-00281|Liao XiaoXia, Zhengchang Mo, Liu Xiaolong, Yuan Xiaolong, Han Junyi, Qiao Xianjie, Guo Li|0.0|2
1626|Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger|In-hand manipulation of objects is an important capability to enable robots to carry-out tasks which demand high levels of dexterity. This work presents a robot systems approach to learning dexterous manipulation tasks involving moving objects to arbitrary 6-DoF poses. We show empirical benefits, both in simulation and sim - to- real transfer, of using keypoint-based representations for object pose in policy observations and reward calculation to train a model-free reinforcement learning agent. By utilizing domain randomization strategies and large-scale training, we achieve a high success rate of 83 % on a real TriFinger system, with a single policy able to perform grasping, ungrasping, and finger gaiting in order to achieve arbitrary poses within the workspace. We demonstrate that our policy can generalise to unseen objects, and success rates can be further improved through finetuning. With the aim of assisting further research in learning in-hand manipulation, we provide a detailed exposition of our system and make the codebase of our system available, along with checkpoints trained on billions of steps of experience, at https://s2r2-ig.github.io|IEEE/RJS International Conference on Intelligent RObots and Systems|2021|10.1109/IROS47612.2022.9981458|Denys Makoviichuk, Manuel Wüthrich, Mayank Mittal, Varun Lodaya, Arthur Allshire, Viktor Makoviychuk, Ankur Handa, F. Widmaier, Stefan Bauer, Animesh Garg|3.75|1
1630|Real-Time Simulation of Indoor Air Flow using the Lattice Boltzmann Method on Graphics Processing Unit|This thesis investigates the usability of the lattice Boltzmann method (LBM) for the simulation of indoor air flows in real-time. It describes the work undertaken during the three years of a Ph.D. study in the School of Mechanical Engineering at the University of Leeds, England. \n \nReal-time fluid simulation, i.e. the ability to simulate a virtual system as fast as the real system would evolve, can benefit to many engineering application such as the optimisation of the ventilation system design in data centres or the simulation of pollutant transport in hospitals. And although real-time fluid simulation is an active field of research in computer graphics, these are generally focused on creating visually appealing animation rather than aiming for physical accuracy. The approach taken for this thesis is different as it starts from a \nphysics based model, the lattice Boltzmann method, and takes advantage of the computational power of a graphics processing unit (GPU) to achieve real-time compute capability while maintaining good physical accuracy. \n \nThe lattice Boltzmann method is reviewed and detailed references are given a variety of models. Particular attention is given to turbulence modelling using the Smagorinsky model in LBM for the simulation of high Reynolds number flow and the coupling of two LBM simulations to simulate thermal flows under the Boussinesq approximation. \n \nA detailed analysis of the implementation of the LBM on GPU is conducted. A special attention is given to the optimisation of the algorithm, and the program kernel is shown to achieve a performance of up to 1.5 billion lattice node updates per second, which is found to be sufficient for coarse real-time simulations. Additionally, a review of the real-time visualisation integrated within the program is \npresented and some of the techniques for automated code generation are introduced. \n \nThe resulting software is validated against benchmark flows, using their analytical solutions whenever possible, or against other simulation results obtained using accepted method from classical computational fluid dynamics (CFD) either as published in the literature or simulated in-house. The LBM is shown to resolve the flow with \nsimilar accuracy and in less time.||2015|10.1007/s12273-015-0232-9|N. Delbosc|3.1|1
1631|GPU acceleration of Monte Carlo simulations for polarized photon scattering in anisotropic turbid media.|In earlier studies, we developed scattering models and the corresponding CPU-based Monte Carlo simulation programs to study the behavior of polarized photons as they propagate through complex biological tissues. Studying the simulation results in high degrees of freedom that created a demand for massive simulation tasks. In this paper, we report a parallel implementation of the simulation program based on the compute unified device architecture running on a graphics processing unit (GPU). Different schemes for sphere-only simulations and sphere-cylinder mixture simulations were developed. Diverse optimizing methods were employed to achieve the best acceleration. The final-version GPU program is hundreds of times faster than the CPU version. Dependence of the performance on input parameters and precision were also studied. It is shown that using single precision in the GPU simulations results in very limited losses in accuracy. Consumer-level graphics cards, even those in laptop computers, are more cost-effective than scientific graphics cards for single-precision computation.|Applied Optics|2016|10.1364/AO.55.007468|Honghui He, Pengcheng Li, Hui Ma, Xianpeng Li, Celong Liu|3.0|1
1632|MIMO Radar Parallel Simulation System Based on CPU/GPU Architecture|The data volume and computation task of MIMO radar is huge; a very high-speed computation is necessary for its real-time processing. In this paper, we mainly study the time division MIMO radar signal processing flow, propose an improved MIMO radar signal processing algorithm, raising the MIMO radar algorithm processing speed combined with the previous algorithms, and, on this basis, a parallel simulation system for the MIMO radar based on the CPU/GPU architecture is proposed. The outer layer of the framework is coarse-grained with OpenMP for acceleration on the CPU, and the inner layer of fine-grained data processing is accelerated on the GPU. Its performance is significantly faster than the serial computing equipment, and satisfactory acceleration effects have been achieved in the CPU/GPU architecture simulation. The experimental results show that the MIMO radar parallel simulation system with CPU/GPU architecture greatly improves the computing power of the CPU-based method. Compared with the serial sequential CPU method, GPU simulation achieves a speedup of 130 times. In addition, the MIMO radar signal processing parallel simulation system based on the CPU/GPU architecture has a performance improvement of 13%, compared to the GPU-only method.|Italian National Conference on Sensors|2022|10.3390/s22010396|Peng Li, Jingjing Cai, Ning Yue, Shuai Wang, G. Qin, Wenbo Yang, Youming Wang, Gao-gao Liu, D. Huang|2.6666666666666665|1
1624|Memory Considerations for Low Energy Ray Tracing|We propose two hardware mechanisms to decrease energy consumption on massively parallel graphics processors for ray tracing. First, we use a streaming data model and configure part of the L2 cache into a ray stream memory to enable efficient data processing through ray reordering. This increases L1 hit rates and reduces off‐chip memory energy substantially through better management of off‐chip memory access patterns. To evaluate this model, we augment our architectural simulator with a detailed memory system simulation that includes accurate control, timing and power models for memory controllers and off‐chip dynamic random‐access memory . These details change the results significantly over previous simulations that used a simpler model of off‐chip memory, indicating that this type of memory system simulation is important for realistic simulations that involve external memory. Secondly, we employ reconfigurable special‐purpose pipelines that are constructed dynamically under program control. These pipelines use shared execution units that can be configured to support the common compute kernels that are the foundation of the ray tracing algorithm. This reduces the overhead incurred by on‐chip memory and register accesses. These two synergistic features yield a ray tracing architecture that reduces energy by optimizing both on‐chip and off‐chip memory activity when compared to a more traditional approach.|Computer graphics forum (Print)|2015|10.1111/cgf.12458|D. Kopta, A. Davis, E. Brunvand, J. Spjut, K. Shkurko|1.6|1
1625|3D laser scanning technique coupled with dem GPU simulations for railway ballasts|One of the key parameters that fundamentally influences the performance of track ballast is particle shape. Particularly settling of the railway on the track ballast depends strongly on the shape, surface roughness and wear characteristics of the ballast. As a consequence railway industries have imposed standards to fix quality requirements regarding ballasts. In parallel, researchers are developing simulation methodologies to tackle the problematic settling of ballasts as freight is hauled within an infrastructure. In particular, the discrete element method (DEM) is classically used to better understand the settling of ballasts under repeated cyclic loading. The computational demands of DEM often limits these studies to 2D simulations or at most 3D simulations using only simplistic spherical shapes to construct lumped particles [1]. Unfortunately, a critical geometric aspect of ballast is particle angularity which is poorly represented using lumped particle representation [1]. This study investigates the potential of a DEM code to model i) realistic particle shape, ii) a large number of particles in a 3D ballast simulation and iii) the potential of GPU based simulations to be used in ballast applications. The simulations are to be validated experimentally using accurate and realistic particle representations. The graphics processing unit (GPU) with its highly parallelized hardware architecture is essential towards achieving ii) as millions of particles are being regularly simulated within reasonable computing times on GPU platforms [2-4], while the specific DEM environment BlazeDEM-3DGPU [2] is crucial for i) as polyhedral shaped convex and non-convex particles can be modelled efficiently allowing for angularity of the ballast to be resolved accurately. Towards capturing the ballast shapes a laser scanning technique is applied to obtain digitized ballast representations of actual track ballast for use in DEM simulations. The detail of the digitized ballast can be controlled by controlling the effective number of faces used in presenting the ballast samples. A benefit is that the critical geometric features could be investigated by conducting DEM simulations using varying detailed ballast samples. In addition, simplified ballast samples allows in particular for large numbers of angular particles to be simulated during loading and unloading investigations of ballast. This study demonstrates that a laser scanning strategy is effective, efficient and practical to quantify the shape properties of the ballasts towards constructing digitized ballast samples for use in DEM simulations. Secondly, this study shows that GPU-enabled DEM can efficiently model the cycling loading on a compression box representative of a typical laboratory ballast experiment.||2017|10.1051/epjconf/201714003071|D. Wilke, P. Pizette, Benjamin Gobe, R. Rajamani, N. Govender, N. Abriak|0.75|1
1633|Graphics processing unit based acceleration of electromagnetic transients simulation|This paper presents a novel parallelization approach to speedup EMT simulation, using GPU-based computing. This paper extends earlier published works in the area, by exploiting additional parallelism to accelerate EMT simulation. A 2D-parallel matrix-vector multiplication is used that is faster than previous 1D-methods. Also this paper implements a simpler GPU-specific sparsity technique to further speed up the simulations as available CPU-based sparse techniques are not suitable for GPUs. Additionally, as an extension to previous works, this paper demonstrates modelling of a power electronic subsystem. A low granularity system, i.e. one with a large cluster of busses connected to others with a few transmission lines is considered, as is also a high granularity where a small cluster of busses is connected to other clusters thereby requiring more interconnecting transmission lines. Computation times for GPU-based computing are compared with the computation times for sequential implementations on the CPU. The paper shows two surprising differences of GPU simulation in comparison with CPU simulation. Firstly, the inclusion of sparsity only makes minor reductions in the GPU-based simulation time. Secondly excessive granularity, even though it appears to increase the number of parallel computable subsystems, significantly slows down the GPU-based simulation.|IEEE Power & Energy Society General Meeting|2016|10.1109/PESGM.2016.7741197|J. Debnath, W. Fung, A. Gole|0.2222222222222222|1
1622|Special issue on selected technologies and applications in smart city computing|Developing smart city is the key to the next generation urbanization process for improving the efficiency, reliability, and security of a traditional city. The concept of smart city includes various aspects such as environmental sustainability, social sustainability, regional competitiveness, natural resources management, cybersecurity, and quality of life improvement. With the massive deployment of networked smart devices/sensors, unprecedentedly large amount of sensory data can be collected and processed by advanced computing paradigms which are the enabling techniques for smart city. For example, given historical environmental, population and economic information, salient modeling, and analytics are needed to simulate the impact of potential city planning strategies, which will be critical for intelligent decision making. Analytics are also indispensable for discovering the underlying structure from retrieved data in order to design the optimal policies for real‐time automatic control in the cyberphysical smart city system. Furthermore, uncertainties and security concerns in the data collected from heterogeneous resources aggravate the problem, which makes smart city planning, operation, monitoring, and control highly challenging. The submitted manuscripts were reviewed by experts from both academia and industry. After two rounds of reviewing, the highest quality manuscripts were accepted for this special issue. Totally, 14 papers are accepted. This special issue will be published by CCPE as special issues. Arunarani et al propose a security and cost aware scheduling algorithm for heterogeneous tasks in scientific workflow executed in a cloud. A core semantics extraction model (CSEM) is proposed by Liu et al to improve the novel and rich semantics of multi‐document summary. Zhou et al study to enhance the service reliability by designing a novel network failure–aware redundant virtual machine placement approach in a cloud data center. Ding et al attempt to answer the fundamental question of “how much information regarding the dynamic property of the original time series can be extracted from these networks.” Lai et al study the issue of improving the performance of Markov chain Monte Carlo method to solve local PageRank problem under General Purpose Graphics Processing Unit environment. In order to effectively solve this problem, Bayes belief model is applied by Xing et al to generate the initial dispensation plan, and learnable ant colony optimization is proposed to solve task scheduling subproblem. Bai analyzes the characteristics and meaning of intelligent manufacturing systems, which leads to problems facing the Intelligent Manufacturing System and then dispersing the benefits of SDN. Two models of the scheme are verifiable by Tang and Cai, supporting location query on homomorphic encrypted ciphertext with searching index and trapdoor. Yang et al aim to present a method to make calibration procedure intelligent and automatic by machine. Wang et al identify that the optimum resolution in the study area is 10 m in digital elevation model. Yan et al formulate the contaminant source identification problem into an optimization problem and then design the cultural algorithm to solve it by considering different sizes of water supply networks as the experimental data. The discussion on the causes of educational service quality was complemented by Li et al from the perspective of social network, especially through analysis at the team level. Zhang et al study the complexity and information flow of stock time series and construct the stock influence network on the basis of transfer entropy. Aiming at lacking in trend prediction and evaluation of network public opinion, the attributes of monitoring index system of network public opinion are reduced by Chen et al to using rough set theory and construct a more scientific monitoring index system of network public opinion combined with the real life, determine the weight of the index using the analytic hierarchy process, and present a novel method of trend prediction and evaluation of network public opinion on the basis of fuzzy comprehensive evaluation model from the aspects of the quantitative and qualitative.|Concurrency and Computation|2017|10.1002/cpe.4363|N. Yen, V. Sugumaran, Zheng Xu|0.125|1
1634|Data structures for SIMD logic simulation|Due to the growth of design size and complexity, design verification is an important aspect of the Logic Circuit development process. The purpose of verification is to validate that the design meets the system requirements and specification. This is done by either functional or formal verification. \nThe most popular approach to functional verification is the use of simulation based techniques. Using models to replicate the behaviour of an actual system is called simulation. \nIn this thesis, a software/data structure architecture without explicit locks is proposed to accelerate logic gate circuit simulation. We call thus system ZSIM. The ZSIM software architecture simulator targets low cost SIMD multi-core machines. Its performance is evaluated on the Intel Xeon Phi and 2 other machines (Intel Xeon and AMD Opteron). \nThe aim of these experiments is to: \n• Verify that the data structure used allows SIMD acceleration, particularly on machines with gather instructions ( section 5.3.1). \n• Verify that, on sufficiently large circuits, substantial gains could be made from multicore parallelism ( section 5.3.2 ). \n• Show that a simulator using this approach out-performs an existing commercial simulator on a standard workstation ( section 5.3.3 ). \n• Show that the performance on a cheap Xeon Phi card is competitive with results reported elsewhere on much more expensive super-computers ( section 5.3.5 ). \nTo evaluate the ZSIM, two types of test circuits were used: \n1. Circuits from the IWLS benchmark suit [1] which allow direct comparison with other published studies of parallel simulators.2. Circuits generated by a parametrised circuit synthesizer. The synthesizer used an algorithm that has been shown to generate circuits that are statistically representative of real logic circuits. The synthesizer allowed testing of a range of very large circuits, larger than the ones for which it was possible to obtain open source files. \nThe experimental results show that with SIMD acceleration and multicore, ZSIM gained a peak parallelisation factor of 300 on Intel Xeon Phi and 11 on Intel Xeon. With only SIMD enabled, ZSIM achieved a maximum parallelistion gain of 10 on Intel Xeon Phi and 4 on Intel Xeon. \nFurthermore, it was shown that this software architecture simulator running on a SIMD machine is much faster than, and can handle much bigger circuits than a widely used commercial simulator (Xilinx) running on a workstation. \nThe performance achieved by ZSIM was also compared with similar pre-existing work on logic simulation targeting GPUs and supercomputers. It was shown that ZSIM simulator running on a Xeon Phi machine gives comparable simulation performance to the IBM Blue Gene supercomputer at very much lower cost. The experimental results have shown that the Xeon Phi is competitive with simulation on GPUs and allows the handling of much larger circuits than have been reported for GPU simulation. \nWhen targeting Xeon Phi architecture, the automatic cache management of the Xeon Phi, handles and manages the on-chip local store without any explicit mention of the local store being made in the architecture of the simulator itself. However, targeting GPUs, explicit cache management in program increases the complexity of the software architecture. Furthermore, one of the strongest points of the ZSIM simulator is its portability. Note that the same code was tested on both AMD and Xeon Phi machines. The same architecture that efficiently performs on Xeon Phi, was ported into a 64 core NUMA AMD Opteron. \nTo conclude, the two main achievements are restated as following: The primary achievement of this work was proving that the ZSIM architecture was faster than previously published logic simulators on low cost platforms. The secondary achievement was the development of a synthetic testing suite that went beyond the scale range that was previously publicly available, based on prior work that showed the synthesis technique is valid.||2016|10.1109/hpcsim.2016.7568358|M. Chimeh|0.1111111111111111|1
1623|Fast GPU simulation of reinforced concrete at the scale of reinforcement ribs by the discrete element method|The paper presents the development of the GPU-based discrete element method (DEM) code for simulating damage and fracture of cohesive solids with application to reinforced concrete at the scale of reinforcement ribs. The solid volume of concrete and steel is modelled by bonded spherical particles. Very fine discretization, containing more than million particles, is applied to describe the 3D reinforcement bar geometry at the scale of ribs and to investigate cracking behaviour of concrete near the reinforcement bar. The numerical model is validated by using experimental results of the double pull-out test. Influence of the discretization scale to the numerical solution is evaluated by using the reinforcement strain profiles and the cracking patterns. The developed GPU-based DEM algorithm efficiently handles interaction of particles, does not require any atomic operation and allows performing fast damage and fracture simulations with large number of particles. The performance measured on GPU is compared with that attained on different CPUs for varying number of particles. The high value of the Cundall number (particle number multiplied by time steps computed per second) equal to 4.3E+07 is measured on NVIDIA® Tesla™ P100 GPU in the case of 1858560 particles.||2019|10.24423/AOM.3148|G. Kaklauskas, A. Kačeniauskas, R. Pacevič, R. Kačianauskas, R. Barauskas|0.0|1
1627|Combined scientific CFD simulation and interactive raytracing with OpenCL|One of the main uses for OpenCL is (scientific) compute applications where graphical rendering is done externally, after the simulation has finished. However separating simulation and rendering has many disadvantages, especially the extreme slowdown caused by copying simulation data from device to host, and needing to store raw data on the hard drive, taking up hundreds of gigabyte, just to visualize preliminary results. A much faster approach is to implement both simulation and rendering in OpenCL. The rendering kernels have direct read-only access to the raw simulation data that resides in ultra-fast GPU memory. This eliminates all PCIe data transfer but camera parameters and finished frames, allowing for interactive visualization of simulation results in real time while the simulation is running. This is an invaluable tool for rapid prototyping. Although OpenCL does not have existing functionality for graphical rendering, being a general compute language, it allows for implementing an entire graphics engine, such that no data has to be moved to the CPU during rendering. On top, specific low-level optimizations make this OpenCL graphics engine outperform any existing rendering solution for this scenario, enabling drawing billions of lines per second and fluid raytracing in real time on even non-RTX GPUs. This combination of simulation and rendering in OpenCL is demonstrated with the software FluidX3D [3] - a lattice Boltzmann method (LBM) fluid dynamics solver. The first part will briefly introduce the numerical method for simulating fluid flow in a physically accurate manner. After introducing the LBM, the optimizations to make it run at peak efficiency are discussed: Being a memory-bound algorithm, coalesced memory access is key. This is achieved through array-of-structures data layout as well as the one-step-pull scheme, a certain variant of the LBM streaming step. One-step-pull leverages the fact that the misaligned read penalty is much smaller than the misaligned write penalty on almost all GPUs. Roofline analysis shows that with these optimizations, the LBM runs at 100% efficiency on the fastest data-center and gaming GPUs [5]. To simulate free surface flows, the LBM is extended with the Volume-of-Fluid (VoF) model. An efficient algorithm has been designed to vastly accelerate the challenging surface tension computation [4]. This extremely efficient VoF-LBM GPU implementation allows covering new grounds in science: FluidX3D has been used to simulate more than 1600 raindrop impacts to statistically evaluate how microplastics transition from the ocean surface into the atmosphere when the spray droplets are generated during drop impact [6]. At the same power consumption, with existing CPU-parallelized codes, compute time would have been several years, whilst with FluidX3D it was about a week. The second part will focus on real time rendering with OpenCL, especially raytracing. Rasterization on the GPU is parallelized not over pixels but lines/triangles instead, making runtime mostly independent of screen resolution and lightning fast. Each line/triangle is transformed with the camera parameters from 3D to 2D screen coordinates and then rasterized onto the frame (integer array) with Bresenham algorithm [2] and z-buffer. The raytracing graphics are based on a combination of fast ray-grid traversal and marching-cubes, leveraging that the computational grid from the LBM already is an ideal acceleration structure for raytracing. The idea of raytracing is simple: Through each pixel on the screen, shoot a reverse light ray out of the camera and see where it intersects with a surface in the scene. Then (recursively) calculate reflected/refracted rays and mix the colors. If a ray doesn’t intersect with anything, its color is determined by the skybox image via UV mapping and bilinear pixel interpolation. With mesh surfaces consisting of many triangles, computation time quickly becomes a problem, as for each ray all triangles have to be tested for intersection. To overcome this, an acceleration structure is required. While computer games often use a bounding volume hierarchy, the LBM already provides an ideal alternative acceleration structure: the simulation grid. The corresponding algorithm is called ray-grid traversal: When a ray shoots through the 3D grid, intersections with the surface only have to be checked for at each traversed grid cell rather than the entire grid. In each traversed grid cell, the 0-5 surface triangles are generated on-the-fly with the marching-cubes algorithm and ray-triangle intersections are checked with the Möller-Trumbore algorithm. If an intersection has been found, only afterwards the normals are calculated on the 8 grid points spanning the cell, and are trilinearly interpolated to the intersection coordinates. The so interpolated surface normal makes the raytraced surface appear perfectly smooth. On the GPU, the ray(s) for each pixel on screen are computed in parallel, vastly speeding up rendering. It is of key importance how to align the OpenCL workgroups on the 2D array of screen pixels: best performance is achieved for 8x8 pixel tiles; this is about 50% faster than 64x1 tiles, because with small, square-ish tiles, all rays of the workgroup are more likely to traverse the same grid cells, greatly improving memory broadcasting. In ray-grid traversal, 8 isovalues spanning a cell have to be loaded from GPU memory for each traversed cell. Once the triangle intersection has been found, the gradient on each of the 8 cell isovalues is calculated with central differences. Instead of loading an additional 6 isovalues for each of the 8 grid points, their isovalues are reused such that only 24 additional isovalues are loaded. For marching-cubes, the algorithm by Paul Bourke [1] is implemented in OpenCL. With 16-/8-bit integers, bit-packing and symmetry, the tables are reduced to 1/8 of their original size and stored in constant memory space. For computing the cube index, branching is eliminated by bit operations. The Möller-Trumbore algorithm [7] is implemented in an entirely branchless manner. This raytracing implementation is fast enough to run in real time for even the largest lattice dimensions that fit into the memory of a GPU. Finally, the combined VoF-LBM simulation and raytracing implementation is demonstrated on the most realistic simulation of an impacting raindrop ever done [8].|International Workshop on OpenCL|2022|10.1145/3529538.3529542|Moritz Lehmann|0.0|1
1628|Comparison and efficiency of GPU accelerated optical light propagation in CORSIKA~8|AI accelerators have proliferated in data centers in recent years and are now almost ubiquitous. In addition, their computational power and, most importantly, their energy efficiency are up to orders of magnitude higher than that of traditional computing. Over the last years, various methods and optimizations have been tested to use these hybrid systems for simulations in the context of astroparticle physics utilizing CORSIKA. The main focus of this talk is the propagation of optical, i.e. fluorescence and Cherenkov, photons through low density inhomogeneous media in the context of the next generation CORSIKA8 simulation framework. Different techniques used and approximations, e.g. the atmospheric model, tested during the development will be presented. The trade-off between performance and precision allows the experiment to achieve its physical precision limited to the real resolution of the experiment and not invest power and time in vanishing precision gains. The additional comparison of classical CPU-based simulations with the new methods validates these methods and allows evaluation against a known baseline.|International Conference on Rebooting Computing|2023|10.22323/1.444.0417|J. Alameddine, D. Baack|0.0|1
1629|Virtualizing and Scheduling FPGA Resources in Cloud Computing Datacenters|Cloud service providers consistently leverage their computing infrastructures by adding reconfigurable hardware platforms such as field-programmable gate arrays (FPGAs) to their existing infrastructures. Adding FPGAs to a cloud environment involves non-trivial challenges. The first challenge is virtualizing FPGAs as part of the cloud resources. As a standard virtualization framework is lacking, there is a need for an efficient framework for virtualizing FPGAs. Furthermore, FPGA resources are used in conjunction with central processing units (CPUs) and graphics processing units (GPUs) to accelerate the execution of tasks. Therefore, to gain the benefits of these powerful accelerating platforms, the second challenge is to optimize the allocation of tasks into the capable resources within a cloud data center. This work proposes an FPGA virtualization framework that abstracts the physical FPGAs into virtual pools of FPGA resources. The work further presents an integer linear programming (ILP) optimization model to minimize the makespan of tasks where FPGA resources are part of the cloud data center. Given the complex nature of the problem, a simulated annealing (SA) metaheuristic is developed to achieve gains in performance compared to the exact method and to scale up and handle many tasks and resources while providing near-optimal solutions. Experimental results show that SA has reduced the makespan of a large dataset with 1000 tasks and 100 resources by up to 30% when compared to first-come-first-served (FCFS) and shortest-deadline-first (SDF) algorithms. Lastly, to quantify the performance of FPGA-enabled cloud datacenters, the work extends the CloudSim simulator (an open-source cloud simulator) to enable FPGA as a resource in its environment. The proposed virtualization framework and the SA scheduler are integrated into the environment. Simulation results show that the execution time of tasks is reduced by up to 78% when FPGA accelerators are used.|IEEE Access|2022|10.1109/ACCESS.2022.3204866|K. El-Fakih, Raafat Aburukba, Abid Farhan, A. Sagahyroon, M. Elnawawy|0.0|1
2246|Scalable molecular dynamics on CPU and GPU architectures with NAMD.|NAMDis a molecular dynamics program designed for high-performance simulations of very large biological objects on CPU- and GPU-based architectures. NAMD offers scalable performance on petascale parallel supercomputers consisting of hundreds of thousands of cores, as well as on inexpensive commodity clusters commonly found in academic environments. It is written in C++ and leans on Charm++ parallel objects for optimal performance on low-latency architectures. NAMD is a versatile, multipurpose code that gathers state-of-the-art algorithms to carry out simulations in apt thermodynamic ensembles, using the widely popular CHARMM, AMBER, OPLS, and GROMOS biomolecular force fields. Here, we review the main features of NAMD that allow both equilibrium and enhanced-sampling molecular dynamics simulations with numerical efficiency. We describe the underlying concepts utilized by NAMD and their implementation, most notably for handling long-range electrostatics; controlling the temperature, pressure, and pH; applying external potentials on tailored grids; leveraging massively parallel resources in multiple-copy simulations; and hybrid quantum-mechanical/molecular-mechanical descriptions. We detail the variety of options offered by NAMD for enhanced-sampling simulations aimed at determining free-energy differences of either alchemical or geometrical transformations and outline their applicability to specific problems. Last, we discuss the roadmap for the development of NAMD and our current efforts toward achieving optimal performance on GPU-based architectures, for pushing back the limitations that have prevented biologically realistic billion-atom objects to be fruitfully simulated, and for making large-scale simulations less expensive and easier to set up, run, and analyze. NAMD is distributed free of charge with its source code at www.ks.uiuc.edu.|Journal of Chemical Physics|2020|10.1063/5.0014475|B. Radak, James C. Phillips, B. Roux, G. Fiorin, J. Hénin, Julio D C Maia, A. Singharoy, R. Skeel, Wei Jiang, L. Kalé, Yi Wang, Marcelo C. R. Melo, E. Tajkhorshid, A. Aksimentiev, João V. Ribeiro, K. Schulten, Ronak Buch, R. McGreevy, C. Chipot, J. Stone, Z. Luthey-Schulten, David J. Hardy, R. Bernardi|406.8|0
1823|DeePMD-kit v2: A software package for deep potential models|DeePMD-kit is a powerful open-source software package that facilitates molecular dynamics simulations using machine learning potentials known as Deep Potential (DP) models. This package, which was released in 2017, has been widely used in the fields of physics, chemistry, biology, and material science for studying atomistic systems. The current version of DeePMD-kit offers numerous advanced features, such as DeepPot-SE, attention-based and hybrid descriptors, the ability to fit tensile properties, type embedding, model deviation, DP-range correction, DP long range, graphics processing unit support for customized operators, model compression, non-von Neumann molecular dynamics, and improved usability, including documentation, compiled binary packages, graphical user interfaces, and application programming interfaces. This article presents an overview of the current major version of the DeePMD-kit package, highlighting its features and technical details. Additionally, this article presents a comprehensive procedure for conducting molecular dynamics as a representative application, benchmarks the accuracy and efficiency of different models, and discusses ongoing developments.|Journal of Chemical Physics|2023|10.1063/5.0155600|Sikai Yao, C. Luo, Jieming Liu, Yuzhi Zhang, Boyang Wang, D. Tisi, Jingchao Zhang, Zi-Tong Li, Jinzhe Zeng, Ye Ding, Wenshuo Liang, Jiahong Zhu, Duoduo Zhang, Feng Yuan, Rhys E. A. Goodall, Yusheng Xia, D. York, Pinghui Mo, Hao-Tong Ye, Q. Zeng, Zeyu Li, Wei Jia, Jiequn Han, Liang Huang, C. Cai, Jiameng Huang, Jiabin Yang, Yifan Li, Yibo Wang, Yingze Wang, Junhan Chang, Koki Muraoka, Denghui Lu, Hanxin Bao, Han Wang, Yinnian Lin, Shaochen Shi, P. Tuo, S. Bore, E. Weinan, R. Car, Yixiao Chen, Anurag Kumar Singh, R. Wentzcovitch, Linfeng Zhang, Mari'an Rynik, Jia-yu Xu|86.5|0
1804|The Parallelized Large-Eddy Simulation Model (PALM) version 4.0 for atmospheric and oceanic flows: model formulation, recent developments, and future perspectives|Abstract. In this paper we present the current version of the Parallelized Large-Eddy Simulation Model (PALM) whose core has been developed at the Institute of Meteorology and Climatology at Leibniz Universitat Hannover (Germany). PALM is a Fortran 95-based code with some Fortran 2003 extensions and has been applied for the simulation of a variety of atmospheric and oceanic boundary layers for more than 15 years. PALM is optimized for use on massively parallel computer architectures and was recently ported to general-purpose graphics processing units. In the present paper we give a detailed description of the current version of the model and its features, such as an embedded Lagrangian cloud model and the possibility to use Cartesian topography. Moreover, we discuss recent model developments and future perspectives for LES applications.||2015|10.5194/GMD-8-2515-2015|M. Gryschka, Rieke Heinze, K. Ketelsen, M. Keck, B. Maronga, M. Sühring, S. Raasch, Farah Kanani-Sühring, M. Letzel, F. Hoffmann|32.2|0
2036|Heterogeneous Computation and Resource Allocation for Wireless Powered Federated Edge Learning Systems|Federated learning (FL) is a popular edge learning approach that utilizes local data and computing resources of network edge devices to train machine learning (ML) models while preserving users’ privacy. Nevertheless, performing efficient learning tasks on the devices and achieving longer battery life are primary challenges faced by federated learning. In this paper, we are the first to study the application of heterogeneous computing (HC) and wireless power transfer (WPT) to federated learning to address these challenges. Especially, we propose a heterogeneous computation and resource allocation framework based on a heterogeneous mobile architecture to achieve effective implementation of FL. To minimize the energy consumption of smart devices and maximize their harvesting energy simultaneously, we formulate an optimization problem featuring multidimensional control, which jointly considers time splitting for WPT, dataset size allocation, transmit power allocation and subcarrier assignment during communications, and processor frequency of processing units (central processing unit (CPU) and graphics processing unit (GPU)). However, the major obstacle is how to design a proper algorithm to solve this optimization problem efficiently. For this purpose, we decouple the optimization variables so as to achieve high efficiency in deriving its solution. Particularly, we first compute the optimal processor frequency and dataset size allocation via employing the Lagrangian dual method, followed by finding the closed-form solution to the optimal time splitting allocation, and finally attain the optimal subcarrier assignment as well as transmit power for transmissions through an iteration algorithm. To evaluate the performance of our proposed scheme, we set up four baseline schemes as comparison, and simulation results show that the proposed scheme converges quite fast and better enhance the energy efficiency of the wireless powered FL system compared with the baseline schemes.|IEEE Transactions on Communications|2022|10.1109/tcomm.2022.3163439|Wenjing Zhang, Xiaodong Lin, Jie Feng, Jinsong Wu, Qingqi Pei|31.333333333333332|0
2074|Micron-scale heterogeneous catalysis with Bayesian force fields from first principles and active learning|—Quantum-mechanically accurate reactive molecular dynamics (MD) at the scale of billions of atoms has been achieved for the heterogeneous catalytic system of H 2 /Pt(111) using the FLARE Bayesian force ﬁeld. This achievement provides accelerated time-to-solution from ﬁrst principles, with Bayesian active learning enabling efﬁcient and autonomous training of the machine learning model. The resulting model is then deployed in LAMMPS on GPUs using the Kokkos performance portability library. The Bayesian force ﬁeld provides quantitative uncertainty of predictions on every atomic environment, critical for detecting conﬁgurations in large reactive simulations that are outside of the training set. Scaling benchmarks were performed using real-application MD of the H 2 /Pt(111) heterogeneous catalysis on the Summit supercomputer, with simulations reaching 0.5 trillion atoms on 4556 GPU nodes.||2022|10.1038/s41467-022-32294-0|Lixin Sun, Cameron J. Owen, A. Johansson, Yu Xie, J. Lim, Jonathan Vandermause, B. Kozinsky|23.333333333333332|0
1801|GPU Methodologies for Numerical Partial Differential Equations|In this thesis we develop techniques to efficiently solve numerical Partial Differential Equations (PDEs) using Graphical Processing Units (GPUs). Focus is put on both performance and re--usability of the methods developed, to this end a library, cuSten, for applying finite--difference stencils to numerical grids is presented herein. On top of this various batched tridiagonal and pentadiagonal matrix solvers are discussed. These have been benchmarked against the current state of the art and shown to improve performance in the solution of numerical PDEs. A variety of other benchmarks and use cases for the GPU methodologies are presented using the Cahn--Hilliard equation as a core example, but it is emphasised the methods are completely general. Finally through the application of the GPU methodologies to the Cahn--Hilliard equation new results are presented on the growth rates of the coarsened domains. In particular a statistical model is built up using batches of simulations run on GPUs from which the growth rates are extracted, it is shown that in a finite domain that the traditionally presented results of 1/3 scaling is in fact a distribution around this value. This result is discussed in conjunction with modelling via a stochastic PDE and sheds new light on the behaviour of the Cahn--Hilliard equation in finite domains.|arXiv.org|2021|10.22331/q-2021-11-10-574|Andrew Gloster|21.0|0
2206|Fast micromagnetic simulations on GPU—recent advances made with mumax3|In the last twenty years, numerical modeling has become an indispensable part of magnetism research. It has become a standard tool for both the exploration of new systems and for the interpretation of experimental data. In the last five years, the capabilities of micromagnetic modeling have dramatically increased due to the deployment of graphical processing units (GPU), which have sped up calculations to a factor of 200. This has enabled many studies which were previously unfeasible. In this topical review, we give an overview of this modeling approach and show how it has contributed to the forefront of current magnetism research.||2018|10.1088/1361-6463/aaab1c|J. D. Clercq, B. Waeyenberge, M. Milošević, J. Leliaert, Jeroen Mulkers, M. Dvornik|19.0|0
1963|Model Predictive Path Integral Control: From Theory to Parallel Computation|In this paper, a model predictive path integral control algorithm based on a generalized importance sampling scheme is developed and parallel optimization via sampling is performed using a graphics processing unit. The proposed generalized importance sampling scheme allows for changes in the drift and diffusion terms of stochastic diffusion processes and plays a significant role in the performance of the model predictive control algorithm. The proposed algorithm is compared in simulation with a model predictive control version of differential dynamic programming on nonlinear systems. Finally, the proposed algorithm is applied on multiple vehicles for the task of navigating through a cluttered environment. The current simulations illustrate the efficiency and robustness of the proposed approach and demonstrate the advantages of computational frameworks that incorporate concepts from statistical physics, control theory, and parallelization against more traditional approaches of optimal control theory.||2017|10.2514/1.G001921|Andrew Aldrich, Evangelos A. Theodorou, Grady Williams|17.375|0
2230|Pseudorapidity distribution and decorrelation of anisotropic flow within the open-computing-language implementation CLVisc hydrodynamics|Studies of fluctuations and correlations of soft hadrons and hard and electromagnetic probes of the dense and strongly interacting medium require event-by-event hydrodynamic simulations of high-energy heavy-ion collisions that are computing intensive. We develop a (3+1)-dimensional viscous hydrodynamic model—CLVisc that is parallelized on a graphics processing unit (GPU) by using the open computing language (OpenCL) with 60 times performance increase for spacetime evolution and more than 120 times for the Cooper–Frye particlization relative to that without GPU parallelization. The model is validated with comparisons with different analytic solutions, other existing numerical solutions of hydrodynamics, and experimental data on hadron spectra in high-energy heavy-ion collisions. The pseudorapidity dependence of anisotropic flow vn(η) are then computed in CLVisc with initial conditions given by the a multiphase transport (ampt) model, with energy density fluctuations both in the transverse plane and along the longitudinal direction. Although the magnitude of vn(η) and the ratios between v2(η) and v3(η) are sensitive to the effective shear viscosity over entropy density ratio ηv/s, the shape of the vn(η) distributions in η do not depend on the value of ηv/s. The decorrelation of vn along the pseudorapidity direction due to the twist and fluctuation of the event planes in the initial parton density distributions is also studied. The decorrelation observable rn(ηa,ηb) between vn{−ηa} and vn{ηa} with the auxiliary reference window ηb is found not to be sensitive to ηv/s when there is no initial fluid velocity. For small ηv/s, the initial fluid velocity from mini-jet partons introduces sizable splitting of rn(ηa,ηb) between the two reference rapidity windows ηb∈[3,4] and ηb∈[4.4,5.0], as has been observed in experiment. The implementation of CLVisc and guidelines on how to efficiently parallelize scientific programs on GPUs are also provided.|Physical Review C|2018|10.1103/PhysRevC.97.064918|Xin-Nian Wang, L. Pang, H. Petersen|16.428571428571427|0
2069|HELIOS: AN OPEN-SOURCE, GPU-ACCELERATED RADIATIVE TRANSFER CODE FOR SELF-CONSISTENT EXOPLANETARY ATMOSPHERES|We present the open-source radiative transfer code named HELIOS, which is constructed for studying exoplanetary atmospheres. In its initial version, the model atmospheres of HELIOS are one-dimensional and plane-parallel, and the equation of radiative transfer is solved in the two-stream approximation with nonisotropic scattering. A small set of the main infrared absorbers is employed, computed with the opacity calculator HELIOS-K and combined using a correlated-k approximation. The molecular abundances originate from validated analytical formulae for equilibrium chemistry. We compare HELIOS with the work of Miller-Ricci & Fortney using a model of GJ 1214b, and perform several tests, where we find: model atmospheres with single-temperature layers struggle to converge to radiative equilibrium; k-distribution tables constructed with cm−1 resolution in the opacity function ( points per wavenumber bin) may result in errors %–10% in the synthetic spectra; and a diffusivity factor of 2 approximates well the exact radiative transfer solution in the limit of pure absorption. We construct “null-hypothesis” models (chemical equilibrium, radiative equilibrium, and solar elemental abundances) for six hot Jupiters. We find that the dayside emission spectra of HD 189733b and WASP-43b are consistent with the null hypothesis, while the latter consistently underpredicts the observed fluxes of WASP-8b, WASP-12b, WASP-14b, and WASP-33b. We demonstrate that our results are somewhat insensitive to the choice of stellar models (blackbody, Kurucz, or PHOENIX) and metallicity, but are strongly affected by higher carbon-to-oxygen ratios. The code is publicly available as part of the Exoclimes Simulation Platform (exoclime.net).||2016|10.3847/1538-3881/153/2/56|Shang-min Tsai, M. Bedell, J. Bean, J. Mendonça, Luc Grosheintz, A. Burrows, M. Malik, D. Kitzmann, B. Lavie, K. Stevenson, K. Heng, S. Grimm, L. Kreidberg|13.555555555555555|0
2301|Development of a GPGPU‐parallelized hybrid finite‐discrete element method for modeling rock fracture|The hybrid finite‐discrete element method (FDEM) is widely used for engineering applications, which, however, is computationally expensive and needs further development, especially when rock fracture process is modeled. This study aims to further develop a sequential hybrid FDEM code formerly proposed by the authors and parallelize it using compute unified device architecture (CUDA) C/C++ on the basis of a general‐purpose graphics processing unit (GPGPU) for rock engineering applications. Because the contact detection algorithm in the sequential code is not suitable for GPGPU parallelization, a different contact detection algorithm is implemented in the GPGPU‐parallelized hybrid FDEM. Moreover, a number of new features are implemented in the hybrid FDEM code, including the local damping technique for efficient geostatic stress analysis, contact damping, contact friction, and the absorbing boundary. Then, a number of simulations with both quasi‐static and dynamic loading conditions are conducted using the GPGPU‐parallelized hybrid FDEM, and the obtained results are compared both quantitatively and qualitatively with those from either theoretical analysis or the literature to calibrate the implementations. Finally, the speed‐up performance of the hybrid FDEM is discussed in terms of its performance on various GPGPU accelerators and a comparison with the sequential code, which reveals that the GPGPU‐parallelized hybrid FDEM can run more than 128 times faster than the sequential code if it is run on appropriate GPGPU accelerators, such as the Quadro GP100. It is concluded that the GPGPU‐parallelized hybrid FDEM developed in this study is a valuable and powerful numerical tool for rock engineering applications.|International journal for numerical and analytical methods in geomechanics (Print)|2019|10.1002/nag.2934|M. Mohammadnejad, D. Fukuda, J. Kodama, Sang-Ho Cho, G. Min, Haoyu Han, Y. Fujii, A. Chan, S. Dehkhoda, Hong-yuan Liu|13.166666666666666|0
1822|Near-global climate simulation at 1 km resolution: establishing a performance baseline on 4888 GPUs with COSMO 5.0|Abstract. The best hope for reducing long-standing global climate model biases is by\nincreasing resolution to the kilometer scale. Here we present results from an\nultrahigh-resolution non-hydrostatic climate model for a near-global setup\nrunning on the full Piz Daint supercomputer on 4888 GPUs (graphics\nprocessing units). The dynamical core of the model has been completely\nrewritten using a domain-specific language (DSL) for performance portability\nacross different hardware architectures. Physical parameterizations and\ndiagnostics have been ported using compiler directives. To our knowledge this\nrepresents the first complete atmospheric model being run entirely on\naccelerators on this scale. At a grid spacing of 930 m (1.9 km), we achieve\na simulation throughput of 0.043 (0.23) simulated years per day and an energy\nconsumption of 596 MWh per simulated year. Furthermore, we propose a new\nmemory usage efficiency (MUE) metric that considers how efficiently the\nmemory bandwidth – the dominant bottleneck of climate codes – is being\nused.||2017|10.5194/GMD-11-1665-2018|O. Fuhrer, X. Lapillonne, Tarun Chadha, D. Leutwyler, T. Schulthess, H. Vogt, T. Hoefler, C. Osuna, C. Schär, D. Lüthi, Grzegorz Kwasniewski|13.125|0
2018|RRAM-Based Analog Approximate Computing|Approximate computing is a promising design paradigm for better performance and power efficiency. In this paper, we propose a power efficient framework for analog approximate computing with the emerging metal-oxide resistive switching random-access memory (RRAM) devices. A programmable RRAM-based approximate computing unit (RRAM-ACU) is introduced first to accelerate approximated computation, and an approximate computing framework with scalability is then proposed on top of the RRAM-ACU. In order to program the RRAM-ACU efficiently, we also present a detailed configuration flow, which includes a customized approximator training scheme, an approximator-parameter-to-RRAM-state mapping algorithm, and an RRAM state tuning scheme. Finally, the proposed RRAM-based computing framework is modeled at system level. A predictive compact model is developed to estimate the configuration overhead of RRAM-ACU and help explore the application scenarios of RRAM-based analog approximate computing. The simulation results on a set of diverse benchmarks demonstrate that, compared with a x86-64 CPU at 2 GHz, the RRAM-ACU is able to achieve 4.06-196.41× speedup and power efficiency of 24.59-567.98 GFLOPS/W with quality loss of 8.72% on average. And the implementation of hierarchical model and X application demonstrates that the proposed RRAM-based approximate computing framework can achieve 12.8× power efficiency than its pure digital implementation counterparts (CPU, graphics processing unit, and field- programmable gate arrays).|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|2015|10.1109/TCAD.2015.2445741|Yi Shan, Yiran Chen, P. Gu, Boxun Li, Huazhong Yang, Yu Wang|10.5|0
1730|Binary optimization by momentum annealing.|One of the vital roles of computing is to solve large-scale combinatorial optimization problems in a short time. In recent years, methods have been proposed that map optimization problems to ones of searching for the ground state of an Ising model by using a stochastic process. Simulated annealing (SA) is a representative algorithm. However, it is inherently difficult to perform a parallel search. Here we propose an algorithm called momentum annealing (MA), which, unlike SA, updates all spins of fully connected Ising models simultaneously and can be implemented on GPUs that are widely used for scientific computing. MA running in parallel on GPUs is 250 times faster than SA running on a modern CPU at solving problems involving 100 000 spin Ising models.|Physical Review E|2019|10.1103/PHYSREVE.100.012111|Tomohiro Sonobe, M. Yamaoka, Takuya Okuyama, K. Kawarabayashi|10.333333333333334|0
1749|Characterizing the Chemical Space of ERK2 Kinase Inhibitors Using Descriptors Computed from Molecular Dynamics Trajectories|Quantitative Structure-Activity Relationship (QSAR) models typically rely on 2D and 3D molecular descriptors to characterize chemicals and forecast their experimental activities. Previously, we showed that even the most reliable 2D QSAR models and structure-based 3D molecular docking techniques were not capable of accurately ranking a set of known inhibitors for the ERK2 kinase, a key player in various types of cancer. Herein, we calculated and analyzed a series of chemical descriptors computed from the molecular dynamics (MD) trajectories of ERK2-ligand complexes. First, the docking of 87 ERK2 ligands with known binding affinities was accomplished using Schrodinger's Glide software; then, solvent-explicit MD simulations (20 ns, NPT, 300 K, TIP3P, 1 fs) were performed using the GPU-accelerated Desmond program. Second, we calculated a series of MD descriptors based on the distributions of 3D descriptors computed for representative samples of the ligand's conformations over the MD simulations. Third, we analyzed the data set of 87 inhibitors in the MD chemical descriptor space. We showed that MD descriptors (i) had little correlation with conventionally used 2D/3D descriptors, (ii) were able to distinguish the most active ERK2 inhibitors from the moderate/weak actives and inactives, and (iii) provided key and complementary information about the unique characteristics of active ligands. This study represents the largest attempt to utilize MD-extracted chemical descriptors to characterize and model a series of bioactive molecules. MD descriptors could enable the next generation of hyperpredictive MD-QSAR models for computer-aided lead optimization and analogue prioritization.|Journal of Chemical Information and Modeling|2017|10.1021/acs.jcim.7b00048|Jeremy R. Ash, D. Fourches|9.625|0
1912|Quantum Chemistry for Solvated Molecules on Graphical Processing Units Using Polarizable Continuum Models.|The conductor-like polarization model (C-PCM) with switching/Gaussian smooth discretization is a widely used implicit solvation model in chemical simulations. However, its application in quantum mechanical calculations of large-scale biomolecular systems can be limited by computational expense of both the gas phase electronic structure and the solvation interaction. We have previously used graphical processing units (GPUs) to accelerate the first of these steps. Here, we extend the use of GPUs to accelerate electronic structure calculations including C-PCM solvation. Implementation on the GPU leads to significant acceleration of the generation of the required integrals for C-PCM. We further propose two strategies to improve the solution of the required linear equations: a dynamic convergence threshold and a randomized block-Jacobi preconditioner. These strategies are not specific to GPUs and are expected to be beneficial for both CPU and GPU implementations. We benchmark the performance of the new implementation using over 20 small proteins in solvent environment. Using a single GPU, our method evaluates the C-PCM related integrals and their derivatives more than 10× faster than that with a conventional CPU-based implementation. Our improvements to the linear solver provide a further 3× acceleration. The overall calculations including C-PCM solvation require, typically, 20-40% more effort than that for their gas phase counterparts for a moderate basis set and molecule surface discretization level. The relative cost of the C-PCM solvation correction decreases as the basis sets and/or cavity radii increase. Therefore, description of solvation with this model should be routine. We also discuss applications to the study of the conformational landscape of an amyloid fibril.|Journal of Chemical Theory and Computation|2015|10.1021/acs.jctc.5b00370|Fang Liu, Nathan Luehr, H. Kulik, T. Martínez|9.3|0
2137|Accelerated CDOCKER with GPUs, parallel simulated annealing and fast Fourier transforms.|Fast Fourier transform (FFT)-based protein ligand docking together with parallel simulated flexible ligand - flexible receptor docking are implemented on GPU- accelerated platforms to significantly enhance the throughput of the CDOCKER and Flexible CDOCKER docking algorithms in the CHARMM program for biomolecule modeling. The FFT-based approach for docking, first applied in protein-protein docking to efficiently search for the binding position and orientation of proteins, is adapted here to search ligand translational and rotational spaces given a ligand conformation in protein-ligand docking. Running on graphical processing units (GPUs), our implementation of FFT docking in CDOCKER achieves a 15,000 fold speedup in the ligand translational and rotational space search in protein-ligand docking problems. With this significant speedup it becomes practical to exhaustively search ligand translational and rotational space when docking a rigid ligand into a protein receptor. We demonstrate in this paper that this provides an efficient way to calculate an upper bound for dock- ing accuracy in the assessment of scoring functions for protein-ligand docking, which is useful for improving scoring functions. The parallel MD simulated annealing, also running on GPUs, aims to accelerate the search algorithm in CDOCKER by running MD simulated annealing in parallel on GPUs. When utilized as part of the general CDOCKER docking protocol, speed ups in excess of 20x are acheived. With this acceleration, we demonstrate that the performance of CDOCKER for re-docking is significantly improved compared with three other popular protein-ligand docking pro- grams on two widely used protein ligand complex datasets - the Astex diverse set and the SB2012 test set. Based on the results presented here, we suggest that the accelerated CDOCKER platform provides a highly competitive docking engine for both rigid-receptor and flexible-receptor docking studies, and will further facilitate continued improvement in the physics-based scoring function employed in CDOCKER docking studies.|Journal of Chemical Theory and Computation|2020|10.1021/acs.jctc.0c00145|Yujin Wu, Jonah Z. Vilseck, Xinqiang Ding, Yanming Wang, C. Brooks|9.0|0
1833|DukeSim: A Realistic, Rapid, and Scanner-Specific Simulation Framework in Computed Tomography|The purpose of this study was to develop a CT simulation platform that is: 1) compatible with voxel-based computational phantoms; 2) capable of modeling the geometry and physics of commercial CT scanners; and 3) computationally efficient. Such a simulation platform is designed to enable the virtual evaluation and optimization of CT protocols and parameters for achieving a targeted image quality while reducing radiation dose. Given a voxelized computational phantom and a parameter file describing the desired scanner and protocol, the developed platform DukeSim calculates projection images using a combination of ray-tracing and Monte Carlo techniques. DukeSim includes detailed models for the detector quantum efficiency, quantum and electronic noise, detector crosstalk, subsampling of the detector and focal spot areas, focal spot wobbling, and the bowtie filter. DukeSim was accelerated using GPU computing. The platform was validated using physical and computational versions of a phantom (Mercury phantom). Clinical and simulated CT scans of the phantom were acquired at multiple dose levels using a commercial CT scanner (Somatom Definition Flash; Siemens Healthcare). The real and simulated images were compared in terms of image contrast, noise magnitude, noise texture, and spatial resolution. The relative error between the clinical and simulated images was less than 1.4%, 0.5%, 2.6%, and 3%, for image contrast, noise magnitude, noise texture, and spatial resolution, respectively, demonstrating the high realism of DukeSim. The runtime, dependent on the imaging task and the hardware, was approximately 2–3 minutes per rotation in our study using a computer with 4 GPUs. DukeSim, when combined with realistic human phantoms, provides the necessary toolset with which to perform large-scale and realistic virtual clinical trials in a patient and scanner-specific manner.|IEEE Transactions on Medical Imaging|2019|10.1109/TMI.2018.2886530|E. Abadi, W. Segars, B. Harrawood, Shobhit Sharma, A. Kapadia, E. Samei|8.833333333333334|0
1756|Fast Realistic MRI Simulations Based on Generalized Multi-Pool Exchange Tissue Model|We present MRiLab, a new comprehensive simulator for large-scale realistic MRI simulations on a regular PC equipped with a modern graphical processing unit (GPU). MRiLab combines realistic tissue modeling with numerical virtualization of an MRI system and scanning experiment to enable assessment of a broad range of MRI approaches including advanced quantitative MRI methods inferring microstructure on a sub-voxel level. A flexible representation of tissue microstructure is achieved in MRiLab by employing the generalized tissue model with multiple exchanging water and macromolecular proton pools rather than a system of independent proton isochromats typically used in previous simulators. The computational power needed for simulation of the biologically relevant tissue models in large 3D objects is gained using parallelized execution on GPU. Three simulated and one actual MRI experiments were performed to demonstrate the ability of the new simulator to accommodate a wide variety of voxel composition scenarios and demonstrate detrimental effects of simplified treatment of tissue micro-organization adapted in previous simulators. GPU execution allowed  $\sim 200\times $  improvement in computational speed over standard CPU. As a cross-platform, open-source, extensible environment for customizing virtual MRI experiments, MRiLab streamlines the development of new MRI methods, especially those aiming to infer quantitatively tissue composition and microstructure.|IEEE Transactions on Medical Imaging|2016|10.1109/TMI.2016.2620961|Fang Liu, Richard Kijowski, W. Block, J. Velikina, A. Samsonov|8.555555555555555|0
2160|Accelerators for Classical Molecular Dynamics Simulations of Biomolecules|Atomistic Molecular Dynamics (MD) simulations provide researchers the ability to model biomolecular structures such as proteins and their interactions with drug-like small molecules with greater spatiotemporal resolution than is otherwise possible using experimental methods. MD simulations are notoriously expensive computational endeavors that have traditionally required massive investment in specialized hardware to access biologically relevant spatiotemporal scales. Our goal is to summarize the fundamental algorithms that are employed in the literature to then highlight the challenges that have affected accelerator implementations in practice. We consider three broad categories of accelerators: Graphics Processing Units (GPUs), Field-Programmable Gate Arrays (FPGAs), and Application Specific Integrated Circuits (ASICs). These categories are comparatively studied to facilitate discussion of their relative trade-offs and to gain context for the current state of the art. We conclude by providing insights into the potential of emerging hardware platforms and algorithms for MD.|Journal of Chemical Theory and Computation|2022|10.1021/acs.jctc.1c01214|Tajana Rosing, Jonathan E. Allen, Niema Moshiri, W. F. Drew Bennett, Derek Jones, M. Gokhale, Yue Yang|8.333333333333334|0
1758|GPU accelerated risk quantification|Factor Analysis of Information Risk (FAIR) is a standard model for quantitatively estimating cybersecurity risks and has been implemented as a sequential Monte Carlo simulation in the RiskLens and FAIR-U applications. Monte Carlo simulations employ random sampling techniques to model certain systems through the course of many iterations. Due to their sequential nature, FAIR simulations in these applications are limited in the number of iterations they can perform in a reasonable amount of time. One method that has been extensively used to speed up Monte Carlo simulations is to implement them to take advantage of the massive parallelization available when using modern Graphics Processing Units (GPUs). Such parallelized simulations have been shown to produce significant speedups, in some cases up to 3,000 times faster than the sequential versions. Due to the FAIR simulation’s need for many samples from various beta distributions, three methods of generating these samples via inverse transform sampling on the GPU are investigated. One method calculates the inverse incomplete beta function directly, and the other two methods approximate this function trading accuracy for improved parallelism. This method is then utilized in a GPU accelerated implementation of the FAIR simulation from RiskLens and FAIR-U using NVIDIA’s CUDA technology.||2018|10.1109/cahpc.2018.8645912|Forrest L. Ireland|8.0|0
1760|Real-time High-accuracy Three-Dimensional Reconstruction with Consumer RGB-D Cameras|We present an integrated approach for reconstructing high-fidelity three-dimensional (3D) models using consumer RGB-D cameras. RGB-D registration and reconstruction algorithms are prone to errors from scanning noise, making it hard to perform 3D reconstruction accurately. The key idea of our method is to assign a probabilistic uncertainty model to each depth measurement, which then guides the scan alignment and depth fusion. This allows us to effectively handle inherent noise and distortion in depth maps while keeping the overall scan registration procedure under the iterative closest point framework for simplicity and efficiency. We further introduce a local-to-global, submap-based, and uncertainty-aware global pose optimization scheme to improve scalability and guarantee global model consistency. Finally, we have implemented the proposed algorithm on the GPU, achieving real-time 3D scanning frame rates and updating the reconstructed model on-the-fly. Experimental results on simulated and real-world data demonstrate that the proposed method outperforms state-of-the-art systems in terms of the accuracy of both recovered camera trajectories and reconstructed models.|ACM Transactions on Graphics|2018|10.1145/3182157|Yan-Pei Cao, L. Kobbelt, Shimin Hu|8.0|0
2161|A massively parallel and scalable multi-GPU material point method|Harnessing the power of modern multi-GPU architectures, we present a massively parallel simulation system based on the Material Point Method (MPM) for simulating physical behaviors of materials undergoing complex topological changes, self-collision, and large deformations. Our system makes three critical contributions. First, we introduce a new particle data structure that promotes coalesced memory access patterns on the GPU and eliminates the need for complex atomic operations on the memory hierarchy when writing particle data to the grid. Second, we propose a kernel fusion approach using a new Grid-to-Particles-to-Grid (G2P2G) scheme, which efficiently reduces GPU kernel launches, improves latency, and significantly reduces the amount of global memory needed to store particle data. Finally, we introduce optimized algorithmic designs that allow for efficient sparse grids in a shared memory context, enabling us to best utilize modern multi-GPU computational platforms for hybrid Lagrangian-Eulerian computational patterns. We demonstrate the effectiveness of our method with extensive benchmarks, evaluations, and dynamic simulations with elastoplasticity, granular media, and fluid dynamics. In comparisons against an open-source and heavily optimized CPU-based MPM codebase [Fang et al. 2019] on an elastic sphere colliding scene with particle counts ranging from 5 to 40 million, our GPU MPM achieves over 100x per-time-step speedup on a workstation with an Intel 8086K CPU and a single Quadro P6000 GPU, exposing exciting possibilities for future MPM simulations in computer graphics and computational science. Moreover, compared to the state-of-the-art GPU MPM method [Hu et al. 2019a], we not only achieve 2x acceleration on a single GPU but our kernel fusion strategy and Array-of-Structs-of-Array (AoSoA) data structure design also generalizes to multi-GPU systems. Our multi-GPU MPM exhibits near-perfect weak and strong scaling with 4 GPUs, enabling performant and large-scale simulations on a 10243 grid with close to 100 million particles with less than 4 minutes per frame on a single 4-GPU workstation and 134 million particles with less than 1 minute per frame on an 8-GPU workstation.|ACM Transactions on Graphics|2020|10.1145/3386569.3392442|Chenfanfu Jiang, Min Tang, S. Slattery, Yu Fang, Minchen Li, Yixin Zhu, Song-Chun Zhu, Xinlei Wang, Dinesh Manocha, Yuxing Qiu|8.0|0
1817|Towards European-scale convection-resolving climate simulations with GPUs: a study with COSMO 4.19|Abstract. The representation of moist convection in climate models represents a major challenge, due to the small scales involved. Using horizontal grid spacings of O(1km), convection-resolving weather and climate models allows one to explicitly resolve deep convection. However, due to their extremely demanding computational requirements, they have so far been limited to short simulations and/or small computational domains. Innovations in supercomputing have led to new hybrid node designs, mixing conventional multi-core hardware and accelerators such as graphics processing units (GPUs). One of the first atmospheric models that has been fully ported to these architectures is the COSMO (Consortium for Small-scale Modeling) model. Here we present the convection-resolving COSMO model on continental scales using a version of the model capable of using GPU accelerators. The verification of a week-long simulation containing winter storm Kyrill shows that, for this case, convection-parameterizing simulations and convection-resolving simulations agree well. Furthermore, we demonstrate the applicability of the approach to longer simulations by conducting a 3-month-long simulation of the summer season 2006. Its results corroborate the findings found on smaller domains such as more credible representation of the diurnal cycle of precipitation in convection-resolving models and a tendency to produce more intensive hourly precipitation events. Both simulations also show how the approach allows for the representation of interactions between synoptic-scale and meso-scale atmospheric circulations at scales ranging from 1000 to 10 km. This includes the formation of sharp cold frontal structures, convection embedded in fronts and small eddies, or the formation and organization of propagating cold pools. Finally, we assess the performance gain from using heterogeneous hardware equipped with GPUs relative to multi-core hardware. With the COSMO model, we now use a weather and climate model that has all the necessary modules required for real-case convection-resolving regional climate simulations on GPUs.||2016|10.5194/GMD-9-3393-2016|O. Fuhrer, X. Lapillonne, D. Leutwyler, C. Schär, D. Lüthi|7.444444444444445|0
1840|GPU assisted self-collisions of cloths|Nowadays, people expectations about high realism in games are very high and computers have to make a huge effort to compute every simple detail that occurs in a virtual 3D scene. Fortunately, we can use power of Graphics Processing Units (GPU) to compute some part of the most computationally heavy algorithms. In this paper, we present method to accelerate computations on GPU using Compute Shaders based on cloth simulation with self-collisions for big number of cloth’s model vertices (more than 2000).||2016|10.1016/j.compositesa.2016.10.026|A. Wojciechowski, T. Gałaj|7.444444444444445|0
1998|Improving the global memory efficiency in GPU-based systems|of the Dissertation Improving the Global Memory Efficiency in GPU-Based Systems by Amir Kavyan Ziabari Doctor of Philosophy in Electrical and Computer Engineering Northeastern University, December 2016 Dr. David Kaeli, Adviser Graphics Processing Units (GPUs) have been used in a wide range of high performance computing domains. Unfortunately, computing with GPU devices presents its own challenges, including inefficiencies in the global memory system. With today’s growing demand for Big Data processing, the need to leverage larger-scale GPUs or multiple GPUs becomes the natural next step. Big Data applications magnify the current limitations of global memory on GPU-based systems. A major source of this global memory inefficiency is due to bottlenecks in the on-chip network associated with this memory. In this dissertation, we describe how to optimize the performance and power efficiency of an on-chip network used on a GPU. We explore the GPU-based Network-on-Chip (NoC) design space, develop execution-driven simulation models, and analyze a range of parallelized applications. We evaluate a number of conventional network topologies, and their impact on performance of a GPU system. We use detailed simulation to characterize memory access patterns present in the GPU applications, and explore electrical on-chip networks that best match the needs of these applications. We incorporate asymmetry into the NoC design as a solution to reduce the power consumption of a network, while providing comparable performance to the best conventional topology. Our solution reduces the Energy-Delay Product (EDP) by as much as 88%. In order to improve the performance of current and future GPUs, we explore the use of silicon-photonic link technology when constructing the NoC. This emerging, low-latency, highbandwidth technology has been incorporated in chip multiproccesors (CMPs). By introducing a hybrid silicon-photonic NoC in the GPU memory system, we are able to improve performance of memory-intensive applications by 3.43×, as compared with the best alternative electrical NoC. Finally, we conduct a thorough analysis of global memory management schemes for multi-GPU systems. We identify limitations of the global memory present in previously proposed||2016|10.1109/iiswc.2016.7581262|Amir Kavyan Ziabari|7.444444444444445|0
1763|GPU optimization of material point methods|The Material Point Method (MPM) has been shown to facilitate effective simulations of physically complex and topologically challenging materials, with a wealth of emerging applications in computational engineering and visual computing. Borne out of the extreme importance of regularity, MPM is given attractive parallelization opportunities on high-performance modern multiprocessors. Parallelization of MPM that fully leverages computing resources presents challenges that require exploring an extensive design-space for favorable data structures and algorithms. Unlike the conceptually simple CPU parallelization, where the coarse partition of tasks can be easily applied, it takes greater effort to reach the GPU hardware saturation due to its many-core SIMT architecture. In this paper we introduce methods for addressing the computational challenges of MPM and extending the capabilities of general simulation systems based on MPM, particularly concentrating on GPU optimization. In addition to our open-source high-performance framework, we also conduct performance analyses and benchmark experiments to compare against alternative design choices which may superficially appear to be reasonable, but can suffer from suboptimal performance in practice. Our explicit and fully implicit GPU MPM solvers are further equipped with a Moving Least Squares MPM heat solver and a novel sand constitutive model to enable fast simulations of a wide range of materials. We demonstrate that more than an order of magnitude performance improvement can be achieved with our GPU solvers. Practical high-resolution examples with up to ten million particles run in less than one minute per frame.|ACM Transactions on Graphics|2018|10.1145/3272127.3275044|Chenfanfu Jiang, Eftychios Sifakis, Xinlei Wang, Andre Pradhana, Kui Wu, Cem Yuksel, Ming Gao|7.285714285714286|0
1778|Multi-Tier CloudVR|The availability of high bandwidth with low-latency communication in 5G mobile networks enables remote rendered real-time virtual reality (VR) applications. Remote rendering of VR graphics in a cloud removes the need for local personal computer for graphics rendering and augments weak graphics processing unit capacity of stand-alone VR headsets. However, to prevent the added network latency of remote rendering from ruining user experience, rendering a locally navigable viewport that is larger than the field of view of the HMD is necessary. The size of the viewport required depends on latency: Longer latency requires rendering a larger viewport and streaming more content. In this article, we aim to utilize multi-access edge computing to assist the backend cloud in such remote rendered interactive VR. Given the dependency between latency and amount and quality of the content streamed, our objective is to jointly optimize the tradeoff between average video quality and delivery latency. Formulating the problem as mixed integer nonlinear programming, we leverage the interpolation between client’s field of view frame size and overall latency to convert the problem to integer nonlinear programming model and then design efficient online algorithms to solve it. The results of our simulations supplemented by real-world user data reveal that enabling a desired balance between video quality and latency, our algorithm particularly achieves the improvements of on average about 22% and 12% in term of video delivery latency and 8% in term of video quality compared to respectively order-of-arrival, threshold-based, and random-location strategies.|ACM Trans. Multim. Comput. Commun. Appl.|2021|10.1145/3429441|Teemu Kämäräinen, M. Siekkinen, Abbas Mehrabi, Antti Ylä-Jääski|7.25|0
1722|Probabilistic Tsunami Hazard Analysis: High Performance Computing for Massive Scale Inundation Simulations|Probabilistic Tsunami Hazard Analysis (PTHA) quantifies the probability of exceeding a specified inundation intensity at a given location within a given time interval. PTHA provides scientific guidance for tsunami risk analysis and risk management, including coastal planning and early warning. Explicit computation of site-specific PTHA, with an adequate discretization of source scenarios combined with high-resolution numerical inundation modelling, has been out of reach with existing models and computing capabilities, with tens to hundreds of thousands of moderately intensive numerical simulations being required for exhaustive uncertainty quantification. In recent years, more efficient GPU-based High-Performance Computing (HPC) facilities, together with efficient GPU-optimized shallow water type models for simulating tsunami inundation, have now made local long-term hazard assessment feasible. A workflow has been developed with three main stages: 1) Site-specific source selection and discretization, 2) Efficient numerical inundation simulation for each scenario using the GPU-based Tsunami-HySEA numerical tsunami propagation and inundation model using a system of nested topo-bathymetric grids, and 3) Hazard aggregation. We apply this site-specific PTHA workflow here to Catania, Sicily, for tsunamigenic earthquake sources in the Mediterranean. We illustrate the workflows of the PTHA as implemented for High-Performance Computing applications, including preliminary simulations carried out on intermediate scale GPU clusters. We show how the local hazard analysis conducted here produces a more fine-grained assessment than is possible with a regional assessment. However, the new local PTHA indicates somewhat lower probabilities of exceedance for higher maximum inundation heights than the available regional PTHA. The local hazard analysis takes into account small-scale tsunami inundation features and non-linearity which the regional-scale assessment does not incorporate. However, the deterministic inundation simulations neglect some uncertainties stemming from the simplified source treatment and tsunami modelling that are embedded in the regional stochastic approach to inundation height estimation. Further research is needed to quantify the uncertainty associated with numerical inundation modelling and to properly propagate it onto the hazard results, to fully exploit the potential of site-specific hazard assessment based on massive simulations.|Frontiers in Earth Science|2020|10.3389/feart.2020.591549|P. Lanucara, L. Pizzimenti, M. Vöge, B. Brizuela, S. Gibbons, M. Castro, Maria Concetta Lorenzino, R. Tonini, A. Babeyko, S. Lorito, C. Sánchez-Linares, M. Volpe, A. Scala, F. Løvholt, S. Glimsdal, J. Selva, José Manuel González Vida, A. Cirella, Massimo Nazaria, F. Romano, Marc de la Asunción, J. Macías|6.8|0
1844|An Extended Algebraic Reconstruction Technique (E-ART) for Dual Spectral CT|Compared with standard computed tomography (CT), dual spectral CT (DSCT) has many advantages for object separation, contrast enhancement, artifact reduction, and material composition assessment. But it is generally difficult to reconstruct images from polychromatic projections acquired by DSCT, because of the nonlinear relation between the polychromatic projections and the images to be reconstructed. This paper first models the DSCT reconstruction problem as a nonlinear system problem; and then extend the classic ART method to solve the nonlinear system. One feature of the proposed method is its flexibility. It fits for any scanning configurations commonly used and does not require consistent rays for different X-ray spectra. Another feature of the proposed method is its high degree of parallelism, which means that the method is suitable for acceleration on GPUs (graphic processing units) or other parallel systems. The method is validated with numerical experiments from simulated noise free and noisy data. High quality images are reconstructed with the proposed method from the polychromatic projections of DSCT. The reconstructed images are still satisfactory even if there are certain errors in the estimated X-ray spectra.|IEEE Transactions on Medical Imaging|2015|10.1109/TMI.2014.2373396|Xing Zhao, Peng Zhang, Yunsong Zhao|6.8|0
2218|Fred: a GPU-accelerated fast-Monte Carlo code for rapid treatment plan recalculation in ion beam therapy|Ion beam therapy is a rapidly growing technique for tumor radiation therapy. Ions allow for a high dose deposition in the tumor region, while sparing the surrounding healthy tissue. For this reason, the highest possible accuracy in the calculation of dose and its spatial distribution is required in treatment planning. On one hand, commonly used treatment planning software solutions adopt a simplified beam–body interaction model by remapping pre-calculated dose distributions into a 3D water-equivalent representation of the patient morphology. On the other hand, Monte Carlo (MC) simulations, which explicitly take into account all the details in the interaction of particles with human tissues, are considered to be the most reliable tool to address the complexity of mixed field irradiation in a heterogeneous environment. However, full MC calculations are not routinely used in clinical practice because they typically demand substantial computational resources. Therefore MC simulations are usually only used to check treatment plans for a restricted number of difficult cases. The advent of general-purpose programming GPU cards prompted the development of trimmed-down MC-based dose engines which can significantly reduce the time needed to recalculate a treatment plan with respect to standard MC codes in CPU hardware. In this work, we report on the development of fred, a new MC simulation platform for treatment planning in ion beam therapy. The code can transport particles through a 3D voxel grid using a class II MC algorithm. Both primary and secondary particles are tracked and their energy deposition is scored along the trajectory. Effective models for particle–medium interaction have been implemented, balancing accuracy in dose deposition with computational cost. Currently, the most refined module is the transport of proton beams in water: single pencil beam dose–depth distributions obtained with fred agree with those produced by standard MC codes within 1–2% of the Bragg peak in the therapeutic energy range. A comparison with measurements taken at the CNAO treatment center shows that the lateral dose tails are reproduced within 2% in the field size factor test up to 20 cm. The tracing kernel can run on GPU hardware, achieving 10 million primary s−1 on a single card. This performance allows one to recalculate a proton treatment plan at 1% of the total particles in just a few minutes.|Physics in Medicine and Biology|2017|10.1088/1361-6560/aa8134|G. Magro, M. Senzacqua, A. Schiavi, G. Battistoni, A. Mairani, V. Patera, S. Pioli, S. Molinelli, M. Ciocca|6.625|0
2259|BrainPy, a flexible, integrative, efficient, and extensible framework for general-purpose brain dynamics programming|Elucidating the intricate neural mechanisms underlying brain functions requires integrative brain dynamics modeling. To facilitate this process, it is crucial to develop a general-purpose programming framework that allows users to freely define neural models across multiple scales, efficiently simulate, train, and analyze model dynamics, and conveniently incorporate new modeling approaches. In response to this need, we present BrainPy. BrainPy leverages the advanced just-in-time (JIT) compilation capabilities of JAX and XLA to provide a powerful infrastructure tailored for brain dynamics programming. It offers an integrated platform for building, simulating, training, and analyzing brain dynamics models. Models defined in BrainPy can be JIT compiled into binary instructions for various devices, including Central Processing Unit, Graphics Processing Unit, and Tensor Processing Unit, which ensures high-running performance comparable to native C or CUDA. Additionally, BrainPy features an extensible architecture that allows for easy expansion of new infrastructure, utilities, and machine-learning approaches. This flexibility enables researchers to incorporate cutting-edge techniques and adapt the framework to their specific needs.|eLife|2023|10.7554/eLife.86365|Shangyang Li, Sichao He, Si Wu, Xiaoyu Chen, Tianqiu Zhang, Chaoming Wang|6.5|0
1999|OpenVCT: a GPU-accelerated virtual clinical trial pipeline for mammography and digital breast tomosynthesis|Virtual clinical trials (VCTs) have a critical role in preclinical testing of imaging systems. A VCT pipeline has been developed to model the human body anatomy, image acquisition systems, display and processing, and image analysis and interpretation. VCTs require the execution of multiple computer simulations in a reasonable time. This study presents the OpenVCT Framework, consisting of graphical software to design a sequence of processing steps for the VCT pipeline; management software that coordinates the pipeline execution, manipulates, and retrieves phantoms and images using a relational database; and a server that executes the individual steps of the virtual patient accrual process using GPU optimized software. The framework is modular and supports various data types, algorithms, and modalities. The framework can be used to conduct massive simulations and several hundred imaging studies can be simulated per day on a single workstation. On average, we can simulate a Tomo Combo (DM + DBT) study using anthropomorphic breast phantoms in less than 9 minutes (voxel size = 100 μm3 and volume = 700 mL). Tomo Combo images from an entire virtual population can be simulated in less than a week. We can accelerate system performance using phantoms with large voxels. The VCT pipeline can also be accelerated by using multiple GPU’s (e.g., using SLI mode, GPU clusters).|Medical Imaging|2018|10.1117/12.2294935|P. Bakic, Andrew D. A. Maidment, B. Barufaldi, D. Higginbotham|6.428571428571429|0
1732|Multi-Scattering software: part I: online accelerated Monte Carlo simulation of light transport through scattering media.|In this article we present and describe an online freely accessible software called Multi-Scattering for the modeling of light propagation in scattering and absorbing media. Part II of this article series focuses on the validation of the model by rigorously comparing the simulated results with experimental data. The model is based on the use of the Monte Carlo method, where billions of photon packets are being tracked through simulated cubic volumes. Simulations are accelerated by the use of general-purpose computing on graphics processing units, reducing the computation time by a factor up to 200x in comparison with a single central processing unit thread. By using four graphic cards on a single computer, the simulation speed increases by a factor of 800x. For an anisotropy factor g = 0.86, this enables the transport path of one billion photons to be computed in 10 seconds for optical depth OD = 10 and in 20 minutes for OD = 500. Another feature of Multi-Scattering is the integration and implementation of the Lorenz-Mie theory in the software to generate the scattering phase functions from spherical particles. The simulations are run from a computer server at Lund University, allowing researchers to log in and use it freely without any prior need for programming skills or specific software/hardware installations. There are countless types of scattering media in which this model can be used to predict light transport, including medical tissues, blood samples, clouds, smoke, fog, turbid liquids, spray systems, etc. An example of simulation results is given here for photon propagation through a piece of human head. The software also includes features for modeling image formation by inserting a virtual collecting lens and a detection matrix which simulate a camera objective and a sensor array respectively. The user interface for setting-up simulations and for displaying the corresponding results is found at: https://multi-scattering.com/.|Optics Express|2020|10.1364/oe.404005|J. Jönsson, E. Berrocal|6.4|0
1782|A High-Performance Integrated Hydrodynamic Modelling System for Urban Flood Simulations|A new High-Performance Integrated hydrodynamic Modelling System (Hi-PIMS) is tested for urban flood simulation. The software solves the two-dimensional shallow water equations using a first-order accurate Godunov-type shock-capturing scheme incorporated with the Harten, Lax and van Leer approximate Riemann solver with the contact wave restored (HLLC) for flux evaluation. The benefits of modern graphics processing units are explored to accelerate large-scale high-resolution simulations. In order to test its performance, the tool is applied to predict flood inundation due to rainfall and a point source surface flow in Glasgow, Scotland, and a hypothetical inundation event at different spatial resolutions in Thamesmead, England, caused by embankment failure. Numerical experiments demonstrate potential benefits for high-resolution modelling of urban flood inundation, and a much-improved level of performance without compromising result quality.||2015|10.2166/HYDRO.2015.029|Q. Liang, L. Smith|6.4|0
1791|Helios: A Scalable 3D Plant and Environmental Biophysical Modeling Framework|This article presents an overview of Helios, a new three-dimensional (3D) plant and environmental modeling framework. Helios is a model coupling framework designed to provide maximum flexibility in integrating and running arbitrary 3D environmental system models. Users interact with Helios through a well-documented open-source C++ API. Version 1.0 comes with model plug-ins for radiation transport, the surface energy balance, stomatal conductance, photosynthesis, solar position, and procedural tree generation. Additional plug-ins are also available for visualizing model geometry and data and for processing and integrating LiDAR scanning data. Many of the plug-ins perform calculations on the graphics processing unit, which allows for efficient simulation of very large domains with high detail. An example modeling study is presented in which leaf-level heterogeneity in water usage and photosynthesis of an orchard is examined to understand how this leaf-scale variability contributes to whole-tree and -canopy fluxes.|Frontiers in Plant Science|2019|10.3389/fpls.2019.01185|B. Bailey|6.166666666666667|0
1867|Virtual Radar: Real-Time Millimeter-Wave Radar Sensor Simulation for Perception-Driven Robotics|"This article presents ViRa<xref ref-type=""fn"" rid=""fn1""><sup>1</sup></xref><fn id=""fn1""><label><sup>1</sup></label><p>Available online: <uri>https://vira.aau.at/</uri>.</p></fn>, a real-time open-source millimeter-wave radar simulation framework for perception-driven robotic applications. ViRa provides <inline-formula><tex-math notation=""LaTeX"">$(i)$</tex-math></inline-formula> raw data of radar sensors in real-time, simulation of <inline-formula><tex-math notation=""LaTeX"">$(ii)$</tex-math></inline-formula> multi-antenna configurations for spatial estimation of objects, <inline-formula><tex-math notation=""LaTeX"">$(iii)$</tex-math></inline-formula> wave penetration of non-conductive objects to infer information in occluded situations, <inline-formula><tex-math notation=""LaTeX"">$(iv)$</tex-math></inline-formula> different radar beam patterns, and, <inline-formula><tex-math notation=""LaTeX"">$(v)$</tex-math></inline-formula> configurations of radar sensors as given by real-world radars. By using ViRa, researchers can simulate radar sensors in different robotic scenarios and investigate radars prior to the installation. This allows an acceleration in the development of radar sensors for robotic applications without the need of real hardware. Contrary to simple model abstractions, which only output loose features, ViRa generates raw radar data using computer graphics techniques on graphics processing unit (GPU) level embedded inside a game engine environment. ViRa allows to feed data directly into machine learning frameworks, which enables further improvement in novel research directions, such as safe human-robot interaction or agile drone flights in obstacle-rich environments. The proposed simulation framework is validated with data from different scenarios in robotics such as human tracking for human-robot interaction. The obtained results are compared with a reference simulation framework and show significantly higher correlation when compared to real-world measurement data."|IEEE Robotics and Automation Letters|2021|10.1109/LRA.2021.3068916|Stephan Mühlbacher-Karrer, H. Zangl, Christian Schöffmann, Barnaba Ubezio, C. Böhm|6.0|0
2084|Medial IPC|We propose a framework of efficient nonlinear deformable simulation with both fast continuous collision detection and robust collision resolution. We name this new framework Medial IPC as it integrates the merits from medial elastics, for an efficient and versatile reduced simulation, as well as incremental potential contact, for a robust collision and contact resolution. We leverage medial axis transform to construct a kinematic subspace. Instead of resorting to projective dynamics, we use classic hyperelastics to embrace real-world nonlinear materials. A novel reduced continuous collision detection algorithm is presented based on the medial mesh. Thanks to unique geometric properties of medial axis and medial primitives, we derive closed-form formulations for identifying between-primitive collision within the reduced medial space. In the meantime, the implicit barrier energy that generates necessary repulsion forces for collision resolution is also formulated with the medial coordinate. In other words, Medial IPC exploits a universal reduced coordinate for simulation, continuous self-/collision detection, and IPC-based collision resolution. Continuous collision detection also allows more aggressive time stepping. In addition, we carefully implement our system with a heterogeneous CPU-GPU deployment such that massively parallelizable computations are carried out on the GPU while few sequential computations are on the CPU. Such implementation also frees us from generating training poses for selecting Cubature points and pre-computing their weights. We have tested our method on complicated deformable models and collision-rich simulation scenarios. Due to the reduced nature of our system, the computation is faster than fullspace IPC or other fullspace methods using continuous collision detection by at least one order. The simulation remains high-quality as the medial subspace captures intriguing and local deformations with sufficient realism.|ACM Transactions on Graphics|2021|10.1145/3450626.3459753|Chenfanfu Jiang, Minchen Li, L. Lan, Junfeng Yao, D. Kaufman, Yin Yang|6.0|0
2276|Advent of Cross‐Scale Modeling: High‐Performance Computing of Solidification and Grain Growth|The application range of computational metallurgy is rapidly expanding thanks to the recent progress in high‐performance computing. In this Progress Report, state‐of‐the‐art collections of large‐scale simulations of solidification and grain growth, performed on the GPU supercomputer, are introduced. One of the notable achievements in this direction is a billion‐atom molecular dynamics simulation for nucleation and solidification, which revealed the heterogeneity in homogeneous nucleation. Moreover, a series of large‐scale phase‐field simulations shed light on the topics at issue including competitive growth of dendrites during the directional solidification, the effect of forced and natural convections on the solidification, and so on. Based on simulation results bridging the gap between atomistic and continuum‐based simulations, a new criterion of multi‐scale modeling is proposed in the age to come. We are now standing at the new era of cross‐scale modeling, in which the overlap between atomistic and continuum simulations creates new research concepts and fields.|Advanced Theory and Simulations|2018|10.1002/adts.201800065|Y. Shibuta, T. Takaki, M. Ohno|6.0|0
2293|Advanced Diagnosis Techniques for Radio Telescopes in Astronomical Applications|The performance of radio telescopes in astronomical applications can be affected by structural variations due to:\n1. Misalignment of the feeding structure, resulting in a lateral or axial displacement of\nthe receiver;\n2. Wind stress;\n3. Gravitational distortion as the antenna is tilted; \n4. Thermal distortion with ambient temperature or sunlight.\nDiagnosis methods are necessary to estimate any deviation of the antenna system from its nominal behavior in order to guarantee the maximum performance. Several approaches have been developed during the years, and among them the electromagnetic diagnosis appears today as the most appealing, because it allows a relatively simple measurement setup and a reduced human intervention. Electromagnetic diagnosis is based on the acquisition of the antenna Far Field Pattern (FFP), with the Antenna Under Test (AUT) working in receiving mode. A natural radio star or a satellite beacon provides the signal source. The acquisition of the FFP typically requires a very large number of field samples to get the complete information about the AUT, and the subsequent measurement process may span over several hours. A prolonged acquisition has significant drawbacks related to the continuous tracking of the source and the inconstancy of the environmental conditions. The purpose of the PhD activity has been focused on an optimized formulation of the diagnosis of radio telescopes aimed at reducing the number of field samples to acquire, and so at minimizing the measurement time. A diagnosis approach has been developed, based on the Aperture Field method for the description of the AUT radiation mechanism. A Principal Component Analysis (PCA) has been employed to restore a linear relationship between the unknowns describing the AUT status and the far field data. An optimal far field sampling grid is selected by optimizing the singular values behavior of the relevant linearized operator. During the activity, a computational tool based on Geometrical Optics (GO) has been developed to improve the diagnosis approach. Indeed, once the Aperture Field is recovered from the inversion of the measured FFP, an additional step is required to assess the AUT status from the phase distribution. Obviously, the computation of the phase distribution should be based on efficient algorithms in order to properly manage electrically large reflectors. The developed GO technique relies on the Fast Marching Method (FMM) for the direct solution of the eikonal equation. A GO approach based on the FMM is appealing because it shows a favorable computational trend. Furthermore, the explicit solution of the eikonal equation opens the possibility to set up an inverse ray tracing scheme, which proves particularly convenient compared to direct ray tracing because it allows to easily select the minimum number of rays to be traced. The FMM is also amenable for parallel execution. In particular, in the present work, the Fast Iterative Method has been implemented on Graphics Processing Units (GPUs). Moreover, the FMM has been accelerated by introducing a tree data structure. The tree allows to manage the mutual interactions between multiple scattering surfaces and the parallelization of the ray tracing step. The method has been numerically tested on simple canonical cases to show its performance in terms of accuracy and speed. Then, it has been applied to the evaluation of the Aperture Field phase required by the reflector diagnosis.\nDuring the research activity, the problem of validating the diagnosis algorithms has been also faced. Obviously, a numerical analysis can been carried out to test the model employed to describe the system and to evaluate the performance of the algorithm. To this end, a reliable commercial software exploited to simulate reflector antennas has been exploited. However, to complete the analysis, the experimental validation becomes mandatory, and an experimental outdoor far field test range is required. Accordingly, a test range has been set up thanks to the collaboration with Istituto Nazionale di Astrofisica (INAF) of Naples, Italy. Its realization has involved the full development of the software to drive an Alt-Azimuth positioner and to remotely control the instrumentation. In addition, an upgrade of the internal connections of a Vector Network Analyzer has been performed in order to allow the interferometric acquisition.||2018|10.3390/antiox7060072|S. Savarese|5.857142857142857|0
1775|Review of Hardware Platforms for Real-Time Simulation of Electric Machines|"This paper presents a review of the research status for real-time simulation of electric machines. The machine models considered are the lumped parameter models, including the phase-domain, <inline-formula> <tex-math notation=""LaTeX"">$d$ </tex-math></inline-formula>–<inline-formula> <tex-math notation=""LaTeX"">$q$ </tex-math></inline-formula>, and voltage-behind-reactance models, as well as the physics-based models, including the finite-element method and magnetic equivalent circuit models. These models are initially presented along with their relative advantages and disadvantages with respect to modeling fidelity and their computational intensity. A field-programmable gate array, a graphics processing unit, a chip multiprocessor, and computer clusters are the main hardware platforms for real-time simulations. An overview of such hardware platforms is presented and their comparative performances are evaluated with respect to real-time simulation of electric machines and drives on the basis of simulation acceleration, machine types, and modeling methodology."|IEEE Transactions on Transportation Electrification|2017|10.1109/TTE.2017.2656141|N. Erdogan, A. Davoudi, S. Mojlish, D. Levine|5.75|0
2277|A high-performance cellular automata model for urban simulation based on vectorization and parallel computing technology|ABSTRACT Cellular automata (CA) models can simulate complex urban systems through simple rules and have become important tools for studying the spatio-temporal evolution of urban land use. However, the multiple and large-volume data layers, massive geospatial processing and complicated algorithms for automatic calibration in the urban CA models require a high level of computational capability. Unfortunately, the limited performance of sequential computation on a single computing unit (i.e. a central processing unit (CPU) or a graphics processing unit (GPU)) and the high cost of parallel design and programming make it difficult to establish a high-performance urban CA model. As a result of its powerful computational ability and scalability, the vectorization paradigm is becoming increasingly important and has received wide attention with regard to this kind of computational problem. This paper presents a high-performance CA model using vectorization and parallel computing technology for the computation-intensive and data-intensive geospatial processing in urban simulation. To transfer the original algorithm to a vectorized algorithm, we define the neighborhood set of the cell space and improve the operation paradigm of neighborhood computation, transition probability calculation, and cell state transition. The experiments undertaken in this study demonstrate that the vectorized algorithm can greatly reduce the computation time, especially in the environment of a vector programming language, and it is possible to parallelize the algorithm as the data volume increases. The execution time for the simulation of 5-m resolution and 3 × 3 neighborhood decreased from 38,220.43 s to 803.36 s with the vectorized algorithm and was further shortened to 476.54 s by dividing the domain into four computing units. The experiments also indicated that the computational efficiency of the vectorized algorithm is closely related to the neighborhood size and configuration, as well as the shape of the research domain. We can conclude that the combination of vectorization and parallel computing technology can provide scalable solutions to significantly improve the applicability of urban CA.|International Journal of Geographical Information Science|2018|10.1080/13658816.2017.1390118|Haijun Wang, Anqi Zhang, Chang Xia, Wenting Zhang|5.714285714285714|0
2304|JAX-ReaxFF: A Gradient-Based Framework for Fast Optimization of Reactive Force Fields.|The reactive force field (ReaxFF) model bridges the gap between traditional classical models and quantum mechanical (QM) models by incorporating dynamic bonding and polarizability. To achieve realistic simulations using ReaxFF, model parameters must be optimized against high fidelity training data which typically come from QM calculations. Existing parameter optimization methods for ReaxFF consist of black box techniques using genetic algorithms or Monte Carlo methods. Due to the stochastic behavior of these methods, the optimization process oftentimes requires millions of error evaluations for complex parameter fitting tasks, thereby significantly hampering the rapid development of high quality parameter sets. Rapid optimization of the parameters is essential for developing and refining Reax force fields because producing a force field which exhibits empirical accuracy in terms of dynamics typically requires multiple refinements to the training data as well as to the parameters under optimization. In this work, we present JAX-ReaxFF, a novel software tool that leverages modern machine learning infrastructure to enable fast optimization of ReaxFF parameters. By calculating gradients of the loss function using the JAX library, JAX-ReaxFF utilizes highly effective local optimization methods that are initiated from multiple guesses in the high dimensional optimization space to obtain high quality results. Leveraging the architectural portability of the JAX framework, JAX-ReaxFF can execute efficiently on multicore CPUs, graphics processing units (GPUs), or even tensor processing units (TPUs). As a result of using the gradient information and modern hardware accelerators, we are able to decrease ReaxFF parameter optimization time from days to mere minutes. Furthermore, the JAX-ReaxFF framework can also serve as a sandbox environment for domain scientists to explore customizing the ReaxFF functional form for more accurate modeling.|Journal of Chemical Theory and Computation|2022|10.1021/acs.jctc.2c00363|A. V. van Duin, Kurt A. O'Hearn, A. Rahnamoun, H. Aktulga, K. Merz, M. C. Kaymak|5.666666666666667|0
2247|Boris computational spintronics—High performance multi-mesh magnetic and spin transport modeling software|This work discusses the design and testing of a new computational spintronics research software. Boris is a comprehensive multi-physics open-source software, combining micromagnetics modelling capabilities with drift-diffusion spin transport modelling and heat flow solver in multi-material structures. A multi-mesh paradigm is employed, allowing modelling of complex multi-layered structures with independent discretization and arbitrary relative positioning between different computational meshes. Implemented micromagnetics models include not only ferromagnetic materials modelling, but also two-sublattice models, allowing simulations of antiferromagnetic and ferrimagnetic materials, fully integrated in the multi-mesh and multi-material design approach. High computational performance is an important design consideration in Boris, and all computational routines can be executed on GPUs, in addition to CPUs. In particular a modified 3D convolution algorithm is used to compute the demagnetizing field on the GPU, termed pipelined convolution, and benchmark comparisons with existing GPU-accelerated software Mumax3 have shown performance improvements up to twice faster.|Journal of Applied Physics|2020|10.1063/5.0024382|S. Lepadatu|5.6|0
2271|Penetration-free projective dynamics on the GPU|We present a GPU algorithm for deformable simulation. Our method offers good computational efficiency and penetration-free guarantee at the same time, which are not common with existing techniques. The main idea is an algorithmic integration of projective dynamics (PD) and incremental potential contact (IPC). PD is a position-based simulation framework, favored for its robust convergence and convenient implementation. We show that PD can be employed to handle the variational optimization with the interior point method e.g., IPC. While conceptually straightforward, this requires a dedicated rework over the collision resolution and the iteration modality to avoid incorrect collision projection with improved numerical convergence. IPC exploits a barrier-based formulation, which yields an infinitely large penalty when the constraint is on the verge of being violated. This mechanism guarantees intersection-free trajectories of deformable bodies during the simulation, as long as they are apart at the rest configuration. On the downside, IPC brings a large amount of nonlinearity to the system, making PD slower to converge. To mitigate this issue, we propose a novel GPU algorithm named A-Jacobi for faster linear solve at the global step of PD. A-Jacobi is based on Jacobi iteration, but it better harvests the computation capacity on modern GPUs by lumping several Jacobi steps into a single iteration. In addition, we also re-design the CCD root finding procedure by using a new minimum-gradient Newton algorithm. Those saved time budgets allow more iterations to accommodate stiff IPC barriers so that the result is both realistic and collision-free. Putting together, our algorithm simulates complicated models of both solids and shells on the GPU at an interactive rate or even in real time.|ACM Transactions on Graphics|2022|10.1145/3528223.3530069|Changxi Zheng, L. Lan, Yin Yang, Chenfanfu Jiang, Minchen Li, Guanqun Ma|5.333333333333333|0
1674|Modeling Noncanonical RNA Base Pairs by a Coarse-Grained IsRNA2 Model.|Noncanonical base pairs contribute crucially to the three-dimensional architecture of large RNA molecules; however, how to accurately model them remains an open challenge in RNA 3D structure prediction. Here, we report a promising coarse-grained (CG) IsRNA2 model to predict noncanonical base pairs in large RNAs through molecular dynamics simulations. By introducing a five-bead per nucleotide CG representation to reserve the three interacting edges of nucleobases, IsRNA2 accurately models various base-pairing interactions, including both canonical and noncanonical base pairs. A benchmark test indicated that IsRNA2 achieves a comparable performance to the atomic model in de novo modeling of noncanonical RNA structures. In addition, IsRNA2 was able to refine the 3D structure predictions for large RNAs in RNA-puzzle challenges. Finally, the graphics processing unit acceleration was introduced to speed up the sampling efficiency in IsRNA2 for very large RNA molecules. Therefore, the CG IsRNA2 model reported here offers a reliable approach to predict the structures and dynamics of large RNAs.|Journal of Physical Chemistry B|2021|10.1021/acs.jpcb.1c07288|Dong Zhang, Shi-jie Chen, R. Zhou|5.0|0
1825|LoCoMOBO: A Local Constrained Multiobjective Bayesian Optimization for Analog Circuit Sizing|A local constrained multiobjective Bayesian optimization (LoCoMOBO) method is introduced to address automatic sizing and tradeoff exploration for analog and RF integrated circuits (ICs). LoCoMOBO applies to constrained optimization problems utilizing multiple Gaussian process (GP) models that approximate the objective and constraint functions locally in the search space. It searches for potential pareto optimal solutions within trust regions of the search space using only a few time-consuming simulations. The trust regions are adaptively updated during the optimization process based on feasibility and hypervolume metrics. In contrast to mainstream Bayesian optimization approaches, LoCoMOBO uses a new acquisition function that can provide multiple query points, therefore allowing for parallel execution of costly simulations. GP inference is also enhanced by using GPU acceleration in order to handle highly constrained problems that require large sample budgets. Combined with a framework for schematic parametrization and simulator calls, LoCoMOBO provides improved performance tradeoffs and sizing results on three real-world circuit examples, while reducing the total runtime up to $\times 43$ times compared to state-of-the-art methods.|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|2021|10.1109/TCAD.2021.3121263|P. Sotiriadis, Konstantinos Touloupas|5.0|0
2016|Accurate Machine-learning Atmospheric Retrieval via a Neural-network Surrogate Model for Radiative Transfer|Atmospheric retrieval determines the properties of an atmosphere based on its measured spectrum. The low signal-to-noise ratios of exoplanet observations require a Bayesian approach to determine posterior probability distributions of each model parameter, given observed spectra. This inference is computationally expensive, as it requires many executions of a costly radiative transfer (RT) simulation for each set of sampled model parameters. Machine learning (ML) has recently been shown to provide a significant reduction in runtime for retrievals, mainly by training inverse ML models that predict parameter distributions, given observed spectra, albeit with reduced posterior accuracy. Here we present a novel approach to retrieval by training a forward ML surrogate model that predicts spectra given model parameters, providing a fast approximate RT simulation that can be used in a conventional Bayesian retrieval framework without significant loss of accuracy. We demonstrate our method on the emission spectrum of HD 189733 b and find good agreement with a traditional retrieval from the Bayesian Atmospheric Radiative Transfer (BART) code (Bhattacharyya coefficients of 0.9843–0.9972, with a mean of 0.9925, between 1D marginalized posteriors). This accuracy comes while still offering significant speed enhancements over traditional RT, albeit not as much as ML methods with lower posterior accuracy. Our method is ∼9× faster per parallel chain than BART when run on an AMD EPYC 7402P central processing unit (CPU). Neural-network computation using an NVIDIA Titan Xp graphics processing unit is 90×–180× faster per chain than BART on that CPU.|The Planetary Science Journal|2020|10.3847/PSJ/abe3fd|J. Harrington, S. Domagal‐Goldman, D. Wright, Frank Soboczenski, A. G. Baydin, Adam D. Cobb, M. O'Beirne, M. D. Himes, G. Arney, Simone Zorzan, Z. Scheffer|5.0|0
1985|Real-Time Nonlinear Model Predictive Control of Robots Using a Graphics Processing Unit|In past robotics applications, Model Predictive Control (MPC) has often been limited to linear models and relatively short time horizons. In recent years however, research in optimization, optimal control, and simulation has enabled some forms of nonlinear model predictive control which find locally optimal solutions. The limiting factor for applying nonlinear MPC for robotics remains the computation necessary to solve the optimization, especially for complex systems and for long time horizons. This letter presents a new solution method which addresses computational concerns related to nonlinear MPC called nonlinear Evolutionary MPC (NEMPC), and then we compare it to several existing methods. These comparisons include simulations on torque-limited robots performing a swing-up task and demonstrate that NEMPC is able to discover complex behaviors to accomplish the task. Comparisons with state-of-the-art nonlinear MPC algorithms show that NEMPC finds high quality control solutions very quickly using a global, instead of local, optimization method. Finally, an application in hardware (a 24 state pneumatically actuated continuum soft robot) demonstrates that this method is tractable for real-time control of high degree of freedom systems.|IEEE Robotics and Automation Letters|2020|10.1109/LRA.2020.2965393|Marc D. Killpack, Phillip Hyatt|4.8|0
2176|Ab initio interactive molecular dynamics on graphical processing units (GPUs).|A virtual molecular modeling kit is developed based on GPU-enabled interactive ab initio molecular dynamics (MD). The code uses the TeraChem and VMD programs with a modified IMD interface. Optimization of the GPU accelerated TeraChem program specifically for small molecular systems is discussed, and a robust multiple time step integrator is employed to accurately integrate strong user-supplied pulling forces. Smooth and responsive visualization techniques are developed to allow interactive manipulation at minimum simulation rates below five MD steps per second. Representative calculations at the Hartree-Fock level of theory are demonstrated for molecular systems containing up to a few dozen atoms.|Journal of Chemical Theory and Computation|2015|10.1021/acs.jctc.5b00419|T. Martínez, Alex G B Jin, Nathan Luehr|4.8|0
2071|Mesoscopic thin film superconductors - A computational framework|Motivated by experiments on superconductors in the microscopic regime and the realization of small superconducting devices such as qubits, we initiate a computational project in the form of a numerical simulation package to model and predict the behavior of these type of systems. It is a multidisciplinary endeavor in that it employs theory, computational science, software design, and to some extent new visualization techniques. Our intention is to create a flexible and modular code library which is capable of simulating a wide variety of superconducting systems, while significantly shortening the startup time for computational projects. \n \nWe base the numerical model in quasiclassical theory with the Eilenberger transport equation, and execute the highly parallel computations on a contemporary graphics card (GPU). In this thesis we touch on the theoretical background, describe the discretization of the theory, and present a thorough overview on how to solve the equations in practice for a general 2-dimensional geometry, as well as review the design principles behind the developed software. Moreover, a few selected results are presented to show that the output is sound, and to highlight some new findings. In paper I we examine a new low temperature phase in which spontaneous edge currents emerge in a d-wave superconducting square grain, breaking time-reversal symmetry. The modeling of such a system is made possible by our development of a simulation domain description which allows for specular boundary conditions in an arbitrarily shaped geometry.||2015|10.1038/nphys3383|M. Håkansson|4.6|0
1662|A tracking system to calculate patient skin dose in real-time during neurointerventional procedures using a biplane x-ray imaging system.|PURPOSE\nNeurovascular interventional procedures using biplane fluoroscopic imaging systems can lead to increased risk of radiation-induced skin injuries. The authors developed a biplane dose tracking system (Biplane-DTS) to calculate the cumulative skin dose distribution from the frontal and lateral x-ray tubes and display it in real-time as a color-coded map on a 3D graphic of the patient for immediate feedback to the physician. The agreement of the calculated values with the dose measured on phantoms was evaluated.\n\n\nMETHODS\nThe Biplane-DTS consists of multiple components including 3D graphic models of the imaging system and patient, an interactive graphical user interface, a data acquisition module to collect geometry and exposure parameters, the computer graphics processing unit, and functions for determining which parts of the patient graphic skin surface are within the beam and for calculating dose. The dose is calculated to individual points on the patient graphic using premeasured calibration files of entrance skin dose per mAs including backscatter; corrections are applied for field area, distance from the focal spot and patient table and pad attenuation when appropriate. The agreement of the calculated patient skin dose and its spatial distribution with measured values was evaluated in 2D and 3D for simulated procedure conditions using a PMMA block phantom and an SK-150 head phantom, respectively. Dose values calculated by the Biplane-DTS were compared to the measurements made on the phantom surface with radiochromic film and a calibrated ionization chamber, which was also used to calibrate the DTS. The agreement with measurements was specifically evaluated with variation in kVp, gantry angle, and field size.\n\n\nRESULTS\nThe dose tracking system that was developed is able to acquire data from the two x-ray gantries on a biplane imaging system and calculate the skin dose for each exposure pulse to those vertices of a patient graphic that are determined to be in the beam. The calculations are done in real-time with a typical graphic update time of 30 ms and an average vertex separation of 3 mm. With appropriate corrections applied, the Biplane-DTS was able to determine the entrance dose within 6% and the spatial distribution of the dose within 4% compared to the measurements with the ionization chamber and film for the SK150 head phantom. The cumulative dose for overlapping fields from both gantries showed similar agreement.\n\n\nCONCLUSIONS\nThe Biplane-DTS can provide a good estimate of the peak skin dose and cumulative skin dose distribution during biplane neurointerventional procedures. Real-time display of this information should help the physician manage patient dose to reduce the risk of radiation-induced skin injuries.|Medical Physics (Lancaster)|2016|10.1118/1.4960368|S. Rudin, D. Bednarek, V. Rana|4.555555555555555|0
1788|Large-Scale Materials Modeling at Quantum Accuracy: Ab Initio Simulations of Quasicrystals and Interacting Extended Defects in Metallic Alloys|Ab initio electronic-structure has remained dichotomous between achievable accuracy and length-scale. Quantum many-body (QMB) methods realize quantum accuracy but fail to scale. Density functional theory (DFT) scales favorably but remains far from quantum accuracy. We present a framework that breaks this dichotomy by use of three interconnected modules: (i) invDFT: a methodological advance in inverse DFT linking QMB methods to DFT; (ii) MLXC: a machine-learned density functional trained with invDFT data, commensurate with quantum accuracy; (iii) DFT-FE-MLXC: an adaptive higher-order spectral finite-element (FE) based DFT implementation that integrates MLXC with efficient solver strategies and HPC innovations in FE-specific dense linear algebra, mixed-precision algorithms, and asynchronous compute-communication. We demonstrate a paradigm shift in DFT that not only provides an accuracy commensurate with QMB methods in ground-state energies, but also attains an unprecedented performance of 659.7 PFLOPS (43.1% peak FP64 performance) on 619,124 electrons using 8,000 GPU nodes of Frontier supercomputer.|International Conference on Software Composition|2023|10.1145/3581784.3627037|Gourab Panigrahi, Sambit Das, V. Gavini, Vishal Subramanian, David M. Rogers, P. Motamarri, Bikash Kanungo, Paul M. Zimmerman|4.5|0
2043|Component-Level Thermo-Electromagnetic Nonlinear Transient Finite Element Modeling of Solid-State Transformer for DC Grid Studies|Highly-detailed equipment models for electromagnetic transient simulation provide an accurate insight into the system characteristics and behavior. In this article, a coupled field-circuit cosimulation employing detailed component-level models is proposed for the solid-state transformer. To reveal comprehensive thermo-electromagnetic information of the equipment, a high-order nonlinear insulated-gate bipolar transistor (IGBT) model is utilized for the modular multilevel converter, while the finite element method (FEM) is adopted in modeling the transformer. The heavy computational challenge posed by the complexity of these models is alleviated by exploiting model parallelism and the subsequent processing by massively parallel architecture of the graphics processing unit, e.g., a pair of coupled voltage-current sources is adopted for reducing the order of the matrix equation in the circuit part, while in the FEM-based models, a matrix-free nodal domain decomposition solution is utilized to parallelize the overall system to the maximum. A multirate scheme is applied for a further computational burden reduction of the cosimulation due to a large disparity in the appropriate time-steps between power semiconductor switches and the magnetic component. Simulation of a multiterminal dc system including the SST is carried out, and the accuracy of proposed models are validated by offline tools such as SaberRD, ANSYS, and PSCAD/EMTDC.|IEEE transactions on industrial electronics (1982. Print)|2021|10.1109/TIE.2020.2967687|V. Dinavahi, Peng Liu, Ning Lin|4.25|0
2219|AMITIS: A 3D GPU-Based Hybrid-PIC Model for Space and Plasma Physics|We have developed, for the first time, an advanced modeling infrastructure in space simulations (AMITIS) with an embedded three-dimensional self-consistent grid-based hybrid model of plasma (kinetic ions and fluid electrons) that runs entirely on graphics processing units (GPUs). The model uses NVIDIA GPUs and their associated parallel computing platform, CUDA, developed for general purpose processing on GPUs. The model uses a single CPU-GPU pair, where the CPU transfers data between the system and GPU memory, executes CUDA kernels, and writes simulation outputs on the disk. All computations, including moving particles, calculating macroscopic properties of particles on a grid, and solving hybrid model equations are processed on a single GPU. We explain various computing kernels within AMITIS and compare their performance with an already existing well-tested hybrid model of plasma that runs in parallel using multi-CPU platforms. We show that AMITIS runs ∼10 times faster than the parallel CPU-based hybrid model. We also introduce an implicit solver for computation of Faraday’s Equation, resulting in an explicit-implicit scheme for the hybrid model equation. We show that the proposed scheme is stable and accurate. We examine the AMITIS energy conservation and show that the energy is conserved with an error < 0.2% after 500,000 timesteps, even when a very low number of particles per cell is used.||2017|10.1088/1742-6596/837/1/012017|A. Poppe, W. Farrell, S. Fatemi, G. Delory|4.25|0
1665|Efficient Lattice Boltzmann Solver for Patient-Specific Radiofrequency Ablation of Hepatic Tumors|Radiofrequency ablation (RFA) is an established treatment for liver cancer when resection is not possible. Yet, its optimal delivery is challenged by the presence of large blood vessels and the time-varying thermal conductivity of biological tissue. Incomplete treatment and an increased risk of recurrence are therefore common. A tool that would enable the accurate planning of RFA is hence necessary. This manuscript describes a new method to compute the extent of ablation required based on the Lattice Boltzmann Method (LBM) and patient-specific, pre-operative images. A detailed anatomical model of the liver is obtained from volumetric images. Then a computational model of heat diffusion, cellular necrosis, and blood flow through the vessels and liver is employed to compute the extent of ablated tissue given the probe location, ablation duration and biological parameters. The model was verified against an analytical solution, showing good fidelity. We also evaluated the predictive power of the proposed framework on ten patients who underwent RFA, for whom pre- and post-operative images were available. Comparisons between the computed ablation extent and ground truth, as observed in postoperative images, were promising (DICE index: 42%, sensitivity: 67%, positive predictive value: 38%). The importance of considering liver perfusion while simulating electrical-heating ablation was also highlighted. Implemented on graphics processing units (GPU), our method simulates 1 minute of ablation in 1.14 minutes, allowing near real-time computation.|IEEE Transactions on Medical Imaging|2015|10.1109/TMI.2015.2406575|Chloé Audigier, D. Carnegie, Viorel Mihalef, Tommaso Mansi, M. Choti, E. Boctor, N. Ayache, A. Kamen, S. Rapaka, D. Comaniciu, H. Delingette|4.2|0
2001|In-Hand Object Pose Tracking via Contact Feedback and GPU-Accelerated Robotic Simulation|Tracking the pose of an object while it is being held and manipulated by a robot hand is difficult for vision-based methods due to significant occlusions. Prior works have explored using contact feedback and particle filters to localize in-hand objects. However, they have mostly focused on the static grasp setting and not when the object is in motion, as doing so requires modeling of complex contact dynamics. In this work, we propose using GPU-accelerated parallel robot simulations and derivative-free, sample-based optimizers to track in-hand object poses with contact feedback during manipulation. We use physics simulation as the forward model for robot-object interactions, and the algorithm jointly optimizes for the state and the parameters of the simulations, so they better match with those of the real world. Our method runs in real-time (30Hz) on a single GPU, and it achieves an average point cloud distance error of 6mm in simulation experiments and 13mm in the real-world ones.|IEEE International Conference on Robotics and Automation|2020|10.1109/ICRA40945.2020.9197117|Karl Van Wyk, D. Fox, Viktor Makoviychuk, Ankur Handa, Jacky Liang, Oliver Kroemer|4.2|0
2086|Evaluation of a Rapid Anisotropic Model for ECG Simulation|State-of-the-art cardiac electrophysiology models that are able to deliver physiologically motivated activation maps and electrocardiograms (ECGs) can only be solved on high-performance computing architectures. This makes it nearly impossible to adopt such models in clinical practice. ECG imaging tools typically rely on simplified models, but these neglect the anisotropic electric conductivity of the tissue in the forward problem. Moreover, their results are often confined to the heart-torso interface. We propose a forward model that fully accounts for the anisotropic tissue conductivity and produces the standard 12-lead ECG in a few seconds. The activation sequence is approximated with an eikonal model in the 3d myocardium, while the ECG is computed with the lead-field approach. Both solvers were implemented on graphics processing units and massively parallelized. We studied the numerical convergence and scalability of the approach. We also compared the method to the bidomain model in terms of ECGs and activation maps, using a simplified but physiologically motivated geometry and 6 patient-specific anatomies. The proposed methods provided a good approximation of activation maps and ECGs computed with a bidomain model, in only a few seconds. Both solvers scaled very well to high-end hardware. These methods are suitable for use in ECG imaging methods, and may soon become fast enough for use in interactive simulation tools.|Frontiers in Physiology|2017|10.3389/fphys.2017.00265|Peter Kal'avský, F. Prinzen, M. Potse, Simone Pezzuto, A. Auricchio, R. Krause|4.125|0
2177|A versatile model for soft patchy particles with various patch arrangements.|We propose a simple and general mesoscale soft patchy particle model, which can felicitously describe the deformable and surface-anisotropic characteristics of soft patchy particles. This model can be used in dynamics simulations to investigate the aggregation behavior and mechanism of various types of soft patchy particles with tunable number, size, direction, and geometrical arrangement of the patches. To improve the computational efficiency of this mesoscale model in dynamics simulations, we give the simulation algorithm that fits the compute unified device architecture (CUDA) framework of NVIDIA graphics processing units (GPUs). The validation of the model and the performance of the simulations using GPUs are demonstrated by simulating several benchmark systems of soft patchy particles with 1 to 4 patches in a regular geometrical arrangement. Because of its simplicity and computational efficiency, the soft patchy particle model will provide a powerful tool to investigate the aggregation behavior of soft patchy particles, such as patchy micelles, patchy microgels, and patchy dendrimers, over larger spatial and temporal scales.|Soft Matter|2016|10.1039/c5sm02125a|Youliang Zhu, Zhan-Wei Li, Zhong-yuan Lu, Zhao-Yan Sun|4.111111111111111|0
1973|A new open-source GPU-based microscopic Monte Carlo simulation tool for the calculations of DNA damages caused by ionizing radiation --- Part I: Core algorithm and validation.|PURPOSE\nMonte Carlo (MC) simulation of radiation interactions with water medium at physical, physicochemical, and chemical stages, as well as the computation of biologically relevant quantities such as DNA damages, are of critical importance for the understanding of microscopic basis of radiation effects. Due to the large problem size and many-body simulation problem in the chemical stage, existing CPU-based computational packages encounter the problem of low computational efficiency. This paper reports our development on a GPU-based microscopic Monte Carlo simulation tool gMicroMC using advanced GPU-acceleration techniques.\n\n\nMETHODS\ngMicroMC simulated electron transport in the physical stage using an interaction-by-interaction scheme to calculate the initial events generating radicals in water. After the physicochemical stage, initial positions of all radicals were determined. Simulation of radicals' diffusion and reactions in the chemical stage was achieved using a step-by-step model using GPU-accelerated parallelization together with a GPU-enabled box-sorting algorithm to reduce the computations of searching for interaction pairs and therefore improve efficiency. A multi-scale DNA model of the whole lymphocyte cell nucleus containing ~6.2 Gbp DNA was built.\n\n\nRESULTS\nAccuracy of physics stage simulation was demonstrated by computing stopping power and track length. The results agreed with published data and the data produced by GEANT4-DNA (version 10.3.3) simulations with 10~20% difference in most cases. Difference of yield values of major radiolytic species from GEANT4-DNA results was within 10%. We computed DNA damages caused by monoenergetic 662 keV photons, approximately representing 137 Cs decay. Single-strand break (SSB) and double-strand break (DSB) yields were 196 ± 8 SSB/Gy/Gbp and 7.3 ± 0.7 DSB/Gy/Gbp, respectively, which agreed with the result of 188 SSB/Gy/Gbp and 8.4 DSB/Gy/Gbp computed by Hsiao et al. Comparing to computation using a single CPU, gMicroMC achieved a speedup factor of ~540x using a NVidia TITAN Xp GPU card.\n\n\nCONCLUSION\nThe achieved accuracy and efficiency made gMicroMC can facilitate research on microscopic radiation transport simulation and DNA damage calculation. gMicroMC is an open-source package available to the research community.|Medical Physics (Lancaster)|2020|10.1002/mp.14037|Y. Chi, Shih-Hao Hung, Min-yu Tsai, Z. Tian, N. Qin, Congchong Yan, X. Jia, Y. Lai|4.0|0
2004|Two Decades of 4D-QSAR: A Dying Art or Staging a Comeback?|A key question confronting computational chemists concerns the preferable ligand geometry that fits complementarily into the receptor pocket. Typically, the postulated ‘bioactive’ 3D ligand conformation is constructed as a ‘sophisticated guess’ (unnecessarily geometry-optimized) mirroring the pharmacophore hypothesis—sometimes based on an erroneous prerequisite. Hence, 4D-QSAR scheme and its ‘dialects’ have been practically implemented as higher level of model abstraction that allows the examination of the multiple molecular conformation, orientation and protonation representation, respectively. Nearly a quarter of a century has passed since the eminent work of Hopfinger appeared on the stage; therefore the natural question occurs whether 4D-QSAR approach is still appealing to the scientific community? With no intention to be comprehensive, a review of the current state of art in the field of receptor-independent (RI) and receptor-dependent (RD) 4D-QSAR methodology is provided with a brief examination of the ‘mainstream’ algorithms. In fact, a myriad of 4D-QSAR methods have been implemented and applied practically for a diverse range of molecules. It seems that, 4D-QSAR approach has been experiencing a promising renaissance of interests that might be fuelled by the rising power of the graphics processing unit (GPU) clusters applied to full-atom MD-based simulations of the protein-ligand complexes.|International Journal of Molecular Sciences|2021|10.3390/ijms22105212|A. Bąk|4.0|0
1723|Designing and Evaluating a Mesh Simplification Algorithm for Virtual Reality|With the increasing accessibility of the mobile head-mounted displays (HMDs), mobile virtual reality (VR) systems are finding applications in various areas. However, mobile HMDs are highly constrained with limited graphics processing units (GPUs) and low processing power and onboard memory. Hence, VR developers must be cognizant of the number of polygons contained within their virtual environments to avoid rendering at low frame rates and inducing simulator sickness. The most robust and rapid approach to keeping the overall number of polygons low is to use mesh simplification algorithms to create low-poly versions of pre-existing, high-poly models. Unfortunately, most existing mesh simplification algorithms cannot adequately handle meshes with lots of boundaries or nonmanifold meshes, which are common attributes of many 3D models. In this article, we present QEM4VR, a high-fidelity mesh simplification algorithm specifically designed for VR. This algorithm addresses the deficiencies of prior quadric error metric (QEM) approaches by leveraging the insight that the most relevant boundary edges lie along curvatures while linear boundary edges can be collapsed. Additionally, our algorithm preserves key surface properties, such as normals, texture coordinates, colors, and materials, as it preprocesses 3D models and generates their low-poly approximations offline. We evaluated the effectiveness of our QEM4VR algorithm by comparing its simplified-mesh results to those of prior QEM variations in terms of geometric approximation error, texture error, progressive approximation errors, frame rate impact, and perceptual quality measures. We found that QEM4VR consistently yielded simplified meshes with less geometric approximation error and texture error than the prior QEM variations. It afforded better frame rates than QEM variations with boundary preservation constraints that create unnecessary lower bounds on overall polygon count reduction. Our evaluation revealed that QEM4VR did not fair well in terms of existing perceptual distance measurements, but human-based inspections demonstrate that these algorithmic measurements are not suitable substitutes for actual human perception. In turn, we present a user-based methodology for evaluating the perceptual qualities of mesh simplification algorithms.|ACM Trans. Multim. Comput. Commun. Appl.|2018|10.1145/3209661|K. Bahirat, Chengyuan Lai, Ryan P. McMahan, B. Prabhakaran|3.857142857142857|0
2194|AMBER-DYES in AMBER: Implementation of fluorophore and linker parameters into AmberTools.|"Molecular dynamics (MD) simulations of explicit representations of fluorescent dyes attached via a linker to a protein allow, e.g., probing commonly used approximations for dye localization and/or orientation or modeling Förster resonance energy transfer. However, setting up and performing such MD simulations with the AMBER suite of biomolecular simulation programs has remained challenging due to the unavailability of an easy-to-use set of parameters within AMBER. Here, we adapted the AMBER-DYES parameter set derived by Graen et al. [J. Chem. Theory Comput. 10, 5505 (2014)] into ""AMBER-DYES in AMBER"" to generate a force field applicable within AMBER for commonly used fluorescent dyes and linkers attached to a protein. In particular, the computationally efficient graphics processing unit (GPU) implementation of the AMBER MD engine can now be exploited to overcome sampling issues of dye movements. The implementation is compatible with state-of-the-art force fields such as GAFF, GAFF2, ff99SB, ff14SB, lipid17, and GLYCAM_06j, which allows simulating post-translationally modified proteins and/or protein-ligand complexes and/or proteins in membrane environments. It is applicable with frequently used water models such as TIP3P, TIP4P, TIP4P-Ew, and OPC. For ease of use, a LEaP-based workflow was created, which allows attaching (multiple) dye/linker combinations to a protein prior to further system preparation steps. Following the parameter development described by Graen et al. [J. Chem. Theory Comput. 10, 5505 (2014)] and the adaptation steps described here, AMBER-DYES in AMBER can be extended by additional linkers and fluorescent molecules."|Journal of Chemical Physics|2020|10.1063/5.0007630|Bastian Schepers, H. Gohlke|3.8|0
1715|Risk-Aware Model Predictive Path Integral Control Using Conditional Value-at-Risk|In this paper, we present a novel Model Predictive Control method for autonomous robot planning and control subject to arbitrary forms of uncertainty. The proposed Risk-Aware Model Predictive Path Integral (RA-MPPI) control utilizes the Conditional Value-at-Risk (CVaR) measure to generate optimal control actions for safety-critical robotic applications. Different from most existing Stochastic MPCs and CVaR optimization methods that linearize the original dynamics and formulate control tasks as convex programs, the proposed method directly uses the original dynamics without restricting the form of the cost functions or the noise. We apply the novel RA-MPPI controller to an autonomous vehicle to perform aggressive driving maneuvers in cluttered environments. Our simulations and experiments show that the proposed RA-MPPI controller can achieve similar lap times with the baseline MPPI controller while encountering significantly fewer collisions. The proposed controller performs online computation at an update frequency of up to 80 Hz, utilizing modern Graphics Processing Units (GPUs) to multi-thread the generation of trajectories as well as the CVaR values.|IEEE International Conference on Robotics and Automation|2022|10.1109/ICRA48891.2023.10161100|Zhiyuan Zhang, P. Tsiotras, Ji Yin|3.6666666666666665|0
1768|A Full 3-D GPU-based Beam-Tracing Method for Complex Indoor Environments Propagation Modeling|Ray tracing method is a popular method for predicting radio channel properties in indoor environments. Due to the discrete sampling of the space, the ray tracing method is time-consuming in obtaining accurate results. In this paper, an accurate and efficient GPU-based kD-tree-accelerated beam-tracing method (GKBT), designed to run on GPU in parallel, is proposed, which overcomes the discrete sampling artifacts of ray tracing methods. The proposed method contains both an efficient beam-triangle intersection algorithm based on Pluecker coordinates and a kD-tree traversal algorithm extended to beam tracing to dramatically reduce the computation time. The simulated result shows great agreement with the measured result published in literature. Numerical experiments show that the GKBT is about ten times faster than the state-of-the-art ray tracing method. Finally, a simulated result for a realistic indoor environment is given, which shows that furniture has great influences on the ultrawide band (UWB) field distribution and channel performance.|IEEE Transactions on Antennas and Propagation|2015|10.1109/TAP.2015.2415036|Jundong Tan, Zhuo Su, Y. Long|3.6|0
1678|A Forward-Adjoint Operator Pair Based on the Elastic Wave Equation for Use in Transcranial Photoacoustic Computed Tomography|Photoacoustic computed tomography (PACT) is an emerging imaging modality that exploits optical contrast and ultrasonic detection principles to form images of the photoacoustically induced initial pressure distribution within tissue. The PACT reconstruction problem corresponds to an inverse source problem in which the initial pressure distribution is recovered from measurements of the radiated wavefield. A major challenge in transcranial PACT brain imaging is compensation for aberrations in the measured data due to the presence of the skull. Ultrasonic waves undergo absorption, scattering and longitudinal-to-shear wave mode conversion as they propagate through the skull. To properly account for these effects, a wave-equation-based inversion method should be employed that can model the heterogeneous elastic properties of the skull. In this work, a forward model based on a finite-difference time-domain discretization of the three-dimensional elastic wave equation is established and a procedure for computing the corresponding adjoint of the forward operator is presented. Massively parallel implementations of these operators employing multiple graphics processing units (GPUs) are also developed. The developed numerical framework is validated and investigated in computer19 simulation and experimental phantom studies whose designs are motivated by transcranial PACT applications.|SIAM Journal of Imaging Sciences|2017|10.1137/16M1107619|M. Anastasio, Joemini Poudel, K. Mitsuhashi, Thomas P. Matthews, A. Garcia-Uribe, Lihong V. Wang|3.5|0
2141|Variable Time-Stepping Modular Multilevel Converter Model for Fast and Parallel Transient Simulation of Multiterminal DC Grid|The efficiency of multiterminal dc (MTDC) grid simulation decreases with an expansion of its scale and the inclusion of accurate component models. Thus, the variable time-stepping scheme is proposed in this paper to expedite the electromagnetic transient computation. A number of criteria are proposed to evaluate the time-step and regulate it dynamically during simulation. Meanwhile, as the accuracy of results is heavily reliant on the switch model in the modular multilevel converter, the nonlinear behavioral model with a greater accuracy is proposed in addition to the classic ideal model, and their corresponding variable time-stepping schemes are analyzed. Circuit partitioning is effective in accelerating the MTDC grid simulation via fine-grained separation of nonlinearities. A subsequent large number of identical circuits enabled a massively parallel implementation on the graphics processing unit, which achieved a remarkable speedup over the CPU-based implementation. The inclusion of variable time-stepping schemes eventually makes the simulation of MTDC grid with highly detailed nonlinear switch models feasible. The results are validated by commercial device-level and system-level simulation tools.|IEEE transactions on industrial electronics (1982. Print)|2019|10.1109/TIE.2018.2880671|V. Dinavahi, Ning Lin|3.5|0
2152|Vehicular Networks Simulation With Realistic Physics|Evaluation of cooperative automated driving applications requires the capability of simulating the vehicle and traffic dynamics as well as the communications with a level of accuracy that most of the current tools still lack. In this paper, we explore the use of game engines in hybrid traffic-network simulators. We describe and validate a novel framework based on this approach: Veneris. Our framework is made of a traffic simulator, implemented on the top of the Unity game engine, which includes a realistic vehicle model and a set of driving and lane change behaviors adapted to a 3D environment that reproduces real-world traffic dynamics; a ray-launching propagation simulator on graphics-processing-unit (GPU), called Opal, and a set of modules, which enable bidirectional coupling with the OMNET++ network simulator. The more relevant and novel mechanisms of Veneris are introduced, but further implementation details can be checked on the source code provided in our repository. We discuss the validation tests we have performed and show how it provides accurate results in three key areas: 1) the fidelity of the vehicle dynamics; 2) the recreation of realistic traffic flows, and; 3) the accuracy of the propagation simulation. In addition, the general results of the expected performance are provided.|IEEE Access|2019|10.1109/ACCESS.2019.2908651|E. Egea-López, J. Pascual-García, F. Losilla, J. Molina-García-Pardo|3.5|0
1986|Medial Elastics|We propose a framework for the interactive simulation of nonlinear deformable objects. The primary feature of our system is the seamless integration of deformable simulation and collision culling, which are often independently handled in existing animation systems. The bridge connecting them is the medial axis transform (MAT), a high-fidelity volumetric approximation of complex 3D shapes. From the physics simulation perspective, MAT leads to an expressive and compact reduced nonlinear model. We employ a semireduced projective dynamics formulation, which well captures high-frequency local deformations of high-resolution models while retaining a low computation cost. Our key observation is that the most compelling (nonlinear) deformable effects are enabled by the local constraints projection, which should not be aggressively reduced, and only apply model reduction at the global stage. From the collision detection (CD)/collision culling (CC) perspective, MAT is geometrically versatile using linear-interpolated spheres (i.e., the so-called medial primitives (MPs)) to approximate the boundary of the input model. The intersection test between two MPs is formulated as a quadratically constrained quadratic program problem. We give an algorithm to solve this problem exactly, which returns the deepest penetration between a pair of intersecting MPs. When coupled with spatial hashing, collision (including self-collision) can be efficiently identified on the GPU within a few milliseconds even for massive simulations. We have tested our system on a variety of geometrically complex and high-resolution deformable objects, and our system produces convincing animations with all of the collisions/self-collisions well handled at an interactive rate.|ACM Transactions on Graphics|2020|10.1145/3384515|Xiaohu Guo, Huamin Wang, Y. Yang, M. Fratarcangeli, L. Lan, Ran Luo, Junfeng Yao, Weiwei Xu|3.4|0
2295|Tsunami inundation simulation by GPU computing and its application to tsunami forecast|"Numerical simulation is effective approach together with stable observation for tsunami forecast, however inundation simulation usually requires high computational cost. We are developing a numerical simulation code by the Compute Unified Device Architecture (CUDA) programming to conduct tsunami propagation and inundation simulation using a Graphics Processing Unit (GPU). The simulation code was optimized on TSUBAME3.0 operated by the Tokyo Institute of Technology. It takes two hours to compute 6 hours of tsunami for the tsunami inundation database of the forecast system developed by the NIED (e.g. Aoi et al. 2015JpGU). The computation area of 1600km by 3000km was represented by ununiformed grid applying nesting method and the minimum grid size was 10 m to compute inundation. We estimated the computational cost of the GPU and CPU computing. The GPU computing is approximate 20 times faster than the computing based on CPU. Numerical simulation using GPU computing would be effective to research tsunami forecasting based on numerical modeling with high computing cost, for example, construction of tsunami inundation database and real-time simulation after tsunami generation. Acknowledgements: This work is supported by ""Joint Usage/Research Center for Interdisciplinary Large-scale Information Infrastructures"" in Japan (Project ID: jh170035-NAJ). This work was also supported by Council for Science, Technology and Innovation (CSTI), Cross-ministerial Strategic Innovation Promotion Program (SIP), “Enhancement of societal resiliency against natural disasters” (Funding agency: JST)."||2018|10.1093/gji/ggy345|S. Aoi, N. Yamamoto, W. Suzuki, T. Miyoshi|3.2857142857142856|0
2264|Simulation du transport de neige|Whether it is for synthetic scenes in movies or video games, the realism of environments passes by the addition of visual effects common to our experiences. Snow is part of complex natural phenomena that make their physical and visual simulation all the more difficult to treat. In fact, snow falls in scenes under the action of wind. It piles up and is compressed in some areas, breaks and falls, is repelled by wind once deposited, liquefies or evaporates, freezes, accumulates impurities, etc. The goal of this research is to develop a simulator of snow transport in synthetic spaces. This simulator is designed as an automatic tool to integrate snow in scenes by adding its precipitation, transport by wind, simulation of melting and evaporation, but also to offer some control to artists in order to produce the desired results with easy and understandable settings such as the snow precipitation rate or the wind direction and speed in the scene. We propose a method for the simulation of snow transport that consists of : 1. A precomputation phase to estimate wind and precipitations in a scene. 2. A physical model of snow transport to capture its evolution through time. 3. A method of implementation on GPU. Our results demonstrate the need to simulate the various phenomena on the realism of snow distribution.||2018|10.1002/ajh.25004|A. Jubert|3.142857142857143|0
2196|An efficient GPU implementation for a faster simulation of unsteady bed-load transport|ABSTRACT Computational tools may help engineers in the assessment of sediment transport during the decision-making processes. The main requirements are that the numerical results have to be accurate and simulation models must be fast. The present work is based on the 2D shallow water equations in combination with the 2D Exner equation. The resulting numerical model accuracy was already discussed in previous work. Regarding the speed of the computation, the Exner equation slows down the already costly 2D shallow water model as the number of variables to solve is increased and the numerical stability is more restrictive. In order to reduce the computational effort required for simulating realistic scenarios, the authors have exploited the use of Graphics Processing Units in combination with non-trivial optimization procedures. The gain in computing cost obtained with the graphic hardware is compared against single-core (sequential) and multi-core (parallel) CPU implementations in two unsteady cases.||2016|10.1080/00221686.2016.1143042|J. Murillo, A. Lacasta, P. García-Navarro, C. Juez|3.111111111111111|0
2269|SFU-Driven Transparent Approximation Acceleration on GPUs|Approximate computing, the technique that sacrifices certain amount of accuracy in exchange for substantial performance boost or power reduction, is one of the most promising solutions to enable power control and performance scaling towards exascale. Although most existing approximation designs target the emerging data-intensive applications that are comparatively more error-tolerable, there is still high demand for the acceleration of traditional scientific applications (e.g., weather and nuclear simulation), which often comprise intensive transcendental function calls and are very sensitive to accuracy loss. To address this challenge, we focus on a very important but long ignored approximation unit on today's commercial GPUs --- the special-function unit (SFU), and clarify its unique role in performance acceleration of accuracy-sensitive applications in the context of approximate computing. To better understand its features, we conduct a thorough empirical analysis on three generations of NVIDIA GPU architectures to evaluate all the single-precision and double-precision numeric transcendental functions that can be accelerated by SFUs, in terms of their performance, accuracy and power consumption. Based on the insights from the evaluation, we propose a transparent, tractable and portable design framework for SFU-driven approximate acceleration on GPUs. Our design is software-based and requires no hardware or application modifications. Experimental results on three NVIDIA GPU platforms demonstrate that our proposed framework can provide fine-grained tuning for performance and accuracy trade-offs, thus facilitating applications to achieve the maximum performance under certain accuracy constraints.|International Conference on Supercomputing|2016|10.1145/2925426.2926255|H. Corporaal, S. Song, Akash Kumar, M. Wijtvliet, Ang Li|3.111111111111111|0
2302|Wetbrush: GPU-based 3D painting simulation at the bristle level|We present a real-time painting system that simulates the interactions among brush, paint, and canvas at the bristle level. The key challenge is how to model and simulate sub-pixel paint details, given the limited computational resource in each time step. To achieve this goal, we propose to define paint liquid in a hybrid fashion: the liquid close to the brush is modeled by particles, and the liquid away from the brush is modeled by a density field. Based on this representation, we develop a variety of techniques to ensure the performance and robustness of our simulator under large time steps, including brush and particle simulations in non-inertial frames, a fixed-point method for accelerating Jacobi iterations, and a new Eulerian-Lagrangian approach for simulating detailed liquid effects. The resulting system can realistically simulate not only the motions of brush bristles and paint liquid, but also the liquid transfer processes among different representations. We implement the whole system on GPU by CUDA. Our experiment shows that artists can use the system to draw realistic and vivid digital paintings, by applying the painting techniques that they are familiar with but not offered by many existing systems.|ACM Transactions on Graphics|2015|10.1145/2816795.2818066|Huamin Wang, Byungmoon Kim, Zhili Chen, Daichi Ito|3.1|0
1669|Simulating P Systems on GPU Devices: A Survey|P systems have been proven to be useful as modeling tools in many fields, such as Systems Biology and Ecological Modeling. For such applications, the acceleration of P system simulation is often desired, given the computational needs derived from these kinds of models. One promising solution is to implement the inherent parallelism of P systems on platforms with parallel architectures. In this respect, GPU computing proved to be an alternative to more classic approaches in Parallel Computing. It provides a low cost, and a manycore platform with a high level of parallelism. The GPU has been already employed to speedup the simulation of P systems. In this paper, we look over the available parallel P systems simulators on the GPU, with special emphasis on those included in the PMCGPU project, and analyze some useful guidelines for future implementations and developments.|Fundamenta Informaticae|2015|10.3233/FI-2015-1157|A. Riscos-Núñez, Miguel A. Martínez-del-Amor, M. Pérez-Jiménez, Luis Valencia-Cabrera, Manuel García-Quismondo, Luis F. Macías-Ramos|3.0|0
1806|Identifying and Overcoming the Sampling Challenges in Relative Binding Free Energy Calculations of a Model Protein:Protein Complex.|Relative alchemical binding free energy calculations are routinely used in drug discovery projects to optimize the affinity of small molecules for their drug targets. Alchemical methods can also be used to estimate the impact of amino acid mutations on protein:protein binding affinities, but these calculations can involve sampling challenges due to the complex networks of protein and water interactions frequently present in protein:protein interfaces. We investigate these challenges by extending a graphics processing unit (GPU)-accelerated open-source relative free energy calculation package (Perses) to predict the impact of amino acid mutations on protein:protein binding. Using the well-characterized model system barnase:barstar, we describe analyses for identifying and characterizing sampling problems in protein:protein relative free energy calculations. We find that mutations with sampling problems often involve charge-changes, and inadequate sampling can be attributed to slow degrees of freedom that are mutation-specific. We also explore the accuracy and efficiency of current state-of-the-art approaches─alchemical replica exchange and alchemical replica exchange with solute tempering─for overcoming relevant sampling problems. By employing sufficiently long simulations, we achieve accurate predictions (RMSE 1.61, 95% CI: [1.12, 2.11] kcal/mol), with 86% of estimates within 1 kcal/mol of the experimentally determined relative binding free energies and 100% of predictions correctly classifying the sign of the changes in binding free energies. Ultimately, we provide a model workflow for applying protein mutation free energy calculations to protein:protein complexes, and importantly, catalog the sampling challenges associated with these types of alchemical transformations. Our free open-source package (Perses) is based on OpenMM and is available at https://github.com/choderalab/perses.|Journal of Chemical Theory and Computation|2023|10.1021/acs.jctc.3c00333|J. Chodera, Sukrit Singh, Dominic A. Rufa, Mike Henry, Laura E. Rosen, Iván Pulido, Ivy Zhang, Kevin Hauser|3.0|0
1815|A Performance-Portable Nonhydrostatic Atmospheric Dycore for the Energy Exascale Earth System Model Running at Cloud-Resolving Resolutions.|We present an effort to port the nonhydrostatic atmosphere dynamical core of the Energy Exascale Earth System Model (E3SM) to efficiently run on a variety of architectures, including conventional CPU, many-core CPU, and GPU. We specifically target cloud-resolving resolutions of 3 km and 1 km. To express on-node parallelism we use the C++ library Kokkos, which allows us to achieve a performance portable code in a largely architecture-independent way. Our C++ implementation is at least as fast as the original Fortran implementation on IBM Power9 and Intel Knights Landing processors, proving that the code refactor did not compromise the efficiency on CPU architectures. On the other hand, when using the GPUs, our implementation is able to achieve 0.97 Simulated Years Per Day, running on the full Summit supercomputer. To the best of our knowledge, this is the most achieved to date by any global atmosphere dynamical core running at such resolutions.|International Conference for High Performance Computing, Networking, Storage and Analysis|2020|10.1109/SC41405.2020.00096|J. Foucar, J. Larkin, S. Rajamanickam, O. Guba, A. Bradley, M. Taylor, Luca Bertagna, A. Salinger|3.0|0
1967|CosmoPower-JAX: high-dimensional Bayesian inference with differentiable cosmological emulators|We present CosmoPower-JAX, a JAX-based implementation of the CosmoPower framework, which accelerates cosmological inference by building neural emulators of cosmological power spectra. We show how, using the automatic differentiation, batch evaluation and just-in-time compilation features of JAX, and running the inference pipeline on graphics processing units (GPUs), parameter estimation can be accelerated by orders of magnitude with advanced gradient-based sampling techniques. These can be used to efficiently explore high-dimensional parameter spaces, such as those needed for the analysis of next-generation cosmological surveys. We showcase the accuracy and computational efficiency of CosmoPower-JAX on two simulated Stage IV configurations. We first consider a single survey performing a cosmic shear analysis totalling 37 model parameters. We validate the contours derived with CosmoPower-JAX and a Hamiltonian Monte Carlo sampler against those derived with a nested sampler and without emulators, obtaining a speed-up factor of $\mathcal{O}(10^3)$. We then consider a combination of three Stage IV surveys, each performing a joint cosmic shear and galaxy clustering (3x2pt) analysis, for a total of 157 model parameters. Even with such a high-dimensional parameter space, CosmoPower-JAX provides converged posterior contours in 3 days, as opposed to the estimated 6 years required by standard methods. CosmoPower-JAX is fully written in Python, and we make it publicly available to help the cosmological community meet the accuracy requirements set by next-generation surveys.|The Open Journal of Astrophysics|2023|10.21105/astro.2305.06347|A. Spurio Mancini, Davide Piras|3.0|0
1975|Fully 3D implementation of the end-to-end deep image prior-based PET image reconstruction using block iterative algorithm|Objective. Deep image prior (DIP) has recently attracted attention owing to its unsupervised positron emission tomography (PET) image reconstruction method, which does not require any prior training dataset. In this paper, we present the first attempt to implement an end-to-end DIP-based fully 3D PET image reconstruction method that incorporates a forward-projection model into a loss function. Approach. A practical implementation of a fully 3D PET image reconstruction could not be performed at present because of a graphics processing unit memory limitation. Consequently, we modify the DIP optimization to a block iteration and sequential learning of an ordered sequence of block sinograms. Furthermore, the relative difference penalty (RDP) term is added to the loss function to enhance the quantitative accuracy of the PET image. Main results. We evaluated our proposed method using Monte Carlo simulation with [18F]FDG PET data of a human brain and a preclinical study on monkey-brain [18F]FDG PET data. The proposed method was compared with the maximum-likelihood expectation maximization (EM), maximum a posteriori EM with RDP, and hybrid DIP-based PET reconstruction methods. The simulation results showed that, compared with other algorithms, the proposed method improved the PET image quality by reducing statistical noise and better preserved the contrast of brain structures and inserted tumors. In the preclinical experiment, finer structures and better contrast recovery were obtained with the proposed method. Significance. The results indicated that the proposed method could produce high-quality images without a prior training dataset. Thus, the proposed method could be a key enabling technology for the straightforward and practical implementation of end-to-end DIP-based fully 3D PET image reconstruction.|Physics in Medicine and Biology|2022|10.1088/1361-6560/ace49c|K. Ote, H. Tashima, T. Yamaya, F. Hashimoto, Yuya Onishi|3.0|0
2030|Wavefield Separation Algorithm of Helmholtz Theory-Based Discontinuous Galerkin Method Using Unstructured Meshes on GPU|In the field of geophysics, the Helmholtz decomposition (HD) formula is mainly numerically discretized by the finite difference method (FDM), which limits its application to a uniform regular grid only. Few scholars note wavefield separation on unstructured grids. In this study, we aim to develop a wavefield separation algorithm to separate P- and S-wavefields on nonuniform grids. Our scheme is based on an isotropic elastic wave equation. We first transform the HD formula into a weak integral form using the discontinuous Galerkin method (DGM). Then, we consider two types of unstructured meshes—triangle and quadrangle, which are more suitable for complex structures. Moreover, to reduce time costs, the single graphic processor unit (GPU) device is used to improve the computational efficiency. We perform a unified DGM operator by transforming unstructured triangles and quadrangles into standard reference elements using coordinate transformation. Our proposed HD operator enables us to effectively separate P- and S-wavefields on unstructured meshes. We carry out the wavefield separation simulation and calculate the numerical solutions in the homogeneous carbonate model, Graben model, and SEG/EAGE model. The homogeneous model verifies the correctness, availability, and superiority of our proposed separated operator, and the other numerical results show excellent performance for P/S-wavefield separation on unstructured meshes.|IEEE Transactions on Geoscience and Remote Sensing|2023|10.1109/TGRS.2023.3234915|Jiandong Huang, Xijun He, Dinghui Yang|3.0|0
2087|EMSim: A Fast Layout Level Electromagnetic Emanation Simulation Framework for High Accuracy Pre-Silicon Verification|Electromagnetic (EM) emanation measurement and evaluation is one important testing for modern integrated circuits (ICs). Severe electromagnetic interference may degrade the performance of electronic devices or even cause system crashes. As a result, modern ICs need to follow strict electromagnetic compatibility (EMC) requirements. Moreover, EM emanations offer a covert channel for adversaries to steal secret information from fabricated ICs, causing side channel attacks. Due to the lack of fast and high-accuracy EM simulation tools, existing EM measurements often happen at the post-silicon stage. Any identification of side channel vulnerability or EM incompatibility may lead to high cost and delay the time-to-market. As a result, design-time EM simulation tools with fast simulation speed and high accuracy for pre-silicon designs are urgently needed. To this end, we propose EMSIM, a layout-level EM simulation framework that significantly speeds up the EM simulation process while maintaining high accuracy of the simulated EM emanations. To achieve this goal, we provide the theoretical explanation for the root cause of EM emanations from ICs. Guiding by this, EMSIM leverages techniques of parasitic network reduction and device model approximation to reduce the computation complexities while still ensuring high simulation accuracy. EMSIM further leverages Graphics Processing Unit (GPU) resources to solve equations for EM simulation. The efficiency and effectiveness of EMSIM are validated by showing the consistency between simulation results and physical measurements obtained from fabricated circuit designs.|IEEE Transactions on Information Forensics and Security|2023|10.1109/TIFS.2023.3239184|Yiqiang Zhao, Haocheng Ma, Yier Jin, Max Panoff, Jiaji He|3.0|0
2116|Wideband Modeling of Power SiC mosfet Module and Conducted EMI Prediction of MVDC Railway Electrification System|The SiC mosfet in the medium-voltage direct-current (MVdc) transportation electrification system features faster switching performance, while simultaneously binging more significant electromagnetic interference (EMI) issues within the rolling stocks, substations, and radiated disturbance into space along the catenaries and tracks. Due to the necessity to involve both the transient characteristics of power semiconductor devices and the stray parameters of all the equipment in the analysis of EMI, it is considerably challenging to perform wideband device-level simulation on traditional commercial software for such a complex system with numerous trains and stations. A computationally efficient method for wideband modeling and simulation of the MVdc high-speed railway system for the assessment of conducted EMI during the project design stage is proposed in this article. Physical characteristics of the semiconductor devices, parasitic parameters of the mosfet package, and converter topology are all taken into consideration to provide not only accurate system-level performance of the system but also an insight into high-frequency characteristics under different operation conditions. The calculation burden is alleviated by a hierarchical circuit partitioning architecture based on the frequency-dependent time-domain transmission line model and the Norton equivalent parameter extraction of each mosfet module to split the whole system into several smaller subcircuits in terms of matrix size, and a fully parallel implementation of the MVdc system is carried out on the graphics processor. The developed program is used to study the case of Jing–Zhang high-speed railway system topology, which is compatible to be modified to the MVdc project. Simulation results show that it is essential to estimate the EMI level comprehensively considering the alternative of speed and dc voltage.|IEEE transactions on electromagnetic compatibility (Print)|2020|10.1109/TEMC.2020.2980563|Tian Liang, V. Dinavahi, Ruimin Zhu, Guishu Liang|3.0|0
2228|Realistic Ultrasound Simulation of Complex Surface Models Using Interactive Monte‐Carlo Path Tracing|Ray‐based simulations have been shown to generate impressively realistic ultrasound images in interactive frame rates. Recent efforts used GPU‐based surface raytracing to simulate complex ultrasound interactions such as multiple reflections and refractions. These methods are restricted to perfectly specular reflections (i.e. following only a single reflective/refractive ray), whereas real tissue exhibits roughness of varying degree at tissue interfaces, causing partly diffuse reflections and refractions. Such surface interactions are significantly more complex and can in general not be handled by conventional deterministic raytracing approaches. However, these can be efficiently computed by Monte‐Carlo sampling techniques, where many ray paths are generated with respect to a probability distribution. In this paper, we introduce Monte‐Carlo raytracing for ultrasound simulation. This enables the realistic simulation of ultrasound‐tissue interactions such as soft shadows and fuzzy reflections. We discuss how to properly weight the contribution of each ray path in order to simulate the behaviour of a beamformed ultrasound signal. Tracing many individual rays per transducer element is easily parallelizable on modern GPUs, as opposed to previous approaches based on recursive binary raytracing. We further propose a significant performance optimization based on adaptive sampling.|Computer graphics forum (Print)|2018|10.1111/cgf.13260|O. Mattausch, O. Goksel, Maxim Makhinya|3.0|0
2154|Model Predictive Path Integral Control using Covariance Variable Importance Sampling|In this paper we develop a Model Predictive Path Integral (MPPI) control algorithm based on a generalized importance sampling scheme and perform parallel optimization via sampling using a Graphics Processing Unit (GPU). The proposed generalized importance sampling scheme allows for changes in the drift and diffusion terms of stochastic diffusion processes and plays a significant role in the performance of the model predictive control algorithm. We compare the proposed algorithm in simulation with a model predictive control version of differential dynamic programming.|arXiv.org|2015|10.3390/e17053352|Andrew Aldrich, Evangelos A. Theodorou, Grady Williams|2.9|0
2217|Hierarchical Jacobi Iteration for Structured Matrices on GPUs using Shared Memory|High fidelity scientific simulations modeling physical phenomena typically require solving large linear systems of equations which result from discretization of a partial differential equation (PDE) by some numerical method. This step often takes a vast amount of computational time to complete, and therefore presents a bottleneck in simulation work. Solving these linear systems efficiently requires the use of massively parallel hardware with high computational throughput, as well as the development of algorithms which respect the memory hierarchy of these hardware architectures to achieve high memory bandwidth. \nIn this paper, we present an algorithm to accelerate Jacobi iteration for solving structured problems on graphics processing units (GPUs) using a hierarchical approach in which multiple iterations are performed within on-chip shared memory every cycle. A domain decomposition style procedure is adopted in which the problem domain is partitioned into subdomains whose data is copied to the shared memory of each GPU block. Jacobi iterations are performed internally within each block's shared memory, avoiding the need to perform expensive global memory accesses every step. We test our algorithm on the linear systems arising from discretization of Poisson's equation in 1D and 2D, and observe speedup in convergence using our shared memory approach compared to a traditional Jacobi implementation which only uses global memory on the GPU. We observe a x8 speedup in convergence in the 1D problem and a nearly x6 speedup in the 2D case from the use of shared memory compared to a conventional GPU approach.|arXiv.org|2020|10.1109/tcad.2019.2944790|M. S. Islam, Qiqi Wang|2.8|0
1694|Computational studies of barrier-crossing in polymer field theory|"Author(s): Carilli, Michael Francis | Advisor(s): Fredrickson, Glenn H; Pincus, Philip A | Abstract: This dissertation is primarily a survey of the zero-temperature string method, a minimum energy path search algorithm, applied to novel barrier-crossing problems in polymer field theory. I apply the method to both self-consistent field theory (SCFT) and a phase-field model (the Landau-Brazovskii model). In the case of SCFT, the focus is on defect annealing problems in homo+copolymer melts; in the case of the Landau-Brazovskii model, the focus is on finding critical nuclei for the disorder-to-lamellar transition, which is known to be a fluctuation-induced first-order phase transition.In SCFT, applying the string method is computationally demanding in both processing time and memory, especially for fully 3-dimensional simulations at industrially relevant system sizes. I successfully address these challenges on state-of-the-art massively parallel computing architectures (NVIDIA graphics processing units). As a result our group is able to identify free energy barriers and transition mechanisms for a wide variety of defect annealing problems relevant to industrial directed self-assembly (DSA).Nucleation in the Landau-Brazovskii model presents its own challenges. The string method as originally formulated is inefficient for nucleation problems, since many images are wasted tracing out unphysical configurations once the nucleus grows to the edges of the simulation cell. I devise a new truncation-based energy weighting (TBEW) scheme that resolves this issue, and will prove valuable to future researchers using the string method to find critical nuclei.Since the bare Landau-Brazovskii model predicts a second-order transition between disorder and lamellae at a mean-field level, naive application of the zero-temperature string method to this model fails to find a barrier. To circumvent this, I instead apply the string method to a renormalized model that incorporates fluctuations at a mean-field level. Using TBEW and the renormalized model, I investigate nucleation pathways for the disorder-to-lamellar transition, finding anisotropic nuclei in agreement with previous predictions and experimental observations. I also conduct a comprehensive search for experimentally observed nuclei containing various exotic defect structures.Finally, I evaluate the validity of the nucleation pathways obtained from the renormalized model by numerically simulating the bare model with explicit fluctuations. I find that the renormalized model makes good predictions for certain quantities, including the location of the order-disorder transition. However, due to sharp dependence of critical nucleus size on proximity to the order-disorder transition, even slight errors in the predicted ODT lead to large errors in predicted nucleus size. I conclude that the renormalized Landau-Brazovskii model is a poor tool for predicting critical nuclei in the fully fluctuating bare theory at experimentally accessible parameters, and recommend that future studies work with the fluctuating bare theory directly. I recommend several strategies to extract barriers and rates."||2016|10.3847/1538-4357/833/1/73|M. Carilli|2.7777777777777777|0
2135|The current state of the methods for calculating global illumination in tasks of realistic computer graphics|Modern realistic computer graphics are based on light transport simulation. In this case, one of the main and difficult to calculate tasks is to calculate the global illumination, i.e. distribution of light in a virtual scene, taking into account multiple reflections and scattering of light and all kinds of its interaction with objects in the scene. Hundreds of publications and describing dozens of methods are devoted to this problem. In this state-of-the-art review, we would like not only to list and briefly describe these methods, but also to give some “map” of existing works, which will allow the reader to navigate, understand their advantages and disadvantages, and, thereby, choose a right method for themselves. Particular attention is paid to such characteristics of the methods as robustness and universality in relation to the used mathematical models, the transparency of the method verification, the possibility of efficient implementation on the GPU, as well as restrictions imposed on the scene or illumination phenomena. In contrast to the existing survey papers, not only the efficiency of the methods is analyzed, but also their limitations and the complexity of software implementation. In addition, we provide the results of our own numerical experiments with various methods that serve as illustrations for the conclusions.||2021|10.15514/ISPRAS-2021-33(2)-1|V. Galaktionov, V. Frolov, A. Voloboy, S. Ershov|2.75|0
2113|Large-Scale Nonlinear Device-Level Power Electronic Circuit Simulation on Massively Parallel Graphics Processing Architectures|Device-level power electronic circuit simulation is so cumbersome that engineers are forced to make model simplification or reduce circuit size to obtain a reasonable execution time for repeated simulation runs. This paper proposes a massive-thread parallel simulation of large-scale power electronic circuits employing device-level modeling on the graphics processors (GPUs) to obtain higher data throughput and lower execution times. Parallel massive-thread modules are proposed for the nonlinear physics-based insulated gate bipolar transistor and power diode components. The nonlinear solution algorithm comprised of Newton–Raphson iterations and partial LU decomposition is fully parallelized on the GPU. Furthermore, the commonly used behavioral model with reduced computational complexity is also employed to represent the switches. The developed simulation codes are used to run large-scale test cases of the modular multilevel converter (MMC) system. The accuracy and efficiency of the GPU-based parallel simulation are compared with sequential CPU-based codes and the SaberRD program to show the advantages of the parallelized simulation; execution time speedups of 15 times and 70 times are reported for the MMC system using the nonlinear physics-based modeling and behavior-based modeling, respectively.|IEEE transactions on power electronics|2018|10.1109/TPEL.2017.2725239|V. Dinavahi, Zhiyin Zhou, Shenhao Yan|2.7142857142857144|0
2231|Time-Domain Power Quality State Estimation Based on Kalman Filter Using Parallel Computing on Graphics Processing Units|A methodology is developed to assess the time-domain power quality state estimation (PQSE) in electrical systems based on the Kalman filter implemented using parallel processing techniques through graphics processing units (GPUs) to reduce the execution time. The measurements used by the state estimation algorithm are taken from the simulation and transient propagation response of the power network. The parallel Kalman filter (PKF) state estimation obtains the waveforms for busbar voltages and line currents with several sources of time-varying electromagnetic transients. The PKF is evaluated using the compute unified device architecture (CUDA) platform and the CUDA basic linear algebra subprograms library, the parallel filter is executed on GPU cards. Case studies are applied to solve the time-domain state estimation using the proposed PKF-PQSE method, obtaining an execution time reduction and including time-varying harmonics, short circuit faults, and load transient conditions. The speed-up depends on the number of state variables modeling the electrical system under analysis. The PKF-PQSE results are successfully compared and validated against the power systems computer aided design/electromagnetic transients including direct current simulator.|IEEE Access|2018|10.1109/ACCESS.2018.2823721|V. Dinavahi, A. Ramos-Paz, A. Medina, R. Cisneros-Magaña|2.7142857142857144|0
1850|Aerophones in flatland|We present the first real-time technique to synthesize full-bandwidth sounds for 2D virtual wind instruments. A novel interactive wave solver is proposed that synthesizes audio at 128,000Hz on commodity graphics cards. Simulating the wave equation captures the resonant and radiative properties of the instrument body automatically. We show that a variety of existing non-linear excitation mechanisms such as reed or lips can be successfully coupled to the instrument's 2D wave field. Virtual musical performances can be created by mapping user inputs to control geometric features of the instrument body, such as tone holes, and modifying parameters of the excitation model, such as blowing pressure. Field visualizations are also produced. Our technique promotes experimentation by providing instant audio-visual feedback from interactive virtual designs. To allow artifact-free audio despite dynamic geometric modification, we present a novel time-varying Perfectly Matched Layer formulation that yields smooth, natural-sounding transitions between notes. We find that visco-thermal wall losses are crucial for musical sound in 2D simulations and propose a practical approximation. Weak non-linearity at high amplitudes is incorporated to improve the sound quality of brass instruments.|ACM Transactions on Graphics|2015|10.1145/2767001|Andrew Allen, N. Raghuvanshi|2.7|0
2195|Modeling, Rendering, and Simulating Knits|Knitting is one of the most popular techniques to manufacture clothing. In this dissertation we present a framework for modeling, fabricating, and simulating knit structures. We first introduce a fully automatic pipeline to convert arbitrary 3D shapes into a stitch mesh, which provides a mesh-based representation of the yarn-level knit structure. We also extend the concept of stitch mesh modeling with a list of novel shaping techniques to allow modeling more complex structures. We then introduce a scheduling algorithm to convert the structure into step-by-step hand knitting instructions for knitters. We further embed low-level machine knitting operations to each face and allow users to edit the stitch mesh in a way that preserves the machine knittability. Moreover, we present a real-time rendering method to display knitwear containing more than a hundred million individual fiber curves at real-time frame rates with shadows and ambient occlusion on the GPU. We also present a GPU parallelization method to address the computational challenges of using the Material Point Method to simulate knits. For my wife, Zhen Li.||2019|10.1109/tvcg.2017.2731949|Kui Wu|2.6666666666666665|0
1929|GPU-Accelerated Simulation of Small Delay Faults|Delay fault simulation is an essential task during test pattern generation and reliability assessment of electronic circuits. With the high sensitivity of current nano-scale designs toward even smallest delay deviations, the simulation of small gate delay faults has become extremely important. Since these faults have a subtle impact on the timing behavior, traditional fault simulation approaches based on abstract timing models are not sufficient. Furthermore, the detection of these faults is compromised by the ubiquitous variations in the manufacturing processes, which causes the actual fault coverage to vary from circuit instance to circuit instance, and makes the use of timing accurate methods mandatory. However, the application of timing accurate techniques quickly becomes infeasible for larger designs due to excessive computational requirements. In this paper, we present a method for fast and waveform-accurate simulation of small delay faults on graphics processing units with exceptional computational performance. By exploiting multiple dimensions of parallelism from gates, faults, waveforms, and circuit instances, the proposed approach allows for timing-accurate and exhaustive small delay fault simulation under process variation for designs with millions of gates.|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|2017|10.1109/TCAD.2016.2598560|M. Kochte, X. Wen, S. Holst, H. Wunderlich, E. Schneider|2.625|0
2048|Initial development of goCMC: a GPU-oriented fast cross-platform Monte Carlo engine for carbon ion therapy|Monte Carlo (MC) simulation is considered as the most accurate method for calculation of absorbed dose and fundamental physics quantities related to biological effects in carbon ion therapy. To improve its computational efficiency, we have developed a GPU-oriented fast MC package named goCMC, for carbon therapy. goCMC simulates particle transport in voxelized geometry with kinetic energy up to 450 MeV u−1. Class II condensed history simulation scheme with a continuous slowing down approximation was employed. Energy straggling and multiple scattering were modeled. δ-electrons were terminated with their energy locally deposited. Four types of nuclear interactions were implemented in goCMC, i.e. carbon–hydrogen, carbon–carbon, carbon–oxygen and carbon–calcium inelastic collisions. Total cross section data from Geant4 were used. Secondary particles produced in these interactions were sampled according to particle yield with energy and directional distribution data derived from Geant4 simulation results. Secondary charged particles were transported following the condensed history scheme, whereas secondary neutral particles were ignored. goCMC was developed under OpenCL framework and is executable on different platforms, e.g. GPU and multi-core CPU. We have validated goCMC with Geant4 in cases with different beam energy and phantoms including four homogeneous phantoms, one heterogeneous half-slab phantom, and one patient case. For each case 3×107 carbon ions were simulated, such that in the region with dose greater than 10% of maximum dose, the mean relative statistical uncertainty was less than 1%. Good agreements for dose distributions and range estimations between goCMC and Geant4 were observed. 3D gamma passing rates with 1%/1 mm criterion were over 90% within 10% isodose line except in two extreme cases, and those with 2%/1 mm criterion were all over 96%. Efficiency and code portability were tested with different GPUs and CPUs. Depending on the beam energy and voxel size, the computation time to simulate 107 carbons was 9.9–125 s, 2.5–50 s and 60–612 s on an AMD Radeon GPU card, an NVidia GeForce GTX 1080 GPU card and an Intel Xeon E5-2640 CPU, respectively. The combined accuracy, efficiency and portability make goCMC attractive for research and clinical applications in carbon ion therapy.|Physics in Medicine and Biology|2017|10.1088/1361-6560/aa5d43|N. Qin, K. Parodi, X. Jia, A. Pompoš, Steve B. Jiang, G. Dedes, M. Pinto, Z. Tian|2.625|0
1902|Parallel Computation of Trajectories Using Graphics Processing Units and Interpolated Gravity Models|Seeking improvements in speed and accuracy in multiobject trajectory simulations, a solution methodology is presented that takes advantage of 1) new high-fidelity geopotential and third-body perturbation models that efficiently trade memory for speed, and 2) a graphics processing unit based integrator to achieve parallelism across multiple objects. The two methods combined lead to multiplicative speedups, making the tool three orders of magnitude faster, in some cases, compared to the same simulation performed in serial on a single central processing unit. The tool is capable of Monte Carlo simulations of a single object or of propagating the mean and covariance of all the objects in a space catalog. The tool performance is demonstrated for 1) a five-day Monte Carlo simulation of the state uncertainty point cloud modeled with over one million objects, and 2) a seven-day simulation of position and velocity states of a full space catalog with over 250,000 objects. The simulations required approximately 1 an...||2015|10.2514/1.G000571|R. Russell, Nitin Arora, V. Vittaldev|2.6|0
2314|A Universal Wideband Device-Level Parallel Simulation Method and Conducted EMI Analysis for More Electric Aircraft Microgrid|The validation of electromagnetic compatibility for the microgrid of a more electric aircraft (MEA) is an essential test item before delivery for a trial flight, and it has always been urgently expected to be involved during the design stage. This article presents a universal method for wideband modeling and simulation of the MEA microgrid system in the time domain, regardless of the fact that motors are driven by which kind of converter, e.g., modular multilevel converter (MMC), 3-L neutral-point clamped (NPC), or 2-L pulsewidth modulation (PWM) converters. The insulated gate bipolar transistor and diodes are modeled with the physics-based dynamic model to emulate not only precise system-level performance of the system, but also to get an insight into the high-frequency oscillation between junction capacitance of the semiconductor modules and the parasitic parameters and high-frequency branch of other components, such as the permanent magnet synchronous motor (PMSM), transformer, and generator. To alleviate the attendant computational challenge, which could be extremely time-consuming (if no nonconvergence problem is encountered) when solved on traditional simulation platform, circuit partition based on transmission line decoupling, Norton equivalent parameter extraction, and TLM-link decoupling of submodules from the MMC bridge arms are utilized. The simulation program is executed on GPU to achieve massively parallel and accelerated solution. The accuracy and efficiency of the GPU-based parallel algorithm are validated by the comparison with the experimentally verified model in ANSYS Simplorer.|IEEE Journal of Emerging and Selected Topics in Industrial Electronics|2020|10.1109/JESTIE.2020.3003317|Zhen Huang, V. Dinavahi, Ruimin Zhu|2.6|0
1680|How much past to see the future: a computational study in calibrating urban cellular automata|The objective of this computational study was to investigate to which extent the availability and the way of use of historical maps may affect the quality of the calibration process of cellular automata (CA) urban models. The numerical experiments are based on a constrained CA applied to a case study. Since the model depends on a large number of parameters, we optimize the CA using cooperative coevolutionary particle swarms, which is an approach known for its ability to operate effectively in search spaces with a high number of dimensions. To cope with the relevant computational cost related to the high number of CA simulations required by our study, we use a parallelized CA model that takes advantage of the computing power of graphics processing units. The study has shown that the accuracy of simulations can be significantly influenced by both the number and position in time of the historical maps involved in the calibration.|International Journal of Geographical Information Science|2015|10.1080/13658816.2014.970190|G. Trunfio, I. Blecic, A. Cecchini|2.5|0
1864|GPU‐specific algorithms for improved solute sampling in grand canonical Monte Carlo simulations|The Grand Canonical Monte Carlo (GCMC) ensemble defined by the excess chemical potential, μex, volume, and temperature, in the context of molecular simulations allows for variations in the number of particles in the system. In practice, GCMC simulations have been widely applied for the sampling of rare gasses and water, but limited in the context of larger molecules. To overcome this limitation, the oscillating μex GCMC method was introduced and shown to be of utility for sampling small solutes, such as formamide, propane, and benzene, as well as for ionic species such as monocations, acetate, and methylammonium. However, the acceptance of GCMC insertions is low, and the method is computationally demanding. In the present study, we improved the sampling efficiency of the GCMC method using known cavity‐bias and configurational‐bias algorithms in the context of GPU architecture. Specifically, for GCMC simulations of aqueous solution systems, the configurational‐bias algorithm was extended by applying system partitioning in conjunction with a random interval extraction algorithm, thereby improving the efficiency in a highly parallel computing environment. The method is parallelized on the GPU using CUDA and OpenCL, allowing for the code to run on both Nvidia and AMD GPUs, respectively. Notably, the method is particularly well suited for GPU computing as the large number of threads allows for simultaneous sampling of a large number of configurations during insertion attempts without additional computational overhead. In addition, the partitioning scheme allows for simultaneous insertion attempts for large systems, offering considerable efficiency. Calculations on the BK Channel, a transporter, including a lipid bilayer with over 760,000 atoms, show a speed up of ~53‐fold through the use of system partitioning. The improved algorithm is then combined with an enhanced μex oscillation protocol and shown to be of utility in the context of the site‐identification by ligand competitive saturation (SILCS) co‐solvent sampling approach as illustrated through application to the protein CDK2.|Journal of Computational Chemistry|2023|10.1002/jcc.27121|Sunhwan Jo, Mingtian Zhao, Anthony Hazel, A. Kognole, A. MacKerell, Aoxiang Tao|2.5|0
2024|The Problem of Effective Evacuation of the Population from Floodplains under Threat of Flooding: Algorithmic and Software Support with Shortage of Resources|Extreme flooding of the floodplains of large lowland rivers poses a danger to the population due to the vastness of the flooded areas. This requires the organization of safe evacuation in conditions of a shortage of temporary and transport resources due to significant differences in the moments of flooding of different spatial parts. We consider the case of a shortage of evacuation vehicles, in which the safe evacuation of the entire population to permanent evacuation points is impossible. Therefore, the evacuation is divided into two stages with the organization of temporary evacuation points on evacuation routes. Our goal is to develop a method for analyzing the minimum resource requirement for the safe evacuation of the population of floodplain territories based on a mathematical model of flood dynamics and minimizing the number of vehicles on a set of safe evacuation schedules. The core of the approach is a numerical hydrodynamic model in shallow water approximation. Modeling the hydrological regime of a real water body requires a multi-layer geoinformation model of the territory with layers of relief, channel structure, and social infrastructure. High-performance computing is performed on GPUs using CUDA. The optimization problem is a variant of the resource investment problem of scheduling theory with deadlines for completing work and is solved on the basis of a heuristic algorithm. We use the results of numerical simulation of floods for the Northern part of the Volga-Akhtuba floodplain to plot the dependence of the minimum number of vehicles that ensure the safe evacuation of the population. The minimum transport resources depend on the water discharge in the Volga river, the start of the evacuation, and the localization of temporary evacuation points. The developed algorithm constructs a set of safe evacuation schedules for the minimum allowable number of vehicles in various flood scenarios. The population evacuation schedules constructed for the Volga-Akhtuba floodplain can be used in practice for various vast river valleys.|De Computis|2023|10.3390/computation11080150|O. Vatyukova, A. Voronin, M. Kharitonov, A. Khoperskov, Anna Vasilchenko, A. Klikunova|2.5|0
2208|Sub-second pencil beam dose calculation on GPU for adaptive proton therapy|Although proton therapy delivered using scanned pencil beams has the potential to produce better dose conformity than conventional radiotherapy, the created dose distributions are more sensitive to anatomical changes and patient motion. Therefore, the introduction of adaptive treatment techniques where the dose can be monitored as it is being delivered is highly desirable. We present a GPU-based dose calculation engine relying on the widely used pencil beam algorithm, developed for on-line dose calculation. The calculation engine was implemented from scratch, with each step of the algorithm parallelized and adapted to run efficiently on the GPU architecture. To ensure fast calculation, it employs several application-specific modifications and simplifications, and a fast scatter-based implementation of the computationally expensive kernel superposition step. The calculation time for a skull base treatment plan using two beam directions was 0.22 s on an Nvidia Tesla K40 GPU, whereas a test case of a cubic target in water from the literature took 0.14 s to calculate. The accuracy of the patient dose distributions was assessed by calculating the γ-index with respect to a gold standard Monte Carlo simulation. The passing rates were 99.2% and 96.7%, respectively, for the 3%/3 mm and 2%/2 mm criteria, matching those produced by a clinical treatment planning system.|Physics in Medicine and Biology|2015|10.1088/0031-9155/60/12/4777|R. Jena, J. da Silva, R. Ansorge|2.5|0
2275|Large-eddy simulations with ClimateMachine v0.2.0: a new open-source code for atmospheric simulations on GPUs and CPUs|Abstract. We introduce ClimateMachine, a new open-source atmosphere modeling framework using the Julia language to be performance portable on central processing units (CPUs) and graphics processing units (GPUs). ClimateMachine uses a common framework both for coarser-resolution global simulations and for high-resolution, limited-area large-eddy simulations (LES). Here, we demonstrate the LES configuration of the atmosphere model in canonical benchmark cases and atmospheric flows, using an energy-conserving nodal discontinuous-Galerkin (DG) discretization of the governing equations. Resolution dependence, conservation characteristics and scaling metrics are examined in comparison with existing LES codes. They demonstrate the utility of ClimateMachine as a modelling tool for limited-area LES flow configurations.\n|Geoscientific Model Development|2021|10.5194/gmd-15-6259-2022|Zhaoyi Shen, Charles Kawczynski, T. Schneider, Simon Byrne, K. Pamnany, F. Giraldo, Valentin Churavy, L. Wilcox, Maciej Waruszewski, J. Kozdon, Thomas H. Gibson, Yassine Tissaoui, S. Marras, A. Sridhar|2.5|0
1925|CAVE-VR and Unity Game Engine for Visualizing City Scale 3D Meshes|Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.|Consumer Communications and Networking Conference|2022|10.1109/CCNC49033.2022.9700515|Emily Lattanzio, Jaired Collins, Shizeng Yao, Bimal Balakrishnan, Joshua Fraser, P. Calyam, Calvin Davis, K. Palaniappan, Ye Duan, Haoxiang Zhang|2.3333333333333335|0
2079|Open Structural Data in Precision Medicine.|Three-dimensional protein structural data at the molecular level are pivotal for successful precision medicine. Such data are crucial not only for discovering drugs that act to block the active site of the target mutant protein but also for clarifying to the patient and the clinician how the mutations harbored by the patient work. The relative paucity of structural data reflects their cost, challenges in their interpretation, and lack of clinical guidelines for their utilization. Rapid technological advancements in experimental high-resolution structural determination increasingly generate structures. Computationally, modeling algorithms, including molecular dynamics simulations, are becoming more powerful, as are compute-intensive hardware, particularly graphics processing units, overlapping with the inception of the exascale era. Accessible, freely available, and detailed structural and dynamical data can be merged with big data to powerfully transform personalized pharmacology. Here we review protein and emerging genome high-resolution data, along with means, applications, and examples underscoring their usefulness in precision medicine. Expected final online publication date for the Annual Review of Biomedical Data Science, Volume 5 is August 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.|Annual Review of Biomedical Data Science|2022|10.1146/annurev-biodatasci-122220-012951|Hyunbum Jang, R. Nussinov, F. Cheng, Chung-Jung Tsai, G. Nir|2.3333333333333335|0
2164|Hare: Exploiting Inter-job and Intra-job Parallelism of Distributed Machine Learning on Heterogeneous GPUs|Distributed machine learning (DML) has shown great promise in accelerating model training on multiple GPUs. To increase GPU utilization, a common practice is to let multiple learning jobs share GPU clusters, where the most fundamental and critical challenge is how to efficiently schedule these jobs on GPUs. However, existing works about DML job scheduling are constrained to settings with homogeneous GPUs. GPU heterogeneity is common in practice, but its influence on multiple DML job scheduling has been seldom studied. Moreover, DML jobs have internal structures that contain great parallelism potentials, which have not yet been fully exploited in the heterogeneous computing environment. In this paper, we propose Hare, a DML job scheduler that exploits both inter-job and intra-job parallelism in a heterogeneous GPU cluster. Hare has three novel designs. First, Hare optimizes GPU execution environment to reduce task switching overhead by exploiting unique features of DML scheduling. Second, Hare adopts a relaxed fixed-scale synchronization scheme that allows independent tasks to be flexibly scheduled within a training round. Finally, we propose a fast heuristic algorithm to minimize the total weighted job completion time by jointly considering job features and hardware heterogeneity. Its theoretical bound is derived. We evaluate Hare using a small-scale testbed and a trace-driven simulator. The results show that it can outperform the state-of-the-art by about 2x.|IEEE International Symposium on High-Performance Parallel Distributed Computing|2022|10.1145/3502181.3531462|Celimuge Wu, Song Guo, Fahao Chen, Peng Li|2.3333333333333335|0
2226|Batched sparse iterative solvers on GPU for the collision operator for fusion plasma simulations|Batched linear solvers, which solve many small related but independent problems, are important in several applications. This is increasingly the case for highly parallel processors such as graphics processing units (GPUs), which need a substantial amount of work to keep them operating efficiently and solving smaller problems one-by-one is not an option. Because of the small size of each problem, the task of coming up with a parallel partitioning scheme and mapping the problem to hardware is not trivial. In recent history, significant attention has been given to batched dense linear algebra. However, there is also an interest in utilizing sparse iterative solvers in a batched form, and this presents further challenges. An example use case is found in a gyrokinetic Particle-In-Cell (PIC) code used for modeling magnetically confined fusion plasma devices. The collision operator has been identified as a bottleneck, and a proxy app has been created for facilitating optimizations and porting to GPUs. The current collision kernel linear solver does not run on the GPU-a major bottleneck. As these matrices are well-conditioned, batched iterative sparse solvers are an attractive option. A batched sparse iterative solver capability has recently been developed in the Ginkgo library. In this paper, we describe how the software architecture can be used to develop an efficient solution for the XGC collision proxy app. Comparisons for the solve times on NVIDIA V100 and A100 GPUs and AMD MI100 GPUs with one dual-socket Intel Xeon Skylake CPU node with 40 OpenMP threads are presented for matrices representative of those required in the collision kernel of XGC. The results suggest that GINKGO's batched sparse iterative solvers are well suited for efficient utilization of the GPU for this problem, and the performance portability of Ginkgo in conjunction with Kokkos (used within XGC as the heterogeneous programming model) allows seamless execution for exascale oriented heterogeneous architectures at the various leadership supercomputing facilities.|IEEE International Parallel and Distributed Processing Symposium|2022|10.1109/ipdps53621.2022.00024|Pratik Nayak, A. Scheinberg, Aditya Kashi, H. Anzt, Paul Lin, Dhruva Kulkarni|2.3333333333333335|0
1926|Splay Thread Cooperation for Load Balancing in Speculative Parallelism and Gpgpu|Abstract- The introduction of the speculative parallelism into any models can improve the performance and provides significant benefits and increases the ILP and TLP. GPGPU is the future computing technology working with both CPU and GPU to solve many real-world problems not only the graphics problems but also the general purpose applications. In this paper a new mechanism or technique is proposed for workload balancing on the Graphics processors and CPU that can be implemented on the graphics processors along with the CPU. As GPU uses   data parallelism tasks, the dynamic memory creation and the splay trees which are self adjusting allows for the increase in throughput and load balancing. The architecture of splay trees is such that   frequently accessed nodes will come   nearer to the root Where they can be accessed were quickly. The frequently used nodes near to the root are an advantage for finding locality of threads as well as for caching and garbage collection. This technique is implemented in this paper for   balancing the load of speculative threads on the GPUs. A GPUOCELOT is a compilation framework, a simulator used for the execution of the programs which has resulted in the increase in the performance of the instructions which uses the amortized cost.||2015|10.1002/hipo.22497|N. Gopalan, S. Suma|2.3|0
2038|GGEMS-Brachy: GPU GEant4-based Monte Carlo simulation for brachytherapy applications|In brachytherapy, plans are routinely calculated using the AAPM TG43 formalism which considers the patient as a simple water object. An accurate modeling of the physical processes considering patient heterogeneity using Monte Carlo simulation (MCS) methods is currently too time-consuming and computationally demanding to be routinely used. In this work we implemented and evaluated an accurate and fast MCS on Graphics Processing Units (GPU) for brachytherapy low dose rate (LDR) applications. A previously proposed Geant4 based MCS framework implemented on GPU (GGEMS) was extended to include a hybrid GPU navigator, allowing navigation within voxelized patient specific images and analytically modeled 125I seeds used in LDR brachytherapy. In addition, dose scoring based on track length estimator including uncertainty calculations was incorporated. The implemented GGEMS-brachy platform was validated using a comparison with Geant4 simulations and reference datasets. Finally, a comparative dosimetry study based on the current clinical standard (TG43) and the proposed platform was performed on twelve prostate cancer patients undergoing LDR brachytherapy. Considering patient 3D CT volumes of 400  × 250  × 65 voxels and an average of 58 implanted seeds, the mean patient dosimetry study run time for a 2% dose uncertainty was 9.35 s (≈500 ms 10−6 simulated particles) and 2.5 s when using one and four GPUs, respectively. The performance of the proposed GGEMS-brachy platform allows envisaging the use of Monte Carlo simulation based dosimetry studies in brachytherapy compatible with clinical practice. Although the proposed platform was evaluated for prostate cancer, it is equally applicable to other LDR brachytherapy clinical applications. Future extensions will allow its application in high dose rate brachytherapy applications.|Physics in Medicine and Biology|2015|10.1088/0031-9155/60/13/4987|Marie-Paule Garcia, O. Pradier, J. Bert, P. Després, Y. Lemaréchal, D. Visvikis, Claire Falconnet, A. Valéri, N. Boussion, U. Schick|2.2|0
2181|GPU implementation of curved-grid finite-difference modelling for non-planar rupture dynamics|A deep understanding of earthquake physics requires a large amount of numerical simulations on seismic wave propagation and dynamic rupture. However, the corresponding intensive computational expense of simulations at traditional CPU (central processing unit) platforms make related researches time-consuming. There are many mature graphics processing unit (GPU) programs that can dramatically accelerate the calculation of seismic wave propagation. Unfortunately, there are few discussions about GPU implementations for rupture dynamics. In this work, we extend our 3-D curved-grid finite-difference method (CG-FDM) for rupture dynamics to the GPU platform using the CUDA (compute unified device architecture) programming language. By taking advantage of the new features of the NVIDIA Volta architecture, we implement the GPU-based program for rupture dynamics that is not only efficient but also easy to maintain. The GPU-based CG-FDM program is two orders of magnitude faster than our previous serial CPU-based program and still has a considerable advantage compared with the parallel one. The reliability and correctness of the program are carefully examined by the comparisons of the benchmarks from the ‘Southern California Earthquake Center/U.S. Geological Survey (SCEC/USGS) Dynamic Earthquake Rupture Code Verification Exercise’. The performance improvements of the GPU-based CG-FDM can save a lot of computing time, allowing researchers to perform much more numerical simulations of rupture dynamics to reveal more details of earthquake physics.||2020|10.1093/gji/ggaa290|Xiaofei Chen, Wenqiang Zhang, Zhenguo Zhang, Mengyang Li|2.2|0
2237|A Work-Time Optimal Parallel Exhaustive Search Algorithm for the QUBO and the Ising model, with GPU implementation|The main contribution of this paper is to present a simple exhaustive search algorithm for the quadratic un-constraint binary optimization (QUBO) problem. It computes the values of the objective function $E(X)$ for all n-bit input vector X in $O(2^{n})$ time. Since $\Omega(2^{n})$ time is necessary to output $E(X)$ for all 2n vectors X, this sequential algorithm is optimal. We also present a work-time optimal parallel algorithm running $O(\log n)$ time using $2^{n}/\log n$ processors on the CREW-PRAM. This parallel algorithm is work optimal, because the total number of computational operations is equal to the running time of the optimal sequential algorithm. Also, it is time optimal because any parallel algorithm using any large number of processors takes at least $\Omega(\log n)$ time for evaluating E(X). Further, we have implemented this parallel algorithm to run on the GPU. The experimental results on NVIDIA GeForce RTX 2080Ti GPU show that our GPU implementation runs more than 1000 times faster than the sequential algorithm running on Intel Corei7-8700K CPU(3.70GHz) for the QUBO with n-bit vector whenever n$\geq$33. We also compare our exhaustive search parallel algorithm with several non-exhaustive search approaches for solving the QUBO including D-Wave 2000Q quantum annealer, simulated annealing algorithm, and Gurobi optimizer.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2020|10.1109/IPDPSW50202.2020.00098|Ryota Yasudo, Masaru Tatekawa, Masaki Tao, Ryota Katsuki, Takashi Yazane, Yasuaki Ito, K. Nakano, Yoko Inaba|2.2|0
1796|Photon mapping in image-based visual comfort assessments with BSDF models of high resolution|Abstract Data-driven models replicate the irregular Bidirectional Scattering Distribution Functions (BSDFs) of optically Complex Fenestration Systems in daylight simulation. RADIANCE employs the tensor tree to store the BSDF at high directional resolution. Its application in backward ray-tracing is however challenging, since the density of stochastic samples must match the model resolution. BSDF proxy and peak extraction address this problem, but are limited to cases when either the fenestration geometry, or the shape and direction of the transmission peak are known. Photon Mapping is proposed to efficiently sample arbitrary BSDFs from the known sun direction. The existing implementation in RADIANCE is extended to account for light sources and their reflections in the field of view, that are of particular importance for visual comfort assessments. The method achieves a high degree of accordance with ray-tracing, and reduces simulation times by ≈95% with data-driven models of high resolution. Abbreviations: BRT: Backward Ray-Tracing; BSDF: Bidirectional Scattering Distribution Function; CBDM: Climate-Based Daylight Modelling; CFS: Complex Fenestration System; DGI: Daylight Glare Index; DGP: Daylight Glare Probability; FPM: Five Phase Method; GPGPU: General Purpose Graphics Processing Unit; LCP: Laser Cut Panel; OoC: Out-of-Core; PM: Photon Mapping; RMSE: Root Mean Squared Error|Journal of Building Performance Simulation, Taylor & Francis|2019|10.1080/19401493.2019.1653994|L. O. Grobe|2.1666666666666665|0
1781|Portable multi-node LQCD Monte Carlo simulations using OpenACC|This paper describes a state-of-the-art parallel Lattice QCD Monte Carlo code for staggered fermions, purposely designed to be portable across different computer architectures, including GPUs and commodity CPUs. Portability is achieved using the OpenACC parallel programming model, used to develop a code that can be compiled for several processor architectures. The paper focuses on parallelization on multiple computing nodes using OpenACC to manage parallelism within the node, and OpenMPI to manage parallelism among the nodes. We first discuss the available strategies to be adopted to maximize performances, we then describe selected relevant details of the code, and finally measure the level of performance and scaling-performance that we are able to achieve. The work focuses mainly on GPUs, which offer a significantly high level of performances for this application, but also compares with results measured on other processors.||2018|10.1142/S0129183118500109|R. Tripiccione, C. Bonati, G. Silvi, F. Negro, S. Schifano, E. Calore, M. Mesiti, M. D’Elia, F. Sanfilippo|2.142857142857143|0
1843|Accelerating Quantum Monte Carlo via Graphics Processing Units|An exact quantum Monte Carlo algorithm for interacting particles in the spatial continuum is extended to exploit the massive parallelism offered by graphics processing units. Its efficacy is tested on the Calogero-Sutherland model describing a system of bosons interacting in one spatial dimension via an inverse square law. Due to the long range nature of the interactions, this model has proved difficult to simulate via conventional path integral Monte Carlo methods running on conventional processors. Using Graphics Processing Units, optimal speedup factors of up to 640 times are obtained for N = 126 particles. The known results for the ground state energy are confirmed and, for the first time, the effects of thermal fluctuations at finite temperature are explored. To my parents, who taught me to ask uncomfortable questions and accept unexpected answers... you are the reason I’ve set myself down this path, and your encouragement has kept me on it even when I thought all was lost.||2018|10.1007/s00211-018-0945-7|Benjamin Himberg, A. Maestro|2.142857142857143|0
1733|Development of regional simulation of seismic ground‐motion and induced liquefaction enhanced by GPU computing|Much research has been conducted for physics‐based ground‐motion simulation to reproduce seismic response of soil and structures precisely and to mitigate damages caused by earthquakes. We aimed at enabling physics‐based ground‐motion simulations of complex three‐dimensional (3D) models with multiple materials, such as a digital twin (high‐fidelity 3D model of the physical world that is constructed in cyberspace). To perform one case of such simulation requires high computational cost and it is necessary to perform a number of simulations for the estimation of parameters or consideration of the uncertainty of underground soil structure data. To overcome this problem, we proposed a fast simulation method using graphics processing unit computing that enables a simulation with small computational resources. We developed a finite‐element‐based method for large‐scale 3D seismic response analysis with small programming effort and high maintainability by using OpenACC, a directive‐based parallel programming model. A lower precision variable format was introduced to achieve further speeding up of the simulation. For an example usage of the developed method, we applied the developed method to soil liquefaction analysis and conducted two sets of simulations that compared the effect of countermeasures against soil liquefaction: grid‐form ground improvement to strengthen the earthquake resistance of existing houses and replacement of liquefiable backfill soil of river wharves for seismic reinforcement of the wharf structure. The developed method accelerates the simulation and enables us to quantitatively estimate the effect of countermeasures using the high‐fidelity 3D soil‐structure models on a small cluster of computers.|Earthquake engineering & structural dynamics (Print)|2020|10.1002/eqe.3369|Takuma Yamaguchi, Ryota Kusakabe, L. Wijerathne, T. Ichimura, M. Hori, K. Fujita|2.0|0
1862|Inference of dynamic spatial GRN models with multi-GPU evolutionary computation|Reverse engineering mechanistic gene regulatory network (GRN) models with a specific dynamic spatial behavior is an inverse problem without analytical solutions in general. Instead, heuristic machine learning algorithms have been proposed to infer the structure and parameters of a system of equations able to recapitulate a given gene expression pattern. However, these algorithms are computationally intensive as they need to simulate millions of candidate models, which limits their applicability and requires high computational resources. Graphics processing unit (GPU) computing is an affordable alternative for accelerating large-scale scientific computation, yet no method is currently available to exploit GPU technology for the reverse engineering of mechanistic GRNs from spatial phenotypes. Here we present an efficient methodology to parallelize evolutionary algorithms using GPU computing for the inference of mechanistic GRNs that can develop a given gene expression pattern in a multicellular tissue area or cell culture. The proposed approach is based on multi-CPU threads running the lightweight crossover, mutation and selection operators and launching GPU kernels asynchronously. Kernels can run in parallel in a single or multiple GPUs and each kernel simulates and scores the error of a model using the thread parallelism of the GPU. We tested this methodology for the inference of spatiotemporal mechanistic gene regulatory networks (GRNs)-including topology and parameters-that can develop a given 2D gene expression pattern. The results show a 700-fold speedup with respect to a single CPU implementation. This approach can streamline the extraction of knowledge from biological and medical datasets and accelerate the automatic design of GRNs for synthetic biology applications.|Briefings Bioinform.|2021|10.1093/bib/bbab104|Sri Harsha Konuru, Reza Mousavi, Daniel Lobo|2.0|0
1884|pyC$^2$Ray: A flexible and GPU-accelerated Radiative Transfer Framework for Simulating the Cosmic Epoch of Reionization|Detailed modelling of the evolution of neutral hydrogen in the intergalactic medium during the Epoch of Reionization, $5 \leq z \leq 20$, is critical in interpreting the cosmological signals from current and upcoming 21-cm experiments such as Low-Frequency Array (LOFAR) and the Square Kilometre Array (SKA). Numerical radiative transfer codes offer the most physically motivated approach for simulating the reionization process. However, they are computationally expensive as they must encompass enormous cosmological volumes while accurately capturing astrophysical processes occurring at small scales ($\lesssim\rm Mpc$). Here, we present pyC$^2$Ray, an updated version of the massively parallel ray-tracing and chemistry code, C$^2$Ray, which has been extensively employed in reionization simulations. The most time-consuming part of the code is calculating the hydrogen column density along the path of the ionizing photons. Here, we present the Accelerated Short-characteristics Octhaedral RAytracing (ASORA) method, a ray-tracing algorithm specifically designed to run on graphical processing units (GPUs). We include a modern Python interface, allowing easy and customized use of the code without compromising computational efficiency. We test pyC$^2$Ray on a series of standard ray-tracing tests and a complete cosmological simulation with volume size $(349\,\rm Mpc)^3$, mesh size of $250^3$ and approximately $10^6$ sources. Compared to the original code, pyC$^2$Ray achieves the same results with negligible fractional differences, $\sim 10^{-5}$, and a speedup factor of two orders of magnitude. Benchmark analysis shows that ASORA takes a few nanoseconds per source per voxel and scales linearly for an increasing number of sources and voxels within the ray-tracing radii.||2023|10.1093/mnras/stad615|S. Giri, Michele Bianco, J. Kneib, Patrick Hirling, G. Mellema, I. Iliev|2.0|0
1927|Fast Reconstruction of 3D Density Distribution around the Sun Based on the MAS by Deep Learning|This study is the first attempt to generate a three-dimensional (3D) coronal electron density distribution based on the pix2pixHD model, whose computing time is much shorter than that of the magnetohydrodynamic (MHD) simulation. For this, we consider photospheric solar magnetic fields as input, and electron density distribution simulated with the MHD Algorithm outside a Sphere (MAS) at a given solar radius is taken as output. We consider 155 pairs of Carrington rotations as inputs and outputs from 2010 June to 2022 April for training and testing. We train 152 deep-learning models for 152 solar radii, which are taken up to 30 solar radii. The artificial intelligence (AI) generated 3D electron densities from this study are quite consistent with the simulated ones from lower radii to higher radii, with an average correlation coefficient 0.97. The computing time of testing data sets up to 30 solar radii of 152 deep-learning models is about 45.2 s using the NVIDIA TITAN XP graphics-processing unit, which is much less than the typical simulation time of MAS. We find that the synthetic coronagraphic images estimated from the deep-learning models are similar to the Solar Heliospheric Observatory (SOHO)/Large Angle and Spectroscopic Coronagraph C3 coronagraph data, especially during the solar minimum period. The AI-generated coronal density distribution from this study can be used for space weather models on a near-real-time basis.|Astrophysical Journal|2023|10.3847/1538-4357/acbd3c|Hyun-jin Jeong, E. Park, Seungheon Shin, S. Bae, Jihye Kang, Y. Moon, Ashraf Siddique, Sumiaya Rahman|2.0|0
2096|Efficient Graphics Processing Unit Modeling of Street‐Scale Weather Effects in Support of Aerial Operations in the Urban Environment|Over the last few years, the concept of incorporating aerial vehicles into the urban environment for diverse purposes has attracted ample interest and investment. These purposes cover a broad spectrum of applications, from larger vehicles designed for passenger transport, to package delivery and inspection/surveillance missions performed by small unmanned drones. While these Advanced Air Mobility (AAM) operations have the potential to alleviate bottlenecks arising from saturated surface transportation networks, there are a number of challenges that need to be addressed to make these operations safe and viable. One challenge is predicting weather effects within the urban environment with the required level of spatiotemporal fidelity, which current operational weather models fail to provide due to the use of coarse grid spacings (a few kilometers) constrained by the predictive performance limitations of traditional computer architectures. Herein, we demonstrate how FastEddy®, a microscale model that exploits the accelerated nature of graphics processing units for high‐performance computing, can be used to understand and predict urban weather impacts from seasonal, day‐to‐day, diurnal, and sub‐hourly scales. To that end, we efficiently perform more than 50 telescoped simulations of microscale urban effects at street‐scale (5 m grid spacing) driven by realistic weather over a 20 km2 region centered at the downtown area of Dallas, Texas. Our analyses demonstrate that urban‐weather interactions at the street‐scale are complex and tightly connected, which is of utmost relevance to AAM operations. These demonstrations reveal the capability of such models to provide real‐time weather hazard avoidance products tailored to capture microscale urban effects.|AGU Advances|2021|10.1029/2021AV000432|P. Hawbecker, R. Sharman, J. Boehnert, H. Shin, M. Steiner, J. Sauer, D. Muñoz‐Esparza, J. Pinto, B. Kosović|2.0|0
2131|Universality of 2+1 dimensional RSOS models|Extensive dynamical simulations of Restricted Solid on Solid models in $D=2+1$ dimensions have been done using parallel multisurface algorithms implemented on graphics cards. Numerical evidence is presented that these models exhibit KPZ surface growth scaling, irrespective of the step heights $N$. We show that by increasing $N$ the corrections to scaling increase, thus smaller step sized models describe better the asymptotic, long wave scaling behavior.|arXiv.org|2016|10.1103/physreve.94.022107|G. Ódor, J. Kelling, S. Gemming|2.0|0
2214|Analytic Solution to the Piecewise Linear Interface Construction Problem and its Application in Curvature Calculation for Volume-of-Fluid Simulation Codes|The plane–cube intersection problem has been discussed in the literature since 1984 and iterative solutions to it have been used as part of piecewise linear interface construction (PLIC) in computational fluid dynamics simulation codes ever since. In many cases, PLIC is the bottleneck of these simulations regarding computing time, so a faster analytic solution to the plane–cube intersection would greatly reduce the computing time for such simulations. We derive an analytic solution for all intersection cases and compare it to the previous solution from Scardovelli and Zaleski (Scardovelli, R.; Zaleski, S. Analytical relations connecting linear interfaces and volume fractions in rectangular grids. J. Comput. Phys. 2000, 164, 228–237), which we further improve to include edge cases and micro-optimize to reduce arithmetic operations and branching. We then extend our comparison regarding computing time and accuracy to include two different iterative solutions as well. We find that the best choice depends on the employed hardware platform: on the CPU, Newton–Raphson is fastest with compiler optimization enabled, while analytic solutions perform better than iterative solutions without. On the GPU, the fastest method is our optimized version of the analytic SZ solution. We finally provide details on one of the applications of PLIC—curvature calculation for the Volume-of-Fluid model used for free surface fluid simulations in combination with the lattice Boltzmann method.|De Computis|2020|10.3390/computation10020021|S. Gekle, Moritz Lehmann|2.0|0
2220|GPU–FPGA-accelerated Radiative Transfer Simulation with Inter-FPGA Communication|The complementary use of graphics processing units (GPUs) and field programmable gate arrays (FPGAs) is a major topic of interest in the high-performance computing (HPC) field. GPU–FPGA-accelerated computing is an effective tool for multiphysics simulations, which encompass multiple physical models and simultaneous physical phenomena. Because the constituent operations in multiphysics simulations exhibit varying characteristics, accelerating these operations solely using GPUs is often challenging. Hence, FPGAs are frequently implemented for this purpose. The objective of the present study was to further improve application performance by employing both GPUs and FPGAs in a complementary manner. Recently, this approach has been applied to the radiative transfer simulation code for astrophysics known as ARGOT, with evaluation results quantitatively demonstrating the resulting improvement in performance. However, the evaluation results in question came from the use of a single node equipped with both a GPU and FPGA. In this study, we extended the GPU–FPGA-accelerated ARGOT code to operate on multiple nodes using the message passing interface (MPI) and an FPGA-to-FPGA communication technology scheme called Communication Integrated Reconfigurable CompUting System (CIRCUS). We evaluated the performance of the ARGOT code with multiple GPUs and FPGAs under weak scaling conditions, and found it to achieve up to 12.8x speedup compared to the GPU-only execution.|International Conference on High Performance Computing in Asia-Pacific Region|2023|10.1145/3578178.3578231|N. Fujita, Ryohei Kobayashi, M. Umemura, T. Boku, Makito Abe, Y. Yamaguchi, K. Yoshikawa|2.0|0
2243|A Graphics Processing Unit (GPU) Approach to Large Eddy Simulation (LES) for Transport and Contaminant Dispersion|Recent advances in the development of large eddy simulation (LES) atmospheric models with corresponding atmospheric transport and dispersion (AT&D) modeling capabilities have made it possible to simulate short, time-averaged, single realizations of pollutant dispersion at the spatial and temporal resolution necessary for common atmospheric dispersion needs, such as designing air sampling networks, assessing pollutant sensor system performance, and characterizing the impact of airborne materials on human health. The high computational burden required to form an ensemble of single-realization dispersion solutions using an LES and coupled AT&D model has, until recently, limited its use to a few proof-of-concept studies. An example of an LES model that can meet the temporal and spatial resolution and computational requirements of these applications is the joint outdoor-indoor urban large eddy simulation (JOULES). A key enabling element within JOULES is the computationally efficient graphics processing unit (GPU)-based LES, which is on the order of 150 times faster than if the LES contaminant dispersion simulations were executed on a central processing unit (CPU) computing platform. JOULES is capable of resolving the turbulence components at a suitable scale for both open terrain and urban landscapes, e.g., owing to varying environmental conditions and a diverse building topology. In this paper, we describe the JOULES modeling system, prior efforts to validate the accuracy of its meteorological simulations, and current results from an evaluation that uses ensembles of dispersion solutions for unstable, neutral, and stable static stability conditions in an open terrain environment.|Atmosphere|2021|10.3390/ATMOS12070890|H. Jonker, M. Sohn, A. Annunzio, P. Bieringer, Richard N. Fry, A. Piña, D. Lorenzetti|2.0|0
1692|Neurite, a Finite Difference Large Scale Parallel Program for the Simulation of Electrical Signal Propagation in Neurites under Mechanical Loading|With the growing body of research on traumatic brain injury and spinal cord injury, computational neuroscience has recently focused its modeling efforts on neuronal functional deficits following mechanical loading. However, in most of these efforts, cell damage is generally only characterized by purely mechanistic criteria, functions of quantities such as stress, strain or their corresponding rates. The modeling of functional deficits in neurites as a consequence of macroscopic mechanical insults has been rarely explored. In particular, a quantitative mechanically based model of electrophysiological impairment in neuronal cells, Neurite, has only very recently been proposed. In this paper, we present the implementation details of this model: a finite difference parallel program for simulating electrical signal propagation along neurites under mechanical loading. Following the application of a macroscopic strain at a given strain rate produced by a mechanical insult, Neurite is able to simulate the resulting neuronal electrical signal propagation, and thus the corresponding functional deficits. The simulation of the coupled mechanical and electrophysiological behaviors requires computational expensive calculations that increase in complexity as the network of the simulated cells grows. The solvers implemented in Neurite—explicit and implicit—were therefore parallelized using graphics processing units in order to reduce the burden of the simulation costs of large scale scenarios. Cable Theory and Hodgkin-Huxley models were implemented to account for the electrophysiological passive and active regions of a neurite, respectively, whereas a coupled mechanical model accounting for the neurite mechanical behavior within its surrounding medium was adopted as a link between electrophysiology and mechanics. This paper provides the details of the parallel implementation of Neurite, along with three different application examples: a long myelinated axon, a segmented dendritic tree, and a damaged axon. The capabilities of the program to deal with large scale scenarios, segmented neuronal structures, and functional deficits under mechanical loading are specifically highlighted.|PLoS ONE|2015|10.1371/journal.pone.0116532|A. Jérusalem, Antonio García-Dopico, Julián A. García-Grajales, J. Peña, G. Rucabado|1.9|0
1753|High-Performance Host-Device Scheduling and Data-Transfer Minimization Techniques for Visualization of 3D Agent-Based Wound Healing Applications.|High-fidelity numerical simulations produce massive amounts of data. Analyzing these numerical data sets as they are being generated provides useful insights into the processes underlying the modeled phenomenon. However, developing real-time in-situ visualization techniques to process large amounts of data can be challenging since the data does not fit on the GPU, thus requiring expensive CPU-GPU data copies. In this work, we present a scheduling scheme that achieve real-time simulation and interactivity through GPU hyper-tasking. Furthermore, the CPU-GPU communications were minimized using an activity-aware technique to reduce redundant copies. Our simulation platform is capable of visualizing 1.7 billion protein data points in situ, with an average frame rate of 42.8 fps. This performance allows users to explore large data sets on remote server with real-time interactivity as they are performing their simulations.|International Conference on Parallel and Distributed Processing Techniques and Applications|2019|10.3390/app9152974|N. Y. Li-Jessen, S. Yuen, Luc Mongeau, G. Yu, J. JáJá, Nuttiiya Seekhao|1.8333333333333333|0
1872|Biopharmaceutical scheduling using a flexible genetic algorithm approach|The goal of biopharmaceutical capacity planning and scheduling is to identify an optimal production schedule (solution) that would satisfy multiple financial and operational objectives. It is a complex combinatorial optimisation problem characterised by features such as multi-product portfolios, variable process durations and yields, long product lead and approval times, and uncertain market forecasts. The bulk of research in the area of biopharmaceutical capacity planning and scheduling has focused on Mixed Integer Linear Programming (MILP) formulations. Heuristic optimisation methods such as Genetic Algorithms (GAs) have received very little attention even though they are reportedly more flexible, easier to implement and, in certain cases, have the potential of outperforming mathematical programming models. Therefore, this thesis addresses this knowledge gap by describing the development of a flexible GA-based Decision Support Tool (DST) for single- and multi-objective biopharmaceutical capacity planning and scheduling under deterministic and uncertain product demand. This thesis makes four broad contributions. Firstly, a GA is designed for solving biopharmaceutical capacity planning and scheduling problems using a discrete-time representation. The effectiveness of the algorithm is demonstrated on two industrial case studies and compared with discrete-time MILP models from the literature. A rolling time horizon strategy is applied to improve solution quality and the performance of the GA. A Particle Swam Optimisation (PSO) algorithm is utilised as a metaoptimiser to automatically tune the parameters of the GA. Secondly, a novel variablelength chromosome structure and an entirely new continuous-time scheduling heuristic are developed for more realistic and efficient medium-term scheduling of biopharmaceutical manufacture. The variable-length chromosome enables the GA to generate production schedules from a single gene. The novel variable-length GA with an embedded continuous-time scheduling heuristic is shown to outperform related discrete- and continuous-time MILP models on two literature-based examples. Thirdly, a multi-objective component is added to the variable-length GA and the continuous-time scheduling heuristic is extended with additional constraints and features, including rolling product sequence-dependent changeovers and lengthy product quality control and assurance (QC/QA) checks. A real-life industrial case study is used to demonstrate the functionality and benefits of the multi-objective optimisation. The multi-objective variable-length GA is used to optimise both the total production throughput and monthly product inventory levels of a multi-product biopharmaceutical facility. Finally, the multi-objective variable-length GA is combined with a Graphics Processing Unit (GPU)-accelerated Monte Carlo simulation for biopharmaceutical capacity planning and scheduling under uncertain product demand. The merits of the approach are highlighted by comparing the production schedules generated when the uncertainty in demand is ignored and when it is accounted for by characterising it with a probability distribution. In the final sections of this thesis an example of a commercial application of this work is presented.||2019|10.1016/j.compchemeng.2019.05.023|Karolis Jankauskas|1.8333333333333333|0
2172|SOFIA: Selection of Medical Features by Induced Alterations in Numeric Labels|This work deals with the improvement of multi-target prediction models through a proposed optimization called Selection Of medical Features by Induced Alterations in numeric labels (SOFIA). This method performs a data transformation when: (1) weighting the features, (2) performing small perturbations on numeric labels and (3) selecting the features that are relevant in the trained multi-target prediction models. With the purpose of decreasing the computational cost in the SOFIA method, we consider those multi-objective optimization metaheuristics that support parallelization. In this sense, we propose an extension of the Natural Optimization (NO) approach for Simulated Annealing to support a multi-objective (MO) optimization. This proposed extension, called MONO, and some multiobjective evolutionary algorithms (MOEAs) are considered when performing the SOFIA method to improve prediction models in a multi-stage migraine treatment. This work also considers the adaptation of these metaheuristics to run on GPUs for accelerating the exploration of a larger space of solutions and improving results at the same time. The obtained results show that accuracies close to 88% are obtained with the MONO metaheuristic when employing eight threads and when running on a GPU. In addition, training times have been decreased from more than 8 h to less than 45 min when running the algorithms on a GPU. Besides, classification models trained with the SOFIA method only require 15 medical features or fewer to predict treatment responses. All in all, the methods proposed in this work remarkably improve the accuracy of multi-target prediction models for the OnabotulinumtoxinA (BoNT-A) treatment, while selecting those relevant features that allow us to know in advance the response to every stage of the treatment.|Electronics|2020|10.3390/electronics9091492|L. Russo, A. García, Franklin Parrales Bravo, J. Ayala|1.8|0
1703|FFSS: The fast fiber simulator software|We introduce FFSS, an optical fiber simulator entirely developed in MATLAB® which takes advantage of parallel calculation on graphic processing units (GPU). This software solves the nonlinear Schrödinger equation (NLSE) - the single-mode fiber wave equation - relying on the well-known split-step Fourier method (SSFM). FFSS operates on either the PMD-Manakov equation (PMD-ME) that averages out random birefringence and polarization mode dispersion (PMD), or on the dual-polarization (DP) NLSE including random birefringence by complementing the SSFM with the waveleplate model. We present results obtained by solving the SSFM with PMD-ME. We tested and benchmarked FFSS using GPU computation on two different hardware solutions, being able to show performance improvements up to 75× for single-precision calculation and 40× for double-precision calculation with respect to the use of computing architectures relying on standard CPU-only computations. We demonstrated the capabilities of FFSS by simulating propagation of 81 PM-QPSK WDM channels on the 50 GHz grid, representing a full C-band WDM comb. We considered a uniform, uncompensated and amplified fiber link, made of 30×100 km spans of standard single-mode fiber (SSMF). We show the accuracy of simulator outcomes obtained by double-precision calculations by comparison with results obtained using theoretical modeling of fiber propagation.|International Conference on Transparent Optical Networks|2017|10.1109/ICTON.2017.8025002|V. Curri, A. Carena, M. Cantono, Dario Pilori|1.75|0
1712|Improving GPU throughput of reservoir simulations using NVIDIA MPS and MIG|Summary In this paper we demonstrated that the overall simulation throughput of full-GPU reservoir simulators can be further improved significantly without any modifications to the software, using NVIDIA’s Multi-Processing-Service and Multi-Instance-GPU infrastructure. For models with just a few thousand cells, a throughput increase of 7x is achieved while for problems with a million cells a 60% improvement is achieved using MPS. Furthermore, when using either MPS or MIG, the smaller models can achieve 80% of the peak achievable performance of larger models. In the context of uncertainty quantification workflows, these performance improvements are significant.|Fifth EAGE Workshop on High Performance Computing for Upstream|2021|10.3997/2214-4609.2021612025|R. Gandham, Yongpeng Zhang, K. Esler, V. Natoli|1.75|0
1793|GPU Based Modelling and Analysis for Parallel Fractional Order Derivative Model of the Spiral-Plate Heat Exchanger|Heat exchangers are commonly used in various industries. A spiral-plate heat exchanger with two fluids is a compact plant that only requires a small space and is excellent in high heat transfer efficiency. However, the spiral-plate heat exchanger is a nonlinear plant with uncertainties, considering the difference between the heat fluid, the heated fluid, and other complex factors. The fractional order derivation model is more accurate than the traditional integer order model. In this paper, a parallel fractional order derivation model is proposed by considering the merit of the graphics processing unit (GPU). Then, the parallel fractional order derivation model for the spiral-plate heat exchanger is constructed. Simulations show the relationships between the output temperature of heated fluid and the orders of fractional order derivatives with two directional fluids impacted by complex factors, namely, the volume flow rate in hot fluid, and the volume flow rate in cold fluid, respectively.|Axioms|2021|10.3390/axioms10040344|M. Deng, Guanqiang Dong|1.75|0
1924|A single‐step and simplified graphics processing unit lattice Boltzmann method for high turbulent flows|In this work, a low‐computational cost graphics processing unit (GPU) lattice Boltzmann Method, coupled with the LES Vreman turbulence model is presented. The algorithm is capable of simulating low‐ and high‐turbulence flows. In contrast to the fractional‐step presented in the Simplified Lattice Boltzmann Method, the proposed work uses a single‐step approach, allowing faster computations of the macroscopic variables without losing any spatial accuracy. Inspired by a recently introduced directional interpolation method for the probability distribution functions, the macroscopic variables for different locations are computed separately, enabling an even further simplification of the steps needed to predict the following time‐step. Similar to the simplified lattice Boltzmann method, this work reduces the required memory allocation by storing only the macroscopic variables. Multiple benchmark cases are presented to compare with results reported in the literature. Excellent agreement with reports in the literature are obtained, while improving the overall computational performance of the algorithm.|International Journal for Numerical Methods in Fluids|2021|10.1002/fld.4976|P. Marzocca, O. Probst, A. Delgado-Gutiérrez, Diego Cárdenas|1.75|0
2110|Reactive Molecular Dynamics on Massively Parallel Heterogeneous Architectures|We present a parallel implementation of the ReaxFF force field on massively parallel heterogeneous architectures, called PuReMD-Hybrid. PuReMD, on which this work is based, along with its integration into LAMMPS, is currently used by a large number of research groups worldwide. Accelerating this important community codebase that implements a complex reactive force field poses a number of algorithmic, design, and optimization challenges, as we discuss in detail. In particular, different computational kernels are best suited to different computing substrates-CPUs or GPUs. Scheduling these computations requires complex resource management, as well as minimizing data movement across CPUs and GPUs. Integrating powerful nodes, each with multiple CPUs and GPUs, into clusters and utilizing the immense compute power of these clusters requires significant optimizations for minimizing communication and, potentially, redundant computations. From a programming model perspective, PuReMD-Hybrid relies on MPI across nodes, pthreads across cores, and CUDA on the GPUs to address these challenges. Using a variety of innovative algorithms and optimizations, we demonstrate that our code can achieve over 565-fold speedup compared to a single core implementation on a cluster of 36 state-of-the-art GPUs for complex systems. In terms of application performance, our code enables simulations of over 1.8M atoms in under 0.68 seconds per simulation time step.|IEEE Transactions on Parallel and Distributed Systems|2017|10.1109/TPDS.2016.2548462|A. Grama, H. Aktulga, Sudhir B. Kylasa|1.75|0
1830|Evaluation of Conflict Resolution Methods for Agent-Based Simulations on the GPU|Graphics processing units (GPUs) have been shown to be well-suited to accelerate agent-based simulations. A fundamental challenge in agent-based simulations is the resolution of conflicts arising when agents compete for simulated resources, which may introduce substantial overhead. A variety of conflict resolution methods on the GPU have been proposed in the literature. In this paper, we systematize and compare these methods and propose two simple new variants. We present performance measurements on the example of the well-known segregation model. We show that the choice of conflict resolution method can substantially affect the simulation performance. Further, although methods in which agents actively indicate their interest in a resource require the use of costly atomic operations, these methods generally outperform the alternatives.|SIGSIM Principles of Advanced Discrete Simulation|2018|10.1145/3200921.3200940|A. Knoll, Philipp Andelfinger, Wentong Cai, Mingyu Yang|1.7142857142857142|0
2186|Application of GPU-Based Large Eddy Simulation in Urban Dispersion Studies|While large eddy simulation has several advantages in microscale air pollutant dispersion modelling, the parametric investigation of geometries is not yet feasible because of its relatively high computational cost. By assuming an analogy between heat and mass transport processes, we utilize a Graphics Processing Unit based software—originally developed for mechanical engineering applications—to model urban dispersion. The software allows for the modification of the geometry as well as the visualization of the transient flow and concentration fields during the simulation, thus supporting the analysis and comparison of different design concepts. By placing passive turbulence generators near the inlet, a numerical wind tunnel was created, capable of producing the characteristic velocity and turbulence intensity profiles of the urban boundary layer. The model results show a satisfactory agreement with wind tunnel experiments examining single street canyons. The effect of low boundary walls placed in the middle of the road and adjacent to the walkways was investigated in a wide parameter range, along with the impact made by the roof slope angle. The presented approach can be beneficially used in the early phase of simulation driven urban design, by screening the concepts to be experimentally tested or simulated with high accuracy models.|Atmosphere|2018|10.3390/ATMOS9110442|G. Kristóf, Bálint Papp|1.7142857142857142|0
1681|Parallel Cloth Simulation Using OpenGL Shading Language|The primary goal of cloth simulation is to express object behavior in a realistic manner and achieve real-time performance by following the fundamental concept of physic. In general, the mass–spring system is applied to real-time cloth simulation with three types of springs. However, hard spring cloth simulation using the mass–spring system requires a small integration time-step in order to use a large stiffness coefficient. Furthermore, to obtain stable behavior, constraint enforcement is used instead of maintenance of the force of each spring. Constraint force computation involves a large sparse linear solving operation. Due to the large computation, we implement a cloth simulation using adaptive constraint activation and deactivation techniques that involve the mass–spring system and constraint enforcement method to prevent excessive elongation of cloth. At the same time, when the length of the spring is stretched or compressed over a defined threshold, adaptive constraint activation and deactivation method deactivates the spring and generate the implicit constraint. Traditional method that uses a serial process of the Central Processing Unit (CPU) to solve the system in every frame cannot handle the complex structure of cloth model in real-time. Our simulation utilizes the Graphic Processing Unit (GPU) parallel processing with compute shader in OpenGL Shading Language (GLSL) to solve the system effectively. In this paper, we design and implement parallel method for cloth simulation, and experiment on the performance and behavior comparison of the mass–spring system, constraint enforcement, and adaptive constraint activation and deactivation techniques the using GPU-based parallel method.|Computer systems science and engineering|2022|10.32604/csse.2022.020685|M. Hong, Min-Hyung Choi, Hongly Va|1.6666666666666667|0
1721|Virtual LiDAR Simulation as a High Performance Computing Challenge: Toward HPC HELIOS++|The software HELIOS++ simulates the laser scanning of a given virtual scene that can be composed of different spatial primitives and 3D meshes with distinct granularity. The high computational cost of this type of simulation software demands efficient computational solutions. Classical solutions based on GPU are not well suited when irregular geometries compose the scene combining different primitives and physics models because they lead to different computation branches. In this paper, we explore the usage of parallelization strategies based on static and dynamic workload balancing and heuristic optimization strategies to speed up the ray tracing process based on a k-dimensional tree (KDT). Using HELIOS++ as our case study, we analyze the performance of our algorithms on different parallel computers, including the CESGA FinisTerrae-II supercomputer. There is a significant performance boost in all cases, with the decrease in computation time ranging from 89.5% to 99.4%. Our results show that the proposed algorithms can boost the performance of any software that relies heavily on a KDT or a similar data structure, as well as those that spend most of the time computing with only a few synchronization barriers. Hence, the algorithms presented in this paper improve performance, whether computed on personal computers or supercomputers.|IEEE Access|2022|10.1109/ACCESS.2022.3211072|B. Höfle, Alberto M. Esmorís, M. Yermo, L. Winiwarter, Hannah Weiser, F. F. Rivera|1.6666666666666667|0
1859|A tool set for random number generation on GPUs in R|We introduce the R package clrng which leverages the gpuR package and is able to generate random numbers in parallel on a Graphics Processing Unit (GPU) with the clRNG (OpenCL) library. Parallel processing with GPU's can speed up computationally intensive tasks, which when combined with R, it can largely improve R's downsides in terms of slow speed, memory usage and computation mode. clrng enables reproducible research by setting random initial seeds for streams on GPU and CPU, and can thus accelerate several types of statistical simulation and modelling. The random number generator in clrng guarantees independent parallel samples even when R is used interactively in an ad-hoc manner, with sessions being interrupted and restored. This package is portable and flexible, developers can use its random number generation kernel for various other purposes and applications.||2022|10.1103/physreva.106.l050401|P. L'Ecuyer, Ruoyong Xu, Patrick R. Brown|1.6666666666666667|0
1880|Graphics Processing Unit-Based Bioheat Simulation to Facilitate Rapid Decision Making Associated with Cryosurgery Training|This study focuses on the implementation of an efficient numerical technique for cryosurgery simulations on a graphics processing unit as an alternative means to accelerate runtime. This study is part of an ongoing effort to develop computerized training tools for cryosurgery, with prostate cryosurgery as a developmental model. The ability to perform rapid simulations of various test cases is critical to facilitate sound decision making associated with medical training. Consistent with clinical practice, the training tool aims at correlating the frozen region contour and the corresponding temperature field with the target region shape. The current study focuses on the feasibility of graphics processing unit-based computation using C++ accelerated massive parallelism, as one possible implementation. Benchmark results on a variety of computation platforms display between 3-fold acceleration (laptop) and 13-fold acceleration (gaming computer) of cryosurgery simulation, in comparison with the more common implementation on a multicore central processing unit. While the general concept of graphics processing unit-based simulations is not new, its application to phase-change problems, combined with the unique requirements for cryosurgery optimization, represents the core contribution of the current study.|Technology in Cancer Research and Treatment|2016|10.1177/1533034615580694|Y. Rabin, Hong Zhang, K. Shimada, R. Keelan|1.6666666666666667|0
1945|A Scheme of Full Kinetic Particle-in-cell Algorithms for GPU Acceleration Using CUDA Fortran Programming|The emerging computable devices, graphical processing units (GPUs), are gradually applied in the simulations of space physics. In this paper, we introduce an approach that implements full kinetic particle-in-cell simulations on GPU architecture devices using the CUDA Fortran language programming for the first time. Using the latest high-performance computing NVIDIA GPUs, this program, which follows the second-order leap-frog iteration method, can speed up the computing process by a factor of 150–285 on a single device compared with the time cost of running with a single core of an Intel Xeon Gold processor. Our scheme improves fast accessibility to the simulation results and provides valuable assistance in studying the physical process.|Astrophysical Journal Supplement Series|2022|10.3847/1538-4365/ac9fd6|K. Jiang, L. Yu, Y. Y. Wei, Q. Xiong, Z. Yuan, J. Zhang, S. Y. Huang, R. Lin, S. B. Xu, Z. Wang|1.6666666666666667|0
1969|A Nonlinear Parallel Model for Reversible Polymer Solutions in Steady and Oscillating Shear Flow|A mathematical model for reversible polymers in steady and oscillating shear flows is presented. Using a mean-field approach, the behavior of the polymer network is characterized by a finitely extensible nonlinear elastic bead-spring model that stochastically transitions between dumbbell states to represent attachments, detachments and loops. An efficient parallel scheme for computation on GPUs utilizes populations of over a million dumbbells to characterize steady, large and small amplitude oscillatory shear (SAOS) flows in Brownian dynamics simulations. In steady-shear a novel attachment species transition function enables shear thickening and shear thinning by the adjustment of either attachment or detachment parameters. Three species simulations show the inclusion of loops modifies the strength of these nonlinear flow responses. In SAOS simulations, three species simulations show an increase in dynamic moduli at higher frequencies not present in two species models. Two approaches for a looped segment transitioning to dangling are explored, and the choice found to have substantial impact on the effect of adding a third species. Pipkin diagrams are also generated using large amplitude oscillatory flows.||2019|10.1016/j.apor.2019.02.002|Erik Palmer|1.6666666666666667|0
2000|Value Function Iteration Toolkit: In Matlab, on the GPU|As part of evaluating economic policies Economists often want to solve Value Function Iteration problems, and then simulate various model outputs. The VFI Toolkit allows the user to easily solve these problems, automatically taking advantage of parallelization on the GPU and CPUs. Using the VFI Toolkit allows Economists to concentrate on the economics of the problem at hand. VFI Toolkit is already available from vfitoolkit.com. A Matlab Toolkit that makes it easy for the user to solve Value Function Iteration problems. Makes automatic use of parallelization on the GPU and CPUs. Roughly, the main command take the Return Function as an input, and gives the Value Function and Optimal Policy Functions as outputs. Work is completed, not preliminary. If it is felt that the presentation at EcoMod would benefit from a more applied example I am happy to construct one based on water management together with a colleague in my department who works on this topic. PS. If it is possible to do a poster in addition to an oral presentation that would be of interest.||2016|10.1183/13993003.01795-2015|R. Kirkby|1.6666666666666667|0
2250|Parallel Computation of Wrench Model for Commutated Magnetically Levitated Planar Actuator|An accurate wrench model is significant for the simulation, manufacture, and control of the commutated magnetically levitated planar actuator (CMLPA). With plenty of coils and permanent magnets employed in the coil set and magnet array of CMLPA, the computational burden of the corresponding wrench model can be substantial. This paper proposes an accurate, universal, and robust parallel massive-thread wrench model (PMWM) for the CMLPA. In PMWM, the magnetic node, Gaussian quadrature, and coordinate transformation are employed to express the interaction between magnet array and coil set. All of these calculation modules are implemented on the graphics processing unit in a massively parallel framework by CUDA. In order to highlight the performance of this PMWM, the wrench model of three different CMLPAs is computed. The computation accuracy and efficiency of proposed PMWM are compared with the finite-element method software Ansys Maxwell and a boundary element method software package named Radia, respectively. The same wrench model is also implemented on a multicore CPU through OpenMP and the comparative results are presented to show significant acceleration of the proposed massive-thread model.|IEEE transactions on industrial electronics (1982. Print)|2016|10.1109/TIE.2016.2592866|V. Dinavahi, Fengqiu Xu, Xianze Xu|1.6666666666666667|0
2060|Towards implementation of cellular automata in Microbial Fuel Cells|The Microbial Fuel Cell (MFC) is a bio-electrochemical transducer converting waste products into electricity using microbial communities. Cellular Automaton (CA) is a uniform array of finite-state machines that update their states in discrete time depending on states of their closest neighbors by the same rule. Arrays of MFCs could, in principle, act as massive-parallel computing devices with local connectivity between elementary processors. We provide a theoretical design of such a parallel processor by implementing CA in MFCs. We have chosen Conway’s Game of Life as the ‘benchmark’ CA because this is the most popular CA which also exhibits an enormously rich spectrum of patterns. Each cell of the Game of Life CA is realized using two MFCs. The MFCs are linked electrically and hydraulically. The model is verified via simulation of an electrical circuit demonstrating equivalent behaviours. The design is a first step towards future implementations of fully autonomous biological computing devices with massive parallelism. The energy independence of such devices counteracts their somewhat slow transitions—compared to silicon circuitry—between the different states during computation.|PLoS ONE|2017|10.1371/journal.pone.0177528|J. Greenman, I. Ieropoulos, Michail-Antisthenis I. Tsompanas, A. Adamatzky, G. Sirakoulis|1.625|0
2133|Acceleration of Linear Finite-Difference Poisson-Boltzmann Methods on Graphics Processing Units.|Electrostatic interactions play crucial roles in biophysical processes such as protein folding and molecular recognition. Poisson-Boltzmann equation (PBE)-based models have emerged as widely used in modeling these important processes. Though great efforts have been put into developing efficient PBE numerical models, challenges still remain due to the high dimensionality of typical biomolecular systems. In this study, we implemented and analyzed commonly used linear PBE solvers for the ever-improving graphics processing units (GPU) for biomolecular simulations, including both standard and preconditioned conjugate gradient (CG) solvers with several alternative preconditioners. Our implementation utilizes the standard Nvidia CUDA libraries cuSPARSE, cuBLAS, and CUSP. Extensive tests show that good numerical accuracy can be achieved given that the single precision is often used for numerical applications on GPU platforms. The optimal GPU performance was observed with the Jacobi-preconditioned CG solver, with a significant speedup over standard CG solver on CPU in our diversified test cases. Our analysis further shows that different matrix storage formats also considerably affect the efficiency of different linear PBE solvers on GPU, with the diagonal format best suited for our standard finite-difference linear systems. Further efficiency may be possible with matrix-free operations and integrated grid stencil setup specifically tailored for the banded matrices in PBE-specific linear systems.|Journal of Chemical Theory and Computation|2017|10.1021/acs.jctc.7b00336|Ruxi Qi, Wesley M. Botello-Smith, R. Luo|1.625|0
2144|Parallel genetic algorithms on the graphics processing units using island model and simulated annealing|To solve a non-deterministic polynomial-hard problem, we can adopt an approximate algorithm for finding the near-optimal solution to reduce the execution time. Although this approach can come up with solutions much faster than brute-force methods, the downside of it is that only approximate solutions are found in most situations. The genetic algorithm is a global search heuristic and optimization method. Initially, genetic algorithms have many shortcomings, such as premature convergence and the tendency to converge toward local optimal solutions; hence, many parallel genetic algorithms are proposed to solve these problems. Currently, there exist many literatures on parallel genetic algorithms. Also, a variety of parallel genetic algorithms have been derived. This study mainly uses the advantages of graphics processing units, which has a large number of cores, and identifies optimized algorithms suitable for computation in single instruction, multiple data architecture of graphics processing units. Furthermore, the parallel simulated annealing method and spheroidizing annealing are also used to enhance performance of the parallel genetic algorithm.||2017|10.1177/1687814017707413|Chu-Hsing Lin, Jung-Chun Liu, Cheng-Chieh Li|1.625|0
1704|Forward Analysis of GPR for Underground Pipes Using CUDA-Implemented Conformal Symplectic Euler Algorithm|Ground-penetrating radar (GPR) is widely used in the detection and positioning of underground facilities. Through the inversion analysis of echo signals of GPR, information such as pipe material, burial depth and location of pipelines can be obtained. Unfortunately, underground pipelines are cylindrical, the ladder approximation method used in traditional forward models produces certain errors. In this study, an accurate and efficient numerical model of GPR forward model in underground pipelines is established using symplectic Euler algorithm, graphics processing unit (GPU) acceleration technology and surface conformal technology. With a Ricker wavelet pulse as the GPR source, the convolution perfectly matched layer (CPML) is incorporated in the symplectic Euler algorithm and shown to be effective to truncate the symplectic Euler computational domain. Through the simulation study of different underground pipeline models, GPR image characteristics of the metal pipeline, plastic pipeline and concrete pipeline filled with air and water are obtained. According to the numerical simulation results, parallel conformal symplectic Euler algorithm effectively reduces the false diffracted waves caused by ladder approximation and improves the computational efficiency of the model in metallic and non-metallic media.|IEEE Access|2020|10.1109/ACCESS.2020.3037811|H. Fang, Yinping Li, Man Yang, Binghan Xue, Jianwei Lei|1.6|0
1831|Discovering Regulatory Network Topologies Using Ensemble Methods on GPGPUs With Special Reference to the Biological Clock of Neurospora crassa|Most genetic networks, such as that for the biological clock, are part of much larger modules controlling fundamental processes in the cell, such as metabolism, development, and response to environmental signals. For example, the biological clock is part of a much larger network controlling the circadian rhythms of about 2418 distinct genes in the genome (with 11 000 genes) of the model system, Neurospora crassa. Predicting and understanding the dynamics of all of these genes and their products in a genetic network describing how the clock functions is a challenge and beyond the current capability of the fastest serial computers. We have implemented a novel variable-topology supernet ensemble method using Markov chain Monte Carlo simulations to fit and discover a regulatory network of unknown topology composed of 2418 genes describing the entire clock circadian network, a network that is found in organisms ranging from bacteria to humans, by harnessing the power of the general-purpose graphics processing unit and exploiting the hierarchical structure of that genetic network. The result is the construction of a genetic network that explains mechanistically how the biological clock functions in the filamentous fungus N. crassa and is validated against over 31 000 data points from microarray experiments. Two transcription factors are identified targeting ribosome biogenesis in the clock network.|IEEE Access|2015|10.1109/ACCESS.2015.2399854|H. Schüttler, T. Taha, Michael T. Judge, J. Arnold, J. Griffith, Ahmad Al-Omari|1.6|0
1979|GPU-accelerated small delay fault simulation|The simulation of delay faults is an essential task in design validation and reliability assessment of circuits. Due to the high sensitivity of current nano-scale designs against smallest delay deviations, small delay faults recently became the focus of test research. Because of the subtle delay impact, traditional fault simulation approaches based on abstract timing models are not sufficient for representing small delay faults. Hence, timing accurate simulation approaches have to be utilized, which quickly become inapplicable for larger designs due to high computational requirements. In this work we present a waveform-accurate approach for fast high-throughput small delay fault simulation on Graphics Processing Units (GPUs). By exploiting parallelism from gates, faults and patterns, the proposed approach enables accurate exhaustive small delay fault simulation even for multimillion gate designs without fault dropping for the first time.|Design, Automation and Test in Europe|2015|10.7873/DATE.2015.0077|E. Schneider, M. Kochte, X. Wen, S. Holst, H. Wunderlich|1.6|0
2098|Parallel genetic algorithm for N‐Queens problem based on message passing interface‐compute unified device architecture|N‐Queens problem derives three variants: obtaining a specific solution, obtaining a set of solutions and obtaining all solutions. The purpose of the variant I is to find a constructive solution, which has been solved. Variant III is aiming to find all solutions and the largest number of queens currently being resolved is 26. Variant II whose purpose is to obtain a set of solutions for larger‐scale problems relies on various intelligent algorithms. In this paper, we use a master‐slave model genetic algorithm that combines the idea of the evolutionary algorithm and simulated annealing algorithm to solve Variant III, and use a parallel fitness function based on compute unified device architecture. Experimental results show that our scheme achieved a maximum 60‐fold speedup over the single‐CPU counterpart. On this basis, a two‐level parallel genetic algorithm based on the island model and master‐slave model is implemented on the GPU cluster by using message passing interface technology. Using two‐node and three‐node GPU cluster, speedup of 1.46 and 2.01 are obtained on average over single‐node, respectively. Compared with the sequential genetic algorithm, the two‐level parallel genetic algorithm makes full use of the parallel computing power of GPU cluster in solving N‐Queen variant II and improves the performance by 99.19 times in the best case.|International Conference on Climate Informatics|2020|10.1111/coin.12300|Yuxin Wang, Jianli Cao, He Guo, Zhikui Chen|1.6|0
2297|Stable and Fast Fluid–Solid Coupling for Incompressible SPH|The solid boundary handling has been a research focus in physically based fluid animation. In this paper, we propose a novel stable and fast particle method to couple predictive–corrective incompressible smoothed particle hydrodynamics and geometric lattice shape matching (LSM), which animates the visually realistic interaction of fluids and deformable solids allowing larger time steps or velocity differences. By combining the boundary particles sampled from solids with a momentum‐conserving velocity‐position correction scheme, our approach can alleviate the particle deficiency issues and prevent the penetration artefacts at the fluid–solid interfaces simultaneously. We further simulate the stable deformation and melting of solid objects coupled to smoothed particle hydrodynamics fluids based on a highly extended LSM model. In order to improve the time performance of each time step, we entirely implement the unified particle framework on GPUs using compute unified device architecture. The advantages of our two‐way fluid–solid coupling method in computer animation are demonstrated via several virtual scenarios.|Computer graphics forum (Print)|2015|10.1111/cgf.12467|W. Wu, X. Shao, N. Magnenat-Thalmann, Z. Zhou|1.6|0
2125|Fast and self-learning indoor airflow simulation based on in situ adaptive tabulation|Fast simulation for stratified indoor airflow distributions is desired for various applications, such as design of advanced indoor environments, emergency management, and coupled annual energy simulation for buildings with stratified air distributions. Reduced order models trained by pre-computed computational fluid dynamics results are fast, but their prediction may be inaccurate when applied for conditions outside the training domain. To overcome this limitation, we propose a fast and self-learning model based on an in situ adaptive tabulation (ISAT) algorithm, which is trained by a fast fluid dynamics (FFD) model as an example. The idea is that the ISAT will retrieve the solutions from an existing data set if the estimated prediction error is within a pre-defined tolerance. Otherwise, the ISAT will execute the FFD simulation, which is accelerated by running in parallel on a graphics processing unit, for a full-scale simulation. This paper systematically investigates the feasibility of the ISAT for indoor airflow simulations by presenting the ISAT-FFD implementation alongside results related to its overall performance. Using a stratified indoor airflow as an example, we evaluated how the training time of ISAT was impacted by four factors (training methods, error tolerances, number of inputs, and number of outputs). Then we demonstrated that a trained ISAT model can predict the key information for inputs both inside and outside the training domain. The ISAT was able to answer query points both inside and close to training domain using retrieve actions within a time less than 0.001 s for each query. Finally, we provided suggestions for using the ISAT for building applications.||2018|10.1080/19401493.2017.1288761|W. Zuo, T. A. Sevilla, W. Tian, M. Wetter, Dan Li|1.5714285714285714|0
1738|Projection Distortion-based Object Tracking in Shader Lamp Scenarios|Shader lamp systems augment the real environment by projecting new textures on known target geometries. In dynamic scenes, object tracking maintains the illusion if the physical and virtual objects are well aligned. However, traditional trackers based on texture or contour information are often distracted by the projected content and tend to fail. In this paper, we present a model-based tracking strategy, which directly takes advantage from the projected content for pose estimation in a projector-camera system. An iterative pose estimation algorithm captures and exploits visible distortions caused by object movements. In a closed-loop, the corrected pose allows the update of the projection for the subsequent frame. Synthetic frames simulating the projection on the model are rendered and an optical flow-based method minimizes the difference between edges of the rendered and the camera image. Since the thresholds automatically adapt to the synthetic image, a complicated radiometric calibration can be avoided. The pixel-wise linear optimization is designed to be easily implemented on the GPU. Our approach can be combined with a regular contour-based tracker and is transferable to other problems, like the estimation of the extrinsic pose between projector and camera. We evaluate our procedure with real and synthetic images and obtain very precise registration results.|IEEE Transactions on Visualization and Computer Graphics|2019|10.1109/TVCG.2019.2932223|Niklas Gard, A. Hilsmann, P. Eisert|1.5|0
1742|Adding GPU Acceleration to an Industrial CPU-Based Simulator, Development Strategy and Results|"\n Running multi-million cell simulation problems in minutes has been a dream for reservoir engineers for decades. Today, with the advancement of Graphic Processing Unit (GPU), we have a real chance to make this dream a reality. Here we present our experience in the step-by-step transformation of a fully developed industrial CPU-based simulator into a fully functional GPU-based simulator. We also demonstrate significant accelerations achieved through the use of GPU technology.\n To achieve the best performance possible, we choose to use CUDA (NVIDIA GPU’s native language), and offload as much computations to GPU as possible. Our CUDA implementation covers all reservoir computes, which include property calculation, linearization, linear solver, etc. The well and Field Management still reside on CPU and need minor changes for their interaction with GPU-based reservoir. Importantly, there is no change to the nonlinear logic. The GPU and CPU parts are overlapped, fully utilizing the asynchronous nature of GPU operations. Each reservoir computation can be run in three modes, CPU_only (existing one), GPU_only, CPU followed by GPU. The latter is only used for result checking and debugging.\n In early 2019, we prototyped two reservoir linearization operations (mass accumulation and mass flux) in CUDA; both showed very strong runtime speed-up of several hundred times, 1 P100-GPU (NVIDIA) vs 1 POWER8NVL CPU core rated at 2.8 GHz (IBM). Encouraged by this success, we moved into linear solver development and managed to move the entire linear solver module into GPU. Again, strong speed-up of ~50 times was achieved (1 GPU vs 1 CPU). The focus for 2019 has been on standard Black-Oil cases. Our implementation was tested with multiple ""million-cell range"" models (SPE10 and other real field cases). In early 2020, we managed to put SPE10 fully on GPU, and finished the entire 2000 day time-stepping in ~35 sec with a single P100 card. After that our effort has switched to compositional AIM (Adaptive Implicit Method), with focus on compositional flash and AIM implementation for reservoir linearization and linear solver, both show early promising results.\n GPU-based reservoir simulation is a future trend for HPC. The development of a reservoir simulator is complex, multi-discipline and time-consuming work. Our paper demonstrates a clear strategy to add tremendous GPU acceleration into an existing CPU-based simulator. Our approach fully utilizes the strength of the existing CPU simulator and minimizes the GPU development effort. This paper is also the first publication targeting GPU acceleration for compositional AIM models."|Day 1 Tue, October 26, 2021|2021|10.2118/203936-ms|N. Gohaud, Rustem Zaydullin, H. Cao, E. Obi, Terrence Liao, G. Darche|1.5|0
1828|Incompressibility Enforcement for Multiple-Fluid SPH Using Deformation Gradient|To maintain incompressibility in SPH fluid simulations is important for visual plausibility. However, it remains an outstanding challenge to enforce incompressibility in such recent multiple-fluid simulators as the mixture-model SPH framework. To tackle this problem, we propose a novel incompressible SPH solver, where the compressibility of fluid is directly measured by the deformation gradient. By disconnecting the incompressibility of fluid from the conditions of constant density and divergence-free velocity, the new incompressible SPH solver is applicable to both single- and multiple-fluid simulations. The proposed algorithm can be readily integrated into existing incompressible SPH frameworks developed for single-fluid, and is fully parallelizable on GPU. Applied to multiple-fluid simulations, the new incompressible SPH scheme significantly improves the visual effects of the mixture-model simulation, and it also allows exploitation for artistic controlling.|IEEE Transactions on Visualization and Computer Graphics|2021|10.1109/TVCG.2021.3062643|Xu Chen, Wei He, Bo Ren, Chenfeng Li|1.5|0
1858|Optimized Parallelization of Boundary Integral Poisson-Boltzmann Solvers|The Poisson-Boltzmann (PB) model governs the electrostatics of solvated biomolecules, i.e., potential, field, energy, and force. These quantities can provide useful information about protein properties, functions, and dynamics. By considering the advantages of current algorithms and computer hardware, we focus on the parallelization of the treecode-accelerated boundary integral (TABI) PB solver using the Message Passing Interface (MPI) on CPUs and the direct-sum boundary integral (DSBI) PB solver using KOKKOS on GPUs. We provide optimization guidance for users when the DSBI solver on GPU or the TABI solver with MPI on CPU should be used depending on the size of the problem. Specifically, when the number of unknowns is smaller than a predetermined threshold, the GPU-accelerated DSBI solver converges rapidly thus has the potential to perform PB model-based molecular dynamics or Monte Carlo simulation. As practical appliations, our parallelized boundary integral PB solvers are used to solve electrostatics on selected proteins that play significant roles in the spread, treatment, and prevention of COVID-19 virus diseases. For each selected protein, the simulation produces the electrostatic solvation energy as a global measurement and electrostatic surface potential for local details.||2023|10.1016/j.jcp.2023.111981|Daniel Reynolds, Reece Iriye, Elyssa Sliheet, Xin Yang, Weihua Geng|1.5|0
2044|A direct Jacobian total Lagrangian explicit dynamics finite element algorithm for real‐time simulation of hyperelastic materials|This article presents a novel direct Jacobian total Lagrangian explicit dynamics (DJ‐TLED) finite element algorithm for real‐time nonlinear mechanics simulation. The nodal force contributions are expressed using only the Jacobian operator, instead of the deformation gradient tensor and finite deformation tensor, for fewer computational operations at run‐time. Owing to this proposed Jacobian formulation, novel expressions are developed for strain invariants and constant components, which are also based on the Jacobian operator. Results show that the proposed DJ‐TLED consumed between 0.70× and 0.88× CPU solution times compared to state‐of‐the‐art TLED and achieved up to 121.72× and 94.26× speed improvements in tetrahedral and hexahedral meshes, respectively, using GPU acceleration. Compared to TLED, the most notable difference is that the notions of stress and strain are not explicitly visible in the proposed DJ‐TLED but embedded implicitly in the formulation of nodal forces. Such a force formulation can be beneficial for fast deformation computation and can be particularly useful if the displacement field is of primary interest, which is demonstrated using a neurosurgical simulation of brain deformations for image‐guided neurosurgery. The present work contributes towards a comprehensive DJ‐TLED algorithm concerning isotropic and anisotropic hyperelastic constitutive models and GPU implementation.|International Journal for Numerical Methods in Engineering|2021|10.1002/nme.6772|Jinao Zhang|1.5|0
2101|Bayesian log‐Gaussian Cox process regression: applications to meta‐analysis of neuroimaging working memory studies|Working memory (WM) was one of the first cognitive processes studied with functional magnetic resonance imaging. With now over 20 years of studies on WM, each study with tiny sample sizes, there is a need for meta‐analysis to identify the brain regions that are consistently activated by WM tasks, and to understand the interstudy variation in those activations. However, current methods in the field cannot fully account for the spatial nature of neuroimaging meta‐analysis data or the heterogeneity observed among WM studies. In this work, we propose a fully Bayesian random‐effects metaregression model based on log‐Gaussian Cox processes, which can be used for meta‐analysis of neuroimaging studies. An efficient Markov chain Monte Carlo scheme for posterior simulations is presented which makes use of some recent advances in parallel computing using graphics processing units. Application of the proposed model to a real data set provides valuable insights regarding the function of the WM.|Journal of the Royal Statistical Society, Series C: Applied Statistics|2017|10.1111/rssc.12295|S. Eickhoff, Shir Atzil, C. Eickhoff, Thomas E. Nichols, Timothy D. Johnson, P. Samartsidis, Lisa Feldman Barrett, T. Wager|1.5|0
2107|Accelerating large-scale phase-field simulations with GPU|A new package for accelerating large-scale phase-field simulations was developed by using GPU based on the semi-implicit Fourier method. The package can solve a variety of equilibrium equations with different inhomogeneity including long-range elastic, magnetostatic, and electrostatic interactions. Through using specific algorithm in Compute Unified Device Architecture (CUDA), Fourier spectral iterative perturbation method was integrated in GPU package. The Allen-Cahn equation, Cahn-Hilliard equation, and phase-field model with long-range interaction were solved based on the algorithm running on GPU respectively to test the performance of the package. From the comparison of the calculation results between the solver executed in single CPU and the one on GPU, it was found that the speed on GPU is enormously elevated to 50 times faster. The present study therefore contributes to the acceleration of large-scale phase-field simulations and provides guidance for experiments to design large-scale functional devices.A new package for accelerating large-scale phase-field simulations was developed by using GPU based on the semi-implicit Fourier method. The package can solve a variety of equilibrium equations with different inhomogeneity including long-range elastic, magnetostatic, and electrostatic interactions. Through using specific algorithm in Compute Unified Device Architecture (CUDA), Fourier spectral iterative perturbation method was integrated in GPU package. The Allen-Cahn equation, Cahn-Hilliard equation, and phase-field model with long-range interaction were solved based on the algorithm running on GPU respectively to test the performance of the package. From the comparison of the calculation results between the solver executed in single CPU and the one on GPU, it was found that the speed on GPU is enormously elevated to 50 times faster. The present study therefore contributes to the acceleration of large-scale phase-field simulations and provides guidance for experiments to design large-scale functional dev...||2017|10.1063/1.5003709|Houbing Huang, Xiaoming Shi, G. Cao, Xingqiao Ma|1.5|0
2171|JAX-LOB: A GPU-Accelerated limit order book simulator to unlock large scale reinforcement learning for trading|Financial exchanges across the world use limit order books (LOBs) to process orders and match trades. For research purposes it is important to have large scale efficient simulators of LOB dynamics. LOB simulators have previously been implemented in the context of agent-based models (ABMs), reinforcement learning (RL) environments, and generative models, processing order flows from historical data sets and hand-crafted agents alike. For many applications, there is a requirement for processing multiple books, either for the calibration of ABMs or for the training of RL agents. We showcase the first GPU-enabled LOB simulator designed to process thousands of books in parallel, whether for identical or different securities, with an up to 75x faster per-message processing time. The implementation of our simulator – JAX-LOB – is based on design choices that aim to best exploit the powers of JAX without compromising on the realism of LOB-related mechanisms. We integrate JAX-LOB with other JAX packages, to provide an example of how one may address an optimal execution problem with reinforcement learning, and to share some preliminary results from end-to-end RL training on GPUs. The project code is available on GitHub 1|International Conference on AI in Finance|2023|10.1145/3604237.3626880|A. Calinescu, Silvia Sapora, Chris Xiaoxuan Lu, J. Foerster, Sascha Frey, Kang Li, S. Zohren, Peer Nagy|1.5|0
2281|Robust Pattern Generation for Small Delay Faults Under Process Variations|Small Delay Faults (SDFs) introduce additional delays smaller than the capture time and require timing-aware test pattern generation. Since process variations can invalidate the effectiveness of such patterns, different circuit instances may show a different fault coverage for the same test pattern set. This paper presents a method to generate test pattern sets for SDFs which are valid for all circuit timings. The method overcomes the limitations of known timing-aware Automatic Test Pattern Generation (ATPG) which has to use fault sampling under process variations due to the computational complexity. A statistical learning scheme maximises the coverage of SDFs in circuits following the variation parameters of a calibrated industrial FinFET transistor model. The method combines efficient ATPG for Transition Faults (TFs) with fast timing-aware fault simulation on GPUs. Simulation experiments show that the size of the pattern set is significantly reduced in comparison to standard N-detection while the fault coverage even increases.|International Test Conference|2023|10.1109/ITC51656.2023.00026|Hanieh Jafarzadeh, F. Klemme, S. Hellebrand, Zahra Paria Najafi-Haghi, H. Amrouch, Jan Dennis Reimer, Hans-Joachim Wunderlich|1.5|0
2080|Molecular Dynamics Simulation by using NAMD-VMD and Gromacs|Molecular Dynamics Simulation is a form of computer simulation, where atoms and molecules are allowed to interact for a period of time under some laws of physics. Calculates the time dependent behaviour of a molecular system. Simulations have provided detailed information on the fluctuations and conformational changes of proteins and nucleic acids. Used to investigate the structure, dynamics and thermodynamics of biological molecules and their complexes. One of the principal tools in the theoretical study of biological molecules. Introduction: Molecular Dynamics simulation is the important tool in the theoretical study of biomolecules. This procedure technique calculates the time dependent behavior of a molecular system. MD simulations have provided careful data on the fluctuations and conformational changes of proteins and nucleic acids [1]. We carry out computer simulations within the hope of understanding the properties of assemblies of molecules in terms of their structure and therefore the microscopic interactions between them. This is a complement to standard experiments, enabling us to find out something new, something that can't be acknowledged in other ways [4]. The two main families of simulation technique are molecular dynamics (MD) and Monte Carlo (MC); additionally, there's an entire range of hybrid techniques which combine features from both. During this study, we shall consider MD. The apparent advantage of MD over MC is that it gives a route to dynamical properties of the system: transport coefficients, time-dependent responses to perturbations, rheological properties and spectra [2]. Computer reenactments go about as an extension between minuscule length and time scales and the plainly visible universe of the research facility: we give a theory at the connections among atoms, and get 'accurate' expectations of mass properties. The forecasts are 'definite' as in they can be made as precise as we like, subject to the confinements forced by our PC spending plan. Simultaneously, the shrouded detail behind mass estimations can be uncovered. A model is the connection between the dissemination coefficient furthermore, speed autocorrelation work (the previous simple to quantify tentatively, the last mentioned a lot harder) [3]. At last we might need to make direct correlations with trial estimations made on explicit materials, in which case a decent model of atomic collaborations is basic. The point of supposed abdominal muscle initio atomic elements is to lessen the measure of fitting what's more, mystery right now a base [3]. Then again, we might be intrigued in marvels of a fairly nonexclusive nature, or we may basically need to segregate between great and awful hypotheses. With regards to points of this sort, it isn't important to have a flawlessly practical sub-atomic model; one that contains the fundamental material science might be very reasonable [4]. To perform a molecular dynamics simulation we use certain software’s like VMD and NAMD, GROMACS, and Desmond. By these we solvation of biomolecules, simulating the molecules to calculate the minimum energy for which we do energy minimization and we analyze the trajectory, rmsd values and the energies of the molecule after the simulation. © 2018 JETIR December 2018, Volume 5, Issue 12 www.jetir.org (ISSN-2349-5162) JETIREB06044 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 370 Significance of MD Simulation  MD simulations permit the study of complex, dynamic processes that occur in biological systems. These include, for example;  Protein Stability  Conformational changes  Protein folding  Molecular recognition: proteins, DNA, membranes, complexes  Ion transport in biological systems  Drug Design  Refinement of the structure determined through X-ray Crystallography and NMR Spectroscopy. Performing MD Simulation:  To run a simulation several things are needed like:  A file containing the coordinates for all atoms  Information on the interactions (bond angles, charges, Vander Waals)  Parameters to control the simulation  Processors to run the simulation The PDB file contains the coordinates for all atoms and is the input structure file for MD Simulation. The interactions are listed in the topology (.top) file and the input parameters are put into a .mdp file. © 2018 JETIR December 2018, Volume 5, Issue 12 www.jetir.org (ISSN-2349-5162) JETIREB06044 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 371 Methodology: VMD and NAMD are the main molecular dynamics simulation software packages, which will work together to know about the structural information of biomolecules. By doing these molecular simulations can give the knowledge to researchers regarding the roles and functions of various biomolecules in life science research [5]. Molecular dynamics simulation has a great significance for biological scientists to study the physical basis of the structure and the function of proteins of bio-molecules since the internal motions of individual atoms play an essential role in the functional mechanism of the proteins in living organisms [6]. NAMD is a molecular dynamics software designed for high performance simulation of large bimolecular system. This software is designed to run on a large number of processors of computer clusters or it can be a large number of cores in graphics processing unit hardware [5]. Usually NAMD works together with molecular graphics software VMD; it provides easy accessible tools, which explore the structural information of bio-molecules. In a glimpse, VMD is visualization software for simulation of a molecule, displaying and analyzing large bio-molecule with the help of 3D graphics and built-in script. VMD and NAMD are available free of cost with source codes and can be supported on different OS platforms like such as Linux, Windows and Mac OS [8]. Download and Install VMD Software in C Drive of your computer. Download the NAMD tutorial files and Save it in VMD folder along with vmd.exe file and all other files related to VMD software [8]. After installation open VMD, go to Extensions tab and open Tk console. Through Tkconsole enter into NAMD tutorial files and then 1-1-build folder. Now open File tab in VMD and go to new molecule and browse the protein of your own (In this tutorial I am using 5eui protein) and load it in VMD [9]. Fig 1: Browse the protein and load it in VMD. To run the simulation we need to have the coordinates file, .psf extension file, parameter file and configuration file. Now in Tk console give the command Set 5eui [atomselect top protein] $5euiwritepdbeuip.pdb This command will extract all the coordinates of your protein molecule and will save it in output in pdb format. Now delete the protein molecule from VMD which have been loaded first and load newly generated coordinates file in VMD. Now go to Extensions tab  Modeling  Automatic PSF Builder and Convert the coordinated file which is in euip.pdb to eui.psf file. Now the solvation of the protein will be in two ways, which are: © 2018 JETIR December 2018, Volume 5, Issue 12 www.jetir.org (ISSN-2349-5162) JETIREB06044 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 372 1. Non-periodic conditions by creating water sphere. 2. Periodic conditions by creating water box. Non-periodic conditions by creating water sphere: Through Tk console go to the NAMD tutorial files and then 1-1-build folder and use these commands. sourcewat_sphere.tcl There will be two output files will be generated which are eui_ws.pdb and eui_ws.psf. Through VMD  File  New molecule  Browse the eui_ws.psf and load it. We will not able to see it in visualizer but we will be able to see it in window. Through VMD  File  New molecule  Browse the eui_ws.pdb and load it. We will be able to see it in visualizer as well as in window. Delete the eui_ws.psf file and eui_ws.pdb file from the VMD window. Fig 2: Non-Periodic condition after creating Water sphere. Periodic conditions by creating water box: Through Tk console go to the NAMD tutorial files and then 1-1-build folder and use these commands. package require solvate solvate eui.psf eui.pdb -t 5 -o eui_wb There will be two output files will be generated which are eui_wb.pdb and eui_wb.psf. Through VMD  File  New molecule  Browse the eui_wb.psf and load it. We will not able to see it in visualizer but we will not able to see it in window. Through VMD  File  New molecule  Browse the eui_wb.pdb and load it. We will be able to see it in visualizer as well as in window. © 2018 JETIR December 2018, Volume 5, Issue 12 www.jetir.org (ISSN-2349-5162) JETIREB06044 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 373 Fig 3: Periodic condition after creating Water Box. Through Tk console in 1-1-build folder use commands Set everyone [atomselect top all] Measureminmax $everyone It helps to measuring the minimum and maximum values of x,y and z coordinates of the entire water system. X min: -41.3409 Y min: -52.1500 Z min: -48.3680 X max: 28.9680 Y max: 18.1560 Z max: 12.9379 Quit VMD. Copy all the 6 files euip.pdb, eui.psf, eui_ws.pdb,eui_ws.psf, eui_wb.pdb and eui_wb.psf into common folder along with some parameter files to run the simulation. Simulation in non-periodic conditions for which we have created water sphere: Go to NAMD tutorial files, open the 1-2-sphere folder, and analyze the configuration file parameters. Minimization is generally performed after setting all the atomic velocities to zero. Download the NAMD Software in C Drive of your computer [9]. As we discussed earlier that NAMD will work together with VMD, so we need to set up NAMD. Right click on my computer  Properties  Advance system settings  Environmental variables  Edit Path and give the complete Path of NAMD. Now use command prompt to enter VMD  NAMD tutorial files  1-2-sphere folder. Now give the command Namd2 eui_ws_eq.conf> eui||2018|10.1016/j.jmgm.2017.12.016|V. Kaushik, Jitendra Sasumana|1.4285714285714286|0
1644|Integrated Massively Parallel Simulation of Thermo-Electromagnetic Fields and Transients of Converter Transformer Interacting With MMC in Multi-Terminal DC Grid|A computationally efficient model to study the transient interaction of the finite-element (FE) converter transformer with modular multi-level converter (MMC) can provide the advanced knowledge of the filed-circuit interactions that can be utilized for the design and test of equipment; however, the runtime of existing simulation tools developed for CPU execution usually takes days or even weeks, which is prohibitively long. In this paper, an integrated thermo-electromagnetic model is proposed for the transient simulation of FE-based transformer interacting with the MMC in a multi-terminal dc gird, with the magnetic field, thermal field, and electrical networks fully coupled. The transmission-line modeling solution is employed for the nonlinear FE problem, and each MMC is split into a number of minimum possible circuits, so that the codes can be sufficiently parallelized and implemented on the graphics processing unit with thousands of Cuda cores to be runtime friendly. The integrated model can provide the transient field distributions within the transformer and the device-level information of the MMC such as switching transients and junction temperatures. Compared with a commercial software package, the execution time of $10^5$ time-steps decreased from several days to only hours with a speedup of more than 47 times while maintaining high accuracy.|IEEE transactions on electromagnetic compatibility (Print)|2020|10.1109/TEMC.2019.2915340|Peng Liu, Ning Lin, V. Dinavahi|1.4|0
1751|Bayesian Neural Ordinary Differential Equations|Recently, Neural Ordinary Differential Equations has emerged as a powerful framework for modeling physical simulations without explicitly defining the ODEs governing the system, but learning them via machine learning. However, the question: Can Bayesian learning frameworks be integrated with Neural ODEs to robustly quantify the uncertainty in the weights of a Neural ODE? remains unanswered. In an effort to address this question, we demonstrate the successful integration of Neural ODEs with two methods of Bayesian Inference: (a) The No-U-Turn MCMC sampler (NUTS) and (b) Stochastic Langevin Gradient Descent (SGLD). We test the performance of our Bayesian Neural ODE approach on classical physical systems, as well as on standard machine learning datasets like MNIST, using GPU acceleration. Finally, considering a simple example, we demonstrate the probabilistic identification of model specification in partially-described dynamical systems using universal ordinary differential equations. Together, this gives a scientific machine learning tool for probabilistic estimation of epistemic uncertainties.|arXiv.org|2020|10.1109/hpec43674.2020.9286178|Vaibhav Dixit, Aslan Garcia-Valadez, Raj Dandekar, Mohamed Tarek, Chris Rackauckas|1.4|0
2174|FD3D_TSN: A Fast and Simple Code for Dynamic Rupture Simulations with GPU Acceleration|\n We introduce FD3D_TSN—an open-source Fortran code for 3D dynamic earthquake rupture modeling based on the staggered grid fourth-order finite-difference method employing a regular cubical spatial discretization. Slip-weakening and fast-velocity-weakening rate-and-state fault friction laws are combined with vertical planar fault geometry, orthogonal to a planar free surface. FD3D_TSN demonstrates good agreement with other methods in a range of benchmark exercises of the Southern California Earthquake Center and U.S. Geological Survey dynamic rupture code verification project. Efficient graphic processing units (GPU) acceleration using the OpenACC framework yields a factor of 10 speed-up in terms of time to solution compared to a single-core solution for current hardware (Intel i9-9900K and Nvidia RTX 2070). The software is fast and easy-to-use and suitable explicitly for data-driven applications requiring a large number of forward simulations such as dynamic source inversion or probabilistic ground-motion modeling. The code is freely available for the scientific community and may be incorporated in physics-based earthquake source imaging and seismic hazard assessment, or for teaching purposes.||2020|10.1785/0220190374|L. Hanyk, J. Premus, F. Gallovič, A. Gabriel|1.4|0
1646|GPU-accelerated generic analytic simulation and image reconstruction platform for multi-pinhole SPECT systems|We introduce a generic analytic simulation and image reconstruction software platform for multi-pinhole (MPH) SPECT systems. The platform is capable of modeling common or sophisticated MPH designs as well as complex data acquisition schemes. Graphics processing unit (GPU) acceleration was utilized to make a high-performance computing software. Herein, we describe the software platform and provide verification studies of the simulation and image reconstruction software.|15th International Meeting on Fully Three-Dimensional Image Reconstruction in Radiology and Nuclear Medicine|2019|10.1117/12.2534523|Benjamin Auer, K. Kalluri, Philip H. Kuo, L. Furenlid, M. King, N. Zeraatkar|1.3333333333333333|0
1647|Accelerating Atmospheric Chemical Kinetics for Climate Simulations|The study of atmospheric chemistry-climate interactions is one of today's great computational challenges. Advances in the architecture of Graphics Processing Units (GPUs) in both raw computational power and memory bandwidth sparked the interest for General-Purpose computing on graphics accelerators in scientific applications. However, the introduction of GPUs in the High Performance Computing (HPC) landscape increased the complexity of software development, due to the inherent heterogeneity requirements of programming models and design approaches, creating a gap in uptake and attainable performance in the presently available scientific community codes. This paper provides an overview of the challenges encountered when using GPU accelerators to achieve optimal performance to calculate the kinetics of chemical tracers in climate models, the techniques used to address them and the insights gained from the process. The paper presents the development of a chemical kinetics code-to-code parser to automatically generate chemical kinetics calculations on three different generations of GPU accelerators (M2070, K80, and P100). The accelerated portion of the application achieves a speedup of up to 22×, equivalent to performance gains of +19 percent up to +90 percent compared with the processor-only version, when using a cluster of 8 Nodes with dual Intel E5-2680 v3 processor and a Kepler architecture (K80), allowing faster completion of the simulations. The paper also provides practical insights and relevant considerations for the development and acceleration of complex applications.|IEEE Transactions on Parallel and Distributed Systems|2019|10.1109/TPDS.2019.2918798|Michail Alvanos, Theodoros Christoudias|1.3333333333333333|0
1710|Intrinsic Performance of Monte Carlo Calibration-Free Algorithm for Laser-Induced Breakdown Spectroscopy|The performance of the Monte Carlo (MC) algorithm for calibration-free LIBS was studied on the example of a simulated spectrum that mimics a metallurgical slag sample. The underlying model is that of a uniform, isothermal, and stationary plasma in local thermodynamical equilibrium. Based on the model, the algorithm generates from hundreds of thousands to several millions of simultaneous configurations of plasma parameters and the corresponding number of spectra. The parameters are temperature, plasma size, and concentrations of species. They are iterated until a cost function, which indicates a difference between synthetic and simulated slag spectra, reaches its minimum. After finding the minimum, the concentrations of species are read from the model and compared to the certified values. The algorithm is parallelized on a graphical processing unit (GPU) to reduce computational time. The minimization of the cost function takes several minutes on the GPU NVIDIA Tesla K40 card and depends on the number of elements to be iterated. The intrinsic accuracy of the MC calibration-free method is found to be around 1% for the eight elements tested. For a real experimental spectrum, however, the efficiency may turn out to be worse due to the idealistic nature of the model, as well as incorrectly chosen experimental conditions. Factors influencing the performance of the method are discussed.|Italian National Conference on Sensors|2022|10.3390/s22197149|T. Völker, I. Gornushkin|1.3333333333333333|0
1875|Accelerating simulations of cardiac electrical dynamics through a multi‐GPU platform and an optimized data structure|Simulations of cardiac electrophysiological models in tissue, particularly in 3D require the solutions of billions of differential equations even for just a couple of milliseconds, thus highly demanding in computational resources. In fact, even studies in small domains with very complex models may take several hours to reproduce seconds of electrical cardiac behavior. Today's Graphics Processor Units (GPUs) are becoming a way to accelerate such simulations, and give the added possibilities to run them locally without the need for supercomputers. Nevertheless, when using GPUs, bottlenecks related to global memory access caused by the spatial discretization of the large tissue domains being simulated, become a big challenge. For simulations in a single GPU, we propose a strategy to accelerate the computation of the diffusion term through a data‐structure and memory access pattern designed to maximize coalescent memory transactions and minimize branch divergence, achieving results approximately 1.4 times faster than a standard GPU method. We also combine this data structure with a designed communication strategy to take advantage in the case of simulations in multi‐GPU platforms. We demonstrate that, in the multi‐GPU approach performs, simulations in 3D tissue can be just 4× slower than real time.|Concurrency and Computation|2019|10.1002/cpe.5528|F. Fenton, M. Zamith, E. C. Vasconcellos, E. Clua|1.3333333333333333|0
2041|MR-iNet Gym: Framework for Edge Deployment of Deep Reinforcement Learning on Embedded Software Defined Radio|Dynamic resource allocation plays a critical role in the next generation of intelligent wireless communication systems. Machine learning has been leveraged as a powerful tool to make strides in this domain. In most cases, the progress has been limited to simulations due to the challenging nature of hardware deployment of these solutions. In this paper, for the first time, we design and deploy deep reinforcement learning (DRL)-based power control agents on the GPU embedded software defined radios (SDRs). To this end, we propose an end-to-end framework (MR-iNet Gym) where the simulation suite and the embedded SDR development work cohesively to overcome real-world implementation hurdles. To prove feasibility, we consider the problem of distributed power control for code-division multiple access (DS-CDMA)-based LPI/D transceivers. We first build a DS-CDMA ns3 module that interacts with the OpenAI Gym environment. Next, we train the power control DRL agents in this ns3-gym simulation environment in a scenario that replicates our hardware testbed. Next, for edge (embedded on-device) deployment, the trained models are optimized for real-time operation without loss of performance. Hardware-based evaluation verifies the efficiency of DRL agents over traditional distributed constrained power control (DCPC) algorithm. More significantly, as the primary goal, this is the first work that has established the feasibility of deploying DRL to provide optimized distributed resource allocation for next-generation of GPU-embedded radios.|WiseML@WiSec|2022|10.1145/3522783.3529530|Jithin Jagannath, Kian Hamedani, Collin Farquhar, Anu Jagannath, Keyvan Ramezanpour|1.3333333333333333|0
2151|Parallel Numerical Calculation on GPU for the 3-Dimensional Mathematical Model in the Walking Beam Reheating Furnace|In this paper, the parallel numerical simulations of 3-dimensional (3D) mathematical model for the walking-beam type reheating furnace have been developed and implemented on the graphics processing unit (GPU) architecture. First, the detailed heat transfer processes in the furnace are described and categorized when building the 3D mathematical model. They consist of the radiative heat exchange into the slab, the heat conduction between the stationary beams and the slabs, the heat convection between the gas flow and slab surfaces, and the heat conduction inside the slabs. Moreover, the proposed 3D mathematical model also accounts for the temperature-dependent material parameters, which is ignored by most published mathematical models. Second, the explicit finite difference method is used to discretize the proposed model to a straightforward parallel computation problem. A detailed analysis of the 3D boundary conditions for the proposed model is introduced and presented. The parallel computing problem is realized by programming on GPU via the platform CUDA in Tesla P100. Finally, the proposed model is verified with industry measurements and the comparison between the GPU-implementation model and the CPU-implementation model is also given to validate the great acceleration. The experimental results prove that the proposed GPU-implementation model declines the computation time from hours to seconds. It is not only orders of magnitude faster but also highly accurate.|IEEE Access|2019|10.1109/ACCESS.2019.2908522|Xiaochuan Luo, Zhi Yang|1.3333333333333333|0
1852|Graphics-Processor-Unit-Based Parallelization of Optimized Baseline Wander Filtering Algorithms for Long-Term Electrocardiography|Long-term electrocardiogram (ECG) often suffers from relevant noise. Baseline wander in particular is pronounced in ECG recordings using dry or esophageal electrodes, which are dedicated for prolonged registration. While analog high-pass filters introduce phase distortions, reliable offline filtering of the baseline wander implies a computational burden that has to be put in relation to the increase in signal-to-baseline ratio (SBR). Here, we present a graphics processor unit (GPU)-based parallelization method to speed up offline baseline wander filter algorithms, namely the wavelet, finite, and infinite impulse response, moving mean, and moving median filter. Individual filter parameters were optimized with respect to the SBR increase based on ECGs from the Physionet database superimposed to autoregressive modeled, real baseline wander. A Monte-Carlo simulation showed that for low input SBR the moving median filter outperforms any other method but negatively affects ECG wave detection. In contrast, the infinite impulse response filter is preferred in case of high input SBR. However, the parallelized wavelet filter is processed 500 and four times faster than these two algorithms on the GPU, respectively, and offers superior baseline wander suppression in low SBR situations. Using a signal segment of 64 mega samples that is filtered as entire unit, wavelet filtering of a seven-day high-resolution ECG is computed within less than 3 s. Taking the high filtering speed into account, the GPU wavelet filter is the most efficient method to remove baseline wander present in long-term ECGs, with which computational burden can be strongly reduced.|IEEE Transactions on Biomedical Engineering|2015|10.1109/TBME.2015.2395456|R. Wildhaber, Thomas Wyss-Balmer, M. Jacomet, J. Goette, R. Vogel, T. Marisa, T. Niederhauser, A. Haeberlin|1.3|0
1854|Protofold II: Enhanced Model and Implementation for Kinetostatic Protein Folding|A reliable prediction of 3D protein structures from sequence data remains a big challenge due to both theoretical and computational difficulties. We have previously shown that our kinetostatic compliance method (KCM) implemented into the Protofold package can overcome some of the key difficulties faced by other de novo structure prediction methods, such as the very small time steps required by the molecular dynamics (MD) approaches or the very large number of samples needed by the Monte Carlo (MC) sampling techniques. In this article, we improve the free energy formulation used in Protofold by including the typically underrated entropic effects, imparted due to differences in hydrophobicity of the chemical groups, which dominate the folding of most water-soluble proteins. In addition to the model enhancement, we revisit the numerical implementation by redesigning the algorithms and introducing efficient data structures that reduce the expected complexity from quadratic to linear. Moreover, we develop and optimize parallel implementations of the algorithms on both central and graphics processing units (CPU/GPU) achieving speed-ups up to two orders of magnitude on the GPU. Our simulations are consistent with the general behavior observed in the folding process in aqueous solvent, confirming the effectiveness of model improvements. We report on the folding process at multiple levels; namely, the formation of secondary structural elements and tertiary interactions between secondary elements or across larger domains. We also observe significant enhancements in running times that make the folding simulation tractable for large molecules.|arXiv.org|2015|10.1115/1.4032759|P. Tavousi, H. Ilies, K. Kazerounian, M. Behandish|1.3|0
1856|SAS Simulations with Procedural Texture and the Point-based Sonar Scattering Model|Recent work has demonstrated the efficacy of Procedural Techniques for simulation of realistic textures emulating rippled-sand and random roughness seafloors, as well as bioturbation by fish feeding pits. Separately, recent work has presented a sonar time series model, which has been shown to agree with theory for the mean, mean square, and spatial coherence of the roughness-scattered acoustic field. In this work, we apply these state of the art environmental generation techniques, inspired by the computer graphics industry, for generation of realistic seafloor textures, combined with the massive parallelization afforded by modern graphics processing units to compute acoustic models, for generation of simulated sonar time series. The resulting time series are then demonstrated to be suitable for coherent synthetic aperture signal processing resulting in a high-fidelity simulated SAS image.|OCEANS 2018 MTS/IEEE Charleston|2018|10.1109/OCEANS.2018.8604616|Daniel C. Brown, Shawn F. Johnson|1.2857142857142858|0
2124|Scalable Cloning on Large-Scale GPU Platforms with Application to Time-Stepped Simulations on Grids|Cloning is a technique to efficiently simulate a tree of multiple what-if scenarios that are unraveled during the course of a base simulation. However, cloned execution is highly challenging to realize on large, distributed memory computing platforms, due to the dynamic nature of the computational load across clones, and due to the complex dependencies spanning the clone tree. We present the conceptual simulation framework, algorithmic foundations, and runtime interface of CloneX, a new system we designed for scalable simulation cloning. It efficiently and dynamically creates whole logical copies of a dynamic tree of simulations across a large parallel system without full physical duplication of computation and memory. The performance of a prototype implementation executed on up to 1,024 graphical processing units of a supercomputing system has been evaluated with three benchmarks—heat diffusion, forest fire, and disease propagation models—delivering a speed up of over two orders of magnitude compared to replicated runs. The results demonstrate a significantly faster and scalable way to execute many what-if scenario ensembles of large simulations via cloning using the CloneX interface.|ACM Transactions on Modeling and Computer Simulation|2018|10.1145/3158669|Srikanth B. Yoginath, K. Perumalla|1.2857142857142858|0
1838|A massively parallel time-domain coupled electrodynamics–micromagnetics solver|We present a high-performance coupled electrodynamics–micromagnetics solver for full physical modeling of signals in microelectronic circuitry. The overall strategy couples a finite-difference time-domain approach for Maxwell’s equations to a magnetization model described by the Landau–Lifshitz–Gilbert equation. The algorithm is implemented in the Exascale Computing Project software framework, AMReX, which provides effective scalability on manycore and GPU-based supercomputing architectures. Furthermore, the code leverages ongoing developments of the Exascale Application Code, WarpX, which is primarily being developed for plasma wakefield accelerator modeling. Our temporal coupling scheme provides second-order accuracy in space and time by combining the integration steps for the magnetic field and magnetization into an iterative sub-step that includes a trapezoidal temporal discretization for the magnetization. The performance of the algorithm is demonstrated by the excellent scaling results on NERSC multicore and GPU systems, with a significant (59×) speedup on the GPU using a node-by-node comparison. We demonstrate the utility of our code by performing simulations of an electromagnetic waveguide and a magnetically tunable filter.|The international journal of high performance computing applications|2021|10.1177/10943420211057906|Yadong Zeng, A. Nonaka, R. Jambunathan, Z. Yao|1.25|0
2059|General simultaneous motion estimation and image reconstruction (G-SMEIR)|To achieve better performance for 4D multi-frame reconstruction with the parametric motion model (MF-PMM), a general simultaneous motion estimation and image reconstruction (G-SMEIR) method is proposed. In G-SMEIR, projection domain motion estimation and image domain motion estimation are performed alternatively to achieve better 4D reconstruction. This method can mitigate the local optimum trapping problem in either domain. To improve computational efficiency, the image domain motion estimation is accelerated by adapting fast convergent algorithms and graphics processing unit (GPU) computing. The proposed G-SMEIR method is tested using a cone-beam computed tomography (CBCT) simulation study of 4D XCAT phantom at different dose levels and compared with 3D total variation-based reconstruction (3D TV), 4D reconstruction with image domain motion estimation (IM4D), and SMEIR. G-SMEIR shows strong denoising capability and achieves similar performance at regular dose and half dose. The root mean squared error (RMSE) of G-SMEIR is the best among the four methods and improved about 12% over SMEIR for all respiratory phase images at full dose. G-SMEIR also achieved the best structural similarity index (SSIM) values among all methods. More importantly, G-SMEIR leads to more than 40% improvement of the mean deviation from the phantom tumor motion over SMEIR. A preliminary patient CBCT image reconstruction also shows better image quality of G-SMEIR than that of the frame-by-frame reconstruction (3D TV) and MF-PMM either using image domain motion estimation (IM4D) or using projection domain motion estimation (SMEIR) alone. G-SMEIR with a flexible combination of image domain and projection domain motion estimation provides an effective tool for 4D tomographic reconstruction.|Biomedical engineering and physics express|2021|10.1088/2057-1976/ac12a4|Shiwei Zhou, Y. Chi, M. Jin, Jing Wang|1.25|0
2256|Parallel Computation of a Dam-Break Flow Model Using OpenACC Applications|AbstractTwo key factors in dam-break modeling are accuracy and speed. Therefore, high-performance calculations are of great importance to the simulation of dam-break events. In this study, we develop a two-dimensional hydrodynamic model based on the finite volume method to simulate the dam-break flow routing process. Roe’s approximate Riemann solution is adopted to solve the interface flux of grid cells and accurately simulate the discontinuous flow. A graphics processing unit (GPU)-based parallel method, OpenACC, is used to realize parallel computing. Because an explicit discrete technique is used to solve the governing equations, and there is no correlation between grid calculations in a single time step, the parallel dam-break model can be easily realized by adding OpenACC directives to the loop structure of the grid calculations. To analyze the performance of the model, we considered the Pangtoupao flood storage area in China using a Nvidia Tesla K20c card and four different grid division schemes. By ...||2017|10.1061/(ASCE)HY.1943-7900.0001225|Yuehua Wu, Rui Yuan, Y. Yi, Shang-hong Zhang|1.25|0
2252|Development of a New Fully-Parallel Finite-Discrete Element Code: Irazu|This paper presents the development and verification of a new, fully-parallel, hydro-mechanically-coupled simulation software (Irazu) based on the finite-discrete element method (FDEM). Irazu is a general-purpose numerical simulation software which combines continuum mechanics principles with discrete element algorithms to simulate the mechanical response of brittle geomaterials. To overcome the computational limitations of FDEM codes, Irazu utilizes the parallel processing power of general-purpose graphics processing units (GPGPUs). As a result, Irazu shows impressive speedups compared with a sequential central processing unit (CPU) FDEM code. The code capabilities are illustrated herein by simulating the complex failure mechanics of bedded rock samples. A case study to assess the stability of the overhang of a mine loading pocket demonstrates Irazu’s application to field-scale problems. Overall, superior physics and computational performance make Irazu a state-of-the-art, commercial simulation tool for tackling complex geomechanical applications in mining, civil, and petroleum engineering. nature of the GPGPU, efficient parallel algorithms have been developed for performing contact detection, hash table implementation, and other functions. In addition, Irazu can use state-of-the-art cloud computing developments to execute the simulations remotely in the cloud, thus minimizing the hardware and maintenance fees typically associated with traditional simulation software. Irazu has been tested rigorously against known analytical solutions, laboratory, and field observations, as well as previous FDEM results obtained using Y-Geo (Geomechanica Inc., 2016). In all cases, Irazu simulations have been able to reproduce the expected behavior, both qualitatively (e.g., fracturing processes) and quantitatively (e.g., peak strength, breakdown pressures). In the current paper, two practical cases have been chosen to demonstrate the accuracy of results: a series of laboratory tests on a bedded rock; and a larger-scale model of mine loading pocket stability. Realistic deformation and fracturing behavior are reproduced in these simulations with substantially decreased computational times compared to typical FDEM simulations (i.e., minutes instead of hours and hours instead of days). 2. IRAZU FDEM PRINCIPLES A brief overview of the major FDEM principles implemented in Irazu are presented here. For further details and mathematical formulations, the reader is referred to the Irazu theory manual (Geomechanica Inc., 2016). 2.1. Governing equation In Irazu, each intact body is discretized with a mesh comprised of 3-noded triangular elements. The generalized governing equation of motion can be expressed as: + + − − − = 0 (1) where M is the lumped mass matrix, C is the viscous damping matrix (applied to dissipate dynamic oscillations in the model), x is the vector of nodal displacements, fint is the vector of internal resisting forces, including elastic reaction forces and crack element bonding forces, fext is vector of external forces, fc is vector of contact forces, and ffl is vector of fluidpressure forces. The equation of motion is integrated at each simulation time step to update the nodal coordinates. 2.2. Material deformation The elastic deformation of intact material is modeled according to the continuum theory of linear elasticity using constant-strain triangular elements (Munjiza, 2004). Element deformation (strain) at each time step is described by the differences between its initial configuration (i.e., undeformed) and current configuration (i.e., deformed). From the strain tensor, the element stress tensor is calculated using constitutive laws for: • isotropic linear elasticity based on plane stress or plane strain assumptions; or • transversely isotropic linear elasticity. 2.3. Contact detection and interaction An FDEM simulation in Irazu can involve a large number of interacting discrete elements. To correctly capture this behavior, contacting couples (i.e., pairs of contacting discrete elements) must first be detected. Detection is accomplished using a spatial hashing contact detection algorithm optimized for the GPGPU. Subsequently, the interaction forces resulting from the detected contacts can be defined. Contact interaction forces are calculated between all pairs of elements that overlap in space. In the normal direction, repulsive forces are applied to enforce a body impenetrability condition, while in the tangential direction, frictional forces are applied. The repulsive forces between contacting elements (i.e., pairs) are calculated using a distributed contact force penalty function method (Munjiza & Andrews, 2000). According to this method, it is assumed that pairs of contacting elements penetrate into each other and thereby generate contact forces that depend on the size and shape of the resulting overlapping area. The frictional forces between contacting couples are calculated using a Coulomb-type friction law (Mahabadi et al., 2012). These frictional forces are applied to intact material and pre-existing and newly-created fractures. Given the explicit integration scheme, the frictional resistance must be mobilized over some finite amount of relative displacement between the interacting edges.||2016|10.1016/j.engstruct.2015.12.037|B. Tatone, A. Lisjak, O. Mahabadi, P. Kaifosh, L. He, G. Grasselli|1.2222222222222223|0
1713|Single Restart with Time Stamps for Parallel Task Processing with Known and Unknown Processors|"We study the problem of scheduling <inline-formula><tex-math notation=""LaTeX"">$n$</tex-math><alternatives><mml:math><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href=""liang-ieq1-2929173.gif""/></alternatives></inline-formula> tasks on <inline-formula><tex-math notation=""LaTeX"">$m+m^{\prime }$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mo>'</mml:mo></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href=""liang-ieq2-2929173.gif""/></alternatives></inline-formula> parallel processors, where the processing times on <inline-formula><tex-math notation=""LaTeX"">$m$</tex-math><alternatives><mml:math><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href=""liang-ieq3-2929173.gif""/></alternatives></inline-formula> processors are known while those on the remaining <inline-formula><tex-math notation=""LaTeX"">$m^{\prime }$</tex-math><alternatives><mml:math><mml:msup><mml:mi>m</mml:mi><mml:mo>'</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=""liang-ieq4-2929173.gif""/></alternatives></inline-formula> processors are not known a priori. This semi-online model is an abstraction of certain heterogeneous computing systems, e.g., with the <inline-formula><tex-math notation=""LaTeX"">$m$</tex-math><alternatives><mml:math><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href=""liang-ieq5-2929173.gif""/></alternatives></inline-formula> known processors representing local CPU cores and the unknown processors representing remote servers with uncertain availability of computing cycles. Our objective is to minimize the makespan of all tasks. We initially focus on the case <inline-formula><tex-math notation=""LaTeX"">$m^{\prime }=1$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mo>'</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=""liang-ieq6-2929173.gif""/></alternatives></inline-formula> and propose a semi-online algorithm termed Single Restart with Time Stamps (SRTS), which has time complexity <inline-formula><tex-math notation=""LaTeX"">$O(n \log n)$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo form=""prefix"">log</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""liang-ieq7-2929173.gif""/></alternatives></inline-formula>. We derive its competitive ratio in comparison with the optimal offline solution. If the unknown processing times are deterministic, the competitive ratio of SRTS is shown to be either always constant or asymptotically constant in practice, respectively in cases where the processing times are independent and dependent on <inline-formula><tex-math notation=""LaTeX"">$m$</tex-math><alternatives><mml:math><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href=""liang-ieq8-2929173.gif""/></alternatives></inline-formula>. A similar result is obtained when the unknown processing times are random. Furthermore, extending the ideas of SRTS, we propose a heuristic algorithm termed SRTS-Multiple (SRTS-M) for the case <inline-formula><tex-math notation=""LaTeX"">$m^{\prime }>1$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mo>'</mml:mo></mml:msup><mml:mo>></mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=""liang-ieq9-2929173.gif""/></alternatives></inline-formula>. Finally, where tasks arrive dynamically with unknown arrival times, we extend SRTS to Dynamic SRTS (DSRTS) and find its competitive ratio. Besides the proven competitive ratios, simulation results further suggest that SRTS and SRTS-M give superior performance on average over randomly generated task processing times, substantially reducing the makespan over the best known alternatives. Interestingly, the performance gain is more significant for task processing times sampled from heavy-tailed distributions."|IEEE Transactions on Parallel and Distributed Systems|2020|10.1109/TPDS.2019.2929173|B. Liang, J. Champati|1.2|0
1728|High-performance computing for computational modelling in built environment-related studies – a scientometric review|\nPurpose\nThis paper aims to present the result of a scientometric analysis conducted using studies on high-performance computing in computational modelling. This was done with a view to showcasing the need for high-performance computers (HPC) within the architecture, engineering and construction (AEC) industry in developing countries, particularly in Africa, where the use of HPC in developing computational models (CMs) for effective problem solving is still low.\n\n\nDesign/methodology/approach\nAn interpretivism philosophical stance was adopted for the study which informed a scientometric review of existing studies gathered from the Scopus database. Keywords such as high-performance computing, and computational modelling were used to extract papers from the database. Visualisation of Similarities viewer (VOSviewer) was used to prepare co-occurrence maps based on the bibliographic data gathered.\n\n\nFindings\nFindings revealed the scarcity of research emanating from Africa in this area of study. Furthermore, past studies had placed focus on high-performance computing in the development of computational modelling and theory, parallel computing and improved visualisation, large-scale application software, computer simulations and computational mathematical modelling. Future studies can also explore areas such as cloud computing, optimisation, high-level programming language, natural science computing, computer graphics equipment and Graphics Processing Units as they relate to the AEC industry.\n\n\nResearch limitations/implications\nThe study assessed a single database for the search of related studies.\n\n\nOriginality/value\nThe findings of this study serve as an excellent theoretical background for AEC researchers seeking to explore the use of HPC for CMs development in the quest for solving complex problems in the industry.\n|Journal of Engineering, Design and Technology|2020|10.1108/jedt-07-2020-0294|O. O. Petinrin, E. Aghimien, D. Aghimien, Lerato Aghimien|1.2|0
1955|A Real-time Ultrasound Rendering with Model-based Tissue Deformation for Needle Insertion|In the course of developing a training simulator for puncture, a novel approach is proposed to render in real-time the ultrasound (US) image of any 3D model. It is combined with the deformation of the soft tissues (due to their interactions with a needle and a probe) according to their physical properties. Our solution reproduces the usual US artifacts at a low cost. It combines the use of textures and ray-tracing with a new way to efficiently render fibrous tissues. Deformations are handled in real-time on the GPU using displacement functions. Our approach goes beyond the usual bottleneck of real-time deformations of 3D models in interactive US simulation. The display of tissues deformation and the possibilities to tune the 3D morphotypes, tissue properties, needle shape, or even specific probe characteristics, is clearly an advantage in such a training environment.|VISIGRAPP|2020|10.5220/0008947302350246|Charles Barnouin, F. Jaillet, F. Zara|1.2|0
1989|Acceleration of discrete stochastic biochemical simulation using GPGPU|For systems made up of a small number of molecules, such as a biochemical network in a single cell, a simulation requires a stochastic approach, instead of a deterministic approach. The stochastic simulation algorithm (SSA) simulates the stochastic behavior of a spatially homogeneous system. Since stochastic approaches produce different results each time they are used, multiple runs are required in order to obtain statistical results; this results in a large computational cost. We have implemented a parallel method for using SSA to simulate a stochastic model; the method uses a graphics processing unit (GPU), which enables multiple realizations at the same time, and thus reduces the computational time and cost. During the simulation, for the purpose of analysis, each time course is recorded at each time step. A straightforward implementation of this method on a GPU is about 16 times faster than a sequential simulation on a CPU with hybrid parallelization; each of the multiple simulations is run simultaneously, and the computational tasks within each simulation are parallelized. We also implemented an improvement to the memory access and reduced the memory footprint, in order to optimize the computations on the GPU. We also implemented an asynchronous data transfer scheme to accelerate the time course recording function. To analyze the acceleration of our implementation on various sizes of model, we performed SSA simulations on different model sizes and compared these computation times to those for sequential simulations with a CPU. When used with the improved time course recording function, our method was shown to accelerate the SSA simulation by a factor of up to 130.|Frontiers in Physiology|2015|10.3389/fphys.2015.00042|K. Sumiyoshi, Kazuki Hirata, N. Hiroi, Akira Funahashi|1.2|0
2010|Application of CUDA-Accelerated GO/PO Method in Calculation of Electromagnetic Scattering From Coated Targets|In this study, fast calculations for the electromagnetic (EM) scattering of complex targets coated by radar absorbing materials have been reported. The Geometric Optics and Physical Optics (GO/PO) methods are combined with the modified surface reflection coefficient method, to solve the problem of EM scattering. Moreover, two acceleration techniques have been employed to improve the efficiency of computations. Firstly, the neighbor search technique is adopted to accelerate the ray tracing process in GO/PO. In this technique, an Octree structure is applied to divide the space into multiple subspaces, namely sub-nodes, where the efficiency of ray tracing can be improved through Morton code transform. In order to further reduce the computational time, parallel acceleration technique, based on GPU platform within the Compute Unified Device Architecture (CUDA) framework is introduced. Our proposed method has been verified from the nice correlation of simulated and multilevel fast multipole method (MLFMM) and CPU-based GO/PO methods. The runtime is compared with that of GO/PO method in serial model, responsible for a good speedup ratio. Finally, the influence of coating thicknesses and types of material on the EM scattering characteristics of the coated targets are also analyzed.|IEEE Access|2020|10.1109/ACCESS.2020.2974770|Li-xin Guo, Xiao Meng, C. Dong|1.2|0
2191|GPU Acceleration for Simulating Massively Parallel Many-Core Platforms|Emerging massively parallel architectures such as a general-purpose processor plus many-core programmable accelerators are creating an increasing demand for novel methods to perform their architectural simulation. Most state-of-the-art simulation technologies are exceedingly slow and the need to model full system many-core architectures adds further to the complexity issues. This paper presents a fast, scalable and parallel simulator, which uses a novel methodology to accelerate the simulation of a many-core coprocessor using GPU platforms. The main idea is to use. The target architecture of the associated. Simulation of many target nodes is mapped to the many hardware-threads available on highly parallel GPU platforms. This paper presents a novel methodology to accelerate the simulation of many-core coprocessors using GPU platforms. We demonstrate the challenges, feasibility and benefits of our idea to use heterogeneous system (CPU and GPU) to simulate future architecture of many-core heterogeneous platforms. The target architecture selected to evaluate our methodology consists of an ARM general purpose CPU coupled with many-core coprocessor with thousands of simple in-order cores connected in a tile network. This work presents optimization techniques used to parallelize the simulation specifically for acceleration on GPUs. We partition the full system simulation between CPU and GPU, where the target general purpose CPU is simulated on the host CPU, whereas the many-core coprocessor is simulated on the NVIDIA Tesla 2070 GPU platform. Our experiments show performance of up to 50 MIPS when simulating the entire heterogeneous chip, and high scalability with increasing cores on coprocessor.|IEEE Transactions on Parallel and Distributed Systems|2015|10.1109/TPDS.2014.2319092|David Atienza Alonso, M. Ruggiero, A. Marongiu, L. Benini, Shivani Raghav, Christian Pinto|1.2|0
1720|A GPU-enabled finite volume solver for large shallow water simulations|This paper presents the implementation of a HLLC finite volume solver using GPU technology for the solution of shallow water problems in two dimensions. It compares both CPU and GPU approaches for implementing all the solver's steps. The technology of graphics and central processors is highlighted with a particular emphasis on the CUDA architecture of NVIDIA. The simple and well-documented Application Programming Interface (CUDA API) facilitates the use of the display card workstation as an additional computer unit to the central processor. Four professional solutions of the NVIDIA Quadro line are tested. Comparison tests between CPU and GPU are carried out on unstructured grids of small sizes (up to 10,000 elements), medium and large sizes (up to 10,000,000 elements). For all test cases, the accuracy of results is of the same order of magnitude for both approaches. Furthermore, the obtained speed gains with the GPU strongly depend on the model of the graphics card, the size of the problem and the simulation time.|arXiv.org|2018|10.1007/s42107-018-0062-z|F. Zaoui|1.1428571428571428|0
1745|A Spark-Based High Performance Computational Approach for Simulating Typhoon Wind Fields|The impacts of typhoons on coastal areas around the globe necessitate the risk assessments of typhoon events to analyze their paths, intensity, and impacts on the environment for disaster prevention and reduction as well as for the scientific assessments of typhoon impacts. To this end, the typhoon wind field needs to be obtained by inversion to attain its temporal and spatial scopes of influence and disaster-inducing intensities using simulation algorithms that are based on the existing typhoon path data. The wind field calculation is a typical kind of compute-intensive processing. It is time-consuming (especially when the temporal resolution and the spatial resolution are high) to an extent, which makes it difficult for a traditional stand-alone computation to meet the application needs for typhoon risk assessments. Therefore, studies need to be conducted to accelerate the wind field calculations. Currently, the most economical and relatively feasible method is to rapidly generate a large quantity of typhoon risk data using multi-thread, graphics processing unit computing or clustering technologies (e.g., cloud computing). Hence, we have attempted to use the latest cloud computing framework, Spark, for the rapid simulation of the typhoon wind field. This paper proposed a storage model for typhoon paths and wind fields under the Spark environment, as well as a wind field parallel acceleration method based on Spark. Using this approach, we have calculated a total of 1038 historical wind fields that impacted the northwestern Pacific Ocean from 1949 to 2014. The results indicate that the Spark-based wind field computation method proposed in this paper has a greater advantage in terms of performance, which facilitates the typhoon risk assessment feasibility.|IEEE Access|2018|10.1109/ACCESS.2018.2850768|Yiran Chen, Shaofu Lin, Zhou Huang, W. Fang, Xinyu Wu|1.1428571428571428|0
1750|Computational Benefit of GPU Optimization for the Atmospheric Chemistry Modeling|Global chemistry‐climate models are computationally burdened as the chemical mechanisms become more complex and realistic. Optimization for graphics processing units (GPU) may make longer global simulation with regional detail possible, but limited study has been done to explore the potential benefit for the atmospheric chemistry modeling. Hence, in this study, the second‐order Rosenbrock solver of the chemistry module of CAM4‐Chem is ported to the GPU to gauge potential speed‐up. We find that on the CPU, the fastest performance is achieved using the Intel compiler with a block interleaved memory layout. Different combinations of compiler and memory layout lead to ~11.02× difference in the computational time. In contrast, the GPU version performs the best when using a combination of fully interleaved memory layout with block size equal to the warp size, CUDA streams for independent kernels, and constant memory. Moreover, the most efficient data transfer between CPU and GPU is gained by allocating the memory contiguously during the data initialization on the GPU. Compared to one CPU core, the speed‐up of using one GPU alone reaches a factor of ~11.7× for the computation alone and ~3.82× when the data transfer between CPU and GPU is considered. Using one GPU alone is also generally faster than the multithreaded implementation for 16 CPU cores in a compute node and the single‐source solution (OpenACC). The best performance is achieved by the implementation of the hybrid CPU/GPU version, but rescheduling the workload among the CPU cores is required before the practical CAM4‐Chem simulation.|Journal of Advances in Modeling Earth Systems|2018|10.1029/2018MS001276|J. Dongarra, S. Tomov, J. Fu, J. Drake, Qingzhao Zhu, Mark Gates, A. Haidar|1.1428571428571428|0
1777|Maximum-Likelihood Estimation of Scintillation Pulse Timing|Including time-of-flight information in positron emission tomography reconstruction increases the signal-to-noise ratio if the timing information is sufficiently accurate. We estimate timing information by analyzing sampled waveforms, where the sampling frequency and number of samples acquired affect the accuracy of timing estimation. An efficient data-acquisition system acquires the minimum number of samples that contains the most timing information for a desired resolution. We describe a maximum-likelihood (ML) estimation algorithm to assign a time stamp to digital pulses. The method is based on a contracting-grid search algorithm that can be implemented in a field-programmable gate array and in graphics processing units. The Fisher-information (FI) matrix quantifies the amount of timing information that can be extracted from the waveforms. FI analyses on different segments of the waveform allow us to determine the smallest amount of data that we need to acquire in order to obtain a desired timing resolution. We describe the model and the procedure used to simulate waveforms for ML estimation and FI analysis, the ML-estimation algorithm and the timing resolution obtained from experimental data using a LaBr3:Ce crystal and two photomultiplier tubes. The results show that for lengthening segments of the pulse, timing resolution approaches a limit. We explored the method as a function of sampling frequency and compared the results to other digital time pickoff methods. This information will be used to build an efficient data-acquisition system with reduced complexity and cost that nonetheless preserves full timing performance.|IEEE Transactions on Radiation and Plasma Medical Sciences|2018|10.1109/TRPMS.2017.2765316|L. Furenlid, Maria Ruiz-Gonzalez, Vaibhav Bora|1.1428571428571428|0
1810|Accelerated DEVS Simulation Using Collaborative Computation on Multi-Cores and GPUs for Fire-Spreading IoT Sensing Applications|Discrete event system specification (DEVS) has been widely used in event-driven simulations for sensor-driven Internet of things (IoT) applications, such as monitoring the spread of fire disaster. Event-driven models for IoT sensor nodes and their communication is described in DEVS and they have to be integrated with continuous models of fire-spreading dynamics so that the hybrid system modeling and simulation approach have to be considered for both continuous behavior of fire-spreading and event-driven communications by large-scale IoT sensor devices. The hybrid-integrated modelling and simulation for fire-spreading in wide area and large-scale IoT devices result in more complex model evaluation, including simulation time synchronization, so that simulation acceleration is important by considering scalability in large-scale IoT-driven applications that sense fire-spreading. In this study, we proposed a scalable simulation acceleration of a DEVS-based hybrid system using heterogeneous architecture based on multi-cores and graphic processing units (GPUs). We evaluated the power consumption comparison of the proposed accelerated-simulation approach in terms of the composition of the event-driven IoT models and continuous fire-spreading models, which are tightly described in differential equations across a large number of cellular models. The demonstrated result shows that the full utilization of CPU-GPU integrated computing resources, on which event-driven models and continuous models are efficiently deployed and optimally distributed, could enable an advantage for high-performance simulation speedup in terms of execution time, although more power consumption is required, but the total energy consumption could be reduced due to fast simulation time.|Applied Sciences|2018|10.3390/APP8091466|Jeonghun Cho, Daejin Park, Seongseop Kim|1.1428571428571428|0
1687|A Fast Synthetic Aperture Radar Raw Data Simulation Using Cloud Computing|Synthetic Aperture Radar (SAR) raw data simulation is a fundamental problem in radar system design and imaging algorithm research. The growth of surveying swath and resolution results in a significant increase in data volume and simulation period, which can be considered to be a comprehensive data intensive and computing intensive issue. Although several high performance computing (HPC) methods have demonstrated their potential for accelerating simulation, the input/output (I/O) bottleneck of huge raw data has not been eased. In this paper, we propose a cloud computing based SAR raw data simulation algorithm, which employs the MapReduce model to accelerate the raw data computing and the Hadoop distributed file system (HDFS) for fast I/O access. The MapReduce model is designed for the irregular parallel accumulation of raw data simulation, which greatly reduces the parallel efficiency of graphics processing unit (GPU) based simulation methods. In addition, three kinds of optimization strategies are put forward from the aspects of programming model, HDFS configuration and scheduling. The experimental results show that the cloud computing based algorithm achieves 4× speedup over the baseline serial approach in an 8-node cloud environment, and each optimization strategy can improve about 20%. This work proves that the proposed cloud algorithm is capable of solving the computing intensive and data intensive issues in SAR raw data simulation, and is easily extended to large scale computing to achieve higher acceleration.|Italian National Conference on Sensors|2017|10.3390/s17010113|Fan Zhang, Haijiang Zhu, Zhixin Li, Wei Li, Ruirui Li, Dandan Su|1.125|0
1767|On the efficient implementation of numerical solvers for the simulation and control of shallow flows on graphical processing units|This work is focused on the design, development and verification of a GPU based computational technique for the simulation and control of the two dimensional shallow water equations. These equations are widely used for modeling free-surface phenomena and they usually require an important computational effort to be solved in practical cases. Traditionally, this effort has been reduced by means of large computational facilities with many computational processing units. The Graphical Processing Units (GPUs) are computing devices which contain a large number of smaller computing elements that, adequately programmed, can perform the same computations as the common CPU units in less time. Thanks to this advance, the simulation tools become attractive also for inverse design purposes. The Shallow Water Equations (SWE) have been traditionally used for forecasting and predictive purposes, but the reduction in computational effort can be used for changing the perspective of their application. In this work, an adjoint based mathematical model is developed and implemented on GPUs. Its application in the one dimensional and two dimensional mathematical model has demonstrated that it can be a useful tool for recovering missing information as well as for control purposes.||2017|10.1080/00221686.2017.1300196|Asier Heradio Lacasta Soto|1.125|0
1943|Single restart with time stamps for computational offloading in a semi-online setting|We study the problem of scheduling n tasks on m + m’ parallel processors, where the processing times on m processors are known while those on the remaining m’ processors are not known a priori. This semi-online model is an abstraction of certain heterogeneous computing systems, e.g., with the m known processors representing local CPU cores and the unknown processors representing remote servers with uncertain availability of computing cycles. Our objective is to minimize the makespan of all tasks. We initially focus on the case m' = 1 and propose a semi-online algorithm termed Single Restart with Time Stamps (SRTS), which has time complexity O(n log n). We derive its competitive ratio in comparison with the optimal offline solution. If the unknown processing times are deterministic, the competitive ratio of SRTS is shown to be either always constant or asymptotically constant in practice, respectively in cases where the processing times are independent and dependent on m. A similar result is obtained when the unknown processing times are random. Furthermore, extending the ideas of SRTS, we propose a heuristic algorithm termed SRTS-Multiple (SRTS-M) for the case m' > 1. Besides the proven competitive ratios, simulation results further suggest that SRTS and SRTS-M give superior performance on average over randomly generated task processing times, substantially reducing the makespan over the best known alternatives. Interestingly, the performance gain is more significant for task processing times sampled from heavy-tailed distributions.|IEEE Conference on Computer Communications|2017|10.1109/INFOCOM.2017.8057149|B. Liang, J. Champati|1.125|0
2012|Visualization of part surfaces for identifying feasible assembly grasp locations|As the first step toward automatic grasp planning in the IPS (Industrial Path Solutions) platform with aDHM (Digital Human Modeling) tool called IMMA (Intelligently Moving Man-ikins), two methods namedPointwise Shortest Distance and Environment Clearance are presented in this paper to color part surfaces by taking environmental constraints into ac-count so that aDHM tooluser can easily identify feasiblegrasp locations for a manikin. Theimplementation of these two methods are robust enoughto handle triangle meshes with commongeometric flaws such as cracks andgaps. In fact, with the help of Visual Shellalgorithmimplemented on Graphics Processing Unit (GPU),even meshes within-consistently oriented normal vectorscan be handled. Currently, we are planning to conduct anindustrial user study, where assembly simulation expertswill be asked to specify man-ikin’s hand grips using IMMA with and without the help of the proposed methods, toprove that the proposed methodsindeedenhance the flexibility and hence the usability for the assembly simulation experts. \n \nVisualization of part surfaces for identifying feasible assembly grasp locations. Available from: https://www.researchgate.net/publication/322581783_Visualization_of_part_surfaces_for_identifying_feasible_assembly_grasp_locations [accessed Jan 19 2018].||2017|10.1016/j.proeng.2017.07.170|J. Carlson, Yi Li, J. Kressin, S. Vajedi|1.125|0
2202|HALWPE: Hardware-assisted light weight performance estimation for GPUs|This paper presents a predictive modeling framework for GPU performance. The key innovation underlying this approach is that performance statistics collected from representative workloads running on current generation GPUs can effectively predict the performance of next-generation GPUs. This is useful when simulators are available for the next-generation device, but simulation times are exorbitant, rendering early design space exploration of microarchitectural parameters and other features infeasible. When predicting performance across three Intel GPU generations (Haswell, Broadwell, Skylake), our models achieved low out-of-sample-errors ranging from 7.45% to 8.91%, while running 30,000–45,000 times faster than cycle-accurate simulation.|Design Automation Conference|2017|10.1145/3061639.3062257|M. Kishinevsky, P. Brisk, Emily J. Shriver, Kenneth O'Neal|1.125|0
2306|GPU Performance Estimation using Software Rasterization and Machine Learning|This paper introduces a predictive modeling framework to estimate the performance of GPUs during pre-silicon design. Early-stage performance prediction is useful when simulation times impede development by rendering driver performance validation, API conformance testing and design space explorations infeasible. Our approach builds a Random Forest regression model to analyze DirectX 3D workload behavior when executed by a software rasterizer, which we have extended with a workload characterizer to collect further performance information via program counters. In addition to regression models, this work produces detailed feature rankings which can provide valuable architectural insight, and accurate performance estimates for an Intel integrated Skylake generation GPU. Our models achieve reasonable out-of-sample-error rates of 14%, with an average simulation speedup of 327x.|ACM Transactions on Embedded Computing Systems|2017|10.1145/3126557|Zack Waters, Ahmed Abousamra, P. Brisk, Emily J. Shriver, Kenneth O'Neal|1.125|0
1708|Light scattering microscopy measurements of single nuclei compared with GPU-accelerated FDTD simulations|Single cell nuclei were investigated using two-dimensional angularly and spectrally resolved scattering microscopy. We show that even for a qualitative comparison of experimental and theoretical data, the standard Mie model of a homogeneous sphere proves to be insufficient. Hence, an accelerated finite-difference time-domain method using a graphics processor unit and domain decomposition was implemented to analyze the experimental scattering patterns. The measured cell nuclei were modeled as single spheres with randomly distributed spherical inclusions of different size and refractive index representing the nucleoli and clumps of chromatin. Taking into account the nuclear heterogeneity of a large number of inclusions yields a qualitative agreement between experimental and theoretical spectra and illustrates the impact of the nuclear micro- and nanostructure on the scattering patterns.|Physics in Medicine and Biology|2016|10.1088/0031-9155/61/7/2749|T. Rothe, A. Kienle, S. Simon, S. Kieß, J. Stark|1.1111111111111112|0
1860|Landrush: Rethinking In-Situ Analysis for GPGPU Workflows|In-situ analysis on the output data of scientific simulations has been made necessary by ever-growing output data volumes and increasing costs of data movement as supercomputing is moving towards exascale. With hardware accelerators like GPUs becoming increasingly common in high end machines, new opportunities arise to co-locate scientific simulations and online analysis performed on the scientific data generated by the simulations. However, the asynchronous nature of GPGPU programming models and the limited context-switching capabilities on the GPU pose challenges to co-locating the scientific simulation and analysis on the same GPU. This paper dives deeper into these challenges to understand how best to co-locate analysis with scientific simulations on the GPUs in HPC clusters. Specifically, our 'Landrush' approach to GPU sharing proposes a solution that utilizes idle cycles on the GPU to provide an improved time-to-answer, that is, the total time to run the scientific simulation and analysis of the generated data. Landrush is demonstrated with experimental results obtained from leadership high-end applications on ORNL's Titan supercomputer, which show that (i) GPU-based scientific simulations have varying degrees of idle cycles to afford useful analysis task co-location, and (ii) the inability to context switch on the GPU at instruction granularity can be overcome by careful control of the analysis kernel launches and software-controlled early completion of analysis kernel executions. Results show that Landrush is superior in terms of time-to-answer compared to serially running simulations followed by analysis or by relying on the GPU driver and hardwired thread dispatcher to run analysis concurrently on a single GPU.|IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing|2016|10.1109/CCGrid.2016.58|S. Klasky, F. Zheng, Anshuman Goswami, G. Eisenhauer, K. Schwan, M. Wolf, Jeffrey S. Young|1.1111111111111112|0
1939|Fluid Simulation: Smoothed Particle Hydrodynamics on the GPU|This report describes the physical concept of fluids as well as a mathematical model for fluids governed by the Navier-Stokes equations. The Smoothed Particle Hydrodynamics method (SPH) for simulating fluids is described, and implementation details of the method are explained. Numerical integration methods such as Euler and Leap-Frog integration are discussed. The presented result is a program with support for real-time simulation and rendering of threedimensional fluids. The properties of the fluid can be adjusted through a graphical interface, and the fluid particles can be rendered either as spheres or as an approximate fluid surface. The program is written in C++ with the SPH simulation implemented in OpenCL and rendering implemented in OpenGL.||2016|10.1007/s40571-015-0097-9|Simon Hedlund, Adam Alsegard, Emil Juopperi, Jonathan Grangien, Benjamin Wiberg|1.1111111111111112|0
1659|Simulation of Time-of-Flight Sensors for Evaluation of Chip Layout Variants|Simulation of time-of-flight (ToF) sensors has mainly been used to evaluate depth data processing algorithms, and existing approaches, therefore, focus on the generation of realistic depth data. Thus, current approaches are of limited usefulness for studying alternatives in sensor chip design, since this application area has different requirements. We propose a new physically based simulation model with a focus on realistic and practical sensor parameterization. The model is suitable for implementation on massively parallel processors such as graphics processing units, to allow fast simulation of many sensor frames across a wide range of parameter sets for meaningful evaluation. We use our implementation to evaluate two alternative approaches in continuous-wave ToF sensor design.|IEEE Sensors Journal|2015|10.1109/JSEN.2015.2409816|A. Kolb, M. Lambers, Stefan Hoberg|1.0|0
1676|Accelerated Spatially Resolved Electrical Simulation of Photovoltaic Devices Using Photovoltaic-Oriented Nodal Analysis|This paper presents photovoltaic-oriented nodal analysis (PVONA), a general and flexible tool for efficient spatially resolved simulations for photovoltaic (PV) cells and modules. This approach overcomes the major problem of the conventional Simulation Program with Integrated Circuit Emphasis-based approaches for solving circuit network models, which is the limited number of nodes that can be simulated due to memory and computing time requirements. PVONA integrates a specifically designed sparse data structure and a graphics processing unit-based parallel conjugate gradient algorithm into a PV-oriented iterative Newton-Raphson solver. This first avoids the complicated and time-consuming netlist parsing, second saves memory space, and third accelerates the simulation procedure. In the tests, PVONA generated the local current and voltage maps of a model with 316 × 316 nodes with a thin-film PV cell in 15 s, i.e., using only 4.6% of the time required by the latest LTSpice package. The 2-D characterization is used as a case study and the potential application of PVONA toward quantitative analysis of electroluminescence are discussed.|IEEE Transactions on Electron Devices|2015|10.1109/TED.2015.2409058|R. Gottschalg, Rajesh Gupta, Archana Sinha, M. Bliss, T. Betts, Xiaofeng Wu|1.0|0
1693|ERF: Energy Research and Forecasting|The Energy Research and Forecasting (ERF) code is a new model that simulates the mesoscale and microscale dynamics of the atmosphere using the latest high-performance computing architectures. It employs hierarchical parallelism using an MPI+X model, where X may be OpenMP on multicore CPU-only systems, or CUDA, HIP, or SYCL on GPU-accelerated systems. ERF is built on AMReX (Zhang et al., 2019, 2021), a block-structured adaptive mesh refinement (AMR) software framework that provides the underlying performance-portable software infrastructure for block-structured mesh operations. The “energy” aspect of ERF indicates that the software has been developed with renewable energy applications in mind. In addition to being a numerical weather prediction model, ERF is designed to provide a flexible computational framework for the exploration and investigation of different physics parameterizations and numerical strategies, and to characterize the flow field that impacts the ability of wind turbines to extract wind energy. The ERF development is part of a broader effort led by the US Department of Energy’s Wind Energy Technologies Office.|Journal of Open Source Software|2023|10.21105/joss.05202|D. Willcox, Riyaz Haque, Weiqun Zhang, E. Quon, Michael Sanders, Pankaj Jha, D. Wiersema, Bruce Perry, A. Lattanzi, A. Almgren, Xingqiu Yuan, J. Mirocha, B. Kosović|1.0|0
1700|Speed Control of PMSM with Finite Control Set Model Predictive Control Using General-purpose Computing on GPU|In this article, novel approach in implementing finite control set predictive control is introduced. Algorithm is implemented using general-purpose computing on graphics processing unit. Predictions are computed using parallel threads on the GPU. Optimal switching state is then selected in dependence on the cost function given by angular speed error and constraints on the current. The algorithm is tested in the PIL simulation using Simulink and Jetson Nano. The ability of the algorithm to ensure the reference tracking and keeping the current within its limits are discussed.|IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society|2020|10.1109/IECON43393.2020.9254381|M. Kozubik, P. Václavek|1.0|0
1741|An implementation of the Social Distances Model using multi-GPU systems|We propose a new approach for using GPUs in large scale simulations of pedestrian evacuation. The Social Distances Model is designed for efficient modeling of pedestrian dynamics. This cellular automata based model, when implemented on the most modern GPUs, can simulate up to 106–108 entities. However, a valuable simulation of pedestrian evacuation must include various factors that govern pedestrian movement, for example, information provided by event organizers and navigation or allocation of other pedestrians. The most common method for introducing such information into simulations is the application of different floor fields. The floor fields provide “local knowledge” that affects pedestrians by modifying the transition functions of an applied cellular automaton. The main disadvantage of this method is its time consuming updating process. We propose a GPU based calculation of static and dynamic floor fields, whereby simulations that use several different floor fields can be efficiently calculated. A single GPU is able to cope with the Social Distance Model calculations, while other GPUs update dynamic floor fields constantly or when required. We also present the classic approach to performing cellular automata based simulations on systems with multiple processing units. The lattice is simply partitioned between the available GPUs. We compare these two approaches in terms of performance and functionality.|The international journal of high performance computing applications|2018|10.1177/1094342016679492|Adrian Klusek, Robert Lubas, P. Topa, Jarosław Wąs|1.0|0
1779|Analysis of GPR Wave Propagation in Complex Underground Structures Using CUDA-Implemented Conformal FDTD Method|Ground penetrating radar (GPR), as a kind of fast, effective, and nondestructive tool, has been widely applied to nondestructive testing of road quality. The finite-difference time-domain method (FDTD) is the common numerical method studying the GPR wave propagation law in layered structure. However, the numerical accuracy and computational efficiency are not high because of the Courant-Friedrichs-Lewy (CFL) stability condition. In order to improve the accuracy and efficiency of FDTD simulation model, a parallel conformal FDTD algorithm based on graphics processor unit (GPU) acceleration technology and surface conformal technique was developed. The numerical simulation results showed that CUDA-implemented conformal FDTD method could greatly reduce computational time and the pseudo-waves generated by the ladder approximation. And the efficiency and accuracy of the proposed method are higher than the traditional FDTD method in simulating GPR wave propagation in two-dimensional (2D) complex underground structures.|International Journal of Antennas and Propagation|2019|10.1155/2019/5043028|H. Fang, Xiaowang Zhang, Haitao Wang, Zibin Wang, X. Ding, Man Yang, Jianwei Lei|1.0|0
1851|3D Modeling of Cities for Virtual Environments|Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display technologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments and typically model the scene as meshes. In this paper, we develop an end-to-end workflow for creating urban scale real world accurate synthetic environments that can be visualized in virtual environments including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction. Four meshing algorithms are evaluated for representation accuracy and city-scale meshes imported into Unity for assessing the quality of the immersive experience.|2021 IEEE International Conference on Big Data (Big Data)|2021|10.1109/BigData52589.2021.9671898|Ye Duan, Haoxiang Zhang, Emily Lattanzio, Jaired Collins, Shizeng Yao, Bimal Balakrishnan, Joshua Fraser, P. Calyam, Calvin Davis, K. Palaniappan|1.0|0
1911|MeshTaichi|Meshes are an indispensable representation in many graphics applications because they provide conformal spatial discretizations. However, mesh-based operations are often slow due to unstructured memory access patterns. We propose MeshTaichi, a novel mesh compiler that provides an intuitive programming model for efficient mesh-based operations. Our programming model hides the complex indexing system from users and allows users to write mesh-based operations using reference-style neighborhood queries. Our compiler achieves its high performance by exploiting data locality. We partition input meshes and prepare the wanted relations by inspecting users' code during compile time. During run time, we further utilize on-chip memory (shared memory on GPU and L1 cache on CPU) to access the wanted attributes of mesh elements efficiently. Our compiler decouples low-level optimization options with computations, so that users can explore different localized data attributes and different memory orderings without changing their computation code. As a result, users can write concise code using our programming model to generate efficient mesh-based computations on both CPU and GPU backends. We test MeshTaichi on a variety of physically-based simulation and geometry processing applications with both triangle and tetrahedron meshes. MeshTaichi achieves a consistent speedup ranging from 1.4× to 6×, compared to state-of-the-art mesh data structures and compilers.|ACM Transactions on Graphics|2022|10.1145/3550454.3555430|Yuanming Hu, Chang Yu, Tiantian Liu, Yi Xu, Ye Kuang|1.0|0
1914|Prevention of Mountain Disasters and Maintenance of Residential Area through Real-Time Terrain Rendering|Climate change increases the frequency of localized heavy rains and typhoons. As a result, mountain disasters, such as landslides and earthworks, continue to occur, causing damage to roads and residential areas downstream. Moreover, large-scale civil engineering works, including dam construction, cause rapid changes in the terrain, which harm the stability of residential areas. Disasters, such as landslides and earthenware, occur extensively, and there are limitations in the field of investigation; thus, there are many studies being conducted to model terrain geometrically and to observe changes in terrain according to external factors. However, conventional topography methods are expressed in a way that can only be interpreted by people with specialized knowledge. Therefore, there is a lack of consideration for three-dimensional visualization that helps non-experts understand. We need a way to express changes in terrain in real time and to make it intuitive for non-experts to understand. In conventional height-based terrain modeling and simulation, there is a problem in which some of the sampled data are irregularly distorted and do not show the exact terrain shape. The proposed method utilizes a hierarchical vertex cohesion map to correct inaccurately modeled terrain caused by uniform height sampling, and to compensate for geometric errors using Hausdorff distances, while not considering only the elevation difference of the terrain. The mesh reconstruction, which triangulates the three-vertex placed at each location and makes it the smallest unit of 3D model data, can be done at high speed on graphics processing units (GPUs). Our experiments confirm that it is possible to express changes in terrain accurately and quickly compared with existing methods. These functions can improve the sustainability of residential spaces by predicting the damage caused by mountainous disasters or civil engineering works around the city and make it easy for non-experts to understand.|Sustainability|2021|10.3390/SU13052950|Eun-Seok Lee, Su-Kyung Sung, B. Shin|1.0|0
1918|Studying performance portability of LAMMPS across diverse GPU‐based platforms|The molecular dynamics simulation software, LAMMPS, utilizes the Kokkos acceleration library to port computation to a diverse set of architectures including those based on GPU accelerators. In addition to Kokkos, LAMMPS contains a vast code base that leverages the CUDA application programming interface using library functions such as cuFFT, CUDA's fast‐fourier transform (FFT) library, and, more recently, also support for AMD's Heterogeneous Interface for Portability (HIP) that is rapidly growing. While preparing LAMMPS tests for the AMD GPU‐based test system precursors to Frontier, we investigated several strategies for accelerating LAMMPS on AMD GPUs, using the AMD Instinct MI100 and MI250X. In this work, we integrated the HIP FFT library, hipFFT, into the particle‐particle particle‐mesh (PPPM) long‐range solver, which allowed the porting of PPPM calculations to the GPUs. Kokkos behavior on the MI100 and MI250X was also investigated through the package kokkos command of LAMMPS, targeting communication, memory usage, and particle grid decomposition. The Tersoff, Reax, Lennard‐Jones (LJ), EAM, Granular, and PPPM potentials were investigated in this effort, and results from these experiments are provided. The selected potentials were run on Spock (AMD Instinct MI100), Crusher (AMD Instinct MI250X), AFW HPC11 (NVIDIA A100) and Summit (NVIDIA V100), for comparison. Operational roofline models were constructed and analyzed for the Tersoff, Reax, and Lennard–Jones potentials on Crusher and Summit.|Concurrency and Computation|2023|10.1002/cpe.7895|V. G. M. Vergara, Nick Hagerty, Arnold Tharrington|1.0|0
1950|Numerical simulation of flattened heat pipe with double heat sources for CPU and GPU cooling application in laptop computers|\n An investigation of the effect of the thermal performance of the flattened heat pipe on its double heat sources acting as central processing unit and graphics processing unit in laptop computers is presented in this work. A finite element method is used for predicting the flattening effect of the heat pipe. The cylindrical heat pipe with a diameter of 6 mm and the total length of 200 mm is flattened into three final thicknesses of 2, 3, and 4 mm. The heat pipe is placed under a horizontal configuration and heated with heater 1 and heater 2, 40 W in combination. The numerical model shows good agreement compared with the experimental data with the standard deviation of 1.85%. The results also show that flattening the cylindrical heat pipe to 66.7 and 41.7% of its original diameter could reduce its normalized thermal resistance by 5.2%. The optimized final thickness or the best design final thickness for the heat pipe is found to be 2.5 mm.|Journal of Computational Design and Engineering|2020|10.1093/jcde/qwaa091|Wisoot Sanhan, N. Kammuang-lue, P. Terdtoon, P. Sakulchangsatjatai, K. Vafai|1.0|0
1952|GPU‐accelerated finite‐difference time‐domain method for dielectric media based on CUDA|The simulation of electromagnetic (EM) waves propagation in the dielectric media is presented using Compute Unified Device Architecture (CUDA) implementation of finite‐difference time‐domain (FDTD) method on graphic processing unit (GPU). The FDTD formulation in the dielectric media is derived in detail, and GPU‐accelerated FDTD method based on CUDA programming model is described in the flowchart. The accuracy and speedup of the presented CUDA‐implemented FDTD method are validated by the numerical simulation of the EM waves propagating into the lossless and lossy dielectric media from the free space on GPU, by comparison with the original FDTD method on CPU. The comparison of the numerical results of CUDA‐implemented FDTD method on GPU and original FDTD method on CPU demonstrates that the CUDA‐implemented FDTD method on GPU can obtain better application speedup performance with reasonable accuracy. © 2016 Wiley Periodicals, Inc. Int J RF and Microwave CAE 26:512–518, 2016.||2016|10.1002/mmce.20997|Song Liu, Xuan Li, S. Zhong, Xi-min Wang|1.0|0
1957|Fast time evolution of matrix product states using the QR decomposition|We propose and benchmark a modified time evolution block decimation (TEBD) algorithm that uses a truncation scheme based on the QR decomposition instead of the singular value decomposition (SVD). The modification reduces the scaling with the dimension of the physical Hilbert space $d$ from $d^3$ down to $d^2$. Moreover, the QR decomposition has a lower computational complexity than the SVD and allows for highly efficient implementations on GPU hardware. In a benchmark simulation of a global quench in a quantum clock model, we observe a speedup of up to three orders of magnitude comparing QR and SVD based updates on an A100 GPU.|Physical review B|2022|10.1103/PhysRevB.107.155133|J. Hauschild, Jakob Unfried, F. Pollmann|1.0|0
1971|Machine-Learned Molecular Surface and Its Application to Implicit Solvent Simulations.|Implicit solvent models, such as Poisson-Boltzmann models, play important roles in computational studies of biomolecules. A vital step in almost all implicit solvent models is to determine the solvent-solute interface, and the solvent excluded surface (SES) is the most widely used interface definition in these models. However, classical algorithms used for computing SES are geometry-based, so that they are neither suitable for parallel implementations nor convenient for obtaining surface derivatives. To address the limitations, we explored a machine learning strategy to obtain a level set formulation for the SES. The training process was conducted in three steps, eventually leading to a model with over 95% agreement with the classical SES. Visualization of tested molecular surfaces shows that the machine-learned SES overlaps with the classical SES in almost all situations. Further analyses show that the machine-learned SES is incredibly stable in terms of rotational variation of tested molecules. Our timing analysis shows that the machine-learned SES is roughly 2.5 times as efficient as the classical SES routine implemented in Amber/PBSA on a tested central processing unit (CPU) platform. We expect further performance gain on massively parallel platforms such as graphics processing units (GPUs) given the ease in converting the machine-learned SES to a parallel procedure. We also implemented the machine-learned SES into the Amber/PBSA program to study its performance on reaction field energy calculation. The analysis shows that the two sets of reaction field energies are highly consistent with a 1% deviation on average. Given its level set formulation, we expect the machine-learned SES to be applied in molecular simulations that require either surface derivatives or high efficiency on parallel computing platforms.|Journal of Chemical Theory and Computation|2021|10.1021/acs.jctc.1c00492|R. Luo, Zekai Zhao, Haixin Wei|1.0|0
2023|Molecular Dynamics of Heterogeneous Systems on GPUs and Their Application to Nucleation in Gas Expanding to a Vacuum.|Expansion of water vapor through a small orifice to a vacuum produces liquid or frozen clusters which in the experiment serve as model particles for atmospheric aerosols. Yet, there are controversies about the shape of these clusters, suggesting that the nucleation process is not fully understood. Such questions can be answered by molecular dynamics simulations; however, they require microsecond-scale runs with thousands of molecules and accurate energy conservation. The available highly parallel codes typically utilize domain decomposition and are inefficient for heterogeneous systems as clusters in a dilute gas. In this work, we present an implementation of molecular dynamics on graphics processing units based on the Verlet list and apply it to several systems for which experimental data are available. We reproduce sufficiently sized clusters but not the experimentally observed clusters of irregular shape.|Journal of Chemical Theory and Computation|2021|10.1021/acs.jctc.1c00736|David Celný, Jiří Kolafa, Martin Klíma|1.0|0
2045|Continuous Control With Swarm Intelligence Based Value Function Approximation|Value function approximation, such as Q-learning, is widely used in the discrete control rather than the continuous one because the optimal action in the discrete setting is more easily selected. Optimizing the action is a non-convex optimization problem with respect to the complex value function. Some notable studies simplify the non-convex optimization problem by assuming the value function as quadratic in the actions or by discretizing the action space. However, the performance of the output policy will decline if these studies’ premises do not hold. In order to address the problem, we propose a framework that combines swarm intelligence algorithms with value-based Reinforcement Learning, where the swarm intelligence algorithms are employed to search for the optimal action with respect to the state and the value function. To ensure the correctness of this framework, we conditionally claim the convergence rate of swarm intelligence algorithms with high probability. We then implement it by searching the batch optimal actions to various states on the GPU platform for the batch training. Furthermore, we employ the population-based atomic actions for the compatibility with the existing related work about solving discrete control problems. Four classical control models and four robot simulation environments are utilized in the comparisons. According to empirical results, our framework outputs a policy comparable with that of the policy-based algorithms by 10% timesteps in the continuous control. Note to Practitioners—This paper is motivated by the exploration-exploitation dilemma of Reinforcement Learning to solve continuous control tasks. To balance the exploration and exploitation, the stochastic exploration and the prioritized exploration are roughly two feasible ways, where the prioritized one is a better choice due to the higher data efficiency than the stochastic one, e.g. $\varepsilon $ -greedy. Normally, the prioritized exploration works well in the value-based Reinforcement Learning algorithms rather than the policy-based ones; meanwhile, the policy-based algorithms are more suitable to continuous control tasks than the value-based ones. To tackle this conflict, we especially design a particle swarm optimization to maximize the Q-value of action in Q-learning. Our design can be hybridized by various swarm intelligence and value-based Reinforcement Learning algorithms. Also, it can be embedded in most intelligent control systems easily. The aim of this study is to solve the continuous control tasks by value-based algorithms as the first step of applying the prioritized exploration. The simulative results verify the effectiveness and efficiency of our design.|IEEE Transactions on Automation Science and Engineering|2024|10.1109/TASE.2023.3234961|Xuelian Li, Jianqing Wu, Bi Wang, Junfu Chen, Yang Chen, Bowen Zeng|1.0|0
2210|MOVING TOWARDS A 5D CARDIAC MODEL|Abstract— The medical diagnosis process requires several steps to identify the types of cardiac pathologies. The \nsegmentation step is used to determine the measurements for cardiac abnormalities on the short axis of the 4D acquired in MRI, but this phase remains limited on the blood flow sequences. The MRI modality allow to the experts to quantify the stenosis and the regurgitation of aortic blood flow. The parameters extracted from the flow sequences, after segmentation, make it possible to identify the valvular pathologies, but they are not sufficient to complete the medical prognosis as well as the lack of precision of these measurements. In this paper, we propose to make a coupling between the 4D cardiac cuts with their study of blood flow through the technique of registration and reconstruction. The interest of the purpose of the blood flow as fifth dimension is to improve the accuracy of the cardiac parameters and the extracting of measurements for valvulopathies in the process of assistance to the decision for the experts An example was introduced in this context through the development of cloud services for patient-specific simulations of blood flows through aortic valves as well as an OsiriX software for 5D visualization that combines a 4D sequence and the functional flow dimension. In this framework, we proposed a processing chain to lead towards a 5D solution. Another problem is raised is the choice of the appropriate architecture to solve the problem of hybrid parallelization for the processing of these cardiac images. To test the constraints of time of the concept of 5D, we need a GPU graphics processor acquired in MRI, as well as a CPU processor to perform the complexity of calculation and the operations applied to the algorithms of image processing.|Journal of Flow Visualization and Image Processing|2019|10.1615/JFLOWVISIMAGEPROC.2018027194|M. Tagina, Houneida Sakly, R. Mahmoudi, M. Akil, Mourad Said|1.0|0
2232|Real-Time Surface-Based Volume Constraints on Mass-Spring Model in Unity3D|This paper describes a parallel method to simulate real-time 3D deformable objects using volume preservation constraints on a mass-spring model (MSM) to achieve plausible results in real-time performance. Instead of considering a volumetric mesh which is mostly used to simulate deformable objects, we purely take the surface of the 3D object into account to reduce the time complexity and obtain a high-quality deformable. In the conventional MSM, we can simply control the shape of the deformable object through the stiffness and damping coefficients which is beneficial for our volume constraint. We use the divergence theorem and implicit constraint enforcement scheme to maintain the volume of the object and deform it freely. The surface-based volume constraint is applied to correct the force of the spring network. The proposed algorithm was designed on compute shader in Unity3D and runs outside the normal rendering pipeline in the graphics processing unit (GPU) in order to utilize the massive parallel process to accelerate the performance of the simulation. The performance of the simulation can be accelerated by using the parallel processing method on the GPU with an average speedup factor of 4.27 using the conventional mass-spring method, and an average speedup factor of 2.54 for the volume preservation constraint method. We present several scenes which demonstrate the volume-preserving deformations using the conventional mass-spring method and volume-preservation constraint method and the volume loss of deformable objects for all 3D models is compared. The volume preservation method obtains a volume loss significantly lower than the conventional MSM for all experimental tests.|IEEE Access|2023|10.1109/ACCESS.2023.3245130|M. Hong, Min-Hyung Choi, Hongly Va|1.0|0
2235|Fast computational aeroelastic analysis of helicopter rotor blades|The use of a new aeroelastic computer framework called Flexit is described and the framework is used to analyse the dynamic aeroelastic behaviour of a four-bladed helicopter main rotor. Flexit implements a loose coupling between unsteady vortex lattice method (UVLM) and numerical solution of the inhomogeneous Euler-Bernoulli partial differential equation (PDE). The framework is fast because most of the intensive computational functionality is performed on GPU using NVIDIA CUDA technology, and this makes it suitable for use in the early design stages. The UVLM algorithm uses a free wake model, and solution of the Euler-Bernoulli PDE is approximated using a finite difference algorithm that includes a term to take account of centrifugal forces. The results of simulations are compared with analysis performed with CFD and FSI tools.||2018|10.2514/6.2018-1044|Mohammad M. Lone, S. Weber, D. Fleischmann|1.0|0
2242|Fluid computational model for mineral and vegetal pigments diffusing in Chinese color-ink painting|In this paper, simulation of artistic vegetable and mineral pigments diffusing effects of Chinese color-ink painting is presented, using a novel physical model according to the Second Fick's diffusing law and Brownian motion theory. Due to the fact that generation of most art effects depends on complicated pigment-water motion simulation such as diffusing and pigment mixing on and under traditional fabric cotton paper (Xuan paper) surface, the proposed model is found effective for simulating the pigment-water motion in art creating process. Implemented on the GPU, the simulation operations in our system can be accomplished in a real-time manner. The effectiveness of the proposed techniques is validated in our developed Digital Painting System, where various art effects can be successfully re-produced including the Initial area-Darkened initial edge-Diffusion area Lightened diffusion edge (IDDL) effect for vegetable pigments, the Initial area-Darkened initial edge-Diffusion area-Darkened diffusion edge (IDDD) effect for mineral pigments, and multistroke superimposing effects and achieves. In addition, quantitative evaluation is also introduced and shows superior performance of the proposed model in comparison with state-of-the-art techniques.|ACM Symposium on Interactive 3D Graphics and Games|2018|10.1109/i-span.2018.00039|Meijun Sun, Yahong Han, Ziqi Zhou, Zheng Wang, Jinchang Ren|1.0|0
2254|Improved hybrid parallel strategy for density matrix renormalization group method|We propose a new heterogeneous parallel strategy for the density matrix renormalization group (DMRG) method in the hybrid architecture with both central processing unit (CPU) and graphics processing unit (GPU). Focusing on the two most time-consuming sections in the finite DMRG sweeps, i.e., the diagonalization of superblock and the truncation of subblock, we optimize our previous hybrid algorithm to achieve better performance. For the former, we adopt OpenMP application programming interface on CPU and use our own subroutines with higher bandwidth on GPU. For the later, we use GPU to accelerate matrix and vector operations involving the reduced density matrix. Applying the parallel scheme to the Hubbard model with next-nearest hopping on the 4-leg ladder, we compute the ground state of the system and obtain the charge stripe pattern which is usually observed in high temperature superconductors. Based on simulations with different numbers of DMRG kept states, we show significant performance improvement and computational time reduction with the optimized parallel algorithm. Our hybrid parallel strategy with superiority in solving the ground state of quasi-two dimensional lattices is also expected to be useful for other DMRG applications with large numbers of kept states, e.g., the time dependent DMRG algorithms.||2020|10.1088/1674-1056/ab8a42|C. Cheng 程, H. Luo 罗, F. Chen 陈|1.0|0
2282|GPU-accelerated Pelton turbine simulation using finite volume particle method coupled with linear eddy viscosity models|The numerical investigation of the unsteady flow patterns around a Pelton bucket can be helpful to improve the overall turbine efficiency by optimizing the bucket design based on identified loss mechanisms. Since the flow is highly turbulent, modeling the effect of turbulence can bring about improved predictions. In this paper, two RANS-based eddy viscosity models (namely the standard and realizable k-ε) have been implemented as a module in a particle-based in-house solver, GPU-SHPEROS. A scalable wall function based on the log-law has been utilized to model the flow in the near-wall region. The solver has been accelerated on GPUs and is based on the Finite Volume Particle Method (FVPM), which is a locally conservative and consistent particle-based method including many of the attractive features of both particle-based methods (e.g. SPH) and conventional mesh-based methods (e.g. FVM). As a mesh-free method based on the Arbitrary Lagrangian Eulerian (ALE) formulation, FVPM is robust in handling free surface flows and large boundary deformations, such as the ones found in rotating Pelton buckets. The validation of the turbulence models implementation within FVPM is presented for internal and free surface flows. Then, the effectiveness of the turbulence models in the case of rotating Pelton buckets is assessed by comparing the predicted torque time histories to experimental data acquired on a model-scale test rig.|IOP Conference Series: Earth and Environment|2019|10.1088/1755-1315/240/7/072018|Takashi Kumashiro, E. Jahanbakhsh, K. Tani, S. Leguizamón, François Avellan, S. Alimirzazadeh, A. Maertens|1.0|0
2305|Parallel Multi-Hypothesis Algorithm for Criticality Estimation in Traffic and Collision Avoidance|Due to the current developments towards autonomous driving and vehicle active safety, there is an increasing necessity for algorithms that are able to perform complex criticality predictions in real-time. Being able to process multi-object traffic scenarios aids the implementation of a variety of automotive applications such as driver assistance systems for collision prevention and mitigation as well as fall-back systems for autonomous vehicles. We present a fully model-based algorithm with a parallelizable architecture. The proposed algorithm can evaluate the criticality of complex, multi-modal (vehicles and pedestrians) traffic scenarios by simulating millions of trajectory combinations and detecting collisions between objects. The algorithm is able to estimate upcoming criticality at very early stages, demonstrating its potential for vehicle safety-systems and autonomous driving applications. An implementation on an embedded system in a test vehicle proves in a prototypical manner the compatibility of the algorithm with the hardware possibilities of modern cars. For a complex traffic scenario with 11 dynamic objects, more than 86 million pose combinations are evaluated in 21 ms on the GPU of a Drive PX 2.|2019 IEEE Intelligent Vehicles Symposium (IV)|2019|10.1109/IVS.2019.8814015|Eduardo Sánchez Morales, M. Botsch, Tobias Dirndorfer, Richard Membarth, Alexander Kammenhuber, P. Slusallek, A. Gaull, C. Lauer|1.0|0
1798|Efficient Lava Flows Simulations with OpenCL: A Preliminary Application for Civil Defence Purposes|GPGPU (General Purpose computing on Graphics Processing Units) has marked a revolution in the field of Parallel Computing allowing to achieve computational performance unimaginable until a few years ago. This hardware has proven to be extremely reliable and suitable to simulate Cellular Automata (CA) models for modeling complex systems whose evolution can be described in terms of local interactions. Starting from previous GPGPU implementations of CA models with CUDA, this paper presents an effective implementation of a well-known numerical model for simulating lava flows on Graphical Processing Units (GPU) based on the OpenCL (Open Computing Language) standard. In addition, a preliminary Civil Defence application related Hazard maps of an area located at Mt. Etna volcano (South Italy), confirms the validity of OpenCL and both low-cost and high-end graphics hardware as an alternative to expensive solutions for the simulation of CA models.|International Conference on P2P, Parallel, Grid, Cloud and Internet Computing|2015|10.1109/3PGCIC.2015.107|M. Macri, D. D'Ambrosio, W. Spataro, A. Rango, D. Spataro|0.9|0
2123|Calculation of Heat Power in Solar Tower Power Plant Based on GPU|Since the solar energy is intermittent and unstable, it's important to estimate the heat power in the solar tower power plant for the optimization of the whole system. The physical model of the heliostat field and the energy conversion model are built to analyze the components of the optical efficiency. Non-parallel incident rays are generated according to the divergence of the sunlight and the solar disk. A new fast calculation method for heat power is presented based on GPU. By exploiting the Single Instruction Multiple Data(SIMD) architecture of GPU, the Monte Carlo ray tracing method is utilized to track the movement of each ray. With the optical efficiency of the heliostat field being calculated, the heat power on the receiver can be obtained with high speed and high precision in the ray tracing procedure. The rapidity and effectiveness of the proposed method is demonstrated by the simulation results of a simulated heliostat field similar to e Solar plant and a real heliostat field.||2015|10.1007/s11595-015-1094-9|Zhou Yi-y|0.9|0
1846|Numerical simulation of disperse particle flows on a graphics processing unit|In both nature and technology, we commonly encounter solid particles being carried within fluid flows, from dust storms to sediment erosion and from food processing to energy generation. The motion of uncountably many particles in highly dynamic flow environments characterizes the tremendous complexity of such phenomena. While methods exist for the full-scale numerical simulation of such systems, current computational capabilities require the simplification of the numerical task with significant approximation using closure models widely recognized as insufficient. There is therefore a fundamental need for the investigation of the underlying physical processes governing these disperse particle flows. In the present work, we develop a new tool based on the Physalis method for the first-principles numerical simulation of thousands of particles (a small fraction of an entire disperse particle flow system) in order to assist in the search for new reduced-order closure models. We discuss numerous enhancements to the efficiency and stability of the Physalis method, which introduces the influence of spherical particles to a fixed-grid incompressible Navier-Stokes flow solver using a local analytic ii||2016|10.1016/j.cpc.2016.05.006|Adam J. Sierakowski|0.8888888888888888|0
2150|ROOM ACOUSTICS SIMULATION USING GPU|Previously many acoustic simulation methods have been introduced. One of The alternatives are Wave-based simulation, but their use for wideband acoustic simulation has been limited by the computing power of available hardware. Nowadays, the computational power, programmability processing power of graphics processing units have improved, and therefore wave-based simulation methods have become potential and efficient alternatives. In many scientific and engineering applications computational modelling and simulation of acoustic spaces is fundamental. The demands vary widely, from interactive simulation in computer games, to highly accurate offline room acoustic computations for musical halls, theatres and other man-made architectural or computer-aided design structures. Capacious multi-room spaces or aircraft cabins with complex geometric shape and properties of the material, as well as outdoor spaces including urban models and open landscapes are inclusive of acoustic spaces. For example, aero plane and jet manufacturers establish large noise engineering laboratories for computation related to noise of aircraft. Other designers of man-made structures such as automobile manufacturers, habitation and urban layouts use acoustic simulation technologies for enhancing design and reducing needs for physical prototypes.||2016|10.4271/2016-01-0342|Chirag Batra|0.8888888888888888|0
2311|Seven Obstacles in the Way of Standard-Compliant Parallel SystemC Simulation|The IEEE 1666-2011 standard defines SystemC based on traditional discrete event simulation (DES) and sequential co-routine semantics, despite explicit parallelism in the model and ample parallel processor cores available in today's host computers. In order to evolve the SystemC standard toward faster parallel DES, substantial hurdles must be overcome. This letter identifies seven obstacles in the standard that stand in the way of efficient parallel SystemC simulation, namely the co-routine semantics, simulator state, lack of thread safety, the role of channels, TLM-2.0, sequential mindset, and temporal decoupling. For each obstacle, we discuss the problem and propose a potential solution toward truly parallel SystemC. This letter to the editor is meant to identify difficulties with IEEE SystemC and stimulate fruitful discussion in the community.|IEEE Embedded Systems Letters|2016|10.1109/LES.2016.2617284|R. Dömer|0.8888888888888888|0
1794|General purpose graphics-processing-unit implementation of cosmological domain wall network evolution.|Topological defects unavoidably form at symmetry breaking phase transitions in the early universe. To probe the parameter space of theoretical models and set tighter experimental constraints (exploiting the recent advances in astrophysical observations), one requires more and more demanding simulations, and therefore more hardware resources and computation time. Improving the speed and efficiency of existing codes is essential. Here we present a general purpose graphics-processing-unit implementation of the canonical Press-Ryden-Spergel algorithm for the evolution of cosmological domain wall networks. This is ported to the Open Computing Language standard, and as a consequence significant speedups are achieved both in two-dimensional (2D) and 3D simulations.|Physical Review E|2017|10.1103/PhysRevE.96.043310|C. Martins, J. Correia|0.875|0
1816|Tree-based mesh-refinement GPU-accelerated tsunami simulator for real-time operation|Abstract. This paper presents a fast and accurate tsunami real-time\noperational model to compute across ocean-wide simulations completely\non GPU (graphics processing unit). The spherical shallow water equations are solved using the method of\ncharacteristics and upwind cubic interpolation to provide high accuracy and\nstability. A customized, user interactive, tree-based mesh-refinement method\nis implemented based on distance from the coast and focal areas to generate\na memory-efficient domain with resolutions of up to 50 m. Three\nspecialized and optimized GPU kernels (Wet, Wall and Inundation) are\ndeveloped to\ncompute the domain block mesh. Multi-GPU is used to further speed up the\ncomputation, and a weighted Hilbert space-filling curve is used to produce a\nbalanced workload. Hindcasting of the 2004 Indonesian tsunami is presented\nto validate and compare the agreement of the arrival times and main peaks at\nseveral gauges. Inundation maps are also produced for Kamala and Hambantota\nto validate the accuracy of our model. Test runs on three Tesla P100 cards\non Tsubame 3.0 could fully simulate 10 h in just under 10 min wall-clock time.\n|Natural Hazards and Earth System Sciences|2017|10.5194/NHESS-18-2561-2018|Marlon Arce Acuña, T. Aoki|0.875|0
2178|A Boundary and Texture Preserving Mesh Simplification Algorithm for Virtual Reality|With the increasing accessibility of the mobile head-mounted displays (HMDs), mobile virtual reality (VR) systems are finding applications in various areas. However, mobile HMDs are highly constrained with limited graphics processing units (GPUs), low processing power and onboard memory. Hence, VR developers must be cognizant of the number of polygons contained within their virtual environments to avoid rendering at low frame rates and inducing simulator sickness. The most robust and rapid approach to keeping the overall number of polygons low is to use mesh simplification algorithms to create low-poly versions of preexisting, high-poly models. Unfortunately, most existing mesh simplification algorithms cannot adequately handle meshes with lots of boundaries or non-manifold meshes, which are common attributes of 3D models made with computer-aided design tools.; AB@In this paper, we present a high-fidelity mesh simplification algorithm specifically designed for VR. This new algorithm, QEM4VR, addresses the deficiencies of prior quadric error metric (QEM) approaches by leveraging the insight that the most relevant boundary edges lie along curvatures while linear boundary edges can be collapsed. Additionally, our QEM4VR algorithm preserves key surface properties, such as normals, texture coordinates, colors, and materials. It pre-processes the 3D models and generate their low-poly approximations offline. We used six publicly available, high-poly models, with and without textures to compare the accuracy and fidelity of our QEM4VR algorithm to previous QEM variations. We also performed a frame rate analysis with original high-poly models and low-poly models obtained using QEM4VR and previous QEM variations. Our results indicate that QEM4VR creates low-poly, high-fidelity virtual environments for VR applications on devices that are constrained by the low number of polygons they can work with.|ACM SIGMM Conference on Multimedia Systems|2017|10.1145/3083187.3083188|Ryan P. McMahan, B. Prabhakaran, K. Bahirat, Chengyuan Lai|0.875|0
2165|Resistive and Multi-fluid RMHD on Graphics Processing Units|In this work we present a proof of concept of CUDA-capable, resistive, multi-fluid models of relativistic magnetohydrodynamics (RMHD). Resistive and multi-fluid codes for simulating models of RMHD suffer from stiff source terms, so it is common to implement a set of semi-implicit time integrators to maintain numerical stability. We show, for the first time, that finite volume IMEX schemes for resistive and two-fluid models of RMHD can be accelerated by execution on graphics processing units, significantly reducing the demand set by these kinds of problems. We report parallel speed-ups of over 21× using double-precision floating-point accuracy, and highlight the optimization strategies required for these schemes, and how they differ from ideal RMHD models. The impact of these results is discussed in the context of the next-generation simulations of neutron star mergers.|Astrophysical Journal Supplement Series|2018|10.3847/1538-4365/aaf1b0|A. Wright, I. Hawke|0.8571428571428571|0
2307|A rapid GPU-based Monte Carlo simulation tool for individualized dose estimations in CT|The rising awareness towards the risks associated with CT radiation has pushed forward the case for patient- specific dose estimation, one of the prerequisites for individualized monitoring and management of radiation exposure. The established technique of using Monte Carlo simulations to provide such dose estimates is computationally intensive, thus limiting their utility towards timely assessment of clinically relevant questions. To overcome this impediment, we have developed a rapid Monte Carlo simulation tool based on the MC-GPU frame- work for individualized dose estimation in CT. This tool utilizes the multi-threaded x-ray transport capability of MC-GPU, scanner-specific geometry and voxelized patient-specific models to produce realistic estimates of radiation dose. To demonstrate its utility, we utilized this tool to provide scanner-specific (LightSpeed VCT, GE Healthcare) organ dose estimates in abdominopelvic CT for a virtual population of 58 adult XCAT patient models. To gauge the accuracy of these estimates, the organ dose values from this new tool were compared against those from a previously published tool based on PENELOPE framework. The comparisons demonstrated the capability of our new simulation tool to produce dose estimates that agree with the published data within 5% for organs within primary field while simultaneously providing speedups as high as 70x over a CPU cluster-based execution model. This high accuracy of dose estimates coupled with the demonstrated speedup provides a viable model for rapid and personalized dose estimation.|Medical Imaging|2018|10.1117/12.2294965|Wanyi Fu, Shobhit Sharma, A. Kapadia, E. Samei, E. Abadi, W. Segars|0.8571428571428571|0
1978|A Parallel Image Registration Algorithm Based on a Lattice Boltzmann Model|Image registration is a key pre-procedure for high level image processing. However, taking into consideration the complexity and accuracy of the algorithm, the image registration algorithm always has high time complexity. To speed up the registration algorithm, parallel computation is a relevant strategy. Parallelizing the algorithm by implementing Lattice Boltzmann method (LBM) seems a good candidate. In consequence, this paper proposes a novel parallel LBM based model (LB model) for image registration. The main idea of our method consists in simulating the convection diffusion equation through a LB model with an ad hoc collision term. By applying our method on computed tomography angiography images (CTA images), Magnet Resonance images (MR images), natural scene image and artificial images, our model proves to be faster than classical methods and achieves accurate registration. In the continuity of 2D image registration model, the LB model is extended to 3D volume registration providing excellent results in domain such as medical imaging. Our method can run on massively parallel architectures, ranging from embedded field programmable gate arrays (FPGAs) and digital signal processors (DSPs) up to graphics processing units (GPUs).|Inf.|2019|10.3390/info11010001|Dongxiang Lu, G. Courbebaisse, Yu Chen|0.8333333333333334|0
1990|Performance analysis of fully explicit and fully implicit solvers within a spectral element shallow-water atmosphere model|Explicit Runge–Kutta methods and implicit multistep methods utilizing a Newton–Krylov nonlinear solver are evaluated for a range of configurations of the shallow-water dynamical core of the spectral element community atmosphere model to evaluate their computational performance. These configurations are designed to explore the attributes of each method under different but relevant model usage scenarios including varied spectral order within an element, static regional refinement, and scaling to the largest problem sizes. This analysis is performed within the shallow-water dynamical core option of a full climate model code base to enable a wealth of simulations for study, with the aim of informing solver development within the more complete hydrostatic dynamical core used for climate research. The limitations and benefits to using explicit versus implicit methods, with different parameters and settings, are discussed in light of the trade-offs with Message Passing Interface (MPI) communication and memory and their inherent efficiency bottlenecks. Given the performance behavior across the configurations analyzed here, the recommendation for future work using the implicit solvers is conditional based on scale separation and the stiffness of the problem. For the regionally refined configurations, the implicit method has about the same efficiency as the explicit method, without considering efficiency gains from a preconditioner. The potential for improvement using a preconditioner is greatest for higher spectral order configurations, where more work is shifted to the linear solver. Initial simulations with OpenACC directives to utilize a Graphics Processing Unit (GPU) when performing function evaluations show improvements locally, and that overall gains are possible with adjustments to data exchanges.|The international journal of high performance computing applications|2019|10.1177/1094342017736373|P. Worley, D. J. Gardner, C. Woodward, M. Norman, Richard Archibald, M. Taylor, K. Evans|0.8333333333333334|0
2091|Machine learned domain decomposition scheme applied to parallel multi-scale muscle simulation|Since multi-scale models of muscles rely on the integration of physical and biochemical properties across multiple length and time scales, they are highly processor and memory intensive. Consequently, their practical implementation and usage in real-world applications is limited by high computational requirements. There are various reported solutions to the problem of parallel computation of various multi-scale models, but due to their inherent complexity, load balancing remains a challenging task. In this article, we present a novel load balancing method for multi-scale simulations based on finite element (FE) method. The method employs a computationally simple single-scale model and machine learning in order to predict computational weights of the integration points within a complex multi-scale model. Employing the obtained weights, it is possible to improve the domain decomposition prior to the complex multi-scale simulation run and consequently reduce computation time. The method is applied to a two-scale muscle model, where the FE on macroscale is coupled with Huxley’s model of cross-bridge kinetics on the microscale. Our massive parallel solution is based on the static domain decomposition policy and operates in a heterogeneous (central processing units + graphics processing units) environment. The approach has been verified on a real-world example of the human tongue, showing high utilization of all processors and ensuring high scalability, owing to the proposed load balancing scheme. The performance analysis shows that the inclusion of the prediction of the computational weights reduces execution time by about 40% compared to the run which uses a trivial load balancer which assumes identical computational weights of all micro-models. The proposed domain decomposition approach possesses a high capability to be applied in a variety of multi-scale models based on the FE method.|The international journal of high performance computing applications|2019|10.1177/1094342019833151|M. Ivanovic, M. Svicevic, A. Kaplarevic-Malisic, S. Mijailovich, B. Stojanovic|0.8333333333333334|0
2143|Compact and high-throughput parameterisable architectures for memory-based FFT algorithms|Designers must carefully choose the best-suited fast Fourier transform (FFT) algorithm among various available techniques for the custom implementation that meets their design requirements, such as throughput, latency, and area. This article, to the best of authors' knowledge, is the first to present a compact and yet high-throughput parameterisable hardware architecture for implementing different FFT algorithms, including radix-2, radix-4, radix-8, mixed-radix, and split-radix algorithms. The designed architectures are fully parameterisable to support a variety of transform lengths and variable word-lengths. The FFT algorithms have been modelled and simulated in double-precision floating-point and fixed-point representations using authors' custom-developed library of numerical operations. The designed FFT architectures are modelled in Verilog hardware description language and their cycle-accurate and bit-true simulation results are verified against their fixed-point simulation models. The characteristics and implementation results of various FFT architectures on a Xilinx Virtex-7 FPGA are presented. Compared to recently published works, authors' memory-based FFT architectures utilise less reconfigurable resources while maintaining comparable or higher operating frequencies. The ASIC implementation results in a standard 45-nm CMOS technology are also presented for the designed memory-based FFT architectures. The execution times of FFTs on a workstation and a graphics processing unit are compared against authors' FPGA implementations.|IET Circuits Devices Syst.|2019|10.1049/IET-CDS.2018.5556|A. Alimohammad, D. Valencia|0.8333333333333334|0
1672|Optimizing communications in multi-GPU Lattice Boltzmann simulations|An increasingly large number of scientific applications run on large clusters based on GPU systems. In most cases the large scale parallelism of the applications uses MPI, widely recognized as the de-facto standard for building parallel applications, while several programming languages are used to express the parallelism available in the application and map it onto the parallel resources available on GPUs. Regular grids and stencil codes are used in a subset of these applications, often corresponding to computational “Grand Challenges”. One such class of applications are Lattice Boltzmann Methods (LB) used in computational fluid dynamics. The regular structure of LB algorithms makes them suitable for processor architectures with a large degree of parallelism like GPUs. Scalability of these applications on large clusters requires a careful design of processor-to-processor data communications, exploiting all possibilities to overlap communication and computation. This paper looks at these issues, considering as a use case a state-of-the-art two-dimensional LB model, that accurately reproduces the thermo-hydrodynamics of a 2D-fluid obeying the equation-of-state of a perfect gas. We study in details the interplay between data organization and data layout, data-communication options and overlapping of communication and computation. We derive partial models of some performance features and compare with experimental results for production-grade codes that we run on a large cluster of GPUs.|International Symposium on High Performance Computing Systems and Applications|2015|10.1109/HPCSim.2015.7237021|D. Marchi, R. Tripiccione, S. Schifano, E. Calore|0.8|0
1890|Physically‐based forehead animation including wrinkles|Physically‐based animation techniques enable more realistic and accurate animation to be created. We present a fully physically‐based approach for efficiently producing realistic‐looking animations of facial movement, including animation of expressive wrinkles. This involves simulation of detailed voxel‐based models using a graphics processing unit‐based total Lagrangian explicit dynamic finite element solver with an anatomical muscle contraction model, and advanced boundary conditions that can model the sliding of soft tissue over the skull. The flexibility of our approach enables detailed animations of gross and fine‐scale soft‐tissue movement to be easily produced with different muscle structures and material parameters, for example, to animate different aged skins. Although we focus on the forehead, our approach can be used to animate any multi‐layered soft body. © 2014 The Authors. Computer Animation and Virtual Worlds published by John Wiley & Sons, Ltd.|Comput. Animat. Virtual Worlds|2015|10.1002/cav.1565|Mark Warburton, S. Maddock|0.8|0
2056|Cosine-type weighted hybrid absorbing boundary based on the second-order Higdon boundary condition and its GPU implementation|In this paper, we analyze the absorption effect of the hybrid absorbing boundary condition (ABC) with various hybrid modes, and propose a cosine-type optimized weighted hybrid mode taking into account the boundary reflection intensity of the inner and outer boundaries of transition area. Additionally, we derive a new finite-difference scheme of the second-order Higdon ABC and the corner equation for Graphic Processing Unit (GPU) acceleration. On this basis, a new type of second-order Higdon hybrid ABC applicable for GPU acceleration is established in the acoustic finite-difference modeling. Numerical experiments demonstrate that the proposed cosine-type weighted hybrid mode can achieve a better absorption effect compared with other weighted hybrid modes; the second-order Higdon ABC based on the proposed new finite-difference scheme can effectively improve the GPU speed-up ratio and is more effective in large-scale wavefield high-precision simulation based on GPU acceleration.||2020|10.1093/jge/gxz102|Zhang Hongyang, Zhang Ruiqi, Xie Chuang, Peng Song, Jun Tan, Zhang Chao, Zhang Xiaobo|0.8|0
2057|A Low Cost High Performance Computing Platform for Cellular Nonlinear Networks Using Python for CUDA|A novel platform (hardware and software) for complex systems modeling is proposed. It exploits the newest developments in both software (Continuum's - Anaconda's Numba and Numbapro Python packages) and hardware (the use of parallel computation on GPU provided by the CUDA computing platform) to ensure high-performance, high-productivity and high-portability in developing and simulating models of cellular nonlinear networks (CNN). A particular example is given in this paper for the case of Reaction Diffusion CNN and its effectiveness it is analyzed. It is shown that the hardware resources are effectively exploited while using a programming style closer to scientific computing and with a short learning cycle. With our low cost implementation we were able to achieve very good performance in implementing a Reaction-Diffusion CNN (about 500 Mcells/second). The platform can be easily extended to support a broader spectrum of computational models similar to CNNs, such as the Finite Difference Time Domain models for various physical processes.|Computer Science in Cars Symposium|2015|10.1109/CSCS.2015.36|R. Dogaru, I. Dogaru|0.8|0
2076|A novel parallel finite element procedure for nonlinear dynamic problems using GPU and mixed-precision algorithm|The purpose of this paper is to improve the computational speed of solving nonlinear dynamics by using parallel methods and mixed-precision algorithm on graphic processing units (GPUs). The computational efficiency of traditional central processing units (CPUs)-based computer aided engineering software has been difficult to satisfy the needs of scientific research and practical engineering, especially for nonlinear dynamic problems. Besides, when calculations are performed on GPUs, double-precision operations are slower than single-precision operations. So this paper implemented mixed precision for nonlinear dynamic problem simulation using Belytschko-Tsay (BT) shell element on GPU.,To minimize data transfer between heterogeneous architectures, the parallel computation of the fully explicit finite element (FE) calculation is realized using a vectorized thread-level parallelism algorithm. An asynchronous data transmission strategy and a novel dependency relationship link-based method, for efficiently solving parallel explicit shell element equations, are used to improve the GPU utilization ratio. Finally, this paper implements mixed precision for nonlinear dynamic problems simulation using the BT shell element on a GPU and compare it to the CPU-based serially executed program and a GPU-based double-precision parallel computing program.,For a car body model containing approximately 5.3 million degrees of freedom, the computational speed is improved 25 times over CPU sequential computation, and approximately 10% over double-precision parallel computing method. The accuracy error of the mixed-precision computation is small and can satisfy the requirements of practical engineering problems.,This paper realized a novel FE parallel computing procedure for nonlinear dynamic problems using mixed-precision algorithm on CPU-GPU platform. Compared with the CPU serial program, the program implemented in this article obtains a 25 times acceleration ratio when calculating the model of 883,168 elements, which greatly improves the calculation speed for solving nonlinear dynamic problems.||2020|10.1108/ec-07-2019-0328|Guangxi Li, Chao-Hsin Wang, Sheng-lin Wang, Yong Cai|0.8|0
2240|Optimized Processing of Localized Collisions in Projective Dynamics|We present a method for the efficient processing of contact and collision in volumetric elastic models simulated using the Projective Dynamics paradigm. Our approach enables interactive simulation of tetrahedral meshes with more than half a million elements, provided that the model satisfies two fundamental properties: the region of the model's surface that is susceptible to collision events needs to be known in advance, and the simulation degrees of freedom associated with that surface region should be limited to a small fraction (e.g. 5%) of the total simulation nodes. In such scenarios, a partial Cholesky factorization can abstract away the behaviour of the collision‐safe subset of the face model into the Schur Complement matrix with respect to the collision‐prone region. We demonstrate how fast and accurate updates of bilateral penalty‐based collision terms can be incorporated into this representation, and solved with high efficiency on the GPU. We also demonstrate iterating a partial update of the element rotations, akin to a selective application of the local step, specifically on the smaller collision‐prone region without explicitly paying the cost associated with the rest of the simulation mesh. We demonstrate efficient and robust interactive simulation in detailed models from animation and medical applications.|Computer graphics forum (Print)|2020|10.1111/cgf.14385|Qisi Wang, C. Cutting, E. Brandt, Eftychios Sifakis, Yutian Tao|0.8|0
1636|Real-time evolutionary model predictive control using a graphics processing unit|With humanoid robots becoming more complex and operating in un-modeled or human environments, there is a growing need for control methods that are scalable and robust, while still maintaining compliance for safety reasons. Model Predictive Control (MPC) is an optimal control method which has proven robust to modeling error and disturbances. However, it can be difficult to implement for high degree of freedom (DoF) systems due to the optimization problem that must be solved. While evolutionary algorithms have proven effective for complex large-scale optimization problems, they have not been formulated to find solutions quickly enough for use with MPC. This work details the implementation of a parallelized evolutionary MPC (EMPC) algorithm which is able to run in real-time through the use of a Graphics Processing Unit (GPU). This parallelization is accomplished by simulating candidate control input trajectories in parallel on the GPU. We show that this framework is more flexible in terms of cost function definition than traditional MPC and that it shows promise for finding solutions for high DoF systems.|IEEE-RAS International Conference on Humanoid Robots|2017|10.1109/HUMANOIDS.2017.8246929|Marc D. Killpack, Phillip Hyatt|0.75|0
1650|On the power consumption modeling for the simulation of Heterogeneous HPC clouds|During the last years, except from the traditional CPU based hardware servers, hardware accelerators are widely used in various HPC application areas. More specifically, Graphics Processing Units (GPUs), Many Integrated Cores (MICs) and Field-Programmable Gate Arrays (FPGAs) have shown a great potential in HPC and have been widely mobilized in supercomputing. With the adoption of HPC from cloud environments, the realization of HPC-Clouds is evolving since many vendors provide HPC capabilities on their clouds. With the increase of the interest on clouds, there has been an analogous increase in cloud simulation frameworks. Cloud simulation frameworks offer a controllable environment for experimentation with various workloads and scenarios, while they provide several metrics such as server utilization and power consumption. For providing these metrics, cloud simulators propose mathematical models that estimate the behavior of the underlying hardware infrastructure. This paper focuses on the power consumption modeling of the main compute elements of heterogeneous HPC servers, i.e. CPU servers and pairs of CPU-accelerators. The modeling approaches of existing cloud simulators are examined and extended, while new models are proposed for estimating the power consumption of accelerators.|CloudNG@EuroSys|2017|10.1145/3068126.3068127|G. Gravvanis, C. Filelis-Papadopoulos, D. Tzovaras, K. M. Giannoutakis, Antonios T. Makaratzis|0.75|0
1656|Preparing an Incompressible-Flow Fluid Dynamics Code for Exascale-Class Wind Energy Simulations|The U.S. Department of Energy has identified exascale-class wind farm simulation as critical to wind energy scientific discovery. A primary objective of the ExaWind project is to build high-performance, predictive computational fluid dynamics (CFD) tools that satisfy these modeling needs. GPU accelerators will serve as the computational thoroughbreds of next-generation, exascale-class supercomputers. Here, we report on our efforts in preparing the Exa Wind unstructured mesh solver, Nalu-Wind, for exascale-class machines. For computing at this scale, a simple port of the incompressible-flow algorithms to GPUs is insufficient. To achieve high performance, one needs novel algorithms that are application aware, memory efficient, and optimized for the latest-generation GPU devices. The result of our efforts are unstructured-mesh simulations of wind turbines that can effectively leverage thousands of GPUs. In particular, we demonstrate a first-of-its-kind, incompressible-flow simulation using Algebraic Multigrid solvers that strong scales to more than 4000 GPUs on the Summit supercomputer.|International Conference for High Performance Computing, Networking, Storage and Analysis|2021|10.1145/3458817.3476185|Stephen J. Thomas, Alan B. Williams, S. Ananthan, M. Sprague, Ruipengyu Li, Jonathan S. Rood, Ashesh Sharma, P. Mullowney|0.75|0
1684|A 3D Bayesian Computed Tomography Reconstruction Algorithm with Gauss-Markov-Potts Prior Model and its Application to Real Data|Iterative reconstruction methods in Computed Tomography (CT) are known to provide better image quality than analytical methods but they are not still applied in many fields because of their computational cost. In the last years, Graphical Processor Units (GPU) have emerged as powerful devices in order to parallelize calculations, but the efficiency of their use is conditionned on applying algorithms that can be massively parallelizable. Moreover, in non-destructive testing (NDT) applications, a segmentation of the reconstructed volume is often needed in order to have an accurate diagnosis on the material health, but performing a segmentation after the reconstruction introduces uncertainties in the diagnosis from both the reconstruction and the segmentation algorithms. In this paper, we propose an iterative reconstruction method for 3D CT that performs a joint reconstruction and segmentation of the controlled object in NDT for industrial applications. The method is based on a 3D Gauss-Markov-Potts prior model in Bayesian framework, which has shown its effective use in many image restoration and super-resolution problems. First, we briefly describe this model, before deriving the expression of the joint posterior distribution of all the unknowns. Next, an effective maximization of this distribution is presented. We use a ray-driven projector and a voxel-driven backprojector implemented on GPU. The algorithm is developed so it can be massively parallelized. * The authors are grateful to Lionel Gay and Nicolas Cochennec for having provided the real IQI phantom used to test the method. They would also like to thank Thomas Boulay for his contribution to the implementation of the projector and the backprojector on GPU. 2 C. Chapdelaine, A. Mohammad-Djafari, N. Gac, E. Parra / 3D Bayesian CT with Gauss-Markov-Potts prior model Finally, we present our results on simulated and real phantoms. In addition, we investigate further reconstruction quality indicators in order to compare our results with other methods.|Fundamenta Informaticae|2017|10.3233/FI-2017-1591|Estelle Parra, A. Mohammad-Djafari, Camille Chapdelaine, N. Gac|0.75|0
1690|The Modular Breeding Program Simulator (MoBPS) allows efficient simulation of complex breeding programs|Context Breeding programs aim at improving the genetic characteristics of livestock populations with respect to productivity, fitness and adaptation, while controlling negative effects such as inbreeding or health and welfare issues. As breeding is affected by a variety of interdependent factors, the analysis of the effect of certain breeding actions and the optimisation of a breeding program are highly complex tasks. Aims This study was conducted to display the potential of using stochastic simulation to analyse, evaluate and compare breeding programs and to show how the Modular Breeding Program Simulator (MoBPS) simulation framework can further enhance this. Methods In this study, a simplified version of the breeding program of Gottingen Minipigs was simulated to analyse the impact of genotyping and optimum contribution selection in regard to both genetic gain and diversity. The software MoBPS was used as the backend simulation software and was extended to allow for a more realistic modelling of pig breeding programs. Among others, extensions include the simulation of phenotypes with discrete observations (e.g. teat count), variable litter sizes, and a breeding value estimation in the associated R-package miraculix that utilises a graphics processing unit. Key results Genotyping with the subsequent use of genomic best linear unbiased prediction (GBLUP) led to substantial increases in genetic gain (15.3%) compared with a pedigree-based BLUP, while reducing the increase of inbreeding by 24.8%. The additional use of optimum genetic selection was shown to be favourable compared with the plain selection of top boars. The use of graphics processing unit-based breeding value estimation with known heritability was ~100 times faster than the state-of-the-art R-package rrBLUP. Conclusions The results regarding the effect of both genotyping and optimal contribution selection are in line with well established results. Paired with additional new features such as the modelling of discrete phenotypes and adaptable litter sizes, this confirms MoBPS to be a unique tool for the realistic modelling of modern breeding programs. Implications The MoBPS framework provides a powerful tool for scientists and breeders to perform stochastic simulations to optimise the practical design of modern breeding programs to secure standardised breeding of high-quality animals and answer associated research questions.|Animal Production Science|2021|10.1071/an21076|C. Reimer, Alexander Freudenberg, H. Simianer, L. F. Mikkelsen, L. Büttgen, M. Schlather, A. Ganesan, T. Pook, J. Geibel, N. Ha|0.75|0
1803|Evaluation of Distributed Tasks in Stencil-based Application on GPUs|In the era of exascale computing, the traditional MPI+X paradigm starts losing its strength in taking advantage of heterogeneous systems. Subsequently, research and development on finding alternative programming models and runtimes have become increasingly popular. This encourages comparison, on competitive grounds, of these emerging parallel programming approaches against the traditional MPI+X paradigm. In this work, an implementation of distributed task-based stencil numerical simulation is compared with a MPI+X implementation of the same application. To be more specific, the Legion task-based parallel programming system is used as an alternative to MPI at out-of-node level, while the underlying CUDA-implemented kernels are kept at node level. Therefore, the comparison is as fair as possible and focused on the distributed aspects of the simulation. Overall, the results show that the task-based approach is on par with the traditional MPI approach in terms of both performance and scalability.|International Workshop on Extreme Scale Programming Models and Middleware|2021|10.1109/ESPM254806.2021.00011|Jonathon M. Anderson, M. Araya-Polo, Jie Meng, Eric Raut|0.75|0
1824|On Unbiased Estimation for Discretized Models|In this article, we consider computing expectations w.r.t. probability measures which are subject to discretization error. Examples include partially observed diffusion processes or inverse problems, where one may have to discretize time and/or space, in order to practically work with the probability of interest. Given access only to these discretizations, we consider the construction of unbiased Monte Carlo estimators of expectations w.r.t. such target probability distributions. It is shown how to obtain such estimators using a novel adaptation of randomization schemes and Markov simulation methods. Under appropriate assumptions, these estimators possess finite variance and finite expected cost. There are two important consequences of this approach: (i) unbiased inference is achieved at the canonical complexity rate, and (ii) the resulting estimators can be generated independently, thereby allowing strong scaling to arbitrarily many parallel processors. Several algorithms are presented, and applied to some examples of Bayesian inference problems, with both simulated and real observed data.|SIAM/ASA J. Uncertain. Quantification|2021|10.1137/21m1460788|A. Tarakanov, J. Heng, K. Law, A. Jasra|0.75|0
1882|Enabling OpenCL and SYCL for RISC-V processors|RISC-V is a non-profit, member managed organization and is gaining momentum in the processor space, with more than 900 members. One of the goals of the organization is to build an open software platform, providing software developers an easy way to harness the familiar benefits already available on CPUs and GPUs. Today, system-on-chip manufacturers are building specialist accelerator processors based on the RISC-V architecture, taking advantage of the Vectorized extensions that match compute performance mostly seen on GPUs today. The availability of a familiar and well defined programming model is an absolute requirement if expecting to successfully bring these new processors to market. This presentation will dive into the details of Codeplay’s work in partnership with NSI-TEXE and Kyoto Microcomputer, describing the components needed to integrate OpenCL and SYCL onto RISC-V using multiple simulators. This project forms part of Japan’s New Energy and Industrial Technology Development Organisation (“NEDO”) project to build a powerful supercomputer. While Codeplay has previously enabled OpenCL for a variety processor architectures, there are a number of technical challenges involved in delivering a generic integration that can be used by multiple RISC-V based systems, and the solution required a change in approach. By adding to the existing LLVM back-end for RISC-V, and creating an integration layer that plugs into OpenCL, we have built a common base architecture for a range of RISC-V processors from different companies. This presentation will explain how Codeplay’s current driver interface works, and how it has been adapted to integrate with multiple RISC-V targets, in particular the riscvOVPsim and Spike RISC-V ISA simulators. We will also talk about some of the RISC-V extensions that are available, and how these can help to to expose features specific to the RISC-V architecture through OpenCL.|International Workshop on OpenCL|2021|10.1145/3456669.3456687|Rod Burns, Aidan Dodds, Colin Davidson|0.75|0
1938|Energy efficiency and portability of oil and gas simulations on multicore and graphics processing unit architectures|Reverse time migration (RTM) simulation is the basis of the seismic imaging tools used by the oil and gas industry. Developers have been porting their simulations to the new high‐performance computing architectures, providing faster and more accurate results at each new generation. However, several challenges arrive when trying to achieve high performance on these new architectures. The first one is to choose the architecture that best fits the kind of simulation. After that, researchers should choose the API used to implement the simulation code. These two decisions are strongly related to the effort, performance, and energy efficiency of the simulations. In this article, we propose three optimizations for an oil and gas application, which reduce the floating‐point operations by changing the equation derivatives. We evaluate these optimizations in different multicore and GPU architectures, investigating the impact of different APIs on the performance, energy efficiency, and portability of the code. Our experimental results show that the dedicated CUDA implementation running on the NVIDIA Volta architecture has the best performance and energy efficiency for RTM on GPUs, while the OpenMP version is the best for Intel Broadwell in the multicore. Also, the OpenACC version, which has a lower programming effort and executes on both architectures, has an up to 20% better performance and energy efficiency than the nonportable ones.|Concurrency and Computation|2021|10.1002/cpe.6212|M. Serpa, A. Azambuja, P. Navaux, P. J. Pavan, Rodrigo L. Machado, J. Panetta, A. Carissimi, E. Cruz|0.75|0
1944|A Parallel Conformal Symplectic Euler Algorithm for GPR Numerical Simulation on Dispersive Media|This letter presents a ground-penetrating radar (GPR) forward model on dispersive media based on parallel conformal symplectic Euler algorithm. We developed a symplectic Euler algorithm combined with conformal meshes and graphics processing unit (GPU) acceleration technology, which proved to be an accurate and effective method to simulating GPR electromagnetic wave propagation in dispersive media.|IEEE Geoscience and Remote Sensing Letters|2021|10.1109/lgrs.2020.3045488|H. Fang, Yinping Li, Binghan Xue, Jianwei Lei|0.75|0
2216|Radiance based method for accurate determination of volume scattering parameters using GPU-accelerated Monte Carlo.|Volume scattering is an important effect in different fields, ranging from biology to lighting. Models for volume scattering usually rely on parameters that are estimated with inverse methods that iteratively fit simulations to experimental data. To obtain accurate estimates for these parameters, the scattered intensity distribution can be used in such fitting methods. However, it has been shown that for samples with long optical path lengths this type of data may result in poor parameter estimates. In this work, an inverse procedure is proposed that fits to scattered radiance distributions. By taking advantage of current generation graphics processing units, the method implemented is sufficiently efficient to allow performing an in-depth simulation study on the difference between using radiance or intensity distributions to estimate the volume scattering parameters of samples. This work shows that for samples with moderate optical path lengths, the intensity distribution contains sufficient information to accurately estimate the volume scattering properties. However, for longer optical path lengths, the descriptive power of the intensity distribution is not enough and radiance distribution based methods, such as the inverse method proposed, are better suited.|Optics Express|2017|10.1364/OE.25.022575|António Correia, Y. Meuret, H. Cornelissen, P. Hanselaer|0.75|0
1802|Modeling Direct Transmission Diseases Using Parallel Bitstring Agent-Based Models|Agent-based models (ABMs) are gaining importance over traditional epidemiological modeling due to advances in computing technology and catalyzed by the need for detailed epidemiological analysis of emergent diseases. Unfortunately, the advantages of ABMs are realized at the cost of significantly large execution times and high memory consumption for large-scale simulations. To address the memory issue, we designed and implemented an ABM using an innovative feature: the bitstring approach, in which the attributes of each agent are represented by an array of bits instead using traditional data structures. To cope with the high computational demands, we developed a parallel version of model aiming multicore CPUs and GPUs architectures using Thrust parallel algorithms library. The results of our model were validated comparing them with data of a spread of Influenza A in the Cascavel City, South Brazil, occurred in 2009. The model presented good qualitative results and an excellent performance on GPUs. The application of bitstring technique is proved to be relevant in economy of memory, allowing to store the same attributes using 41% less memory space and improving the data copy time between CPU and GPU up to 52%.|IEEE Transactions on Computational Social Systems|2018|10.1109/TCSS.2018.2871625|Claudia Brandelero Rizzi, W. L. Kaizer, Rogério Luís Rizzi, Guilherme Galante, F. Coelho|0.7142857142857143|0
1685|Gell: A GPU-powered 3D hybrid simulator for large-scale multicellular system|As a powerful but computationally intensive method, hybrid computational models study the dynamics of multicellular systems by evolving discrete cells in reacting and diffusing extracellular microenvironments. As the scale and complexity of studied biological systems continuously increase, the exploding computational cost starts to limit large-scale cell-based simulations. To facilitate the large-scale hybrid computational simulation and make it feasible on easily accessible computational devices, we develop a fast and memory-efficient open-source GPU-based hybrid computational modeling platform Gell (GPU Cell), for large-scale system modeling. We fully parallelize the simulations on GPU for high computational efficiency and propose a novel voxel sorting method to further accelerate the modeling of massive cell-cell mechanical interaction with negligible additional memory footprint. As a result, Gell efficiently handles simulations involving tens of millions of cells on a personal computer. We compare the performance of Gell with a state-of-the-art paralleled CPU-based simulator on a hanging droplet spheroid growth task and further demonstrate Gell with a ductal carcinoma in situ (DCIS) simulation. Gell affords ~150X acceleration over the paralleled CPU method with one-tenth of the memory requirement. Author Summary Numerical cell simulations provide indispensable insight into the cell-to-tumor tissue transition and help reduce biological experimental variables. However, the availability and practicality of large-scale cell simulation tools have been limited by high computational cost, slow performance, or proprietary. Recent developments in open-source simulation codes and GPU implementation have partially addressed the challenge. We further optimized the cell simulation platform for GPU implementation in this work. As a result, benchmark cell simulation experiments can be performed efficiently on a personal computer with a modern GPU. We made the platform open source to encourage community adoption and collective development.|bioRxiv|2022|10.1371/journal.pone.0288721|Lihua Jin, Jiayi Du, K. Sheng, Yu Zhou|0.6666666666666666|0
1719|Visual analysis of three-dimensional flow field based on WebVR|\n With the rapid development of internet technologies, it is possible to provide computing services and visualize calculated results on the internet. A three-dimensional flow field visualization method based on WebVR is presented in this paper. We devised and built an immersive and interactive three-dimensional virtual reality scene employing web-standard technologies (i.e., HTML5, JavaScript, WebGL, and Ajax) and computing services provided by hydrodynamic software, using GPUs to accelerate the display of flow field in the browser, without the use of plug-ins. On the basis of three-dimensional topography and surface flow field, a three-dimensional flow field presentation method of superimposing multiple sections into the original computational domain was proposed. Furthermore, the description of tracer sphere and path line was adopted to describe the structure characteristics of the flow field. Replacing complete three-dimensional sphere models with textured stylized particles improved the frame rate of the browser greatly when rendering animations. This research enables developers and users of the hydrodynamic model to be immersed in their data of flow field using Google Cardboard. As far as we know, this is the first time that WebVR technology has been applied in three-dimensional hydrodynamic simulation.|Journal of Hydroinformatics|2019|10.2166/HYDRO.2019.101|S. Zhao, Sheng Jin, N. Zhang, C. Ai|0.6666666666666666|0
1736|Improved Numerical Methodologies on Power System Dynamic Simulation Using GPU Implementation|This paper proposes an improved numerical methodology for faster than real time (FTRT) power system dynamic simulation. It is implemented on a graphics processing unit (GPU) with parallel programming, and can handle large-scale power systems. In power system dynamic simulation, the biggest challenge for FTRT simulation is the matrix solver for the network equation. State of the art using the analog approach or parallel high-performance computing (HPC) can barely meet the real-time requirement on a system with approximately 10,000 buses and 2,000 generator-buses. We use the Sherman-Morrison-Woodbury formula to Gauss Block Elimination to take advantage of the low-rank change in the admittance matrix while lowering the computational dimension. With the proposed numerical methodology, we implemented the dynamic simulation on GPUs with classical generator models. To validate its FTRT capability of our methodology, we demonstrate this with the example of Western Electricity Coordinating Council (WECC) systems with over 20,000 buses and over 2,000 generator-buses being simulated, which is very capable of running over four times faster than real-time. The promising results exhibit outstanding scalable FTRT performance.|IEEE PES Innovative Smart Grid Technologies Conference|2019|10.1109/ISGT.2019.8791667|A. Chakrabortty, Xiaoming Feng, Zhao Wang, C. T. Kelley, P. Franzon|0.6666666666666666|0
1764|Process Simulation of Complex Biological Pathways in Physical Reactive Space and Reformulated for Massively Parallel Computing Platforms|Biological systems encompass complexity that far surpasses many artificial systems. Modeling and simulation of large and complex biochemical pathways is a computationally intensive challenge. Traditional tools, such as ordinary differential equations, partial differential equations, stochastic master equations, and Gillespie type methods, are all limited either by their modeling fidelity or computational efficiency or both. In this work, we present a scalable computational framework based on modeling biochemical reactions in explicit 3D space, that is suitable for studying the behavior of large and complex biological pathways. The framework is designed to exploit parallelism and scalability offered by commodity massively parallel processors such as the graphics processing units (GPUs) and other parallel computing platforms. The reaction modeling in 3D space is aimed at enhancing the realism of the model compared to traditional modeling tools and framework. We introduce the Parallel Select algorithm that is key to breaking the sequential bottleneck limiting the performance of most other tools designed to study biochemical interactions. The algorithm is designed to be computationally tractable, handle hundreds of interacting chemical species and millions of independent agents by considering all-particle interactions within the system. We also present an implementation of the framework on the popular graphics processing units and apply it to the simulation study of JAK-STAT Signal Transduction Pathway. The computational framework will offer a deeper insight into various biological processes within the cell and help us observe key events as they unfold in space and time. This will advance the current state-of-the-art in simulation study of large scale biological systems and also enable the realistic simulation study of macro-biological cultures, where inter-cellular interactions are prevalent.|IEEE/ACM Transactions on Computational Biology & Bioinformatics|2016|10.1109/TCBB.2015.2443784|Hanyu Jiang, N. Ganesan, Jie Li, Vishakha Sharma, Adriana B. Compagnoni|0.6666666666666666|0
1836|Iterative hard thresholding for model selection in genome‐wide association studies|A genome‐wide association study (GWAS) correlates marker and trait variation in a study sample. Each subject is genotyped at a multitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here, we assume that subjects are randomly collected unrelateds and that trait values are normally distributed or can be transformed to normality. Over the past decade, geneticists have been remarkably successful in applying GWAS analysis to hundreds of traits. The massive amount of data produced in these studies present unique computational challenges. Penalized regression with the ℓ1 penalty (LASSO) or minimax concave penalty (MCP) penalties is capable of selecting a handful of associated SNPs from millions of potential SNPs. Unfortunately, model selection can be corrupted by false positives and false negatives, obscuring the genetic underpinning of a trait. Here, we compare LASSO and MCP penalized regression to iterative hard thresholding (IHT). On GWAS regression data, IHT is better at model selection and comparable in speed to both methods of penalized regression. This conclusion holds for both simulated and real GWAS data. IHT fosters parallelization and scales well in problems with large numbers of causal markers. Our parallel implementation of IHT accommodates SNP genotype compression and exploits multiple CPU cores and graphics processing units (GPUs). This allows statistical geneticists to leverage commodity desktop computers in GWAS analysis and to avoid supercomputing. Availability: Source code is freely available at https://github.com/klkeys/IHT.jl.|Genetic Epidemiology|2016|10.1002/gepi.22068|Kevin L. Keys, Gary K. Chen, K. Lange|0.6666666666666666|0
1839|SWIFT: Switch-Level Fault Simulation on GPUs|Current nanometer CMOS circuits show an increasing sensitivity to deviations in first-order parameters and suffer from process variations during manufacturing. To properly assess and support test validation of digital designs, low-level fault simulation approaches are utilized to accurately capture the behavior of CMOS cells under parametric faults and process variations as early as possible throughout the design phase. However, low-level simulation approaches exhibit a high computational complexity, especially when variation has to be taken into account. In this paper, a high-throughput parallel fault simulation at switch level is presented. First-order electrical parameters are utilized to capture CMOS-specific functional and timing behavior of complex cells allowing to model faults with transistor granularity and without the need of logic abstraction. Furthermore, variation modeling in cells and transistor devices enables broad and efficient variation analyses of faults over many circuit instances for the first time. The simulation approach utilizes massive parallelization on graphics processing units by exploiting parallelism from cells, stimuli, faults, and circuit instances. Despite the lower abstraction levels of the approach, it processes designs with millions of gates and outperforms conventional fault simulation at logic level in terms of speed and accuracy.|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|2019|10.1109/TCAD.2018.2802871|H. Wunderlich, E. Schneider|0.6666666666666666|0
1886|Wireless Access Point Positioning Optimization|This work presents the development of a wireless Access Point (AP) placement software that uses microwave signal propagation models. We use the Simulated Annealing metaheuristic, to improve the signal coverage according to the physical features of the architectural floor plan. In the experiments, a Graphics Processing Unit (GPU) with CUDA is used to parallelize the simulations, speeding up the runtime required. It is worth noting that our software makes possible to test the provisions of APs without the operational cost of having to move them physically. It proposes a spatial arrangement of APs that provides a greater coverage with usable signal intensity within the simulated environment. Using a limited amount of APs, the test case results showed that it is possible to obtain a significant improvement in the coverage of the Wi-Fi signal by adjusting the APs positions to the metaheuristic proposed solution. In addition, our software is capable of performing simulations with an increasing number of APs if the current quantity is not enough to achieve better quality and signal coverage.|International Conference on Software, Telecommunications and Computer Networks|2019|10.23919/SOFTCOM.2019.8903880|Everthon Valadão, R. L. Rosa, D. Z. Rodríguez, Samuel Terra Vieira|0.6666666666666666|0
1909|Meshless single grain cutting simulations on the GPU|Physically sound prediction of grinding results necessarily starts with the single grain interaction with the material, which is especially important for engineered grinding tools (EGT). The deformations encountered in single grain simulations are severe, especially due to the large negative rake angles. Conventional software tools using FEM suffer from the induced extreme distortions of the grid, necessitating costly remeshing operations. Meshless methods are not limited in the amount of deformation they can reproduce and thus are a promising alternative, which also has the potential for extreme parallelisation on the graphics co-processor (GPU). With the drastically increased computational speed unprecedented simulation resolutions can be achieved at reasonable computation times allowing parameter studies on different friction and constitutive models with first order complete meshless formulations. In the interest of reproducibility, the software tool developed is made open source and can be obtained from https://github.com/mroethli/iwf_mfree_gpu_3d.|International Journal of Mechatronics and Manufacturing Systems|2019|10.1504/ijmms.2019.10025073|M. Röthlin, H. Klippel, K. Wegener, M. Afrasiabi|0.6666666666666666|0
1983|An Efficient Compressible Multicomponent Flow Solver for Heterogeneous CPU/GPU Architectures|We present a solver for three-dimensional compressible multicomponent flow based on the compressible Euler equations. The solver is based on a finite volume scheme for structured grids and advances the solution using an explicit Runge-Kutta time stepper. The numerical scheme requires the computation of the flux divergence based on an approximate Riemann problem. The computation of the divergence quantity is the most expensive task in the algorithm. Our implementation takes advantage of the compute capabilities of heterogeneous CPU/GPU architectures. The computational problem is organized in subdomains small enough to be placed into the GPU memory. The compute intensive stencil scheme is offloaded to the GPU accelerator while advancing the solution in time on the CPU. Our method to implement the stencil scheme on the GPU is not limited to applications in fluid dynamics. The performance of our solver was assessed on Piz Daint, a XC30 supercomputer at CSCS. The GPU code is memory-bound and achieves a per-node performance of 462Gflop/s, outperforming by 3.2x the multicore-based Gordon Bell winning CUBISM-MPCF solver [16] for the offloaded computation on the same platform. The focus of this work is on the per-node performance of the heterogeneous solver. In addition, we examine the performance of the solver across 4096 compute nodes. We present simulations for the shock-induced collapse of an aligned row of air bubbles submerged in water using 4 billion cells. Results show a final pressure amplification that is 100x stronger than the strength of the initial shock.|Platform for Advanced Scientific Computing Conference|2016|10.1145/2929908.2929914|P. Hadjidoukas, B. Hejazialhosseini, D. Rossinelli, P. Koumoutsakos, F. Wermelinger|0.6666666666666666|0
2021|Celeritas: GPU-accelerated particle transport for detector simulation in High Energy Physics experiments|Within the next decade, experimental High Energy Physics (HEP) will enter a new era of scientific discovery through a set of targeted programs recommended by the Particle Physics Project Prioritization Panel (P5), including the upcoming High Luminosity Large Hadron Collider (LHC) HL-LHC upgrade and the Deep Underground Neutrino Experiment (DUNE). These efforts in the Energy and Intensity Frontiers will require an unprecedented amount of computational capacity on many fronts including Monte Carlo (MC) detector simulation. In order to alleviate this impending computational bottleneck, the Celeritas MC particle transport code is designed to leverage the new generation of heterogeneous computer architectures, including the exascale computing power of U.S. Department of Energy (DOE) Leadership Computing Facilities (LCFs), to model targeted HEP detector problems at the full fidelity of Geant4. This paper presents the planned roadmap for Celeritas, including its proposed code architecture, physics capabilities, and strategies for integrating it with existing and future experimental HEP computing workflows.||2022|10.2172/1863002|S. Johnson, P. Romano, A. Lund, V. Pascuzzi, S. Tognini, P. Canal, T. Evans, S. Jun, G. Lima|0.6666666666666666|0
2100|DCGrid: An Adaptive Grid Structure for Memory-Constrained Fluid Simulation on the GPU|We introduce Dynamic Constrained Grid (DCGrid), a hierarchical and adaptive grid structure for fluid simulation combined with a scheme for effectively managing the grid adaptations. DCGrid is designed to be implemented on the GPU and used in high-performance simulations. Specifically, it allows us to efficiently vary and adjust the grid resolution across the spatial domain and to rapidly evaluate local stencils and individual cells in a GPU implementation. A special feature of DCGrid is that the control of the grid adaption is modeled as an optimization under a constraint on the maximum available memory, which addresses the memory limitations in GPU-based simulation. To further advance the use of DCGrid in high-performance simulations, we complement DCGrid with an efficient scheme for approximating collisions between fluids and static solids on cells with different resolutions. We demonstrate the effectiveness of DCGrid for smoke flows and complex cloud simulations in which terrain-atmosphere interaction requires working with cells of varying resolution and rapidly changing conditions. Finally, we compare the performance of DCGrid to that of alternative adaptive grid structures.|Proceedings of the ACM on Computer Graphics and Interactive Techniques|2022|10.1145/3522608|Jorge Alejandro Amador Herrera, K. Hildebrandt, S. Pirk, Torsten Hädrich, D. Banuti, D. Michels, Wouter Raateland, Wojciech Palubicki|0.6666666666666666|0
2253|A Massively Parallel Algebraic Multiscale Solver for Reservoir Simulation on the GPU Architecture|\n In this work, the scalability of the Algebraic Multiscale Solver (AMS) (Wang et al. 2014) for the pressure equation arising from incompressible flow in heterogeneous porous media is investigated on the GPU massively parallel architecture. The solvers robustness and scalability is compared against its carefully optimized implementation on the shared-memory multi-core architecture (Manea et al. 2016), which this work is directly extending. Although several components in the AMS algorithm are directly parallelizable, its scalability on GPU's depends heavily on the underlying algorithmic details and data-structures design of each step, where one needs to ensure favorable control- and data-flow on the GPU, while extracting enough parallel work for a massively parallel environment. In addition, the type of the algorithm chosen for each step greatly influences the overall robustness of the solver. Taking all these constraints into account, we have developed a GPU-based AMS that exploits the parallelism in every module of the solver, including both the setup phase and the solution phase. The performance of AMS—with our carefully optimized algorithmic choices on the GPU for both the setup phase and the solution phase, is demonstrated using highly heterogeneous 3D problems derived from the SPE10 Benchmark (Christie et al. 2001). Those problems range in size from millions to tens of millions of cells. The GPU implementation is benchmarked on a massively parallel architecture consisting of NVIDIA Kepler K80 GPU's, where its performance is compared to the multi-core CPU architecture using an optimized multi-core AMS implementation (Manea et al. 2016) running on a shared memory multi-core architecture consisting of two packages of Intel's Haswell-EP Xeon(R) CPU E5-2667. While the GPU-based AMS parallel implementation shows good scalability for the solution stage, its setup stage is less efficient compared to the CPU, mainly due to the dependence on a QR-based basis functions solver.|Day 1 Wed, April 10, 2019|2019|10.2118/193880-MS|T. Almani, A. Manea|0.6666666666666666|0
1755|Addressing Global Data Dependencies in Heterogeneous Asynchronous Runtime Systems on GPUs|Large-scale parallel applications with complex global data dependencies beyond those of reductions pose significant scalability challenges in an asynchronous runtime system. Internodal challenges include identifying the all-to-all communication of data dependencies among the nodes. Intranodal challenges include gathering together these data dependencies into usable data objects while avoiding data duplication. This paper addresses these challenges within the context of a large-scale, industrial coal boiler simulation using the Uintah asynchronous many-task runtime system on GPU architectures. We show significant reduction in time spent analyzing data dependencies through refinements in our dependency search algorithm. Multiple task graphs are used to eliminate subsequent analysis when task graphs change in predictable and repeatable ways. Using a combined data store and task scheduler redesign reduces data dependency duplication ensuring that problems fit within host and GPU memory. These modifications did not require any changes to application code or sweeping changes to the Uintah runtime system. We report results running on the DOE Titan system on 119K CPU cores and 7.5K GPUs simultaneously. Our solutions can be generalized to other task dependency problems with global dependencies among thousands of nodes which must be processed efficiently at large scale.|ESPM2@SC|2017|10.1145/3152041.3152082|M. Berzins, John A. Schmidt, B. Peterson, A. Humphrey|0.625|0
1655|Evaluation of gas sales agreements with indexation using tree and least-squares Monte Carlo methods on graphics processing units|A typical gas sales agreement, also called a gas swing contract, is an agreement between a supplier and purchaser for the delivery of variable daily quantities of gas between specified minimum and maximum daily limits. The primary constraint of such agreements that makes them difficult to value is that the strike price is set based on the indexation principle, under which the strike price is called the index. Each month, the value of the index is determined by the weighted average price of certain energy products (e.g. crude oil) in the previous month. We propose a lattice-based method (trinomial trees) and a simulation-based method (least-squares Monte Carlo simulations) for pricing such swing contracts with indexation. With the help of graphics processing unit (GPU) technology, we can efficiently evaluate the algorithms. We also provide a detailed analysis using several numerical examples of the indexation and how different model parameters will affect both the optimal value and optimal decisions.||2020|10.1080/14697688.2020.1775283|W. Dong, B. Kang|0.6|0
1829|Slime mould computing|Slime mould, Physarum polycephalum, is a large single cell with intriguingly smart behaviour. The slime mould shows outstanding abilities to adapt its protoplasmic network to varying environmental conditions. The slime mould can solve tasks of computational geometry, image processing, logics and arithmetics when data are represented by configurations of attractants and repellents. In this special issue, we present groundbreaking results on implementation of computing devices with living and simulated slime mould. The showcases include obstacle-free path planning, implementation of cellular automata, laboratory realization of logical gates, formalization of the slime mould’s behaviour, constructions of the slime mould programming language, production of hybrid computing elements, designing evacuation procedures and formalization of uncertainties of the slime mould’s behaviour. Path planning is amongst the biblical problems of computer science. Using a particlebased model of the slime mould, Jeff Jones – “A Morphological Adaptation Approach to Path Planning Inspired by Slime Mould” – shows how a collision-free path can be approximated via morphological adaptations of the slime mould. Slime mould computes the path while guided by gradient of attractants towards the destination and avoids obstacles because they emit repellents. Tomohiro Shirakawa and his colleagues grow the slime mould on a regular two-dimensional array of agar blobs. The surfaces are occupied by the slime mould and the connections between neighbouring blobs are made of the protoplasmic tubes. The whole architecture reminds a locally connecting parallel processor. Some cellular-automata-models proposed in their paper “Construction of Living Cellular Automata using the Physarum Plasmodium” enlighten us on how the slime mould propagates and interacts with its environment. Richard Mayne and Andrew Adamatzky employ plasmodial phototactic responses to construct laboratory prototypes of NOT and NAND logical gates with electrical inputs/outputs and optical coupling. Their paper “Slime Mould Foraging Behaviour as Optically Coupled Logical Operations” shows that the slime mould gate constructed is reusable and fault tolerant. Krzysztof Pancerz and Andrew Schumann – “Rough Set Models of Physarum Machines” – formalize behaviour of the slime mould using transition systems over rough sets. They develop a formal language of the slime and apply their findings to discover predecessor anticipators in the slime mould’s state transitions. Qing Wang and co-authors develop a unique algorithm of shortest path computation inspired by foraging behaviour of the slime mould. In their paper “An Anticipation Mechanism for the Shortest Path Problem Based on Physarum polycephalum” they present an iterative modification of the famous “amoeba algorithm”, their version incorporates some anticipatory mechanisms. Tatiana Berzina and colleagues – “Hybrid Slime Mould-containing System for Unconventional Computing” – overview their results on internalization of functional nanoparticles in the slime mould and control of the particle transportation inside the slime mould using attractants and repellents. By growing the slime mould on conductive polymers, they modify properties of the polymers and thus pave a way towards novel bio-electronic devices.|International Journal of General Systems|2015|10.1080/03081079.2014.997525|A. Adamatzky|0.6|0
2140|Real-time VR Simulation of Laparoscopic Cholecystectomy based on Parallel Position-based Dynamics in GPU|In recent years, virtual reality (VR) based training has greatly changed surgeons learning mode. It can simulate the surgery from the visual, auditory, and tactile aspects. VR medical simulator can greatly reduce the risk of the real patient and the cost of hospitals. Laparoscopic cholecystectomy is one of the typical representatives in minimal invasive surgery (MIS). Due to the large incidence of cholecystectomy, the application of its VR-based simulation is vital and necessary for the residents’ surgical training. In this paper, we present a VR simulation framework based on position-based dynamics (PBD) for cholecystectomy. To further accelerate the deformation of organs, PBD constraints are solved in parallel by a graph coloring algorithm. We introduce a bio-thermal conduction model to improve the realism of the fat tissue electrocautery. Finally, we design a hybrid multi-model connection method to handle the interaction and simulation of the liver-gallbladder separation. This simulation system has been applied to laparoscopic cholecystectomy training in several hospitals. From the experimental results, users can operate in real-time with high stability and fidelity. The simulator is also evaluated by a number of digestive surgeons through preliminary studies. They believed that the system can offer great help to the improvement of surgical skills.|IEEE Conference on Virtual Reality and 3D User Interfaces|2020|10.1109/VR46266.2020.1580817835575|Peng Yu, Hong Qin, Hong Wang, Haimin Hao, Yang Shen, Leiyu Zhang, Junjun Pan|0.6|0
2182|Conditional Monte Carlo Learning for Diffusions II: extended methodology and application to risk measures and early stopping problems|In Conditional Monte Carlo Learning for Diffusions part I (CMCLDI) [2], we presented a One-layered Nested Monte Carlo (1NMC) to simulate functionals U of a Markov process X. Based on a judicious combination between regression and 1NMC used for localization purpose, this methodology allows to simulate U_{t≥s} conditionally on X{s}. The parallel suitability and scalability of 1NMC makes this algorithm very competitive to simulate quantities that are almost impossible to simulate with other methods. In this paper, using the double layer of trajectories, we explain further the mathematical background of the control on the bias propagation. With this double layer structure, we also detail how to adjust the variance to get a better approximation of the second moment from the regression. In normal and log-normal models, this variance adjustment allows a better description of tail events. Since we applied this algorithm on Backward Stochastic Differential Equations in CMCLDI, we show here its strength for the simulation of risk measures and optimal stopping problems. Two highly dimensional numerical examples are executed in few minutes on one Graphics Processing Unit (GPU).||2020|10.1007/s11009-019-09751-3|L. Abbas-Turki, B. Diallo, G. Pagès|0.6|0
2188|Switch Level Time Simulation of CMOS Circuits with Adaptive Voltage and Frequency Scaling|Design and test validation of systems with adaptive voltage-and frequency scaling (AVFS) requires timing simulation with accurate timing models under multiple operating points. Such models are usually located at logic level and compromise accuracy and simulation speed due to the runtime complexity.This paper presents the first massively parallel time simulator at switch level that uses parametric delay modeling for efficient timing-accurate validation of systems with AVFS. It provides full glitch-accurate switching activity information of designs under varying supply voltage and temperature. Offline statistical learning with regression analysis is employed to generate polynomials for dynamic delay modeling by approximation of the first-order electrical parameters of CMOS standard cells. With the parallelization on graphics processing units and simultaneous exploitation of multiple dimensions of parallelism the simulation throughput is maximized and scalable-design space exploration of AVFS-based systems is enabled. Results demonstrate the accuracy and efficiency with speedups of up to 159× over conventional logic level time simulation with static delays.|IEEE VLSI Test Symposium|2020|10.1109/VTS48691.2020.9107642|E. Schneider, H. Wunderlich|0.6|0
2234|Design of Navigation System for Liver Surgery Guided by Augmented Reality|The traditional surgical navigation system combining preoperative CT and intraoperative ultrasound is widely used in open hepatectomy, but the problem of this system is that there are some errors in terms of the precision of the time and the space during the surgery. In order to solve the above problems, this paper introduces augmented reality technology into the surgical navigation system. In order to accurately describe the biomechanical characteristics of the liver and let the navigation system perform more accurately, this paper uses Tetgen to perform tetrahedral partition on the triangle mesh data obtained after the three-dimensional CT was reconstructed, and then the surface mesh data and the internal tetrahedral data were obtained and they were used to describe a whole liver model. The surface triangle mesh data is used to render graphics and describe the surface topology change, and the internal tetrahedral data combined with mass-spring theory is used to simulate deformation. Subsequently, the ex vivo pig liver was used to experimentally verify the accuracy of gravity deformation of the liver model, and the results show that the error was mainly distributed between −2mm and −2.5mm. At the same time, this paper uses the NDI Polaris infrared tracking system to carry out precision experiments on the augmented reality module, and the measured error is 1.55± 0.29mm. Finally, various modules of the system are integrated to finish the experiment in which the ring-shaped lesions are cut from the ex vivo pig liver with the aid of augmented reality. The experimental error is 0± 1.26mm, and with the assistance of the general purpose graphics processing unit (GPGPU), the refresh rate is above 200FPS. The results prove that the liver surgery navigation system proposed in this paper is excellent in terms of real-time performance and accuracy, which can help doctors accurately locate the tumor during surgery and perform ideal resection.|IEEE Access|2020|10.1109/ACCESS.2020.3008690|Liqiang Sun, Shi Zhang, Fengfeng Zhang, Kai Zhong, Lingtao Yu|0.6|0
2263|Parallel GPU Implementation for Fast Generating System Adequacy Assessment via Sequential Monte Carlo Simulation|The Sequential Monte Carlo Simulation (SMCS) is a powerful and flexible method commonly used for generating system adequacy assessment. By sampling outage events in sequence and their respective duration, this method can easily incorporate time-dependent issues such as renewable power production, the capacity of hydro units, scheduled maintenance, complex correlated load models, etc, and is the only method that provides probability distributions for the reliability indexes. Despite these advantages, the SMCS method requires considerably more simulation time than the Non-sequential Monte Carlo Simulation approach to provide accurate estimates for the reliability indexes. In an attempt to reduce the simulation time, the SMCS method has been implemented in parallel using a Graphics Processing Unit (GPU) to take advantage of the fast calculations provided by these computing platforms. Two parallelization strategies are proposed: Strategy A, which creates and evaluates yearly samples in a completely parallel approach and while the estimates of the reliability indexes are computed in the CPU; and Strategy B, which consists on concurrently sampling the outage events for the generating units while the state evaluation and the index estimation stages are executed in serial. Simulation results for the IEEE RTS 79, IEEE RTS 96, and the new IEEE RTS GMLC test systems, show that both implementations lead to a significant acceleration of the SMCS method while keeping all its advantages. In addition, it was observed that Strategy B results in less simulation time than Strategy A for generation system adequacy assessment.|IEEE International Conference on Probabilistic Methods Applied to Power Systems|2020|10.1109/PMAPS47429.2020.9183554|Inês M. Alves, L. Carvalho, Vladimiro Miranda|0.6|0
2291|Wrinkles, Folds, Creases, Buckles: Small-Scale Surface Deformations as Periodic Functions on 3D Meshes|We propose a method for adding small-scale details to surfaces of 3D geometries in the context of interactive deformation computation of elastic objects. This is relevant in real-time applications, for instance, in surgical simulation or interactive animation. The key idea is the procedural generation of surface details via a weighted sum of periodic functions, applied as an on-surface displacement field. We first calculate local deformation strains of a low-resolution 3D input mesh, which are then employed to estimate amplitudes, orientations, and positions of high-resolution details. The shapes and spatial frequencies of the periodic details are obtained from mechanical parameters, assuming the physical model of a film-substrate aggregate. Finally, our approach creates the highly-detailed output mesh fully on the GPU. The performance is independent of the spatial frequency of the inserted details as well as, within certain limits, of the resolution of the output mesh. We can reproduce numerous commonly observed, characteristic surface deformation patterns, such as wrinkles or buckles, allowing for the representation of a wide variety of simulated materials and interaction processes. We highlight the performance of our method with several examples.|IEEE Transactions on Visualization and Computer Graphics|2020|10.1109/TVCG.2019.2914676|M. Harders, E. Zuenko|0.6|0
1773|Towards Controlled Single-Molecule Manipulation Using “Real-Time” Molecular Dynamics Simulation: A GPU Implementation|Molecular electronics saw its birth with the idea to build electronic circuitry with single molecules as individual components. Even though commercial applications are still modest, it has served an important part in the study of fundamental physics at the scale of single atoms and molecules. It is now a routine procedure in many research groups around the world to connect a single molecule between two metallic leads. What is unknown is the nature of this coupling between the molecule and the leads. We have demonstrated recently (Tewari, 2018, Ph.D. Thesis) our new setup based on a scanning tunneling microscope, which can be used to controllably manipulate single molecules and atomic chains. In this article, we will present the extension of our molecular dynamic simulator attached to this system for the manipulation of single molecules in real time using a graphics processing unit (GPU). This will not only aid in controlled lift-off of single molecules, but will also provide details about changes in the molecular conformations during the manipulation. This information could serve as important input for theoretical models and for bridging the gap between the theory and experiments.|Micromachines|2018|10.3390/mi9060270|J. V. van Ruitenbeek, F. Verbeek, Dyon van Vreumingen, S. Tewari|0.5714285714285714|0
1812|Graphics Processing Unit–Enhanced Genetic Algorithms for Solving the Temporal Dynamics of Gene Regulatory Networks|Understanding the regulation of gene expression is one of the key problems in current biology. A promising method for that purpose is the determination of the temporal dynamics between known initial and ending network states, by using simple acting rules. The huge amount of rule combinations and the nonlinear inherent nature of the problem make genetic algorithms an excellent candidate for finding optimal solutions. As this is a computationally intensive problem that needs long runtimes in conventional architectures for realistic network sizes, it is fundamental to accelerate this task. In this article, we study how to develop efficient parallel implementations of this method for the fine-grained parallel architecture of graphics processing units (GPUs) using the compute unified device architecture (CUDA) platform. An exhaustive and methodical study of various parallel genetic algorithm schemes—master-slave, island, cellular, and hybrid models, and various individual selection methods (roulette, elitist)—is carried out for this problem. Several procedures that optimize the use of the GPU’s resources are presented. We conclude that the implementation that produces better results (both from the performance and the genetic algorithm fitness perspectives) is simulating a few thousands of individuals grouped in a few islands using elitist selection. This model comprises 2 mighty factors for discovering the best solutions: finding good individuals in a short number of generations, and introducing genetic diversity via a relatively frequent and numerous migration. As a result, we have even found the optimal solution for the analyzed gene regulatory network (GRN). In addition, a comparative study of the performance obtained by the different parallel implementations on GPU versus a sequential application on CPU is carried out. In our tests, a multifold speedup was obtained for our optimized parallel implementation of the method on medium class GPU over an equivalent sequential single-core implementation running on a recent Intel i7 CPU. This work can provide useful guidance to researchers in biology, medicine, or bioinformatics in how to take advantage of the parallelization on massively parallel devices and GPUs to apply novel metaheuristic algorithms powered by nature for real-world applications (like the method to solve the temporal dynamics of GRNs).|Evolutionary bioinformatics online|2018|10.1177/1176934318767889|Raúl García-Calvo, J. Guisado, F. Díaz-del-Río, A. Córdoba, F. Jiménez-Morales|0.5714285714285714|0
1871|Development of Haptic-Enabled Virtual Reality Simulator for Video-Assisted Thoracoscopic Right Upper Lobectomy|Video-assisted thoracoscopic surgery (VATS), referred to as the commonest minimum invasive excision for located T1 or T2 lung carcinomas, requires a steep learning curve for the novice residents to acquire highly deliberate skills to achieve surgical competence. The aim of this study is to propose a virtual reality-based (VR) surgical educative simulator with realistic performance in both visual and haptic sensation for the VAST procedures. To provide an immersive and perceptual user interface, we combined the customized haptic-enabled thoracoscopic instruments with HTC VIVE helmet set in our simulation system. In particular, position based deformation (PBD) method on the GPU and a novel haptic rendering algorithm of surgical grasps and stapling operations are also been implemented for the surgical scene, respectively for the soft tissue deformation and intraoperative force manipulation simulation. Experiments by thoracic surgery professors and novices' evaluation results on our framework demonstrated a high performance and distinguished accurately. These study findings suggested a new cognitive model for the VATS surgical education integrate with haptic and VR implementation.|IEEE International Conference on Systems, Man and Cybernetics|2018|10.1109/SMC.2018.00511|Yonghang Tai, Jun Peng, S. Nahavandi, Lei Wei, Hailing Zhou, Qiong Li, Junsheng Shi|0.5714285714285714|0
1874|Performance Evaluation of Boids on the GPU and CPU|Context. Agent based models are used to simulate complex systems by using multiple agents that follow a set of rules. One such model is the boid model which is used to simulate movements of synchro ...||2018|10.1111/1471-0528.15490|S. Lindqvist|0.5714285714285714|0
2279|3-D Simulation of Double-Bar Plush Fabrics with Jacquard Patterns|Abstract A realistic computerized simulation of double-bar plush fabrics can result in a time-saving development process with high quality. Based on basic analysis of jacquard principles, a fast 3-D simulation method of warp-knitted plush fabrics is proposed by using a geometry shader on GPU. Firstly, pile areas and non-pile areas are identified according to the jacquard design graphs and chain notations. According to the directions of observation and raised pile, two layered chips are formed in the geometry shader with an approach of multi-layered textures. To ensure that the simulated piles resemble the real ones, the directions of the piles are randomized with the Perlin noise method. One pile is generated along its length with numerous layers in the plush fabric model. Simulation results of piles on both the technical face and technical back are obtained via the model built above, which is confirmed with practicability and efficiency. This 3D simulation approach improves the visualization appearance of piles just as they are actually raised.|Autex Research Journal|2018|10.1515/aut-2017-0029|Honglian Cong, Zhijia Dong, Aijun Zhang, Xinxin Li, G. Jiang|0.5714285714285714|0
1640|An analysis of the feasibility and benefits of GPU/multicore acceleration of the Weather Research and Forecasting model|There is a growing need for ever more accurate climate and weather simulations to be delivered in shorter timescales, in particular, to guard against severe weather events such as hurricanes and heavy rainfall. Due to climate change, the severity and frequency of such events – and thus the economic impact – are set to rise dramatically. Hardware acceleration using graphics processing units (GPUs) or Field‐Programmable Gate Arrays (FPGAs) could potentially result in much reduced run times or higher accuracy simulations. In this paper, we present the results of a study of the Weather Research and Forecasting (WRF) model undertaken in order to assess if GPU and multicore acceleration of this type of numerical weather prediction (NWP) code is both feasible and worthwhile. The focus of this paper is on acceleration of code running on a single compute node through offloading of parts of the code to an accelerator such as a GPU. The governing equations set of the WRF model is based on the compressible, non‐hydrostatic atmospheric motion with multi‐physics processes. We put this work into context by discussing its more general applicability to multi‐physics fluid dynamics codes: in many fluid dynamics codes, the numerical schemes of the advection terms are based on finite differences between neighboring cells, similar to the WRF code. For fluid systems including multi‐physics processes, there are many calls to these advection routines. This class of numerical codes will benefit from hardware acceleration. We studied the performance of the original code of the WRF model and proposed a simple model for comparing multicore CPU and GPU performance. Based on the results of extensive profiling of representative WRF runs, we focused on the acceleration of the scalar advection module. We discuss the implementation of this module as a data‐parallel kernel in both OpenCL and OpenMP. We show that our data‐parallel kernel version of the scalar advection module runs up to seven times faster on the GPU compared with the original code on the CPU. However, as the data transfer cost between GPU and CPU is very high (as shown by our analysis), there is only a small speed‐up (two times) for the fully integrated code. We show that it would be possible to offset the data transfer cost through GPU acceleration of a larger portion of the dynamics code. In order to carry out this research, we also developed an extensible software system for integrating OpenCL code into large Fortran code bases such as WRF. This is one of the main contributions of our work. We discuss the system to show how it allows the replacement of the sections of the original codebase with their OpenCL counterparts with minimal changes – literally only a few lines – to the original code. Our final assessment is that, even with the current system architectures, accelerating WRF – and hence also other, similar types of multi‐physics fluid dynamics codes – with a factor of up to five times is definitely an achievable goal. Accelerating multi‐physics fluid dynamics codes including NWP codes is vital for its application to weather forecasting, environmental pollution warning, and emergency response to the dispersion of hazardous materials. Implementing hardware acceleration capability for fluid dynamics and NWP codes is a prerequisite for up‐to‐date and future computer architectures. Copyright © 2015 John Wiley & Sons, Ltd.|Concurrency and Computation|2016|10.1002/cpe.3522|W. Vanderbauwhede, T. Takemi|0.5555555555555556|0
1889|Detailed numerical simulation of shock-body interaction in 3D multicomponent flow using the RKDG numerical method and ”DiamondTorre” GPU algorithm of implementation|Interaction between a shock wave and an inhomogeneity in fluid has complicated behavior, including vortex and turbulence generating, mixing, shock wave scattering and reflection. In the present paper we deal with the numerical simulation of the considered process. The Euler equations of unsteady inviscid compressible three-dimensional flow are used into the four-equation model of multicomponent flow. These equations are discretized using the RKDG numerical method. It is implemented with the help of the DiamondTorre algorithm, so the effective GPGPU solver is obtained having outstanding computing properties. With its use we carry out several sets of numerical experiments of shock-bubble interaction problem. The bubble deformation and mixture formation is observed.||2016|10.1088/1742-6596/681/1/012046|B. Korneev, V. Levchenko|0.5555555555555556|0
1919|GPU accelerated coverage path planning optimized for accuracy in robotic inspection applications|In this paper, we introduce a coverage path planning algorithm for inspecting large structures optimized to generate highly accurate 3D models. Robotic inspection of structures such as aircrafts, bridges and buildings, is considered a critical task since missing any detail could affect the performance and integrity of the structures. Additionally, it is a time and resource intensive task that should be performed as efficiently and accurately as possible. The method we propose is a model based coverage path planning approach that generates an optimized path that passes through a set of admissible waypoints to cover a complex structure. The coverage path planning algorithm is developed with a heuristic reward function that exploits our knowledge of the structure mesh model, and the UAV's onboard sensors' models to generate optimal paths that maximizes coverage and accuracy, and minimizes distance travelled. Moreover, we accelerated critical components of the algorithm utilizing the Graphics Processing Unit (GPU) parallel architecture. A set of experiments were conducted in a simulated environment to test the validity of the proposed algorithm.|Midwest Symposium on Circuits and Systems|2016|10.1109/MWSCAS.2016.7869968|Randa Almadhoun, Guowei Cai, T. Taha, J. Dias, L. Seneviratne|0.5555555555555556|0
2184|CUDA parallel programming for simulation of epidemiological models based on individuals|Mathematical models are of great value in epidemiology to help understand the dynamics of the various infectious diseases, as well as in the conception of effective control strategies. The classical approach is to use differential equations to describe, in a quantitative manner, the spread of diseases within a particular population. An alternative approach is to represent each individual in the population as a string or vector of characteristic data and simulate the contagion and recovery processes by computational means. This type of model, referred in the literature as MBI (models based on individuals), has the advantage of being flexible as the characteristics of each individual can be quite complex, involving, for instance, age, sex, pre‐existing health conditions, environmental factors, social habits, etc. However, when it comes to simulations involving large populations, MBI may require a large computational effort in terms of memory storage and processing time. In order to cope with the problem of heavy computational effort, this paper proposes a parallel implementation of MBI using a graphics processor unit compatible with CUDA. It was found that, even in the case of a simple susceptible–infected–recovered model, the computational gains in terms of processing time are significant. Copyright © 2015 John Wiley & Sons, Ltd.||2016|10.1002/mma.3490|T. W. Lima, A. R. G. Filho, C. Coelho, A. Soares, L. C. M. D. Paula|0.5555555555555556|0
2316|A Platform-Oblivious Approach for Heterogeneous Computing: A Case Study with Monte Carlo-based Simulation for Medical Applications|Light is important and helpful in many medical applications, such as cancer treatment. Computer modeling and simulation of light transport are often adopted to improve the quality of medical treatments. In particular, Monte Carlo-based simulations are considered to deliver accurate results, but require intensive computational resources. While several attempts to accelerate the Monte Carlo-based methods for the simulation of photon transport with platform-specific programming schemes, such as CUDA on GPU and HDL on FPGA, have been proposed, the approach has limited portability and prolongs software updates. In this paper, we parallelize the Monte Carlo modeling of light transport in multi-layered tissues (MCML) program with OpenCL, an open standard supported by a wide range of platforms. We characterize the performance of the parallelized MCML kernel program runs on CPU, GPU and FPGA. Compared to platform-specific programming schemes, our platform-oblivious approach provides a unified, highly portable code and delivers competitive performance and power efficiency.|Symposium on Field Programmable Gate Arrays|2016|10.1145/2847263.2847335|Min-yu Tsai, Shih-Hao Hung, B. Huang, Chia-Heng Tu|0.5555555555555556|0
1642|Fast and universal single molecule localization using multi-dimensional point spread functions|The recent development of single molecule imaging techniques has enabled not only high accuracy spatial resolution imaging but also information rich functional imaging. Abundant information of the single molecules can be encoded in its diffraction pattern and be extracted precisely (e.g. 3D position, wavelength, dipole orientation). However, sophisticated high dimensional point spread function (PSF) modeling and analyzing methods have greatly impeded the broad accessibility of these techniques. Here, we present a graphics processing unit (GPU)-based B-spline PSF modeling method which could flexibly model high dimensional PSFs with arbitrary shape without greatly increasing the model parameters. Our B-spline fitter achieves 100 times speed improvement and minimal uncertainty for each dimension, enabling efficient high dimensional single molecule analysis. We demonstrated, both in simulations and experiments, the universality and flexibility of our B-spline fitter to accurately extract the abundant information from different types of high dimensional single molecule data including multicolor PSF (3D + color), multi-channel four-dimensional 4Pi-PSF (3D + interference phase) and five-dimensional vortex PSF (3D + dipole orientation).|bioRxiv|2023|10.1101/2023.10.17.562517|Yiming Li, Wei Shi, Lulu Zhou, Shuang Fu, Sheng Liu, Mengfan Li, Yue Fei|0.5|0
1651|Autonomic Management of 3D Cardiac Simulations|Large scale scientific applications in general and especially cardiac simulations experience different execution phases at runtime and each phase has different computational and communication requirements. An optimal solution or numerical scheme for one execution phase might not be appropriate for the next phase of the application execution. We propose an autonomic management framework, which is built on the physics aware programming (PAP) paradigm for accelerating the cardiac simulations further beyond what can be achieved through traditional parallelization efforts. This approach effectively exploits the physical properties of the cardiac simulation by being smart in the development of simulation algorithms. The cardiac simulation phase is periodically monitored and analyzed to identify its current execution phase. We apply machine learning techniques to detect the phase of the simulation during each time step of the 3D model of a human ventricular epicardial myocyte simulation. For each change in the simulation phase, we exploit the spatial and temporal attributes, dynamically change the resolution of the simulation, and select the numerical algorithms/solvers that optimize its performance without sacrificing the accuracy of the simulation. We compare the performance of the PAP-based algorithm in terms of simulation accuracy and execution time with respect to the reference simulation, which is considered the high-precision implementation. We achieve an overall speedup of 28.4× with a simulation accuracy of 99.9% with the PAP-based cardiac simulations. We also couple the PAP with a multi-graphics processing units (GPU) implementation, and show up to 191× speedup on a 16-GPU system.|International Conference on Cloud and Autonomic Computing|2017|10.1109/ICCAC.2017.8|G. Ditzler, S. Hariri, J. Szep, A. Akoglu, T. Moukabary, E. Esmaili|0.5|0
1673|Parallel Makespan Calculation for Flow Shop Scheduling Problem with Minimal and Maximal Idle Time|In this paper, a flow shop scheduling problem with minimal and maximal machine idle time with the goal of minimizing makespan is considered. The mathematical model of the problem is presented. A generalization of the prefix sum, called the job shift scan, for computing required shifts for overlapping jobs is proposed. A work-efficient algorithm for computing the job shift scan in parallel for the PRAM model with n processors is proposed and its time complexity of O(logn) is proven. Then, an algorithm for computing the makespan in time O(mlogn) in parallel using the prefix sum and job shift scan is proposed. Computer experiments on GPU were conducted using the CUDA platform. The results indicate multi-thread GPU vs. single-thread GPU speedups of up to 350 and 1000 for job shift scan and makespan calculation algorithms, respectively. Multi-thread GPU vs. single-thread CPU speedups up to 4.5 and 14.7, respectively, were observed as well. The experiments on the Taillard-based problem instances using a simulated annealing solving method and employing the parallel makespan calculation show that the method is able to perform many more iterations in the given time limit and obtain better results than the non-parallel version.|Applied Sciences|2021|10.3390/app11178204|J. Rudy|0.5|0
1724|SiSyPHE: A Python package for the Simulation of Systems of interacting mean-field Particles with High Efficiency|Over the past decades, the study of systems of particles has become an important part of many research areas, from theoretical physics to applied biology and computational mathematics. One of the main motivations in mathematical biology is the modelling of large animal societies and the emergence of complex patterns from simple behavioral rules, e.g., flocks of birds, fish schools, ant colonies, etc. In the microscopic world, particle systems are used to model a wide range of phenomena, from the collective motion of spermatozoa to the anarchical development of cancer cells. Within this perspective, there are at least three important reasons to conduct large scale computer simulations of particle systems. First, numerical experiments are essential to calibrate the models and test the influence of each parameter in a controlled environment. For instance, the renowned Vicsek model (Vicsek et al., 1995) is a minimal model of flocking, which exhibits a complex behavior, studied numerically in particular in (Chaté et al., 2008). Secondly, particle simulations are used to check the validity of macroscopic models that describe the statistical behavior of particle systems. These models are usually based on partial differential equations (PDE) derived using phenomenological considerations that are often difficult to justify mathematically (Degond et al., 2021; Degond & Motsch, 2008; Dimarco & Motsch, 2016). Finally, inspired by models in biology, there is an ever growing literature on the design of algorithms based on the simulation of artificial particle systems to solve tough optimization problems (Grassi & Pareschi, 2020; Kennedy & Eberhart, 1995; Pinnau et al., 2017; Totzeck, 2021) and to construct new more efficient Markov Chain Monte Carlo methods (Cappé et al., 2004; Clarté et al., 2021; Del Moral, 1998, 2013; Doucet et al., 2001). The simulation of systems of particles is also at the core of molecular dynamics (Leimkuhler & Matthews, 2015), although the present library is not specifically written for this purpose. The SiSyPHE library builds on recent advances in hardware and software for the efficient simulation of large scale interacting mean-field particle systems, both on the GPU and on the CPU. The versatile object-oriented Python interface of the library is designed for the simulation and comparison of new and classical many-particle models of collective dynamics in mathematics and active matter physics, enabling ambitious numerical experiments and leading to novel conjectures and results.|Journal of Open Source Software|2021|10.21105/joss.03653|A. Diez|0.5|0
1731|Improving Ray Tracing Based Radio Propagation Model Performance Using Spatial Acceleration Structures|Ray tracing based propagation models are suited to simulate wireless networks in complex environments and can stand in for physical measurements if they are inaccessible. However, due to the associated computational cost, engineers have to trade off the size of modelled scenarios, level of detail and time resolution to keep simulation times feasible. We present a Monte Carlo ray tracing model that is implemented entirely on the GPU. State of the art spatial acceleration structures are used to optimize the underlying ray tracing routines. The model is verified and the implementations performance is measured in five demo scenes. We can report interactive frame rates in scenes with up to 7M triangles, 2M rays and multiple bounces.|Q2S and Security for Wireless and Mobile Networks|2021|10.1145/3479242.3487318|J. Roßmann, Jan Reitz, Moritz Alfrink|0.5|0
1744|Implementation of Particle Filters for Single Target Tracking Using CUDA|In order to implement Sequential Bayesian estimator using Monte carlo simulation and to get rid of limitations of Kalman filter, Particle filtering techniques plays a very crucial role for target tracking applications in state space where Importance sampling approximately distributed by posterior distribution with multimodel feature and robustness to noise. However as the particles becomes very large, the Monte Carlo representation becomes nearly equivalent to analytical description characterization for posterior distributions and has some deficiencies such as high computational cost and low sampling efficiency. Therefore, emerging computing platform, CUDA may be regarded as most appealing platform for such implementaion. Representation provided with set of samples for target distribution of state leads to increase in sampling efficiency so that graphics processing unit (GPUPU) becomes more appealing to use in Particle filter. The modification on mapping architecture are evaluated with qualitative analysis. The proposed design will be 3.5 times faster than direct design.|2015 Fifth International Conference on Advances in Computing and Communications (ICACC)|2015|10.1109/ICACC.2015.111|Tarun Budhraja, C. Shivakumar, Roheet Bhatnagar, Bhavya Goyal|0.5|0
1752|Vectorized Uncertainty Propagation and Input Probability Sensitivity Analysis|In this article we construct a theoretical and computational process for assessing Input Probability Sensitivity Analysis (IPSA) using a Graphics Processing Unit (GPU) enabled technique called Vectorized Uncertainty Propagation (VUP). VUP propagates probability distributions through a parametric computational model in a way that's computational time complexity grows sublinearly in the number of distinct propagated input probability distributions. VUP can therefore be used to efficiently implement IPSA, which estimates a model's probabilistic sensitivity to measurement and parametric uncertainty over each relevant measurement location. Theory and simulation illustrate the effectiveness of these methods.||2019|10.1115/detc2019-97766|Kevin Vanslette, A. Alanqari, Zeyad Alawwad, K. Youcef-Toumi|0.5|0
1795|Fly4Arts: Evolutionary Digital Art with the Fly Algorithm|The aim of this study is to generate artistic images, such as digital mosaics, as an optimisation problem without the introduction of any a priori knowledge or constraint other than an input image. The usual practice to produce digital mosaic images heavily relies on Centroidal Voronoi diagrams. We demonstrate here that it can be modelled as an optimisation problem solved using a cooperative co-evolution strategy based on the Parisian evolution approach, the Fly algorithm. An individual is called a fly. Its aim of the algorithm is to optimise the position of infinitely small 3-D points (the flies). The Fly algorithm has been initially used in real-time stereo vision for robotics. It has also demonstrated promising results in image reconstruction for tomography. In this new application, a much more complex representation has been study. A y is a tile. It has its own position, size, colour, and rotation angle. Our method takes advantage of graphics processing units (GPUs) to generate the images using the modern OpenGL Shading Language (GLSL) and Open Computing Language (OpenCL) to compute the difference between the input image and simulated image. Different types of tiles are implemented, some with transparency, to generate different visual effects, such as digital mosaic and spray paint. An online study with 41 participants has been conducted to compare some of our results with those generated using an open source software for image manipulation. It demonstrates that our method leads to more visually appealing images.||2017|10.1007/978-3-319-55849-3_30|F. Vidal, Z. Abbood|0.5|0
1799|A GPU-parallel construction of volumetric tree|Volume data has extensive uses in medical imaging, like, MRI (magnetic resonance imaging) scan, visual effects production, including volume rendering and fluid simulation, computer-aided design and manufacturing (CAD/CAM) in advanced prototyping, such as, 3D Printing, among others. This work presents a compact hierarchical data structure, dubbed HDT, for extreme-scale volume modeling. The name is shorthand for hybrid dynamic tree. HDT is hybrid in that it fuses two contrasting structures, namely, octree and tiled grid. Such fusion of two opposing data layouts alleviate the limitations inherent to the respective structures. We describe the HDT construction algorithm to generate volumetric representation of triangle mesh on GPUs. While HDT mirrors much of the existing works on sparse 3D data modeling, our evaluation and comparative studies with prior researches demonstrate that HDT is arguably the most storage-effective representation. The geometric topology in HDT consumes just short of two bits per voxel --- five times compact relative to the current state-of-the-art volumetric structure.|IA3@SC|2015|10.1145/2833179.2833191|Thomas M. Tucker, Mohammad M. Hossain, T. Kurfess, R. Vuduc|0.5|0
1800|A GPU Implementation of Watercolor Painting Image Generation|Stroke-based rendering is a technique to generate images with the painting effect of an actual brush stroke by repeating a drawing, called a stroke, many times. In this paper, we propose a watercolor image generation with stroke-based rendering. We use a physical model that simulates a watercolor painting on paper by computing the movement of water and pigment. This method requires a large number of strokes to be drawn one by one, and the computational cost of watercolor simulation for each stroke drawn is also very high. Therefore, to reduce the generation time, we propose a parallel algorithm for drawing multiple strokes simultaneously. The idea of the parallel algorithm is to generate multiple strokes independently. However, since some of the generated strokes have overlaps, such strokes cannot be drawn simultaneously. In our approach, we find non-overlapping strokes by reducing this problem to the independent point set problem and solving it instead. Furthermore, we implement this parallel algorithm on the GPU. Experimental results show that the proposed GPU implementation on NVIDIA GeForce RTX 3090 attains a speed-up factor of up to 75 over a sequential execution on the CPU.|2021 Ninth International Symposium on Computing and Networking Workshops (CANDARW)|2021|10.1109/CANDARW53999.2021.00031|Jiamian Huang, Yasuaki Ito, K. Nakano|0.5|0
1807|High-performance mesoscopic traffic simulation with GPU for large scale networks|Mesoscopic Traffic Simulation is an important tool in traffic analysis and traffic management support. The balance between traffic modeling details and performance has made Mesoscopic Traffic Simulation one of the key solutions for traffic controllers and policy makers. Mesoscopic traffic simulators offer acceptable speed in simulating normal traffic. However, when traffic prediction and optimization for large scale networks come into context, the performance of mesoscopic traffic simulators is unsatisfactory in optimizing a massive number of control parameters for a much longer prediction horizon. This issue again emphasizes the need to further speed up mesoscopic traffic simulation. This paper proposes a comprehensive framework to massively speed up mesoscopic traffic simulation using GPU without compromising its correctness and realistic modeling property. It also gives an in-depth analysis into the trade-off between simulation correctness and performance speedup. By combining the power of GPU with optimal design and data structures, mesoscopic traffic simulation is able to speed up to more than 6 times compared to original CPU implementation.|IEEE International Symposium on Distributed Simulation and Real-Time Applications|2017|10.1109/DISTRA.2017.8167676|Gary S. H. Tan, V. Vu|0.5|0
1814|Modeling of a spiral heat exchanger using fractional order equation and GPU|A spiral heat exchanger is a nonliear system with high heat transfer efficiency and delay time. Its mathematics model constructed by fractional differential equations is more accurate than that constructed by integer differential equation in traditional method. In this paper, nonlinear system for a spiral heat exchanger with bounded perturbations described by fractional order equation is considered. Firstly, some preliminary concepts of fractional order and parallel numerical model of fractional order equation are introduced. Secondly, parallel numerical model of fractional order (PNMFO) description for a spiral heat exchanger model on GPU is proposed. Finally, the proposed model is simulated and analysed.|International Conference on Advanced Mechatronic Systems|2019|10.1109/ICAMechS.2019.8861652|M. Deng, Guanqiang Dong, Longguo Jin|0.5|0
1842|Scaling soft matter physics to thousands of graphics processing units in parallel|We describe a multi-graphics processing unit (GPU) implementation of the Ludwig application, which specialises in simulating a variety of complex fluids via lattice Boltzmann fluid dynamics coupled to additional physics describing complex fluid constituents. We describe our methodology in augmenting the original central processing unit (CPU) version with GPU functionality in a maintainable fashion. We present several optimisations that maximise performance on the GPU architecture through tuning for the GPU memory hierarchy. We describe how we implement particles within the fluid in such a way to avoid a major diversion of the CPU and GPU codebases, whilst minimising data transfer at each time step. We detail our halo-exchange communication phase for the code, which exploits overlapping to allow efficient parallel scaling to many GPUs. We present results showing that the application demonstrates excellent scaling to at least 8192 GPUs in parallel, the largest system tested at the time of writing. The GPU version (on NVIDIA K20X GPUs) is around 3.5–5 times faster that the CPU version (on fully utilised AMD Opteron 6274 16-core CPUs), comparing equal numbers of CPUs and GPUs.|The international journal of high performance computing applications|2015|10.1177/1094342015576848|O. Henrich, A. Hart, A. Gray, K. Stratford|0.5|0
1896|DEVELOPMENT OF ADVANCED COMPUTATIONAL FLUID DYNAMICS TOOLS AND THEIR APPLICATION TO SIMULATION OF INTERNAL TURBULENT FLOWS|Modern graphics processing units (GPU) provide architectures and new programming models that enable to harness their large processing power and to design computational fluid dynamics (CFD) simulations at both high performance and low cost. Possibilities of the use of GPUs for the simulation of internal fluid flows are discussed. The finite volume method is applied to solve three-dimensional (3D) unsteady compressible Euler and Navier-Stokes equations on unstructured meshes. Compute Inified Device Architecture (CUDA) technology is used for programming implementation of parallel computational algorithms. Solution of some fluid dynamics problems on GPUs is presented and approaches to optimization of the CFD code related to the use of different types of memory are discussed. Speedup of solution on GPUs with respect to the solution on central processor unit (CPU) is compared with the use of different meshes and different methods of distribution of input data into blocks. Performance measurements show that numerical schemes developed achieve 20 to 50 speedup on GPU hardware compared to CPU reference implementation. The results obtained provide promising perspective for designing a GPU-based software framework for applications in CFD.||2015|10.1051/EUCASS/201507247|K. Volkov, A. Karpenko, V. Emelyanov|0.5|0
1930|Reducing Data Motion and Energy Consumption of Geospatial Modeling Applications Using Automated Precision Conversion|The burgeoning interest in large-scale geospatial modeling, particularly within the domains of climate and weather prediction, underscores the concomitant critical importance of accuracy, scalability, and computational speed. Harnessing these complex simulations’ potential, however, necessitates innovative computational strategies, especially considering the increasing volume of data involved. Recent advancements in Graphics Processing Units (GPUs) have opened up new avenues for accelerating these modeling processes. In particular, their efficient utilization necessitates new strategies, such as mixed-precision arithmetic, that can balance the trade-off between computational speed and model accuracy. This paper leverages PaRSEC runtime system and delves into the opportunities provided by mixed-precision arithmetic to expedite large-scale geospatial modeling in heterogeneous environments. By using an automated conversion strategy, our mixed-precision approach significantly improves computational performance (up to 3X) on Summit supercomputer and reduces the associated energy consumption on various Nvidia GPU generations. Importantly, this implementation ensures the requisite accuracy in environmental applications, a critical factor in their operational viability. The findings of this study bear significant implications for future research and development in high-performance computing, underscoring the transformative potential of mixed-precision arithmetic on GPUs in addressing the computational demands of large-scale geospatial modeling and making a stride toward a more sustainable, efficient, and accurate future in large-scale environmental applications.|IEEE International Conference on Cluster Computing|2023|10.1109/CLUSTER52292.2023.00035|M. Genton, David E. Keyes, Qinglei Cao, G. Bosilca, Sameh Abdulah, H. Ltaief|0.5|0
1933|A GPU-Accelerated 3D Visual Simulation Platform of X-Ray Digital Imaging Based on Unity|X-ray digital imaging is an advanced and widely used non-destructive testing technology, X-ray digital imaging system faces some problems in actual inspection such as high costs and radiation effects. Computer simulation technology, which comprehensively applies mathematics, computer graphics and image processing technology, can solve these problems well and has its own advantages. At present, most of the research on X-ray digital imaging simulation has slow imaging speed, poor visibility and poor interaction. In this paper, we make a mathematical model for X-ray digital imaging and build a 3D visual simulation platform in Unity. By using GPU acceleration technology, the imaging speed is reduced to only a fraction of a second which can satisfy real-time interaction. This simulation platform has good demonstration and comfortable user experience based on the 3D visualization technology. This work will promote the applications of computer simulation technology in X-ray digital imaging technology.|International Conference on Computer and Automation Engineering|2023|10.1109/ICCAE56788.2023.10111203|Junyi Lu, J. Fu, Zhiyu Gao, Xianliang Gao|0.5|0
2022|Optimizing Intersection and Reflection Step of Geometrical Optics using GPUs|Ray tracing method (geometrical optics) is a technique to trace the path of waves or particles. It is used in various areas like computer graphics or physics. However, the ray tracing method is very slow to compute on CPU because it includes a large amount of computations. Therefore, it is important to reduce the computation time. For example, in physics, a fast ray tracing method can simulate propagation modeling on a lot of cases within limited time, and in computer graphics, the fast method can display 3-D images in your monitor as real time. In this paper, we develop the ray tracing method using Graphic Processing Unit (GPU) to compute intersection and reflection test. Because there are a lot of simple vector operations in ray tracing method, we accelerate the method using GPU that is powerful product to compute many simple calculations like vector operations. Therefore, we can extremely improve the performance of the ray tracing method compared to the conventional CPU-based ray tracing method. Based on our evaluation, we achieve a performance improvement of about 1,740 times.||2017|10.1109/tpds.2017.2704080|M. Yoon, H. Chung, W. Ro|0.5|0
2078|Locality data properties of 3D data orderings with application to parallel molecular dynamics simulations|General-purpose computing on GPUs is widely adopted for scientific applications, providing inexpensive platforms for massively parallel computation. This has motivated us to investigate GPU performance in terms of speed and memory usage, specifically in relation to data locality in molecular dynamics simulations. The assumption is that enhancing data locality of these applications will lower the cost of data movement across the GPU memory hierarchy. In this research, we analyse spatial data locality and data reuse (temporal data locality) characteristics for row-major, Hilbert, and Morton data orderings, and hybrid variants of these, and assess their impact on the performance of molecular dynamics simulations (MDS). Data locality in MDS applications, based on the relationship between a bin and its neighbouring bins, that are generated using an approximately spherical stencil, previously has not been widely studied. In this research, a simple cache model is presented, and this is found to yield results that are consistent with timing results for the particle force computation obtained on NVIDIA Geforce GTX960 and Tesla P100 graphical processing units (GPUs). The NVIDIA profiling tool is used to investigate the execution time results and to observe the memory usage in terms of cache hits and the number of memory transactions. The analysis also provides a more detailed explanation of execution behaviour for the different orderings. To the best of our knowledge, this is the first study to investigate memory analysis and data locality issues for molecular dynamics simulations of Lennard-Jones fluids on NVIDIA’s Maxwell and Tesla architectures.||2019|10.1177/1094342019846282|Ibrahim Al Kharusi|0.5|0
2097|Evolutionary Game Theory-Based Optimal Scheduling Strategy for Heterogeneous Computing|With the development of intelligent applications, simply relying on traditional single type of computing unit cannot efficiently satisfy diverse cloud requirements. The emergence of heterogeneous computing can efficiently achieve the adaptation of these intelligent applications by using different types of processing units such as Graphics Processing Unit (GPU) and Field Programmable Gate Array (FPGA). However, the trade-off between profit and costs in the process of scheduling heterogeneous computing resources is also an issue worthy of attention. To address this challenge, this work establishes a heterogeneous computing resource scheduling model based on Stackelberg differential game, which includes three roles Computing Power Trading Platforms (CPTPs), Heterogeneous Computing Service Providers (HCSPs), and Heterogeneous Computing Application Providers (HCAPs). The objective is to maximize utility function of CPTPs and HCSPs subject to rental ratio, pricing strategy and energy consumption of resource scheduling, which has proved that there exists a Stackelberg Nash Equilibrium (NE) solution. The Support Vector Machine based on Artificial Fish (SVM-AF) is proposed to predict the access times of heterogeneous computing applications. In addition, the distributed iteration method and Cauchy distribution is adopted to optimize the computing price strategy and improve its convergence performance. The simulation results show that compared with other strategies, the proposed strategy can effectively improve computing revenue of user experience and while reducing energy consumption in the process of resource scheduling.|IEEE Access|2023|10.1109/ACCESS.2023.3272732|Wei Zhao, Rui She|0.5|0
2127|Creating 3D Virtual Driving Environments for Simulation-Aided Development of Autonomous Driving and Active Safety|Recreating traffic scenarios for testing autonomous driving in the real world requires significant time, resources and expense, and can present a safety risk if hazardous scenarios are tested. Using a 3D virtual environment to enable testing of many of these traffic scenarios on the desktop or cluster significantly reduces the amount of required road tests. In order to facilitate the development of perception and control algorithms for level 4 autonomy, a shared memory interface between MATLAB, Simulink, and Unreal Engine 4 can send information (such as vehicle control signals) back to the virtual environment. The shared memory interface conveys arbitrary numerical data, RGB image data, and point cloud data for the simulation of LiDAR sensors. The interface consists of a plugin for Unreal Engine, which contains the necessary read/write functions, and a beta toolbox for MATLAB, capable of reading and writing from the same shared memory locations specified in Unreal Engine, MATLAB, and Simulink. The LiDAR sensor model was tested by generating point clouds with beam patterns that mimic Velodyne HDL-32E (32 beam) sensors and is demonstrated to run at sufficient frame rates for real-time computations by leveraging the Graphics Processing Unit (GPU).||2017|10.4271/2017-01-0107|A. Micks, Arvind Jayaraman, E. Gross|0.5|0
2189|Heuristic Tree-Partition-Based Parallel Method for Biophysically Detailed Neuron Simulation|Abstract Biophysically detailed neuron simulation is a powerful tool to explore the mechanisms behind biological experiments and bridge the gap between various scales in neuroscience research. However, the extremely high computational complexity of detailed neuron simulation restricts the modeling and exploration of detailed network models. The bottleneck is solving the system of linear equations. To accelerate detailed simulation, we propose a heuristic tree-partition-based parallel method (HTP) to parallelize the computation of the Hines algorithm, the kernel for solving linear equations, and leverage the strong parallel capability of the graphic processing unit (GPU) to achieve further speedup. We formulate the problem of how to get a fine parallel process as a tree-partition problem. Next, we present a heuristic partition algorithm to obtain an effective partition to efficiently parallelize the equation-solving process in detailed simulation. With further optimization on GPU, our HTP method achieves 2.2 to 8.5 folds speedup compared to the state-of-the-art GPU method and 36 to 660 folds speedup compared to the typical Hines algorithm.|Neural Computation|2023|10.1162/neco_a_01565|Tiejun Huang, Kai Du, Yichen Zhang|0.5|0
2267|Simulating Complex Fluids with Smoothed Particle Hydrodynamics|Complex fluid dynamics encompasses a large variety of flows, such as fluids with non-Newtonian rheology, multi-phase and multi-fluid flows (suspensions, lather, solid/fluid interaction with floating objects, etc), violent flows (breaking waves, dam-breaks, etc), fluids with thermal dependencies and phase transition or free-surface flows. Correctly modeling the behavior of such flows can be quite challenging, and has led to significant advances in the field of Computational Fluid Dynamics (CFD). Recently, the Smoothed Particle Hydrodynamics (SPH) method has emerged as a powerful alternative to more classic CFD methods (such as finite volumes or finite elements) in many fields, including oceanography, volcanology, structural engineering, nuclear physics and medicine. With SPH, the fluid is discretized by means of particles and thanks to the meshless, Lagrangian nature of the model, it easily allows the modeling and simulation of both simple and complex fluids, simplifying the treatment of aspects which can be challenging with more traditional methods: dynamic free surfaces, large deformations, phase transition, fluid/solid interaction and complex geometries. In addition, the most common SPH formulations are fully parallelizable, which favors implementation on high-performance parallel computing hardware, such as modern Graphics Processing Units (GPUs). We present here how GPUSPH, an implementation of the SPH method that runs on GPUs, can model a variety of complex fluids, highlighting the computational challenges that arise in its applications to problem of great interest in volcanology.||2017|10.4401/AG-7362|C. Negro, R. Dalrymple, L. Fortuna, G. Bilotta, V. Zago, G. Ganci, A. Cappello, A. Hérault|0.5|0
2284|Towards Realtime|This paper introduces a hair simulator optimized for real-time applications, including console and cloud gaming, avatar live-streaming, and metaverse environments. We view the collisions between strands as a mechanism to preserve the overall volume of the hair and adopt explicit Material Point Method (MPM) to resolve the strand-strand collision. For simulating single-strand behavior, a semi-implicit Discrete Elastic Rods (DER) model is used. We build upon a highly efficient GPU MPM framework recently presented by Fei et al. [2021b] and propose several schemes to largely improve the performance of building and solving the semi-implicit DER systems on GPU. We demonstrate the efficiency of our pipeline by a few practical scenes that achieve up to 260 frames-per-second (FPS) with more than two thousand simulated strands on Nvidia GeForce RTX 3080.|Proceedings of the ACM on Computer Graphics and Interactive Techniques|2023|10.1145/3606937|Ming-Xin Gao, Li Huang, Yu Ju Chen, Chen Wei, Chun Yuan, Fan Yang|0.5|0
2286|Forecast Density Combinations with Dynamic Learning for Large Data Sets in Economics and Finance|A flexible forecast density combination approach is introduced that can deal with large data sets. It extends the mixture of experts approach by allowing for model set incompleteness and dynamic learning of combination weights. A dimension reduction step is introduced using a sequential clustering mechanism that allocates the large set of forecast densities into a small number of subsets and the combination weights of the large set of densities are modelled as a dynamic factor model with a number of factors equal to the number of subsets. The forecast density combination is represented as a large finite mixture in nonlinear state space form. An efficient simulation-based Bayesian inferential procedure is proposed using parallel sequential clustering and filtering, implemented on graphics processing units. The approach is applied to track the Standard & Poor 500 index combining more than 7000 forecast densities based on 1856 US individual stocks that are are clustered in a relatively small subset. Substantial forecast and economic gains are obtained, in particular, in the tails using Value-at-Risk. Using a large macroeconomic data set of 142 series, similar forecast gains, including probabilities of recession, are obtained from multivariate forecast density combinations of US real GDP, Inflation, Treasury Bill yield and Employment. Evidence obtained on the dynamic patterns in the financial as well as macroeconomic clusters provide valuable signals useful for improved modelling and more effective economic and financial policies.|Social Science Research Network|2019|10.2139/ssrn.3363556|R. Casarin, S. Grassi, H. V. Dijk, Francesco Ravazzollo|0.5|0
2315|Vehicles or Pedestrians: On the gNB Placement in Ultradense Urban Areas|This paper tackles the problem of base stations placement to guarantee line of sight connectivity to vehicles in urban areas, when high frequency communications (mmWave or TeraHertz) are used. Our novel methodology takes advantage of vehicular traffic simulation to generate a realistic demand model for vehicles in urban areas. Then, through a bounded error heuristic, find the maximal coverage that can be achieved with a given density of base stations. The heuristic is implemented on GPU and used to evaluate the coverage in a densely urbanized area in the city of Luxembourg. Our results indicate that a reasonably low density (20 base stations per km2) is sufficient to provide coverage for vehicles in urban environments. However, optimizing solely on vehicles negatively affects the coverage of pedestrians.|Wireless on Demand Network Systems and Service|2023|10.23919/WONS57325.2023.10062000|Gabriele Gemmi, L. Maccari, Michele Segata|0.5|0
1698|High-Throughput Transistor-Level Fault Simulation on GPUs|Deviations in the first-order parameters of CMOS cells can lead to severe errors in the functional and time domain. With increasing sensitivity of these parameters to manufacturing defects and variation, parametric and parasitic-aware fault simulation is becoming crucial in order to support test pattern generation. Traditional approaches based on gate-level models are not sufficient to represent and capture the impact of deviations in these parameters in either an efficient or accurate manner. Evaluation at electrical level, on the other hand, severely lacks execution speed and quickly becomes inapplicable to larger designs due to high computational demands.This work presents a novel fault simulation approach considering first-order parameters in CMOS circuits to explicitly capture CMOS-specific behavior in the functional and time domain with transistor granularity. The approach utilizes massive parallelization in order to achieve high-throughput acceleration on Graphics Processing Units (GPUs) by exploiting parallelism of cells, stimuli and faults. Despite the more precise level of abstraction, the simulator is able to process designs with millions of gates and even outperforms conventional simulation at logic level in terms of modeling accuracy and simulation speed.|Asian Test Symposium|2016|10.1109/ATS.2016.9|H. Wunderlich, E. Schneider|0.4444444444444444|0
1774|GPU based parallel matrix exponential algorithm for large scale power system electromagnetic transient simulation|In order to meet the demand of fast electromagnetic transient simulation for large-scale interconnected power systems, a new method of GPU based parallel matrix exponential algorithm for power system electromagnetic transient simulation is presented in this paper. Firstly, the hardware structure and programming model of GPU are introduced. Then the high data parallelism based on matrix exponential integration algorithm is proposed. Then, a simulation test is carried out for a wind farm system including 17 WTGs. The accuracy and efficiency of the proposed method are verified. The results show that the speed of parallel computing based on GPU is about 2 times faster than that of CPU and Matlab/SimPowerSystems.|2016 IEEE Innovative Smart Grid Technologies - Asia (ISGT-Asia)|2016|10.1109/ISGT-ASIA.2016.7796370|Juntao Liu, Peng Li, G. Song, X. Fu, Chengshan Wang, Jinli Zhao|0.4444444444444444|0
1813|Metos3D: the Marine Ecosystem Toolkit for Optimization and Simulation in 3-D – Part 1: Simulation Package v0.3.2|Abstract. We designed and implemented a modular software framework for the offline simulation of steady cycles of 3-D marine ecosystem models based on the transport matrix approach. It is intended for parameter optimization and model assessment experiments. We defined a software interface for the coupling of a general class of water column-based biogeochemical models, with six models being part of the package. The framework offers both spin-up/fixed-point iteration and a Jacobian-free Newton method for the computation of steady states. The simulation package has been tested with all six models. The Newton method converged for four models when using standard settings, and for two more complex models after alteration of a solver parameter or the initial guess. Both methods delivered the same steady states (within a reasonable precision) on convergence for all models employed, with the Newton iteration generally operating 6 times faster. The effects on performance of both the biogeochemical and the Newton solver parameters were investigated for one model. A profiling analysis was performed for all models used in this work, demonstrating that the number of tracers had a dominant impact on overall performance. We also implemented a geometry-adapted load balancing procedure which showed close to optimal scalability up to a high number of parallel processors.||2016|10.5194/GMD-9-3729-2016|J. Piwonski, T. Slawig|0.4444444444444444|0
1845|Solving Random Ordinary Differential Equations on GPU Clusters using Multiple Levels of Parallelism|Random ordinary differential equations (RODEs) perfectly describe classes of time-dependent problems with stochastic disturbances that are of utmost importance in science and engineering. Both their pathwise solution concept leading to a massive amount of simulations and the form of the numerical solvers for RODEs contain high potential for efficient parallelization approaches. We analyze for the first time a high performance computing parallelization relying on GPU clusters to exploit the underlying three levels of parallelism for the example of the Kanai--Tajimi earthquake model in its RODE form. We identify four basic building blocks of the application which are valid also for general RODE scenarios. We optimized and benchmarked the implementation of the four building blocks separately to be able to select the best individual parameter settings. This allows for a comparison of the total performance of the overall application which shows excellent scaling results also on large GPU clusters. The results ...|SIAM Journal on Scientific Computing|2016|10.1137/15M1036014|C. Riesinger, T. Neckel, Florian Rupp|0.4444444444444444|0
1895|GPU-Accelerated Solution of Activated Sludge Model's System of ODEs with a High Degree of Stiffness|Simulation of activated sludge model (ASM) including detailed biokinetic reaction network often requires the solution of a large system of ordinary differential equations (ODEs) at each time frame, which requires long computing times. In this study, an adaptive time step backward differentiation formula (BDF) is proposed to solve the ASM's system of ODEs that mainly contains a high degree of stiffness. A multi-tile CUDA-based parallel Gauss-Jordan (GJ) algorithm for matrix inversion is applied as a part of the adaptive BDF algorithm to accelerate the overall simulation of the ASM. The results indicate that the performance of parallel GJ algorithm on GPU is highly dependent on the size of the matrix and is effective when the size of the matrix is greater than 128×128. Matrix inversion runtime analysis showed that the parallel matrix inversion on GPU could be achieved speedups of up to 15-fold and 430-fold over comparable serial single-CPU implementations for arbitrary matrices of size 256×256 and 2048×2048, respectively. A range of bioreactor configurations in ASM with different number of ODEs are assessed, and the combination of adaptive BDF with GPU-based parallel matrix inversion achieved an overall runtime reduction up to a 50% compared to single CPU implementations.|2016 International Conference on Computational Science and Computational Intelligence (CSCI)|2016|10.1109/CSCI.2016.0111|Ujjal K. Bhowmika, Jamal Alikhania, Arash Massoudiehb|0.4444444444444444|0
1903|GPU‐based parallel fuzzy c‐mean clustering model via genetic algorithm|Detection of white matter changes in brain tissue using magnetic resonance imaging has been an increasingly active and challenging research area in computational neuroscience. A genetic algorithm based on a fuzzy c‐mean clustering method (GAFCM) was applied to simulated images to separate foreground spot signal information from the background, and the results were compared. The strength of this algorithm was tested by evaluating the segmentation matching factor, coefficient of determination, concordance correlation, and gene expression values. The experimental results demonstrated that the segmentation ability of GAFCM was better than that of fuzzy c‐means and K‐means algorithms. However, GAFCM is computationally expensive. This study presents a new GPU‐based parallel GAFCM algorithm to improve the performance of GAFCM. The experimental results show that computational performance can be increased by a factor of approximately 20 over the CPU‐based GAFCM algorithm while maintaining the quality of the processed images. Thus, the proposed GPU‐based parallel GAFCM algorithm can achieve the same results and significantly decrease processing time. Copyright © 2015 John Wiley & Sons, Ltd.|Concurrency and Computation|2016|10.1002/cpe.3731|Che-Lun Hung, Yuan-Huai Wu|0.4444444444444444|0
2046|An NoC Simulator That Supports Deflection Routing, GPU/CPU Integration, and Co-Simulation|We present deflection routing network on chip simulator (DNOC), a network-on-chip simulator. DNOC is primarily a deflection routing simulator, it simulates custom network topologies with detailed deflection router models, and a basic virtual channel router. DNOC can generate various statistics, such as network latency and power. We evaluate the simulator in three typical use cases. In stand-alone simulation, synthetic traffic generators are used to offer load to the network. In synchronous co-simulation, the simulator is integrated as a module within a larger system simulator with synchronization every simulated cycle. In the faster model-based co-simulation mode, a latency model is built, and retuned periodically at longer time intervals. We demonstrate co-simulation by running applications from the Rodinia and SPLASH-2 benchmark sets on mesh variants. DNOC is also able to run on multicore processors, speeding up the simulation of large networks.|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|2016|10.1109/TCAD.2016.2527698|S. Weiss, G. Oxman|0.4444444444444444|0
2128|A Study of a GPU Based Real-time 3D Live Controlled Molecular Kinetics Simulation on Multiple Computing Platforms|In the emerging field of molecular robotics researchers are faced with the problem of creating controlled movement within organic systems. Due to the difficulty in this problem we have been developing a molecular kinetics simulation to aid in the study of the microtubule swarm motion. However creating such a system poses many research challenges, since the scale of the simulation is far greater than past molecular simulations, and new simulation approaches must be taken. \nThis problem is especially difficult due to the scale of the biological systems. Some past works have looked at making general models which have been able to reproduce swarm motion but are not able to elucidate the cause of these swarms. Other works have looked at explicitly simulating microtubule motion, but due to performance limitations have not been able to verify the creation of swarms based on their model. In order to combine the strengths of these two approaches, large scales as well as simulating the interacting elements of the biological system explicitly, we need to face challenges such as the immense amount of data which is produced, computational demands, as well as memory demands. In addition platform flexibility and usability must be addressed; otherwise, the impact of the work may be limited by hardware demands for the users or ease of use. \nWe have solved these problems by creating a real-time live controlled 3D molecular kinetics simulation. Choosing a real-time 3D simulation helps to reduce the amount of data produced, by reducing storage demands and post processing or rendering. However, choosing real-time facilities further increases the performance demands by giving a very small time budget per update. To solve this, we have chosen to use general purpose graphical processing units which offer a high level of computational performance. We have also developed specialized algorithms designed to efficiently manage the memory demands alongside of the computation. Lastly, we have applied these methodologies in various ways to ensure platform flexibility and usability for future studies with the simulation.\nBy using the proposed methods formulated throughout my graduate studies we have been able to reach the needed performance for simulating large-scale molecular swarms explicitly in real-time. In addition to adding to the emerging field of real-time visual simulations, this work has also created a tool for scientists to test and observe molecular kinetics based interactions, specifically microtubule gliding assay swarms. This will open a door for future research and study of molecular swarms. The scope of my graduate studies has been the creation of the entire simulation software and model.||2016|10.1007/s11633-015-0947-1|G. Gutmann|0.4444444444444444|0
1717|Extended Playing Techniques on an Augmented Virtual Percussion Instrument|Abstract Innovation and tradition are two fundamental factors in the design of new digital musical instruments. Although apparently mutually exclusive, novelty does not imply a total disconnection from what we have inherited from hundreds of years of traditional design, and the balance of these two factors often determines the overall quality of an instrument. Inspired by this rationale, in this article we introduce the Hyper Drumhead, a novel augmented virtual instrument whose design is deeply rooted in traditional musical paradigms, yet aimed at the exploration of unprecedented sounds and control. In the first part of the article we analyze the concepts of designing an augmented virtual instrument, explaining their connection with the practice of augmenting traditional instruments. Then we describe the design of the Hyper Drumhead in detail, focusing on its innovative physical modeling implementation. The finite-difference time-domain solver that we use runs on the parallel cores of a commercially available graphics card and permits the simulation of real-time 2-D wave propagation in massively sized domains. Thanks to the modularity of this implementation, musicians can create several 2-D virtual percussive instruments that support realistic playing techniques but whose affordances can be enhanced beyond most of the limits of traditional augmentation.|Computer Music Journal|2018|10.1162/comj_a_00457|Andrew Allen, Victor Zappi, S. Fels|0.42857142857142855|0
1881|Parallel Pair-Wise Interaction for Multi-Agent Immune Systems Modelling|Agent Based Modelling (ABM), is an approach for modelling dynamic systems and studying complex and emergent behaviour. ABM approach is a very common technique in biological domain due to high demand for a large scale analysis tool to collect and interpret information to solve biological problems. However, simulating large scale cellular level models (i.e. large number of agents/entities) require a high degree of computational power which is achievable through parallel computing methods such as Graphics Processing Units (GPUs). The use of parallel approaches in ABMs is growing rapidly specifically when modelling in continuous space system (particle based). Parallel implementation of particle based simulation within continuum space where agents contain quantities of chemicals/substances is very challenging. Pair-wise interactions are different abstraction to continuous space (particle) models which is commonly used for immune system modelling. This paper describes an approach to parallelising the key component of biological and immune system models (pair-wise interactions) within an ABM model. Our performance results demonstrate the applicability of this method to a broader class of biological systems with the same type of cell interactions and that it can be used as the basis for developing complete immune system models on parallel hardware.|IEEE International Conference on Bioinformatics and Biomedicine|2018|10.1109/BIBM.2018.8621404|M. Chimeh, P. Richmond, M. Pennisi, F. Pappalardo, Peter Heywood|0.42857142857142855|0
2034|Efficient utilization of multi-core processors and many-core co-processors on supercomputer beacon for scalable geocomputation and geo-simulation over big earth data|Abstract Digital earth science data originated from sensors aboard satellites and platforms such as airplane, UAV, and mobile systems are increasingly available with high spectral, spatial, vertical, and temporal resolution data. When such big earth science data are processed and analyzed via geocomputation solutions, or utilized in geospatial simulation or modeling, considerable computing power and resources are necessary to complete the tasks. While classic computer clusters equipped by central processing units (CPUs) and the new computing resources of graphics processing units (GPUs) have been deployed in handling big earth data, coprocessors based on the Intel’s Many Integrated Core (MIC) Architecture are emerging and adopted in many high-performance computer clusters. This paper introduces how to efficiently utilize Intel’s Xeon Phi multicore processors and MIC coprocessors for scalable geocomputation and geo-simulation by implementing two algorithms, Maximum Likelihood Classification (MLC) and Cellular Automata (CA), on supercomputer Beacon, a cluster of MICs. Four different programming models are examined, including (1) the native model, (2) the offload model, (3) the symmetric model, and (4) the hybrid-offload model. It can be concluded that while different kinds of parallel programming models can enable big data handling efficiently, the hybrid-offload model can achieve the best performance and scalability. These different programming models can be applied and extended to other types of geocomputation to handle big earth data.||2018|10.1080/20964471.2018.1434265|Xuan Shi, Chenggang Lai, Miaoqing Huang|0.42857142857142855|0
2209|Ink Wash Painting Style Rendering With Physically-based Ink Dispersion Model|This paper presents a real-time rendering method based on the GPU programmable pipeline for rendering the 3D scene in ink wash painting style. The method is divided into main three parts: First, render the ink properties of 3D model by calculating its vertex curvature. Then, cached the ink properties to a paper structure and using an ink dispersion model which is defined by referencing the theory of porous media to simulate the dispersion of ink. Finally, convert the ink properties to the pixel color information and render it to the screen. This method has a better performance than previous methods in visual quality.||2018|10.1088/1742-6596/1004/1/012026|Qing Zhu, Weiran Li, Y. Wang|0.42857142857142855|0
2251|Stochastic approximation schemes for economic capital and risk margin computations|We consider the problem of the numerical computation of its economic capital by an insurance or a bank, in the form of a value-at-risk or expected shortfall of its loss over a given time horizon. This loss includes the appreciation of the mark-to-model of the liabilities of the firm, which we account for by nested Monte Carlo à la Gordy and Juneja [17] or by regression à la Broadie, Du, and Moallemi [10]. Using a stochastic approximation point of view on value-at-risk and expected shortfall, we establish the convergence of the resulting economic capital simulation schemes, under mild assumptions that only bear on the theoretical limiting problem at hand, as opposed to assumptions on the approximating problems in [17] and [10]. Our economic capital estimates can then be made conditional in a Markov framework and integrated in an outer Monte Carlo simulation to yield the risk margin of the firm, corresponding to a market value margin (MVM) in insurance or to a capital valuation adjustment (KVA) in banking parlance. This is illustrated numerically by a KVA case study implemented on GPUs.|ESAIM Proceedings and Surveys|2018|10.1051/proc/201965182|E. Gobet, D. Barrera, S. Crépey, G. Fort, Uladzislau Stazhynski, B. Diallo|0.42857142857142855|0
1639|Collaborative Parallel Hybrid Metaheuristics on Graphics Processing Unit|Metaheuristics are nondeterministic optimization algorithms used to solve complex problems for which classic approaches are unsuitable. Despite their effectiveness, metaheuristics require considerable computational power and cannot easily be used in time critical applications. Fortunately, those algorithms are intrinsically parallel and have been implemented on shared memory systems and more recently on graphics processing units (GPUs). In this paper, we present highly efficient parallel implementations of the particle swarm optimization (PSO), the genetic algorithm (GA) and the simulated annealing (SA) algorithm on GPU using CUDA. Our approach exploits the parallelism at the solution level, follows an island model and allows for speedup up to 346× for different benchmark functions. Most importantly, we also present a strategy that uses the generalized island model to integrate multiple metaheuristics into a parallel hybrid solution adapted to the GPU. Our proposed solution uses OpenMP to heavily exploit the concurrent kernel execution feature of recent NVIDIA GPUs, allowing for the parallel execution of the different metaheuristics in an asynchronous manner. Asynchronous hybrid metaheuristics has been developed for multicore CPU, but never for GPU. The speedup offered by the GPU is far superior and key to the optimization of solutions to complex engineering problems.|International Journal of Computational Intelligence and Applications|2015|10.1142/S1469026815500029|Vincent Roberge, M. Tarbouchi, F. Okou|0.4|0
1670|Large Scale Tissue Morphogenesis Simulation on Heterogenous Systems Based on a Flexible Biomechanical Cell Model|The complexity of biological tissue morphogenesis makes in silico simulations of such system very interesting in order to gain a better understanding of the underlying mechanisms ruling the development of multicellular tissues. This complexity is mainly due to two elements: firstly, biological tissues comprise a large amount of cells; secondly, these cells exhibit complex interactions and behaviors. To address these two issues, we propose two tools: the first one is a virtual cell model that comprise two main elements: firstly, a mechanical structure (membrane, cytoskeleton, and cortex) and secondly, the main behaviors exhibited by biological cells, i.e., mitosis, growth, differentiation, molecule consumption, and production as well as the consideration of the physical constraints issued from the environment. An artificial chemistry is also included in the model. This virtual cell model is coupled to an agent-based formalism. The second tool is a simulator that relies on the OpenCL framework. It allows efficient parallel simulations on heterogenous devices such as micro-processors or graphics processors. We present two case studies validating the implementation of our model in our simulator: cellular proliferation controlled by cell signalling and limb growth in a virtual organism.|IEEE/ACM Transactions on Computational Biology & Bioinformatics|2015|10.1109/TCBB.2015.2418994|P. Ballet, V. Rodin, A. Jeannin-Girardon|0.4|0
1739|FPGA-Based Stochastic Activity Networks for Online Reliability Monitoring|Stochastic activity network (SAN) is a flexible formalism that permits performing reliability and prognostics analysis of complex degrading systems. However, their use for the analysis of high-reliability equipment including fast evolving and degrading mechanisms requires long simulation times and large hardware resources. This paper presents, a field programmable gate array (FPGA)-based architecture that permits performing online Monte Carlo simulations of SAN models of high reliability systems. The synthesis is automated and the result preserves the structure of the models, permitting model inspection, and validation. The architecture runs without the aid of a central processing unit (CPU) and the simulation results are directly recorded into CPU-accessible random access memory (RAM). The design is validated on reliability monitors for an overheating detector and for degrading underground cables. The resource usage and simulation time are compared to the performance of software solutions that are run on parallel processors and large computer resources. Obtained results confirm that the proposed FPGA architecture can be employed for the accelerated runtime reliability analysis of equipment.|IEEE transactions on industrial electronics (1982. Print)|2020|10.1109/TIE.2019.2928244|M. Mendicute, Unai Garro, J. Aizpurua, E. Muxika|0.4|0
1747|FALPEM: Framework for Architectural-Level Power Estimation and Optimization for Large Memory Sub-Systems|Framework is developed for estimation of power at pre register transfer level (RTL) stage for structured memory sub-systems. Power estimation model is proposed specifically targeting power consumed by clock network and interconnect. The model is validated with VCD-based simulation on back-annotated netlist of an 8 MB memory sub-system used as video RAM (VRAM) for high-end graphics applications. This methodology also forms the basis for low-power exploration driving floor plan choice, gating structure of data, and clock network. We demonstrate 57% reduction in dynamic power by using low-power techniques for the 8 MB VRAM used as frame buffer in a graphics processor. FALPEM can be extended to other applications like processor cache and ASIC designs.|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|2015|10.1109/TCAD.2014.2387859|Promod Kumar, Harsh Rawat, L. Bergher, Mohit Jain, Pascal Tessier, Daniel Pierredon, A. Chhabra|0.4|0
1958|Performance Analysis of Thread Block Schedulers in GPGPU and Its Implications|GPGPU (General-Purpose Graphics Processing Unit) consists of hardware resources that can execute tens of thousands of threads simultaneously. However, in reality, the parallelism is limited as resource allocation is performed by the base unit called thread block, which is not managed judiciously in the current GPGPU systems. To schedule threads in GPGPU, a specialized hardware scheduler allocates thread blocks to the computing unit called SM (Stream Multiprocessors) in a Round-Robin manner. Although scheduling in hardware is simple and fast, we observe that the Round-Robin scheduling is not efficient in GPGPU, as it does not consider the workload characteristics of threads and the resource balance among SMs. In this article, we present a new thread block scheduling model that has the ability of analyzing and quantifying the performances of thread block scheduling. We implement our model as a GPGPU scheduling simulator and show that the conventional thread block scheduling provided in GPGPU hardware does not perform well as the workload becomes heavy. Specifically, we observe that the performance degradation of Round-Robin can be eliminated by adopting DFA (Depth First Allocation), which is simple but scalable. Moreover, as our simulator consists of modular forms based on the framework and we publicly open it for other researchers to use, various scheduling policies can be incorporated into our simulator for evaluating the performance of GPGPU schedulers.|Applied Sciences|2020|10.3390/app10249121|H. Bahn, Kyungwoon Cho|0.4|0
1961|Accelerating electromagnetic transient simulation of electrical power systems using graphics processing units|This thesis presents the application of graphics processing unit (GPU) based parallel computing technique to speed up electromagnetic transients (EMT) simulation for large power systems. Nowadays, desktop computers come with GPUs that support extra computing capability to handle gaming and animation related applications. GPUs are built with highly parallel computing architecture to support the high demand of graphics for various applications mainly related to computer graphics, video processing, and playback. It is possible to use these GPUs for general-purpose computations, such as EMT simulation. Power system components are mathematically modeled in their highest detail in EMT simulation. Traditionally, EMT simulation tools are implemented on central processing unit (CPU) based computers, where simulation is performed in a sequential manner. With the increase in network size there is a drastic increase in simulation time with conventional CPU based simulation tools. This research shows that the use of GPU computing considerably reduces the total simulation time. In this approach, GPU performs computationally intensive parts of the EMT simulation in parallel on its built-in massively parallel processing cores, and the CPU handles various sequential jobs such as flow control of the simulation, storing the output variables, etc. This thesis proposes a parallel computing algorithm for EMT simulations on the GPU platforms, and demonstrates the algorithm by simulating large power systems. Total computation time for GPU computing, using ’compute unified device architecture’ (CUDA)-based C programming is compared with the total computation time for the sequential implementations on the CPU using ANSI-C programming for systems of various sizes and types.||2015|10.3329/dujs.v62i2.21977|J. Debnath|0.4|0
2050|HPC simulations of brownout: A noninteracting particles dynamic model|Helicopters can experience brownout when flying close to a dusty surface. The uplifting of dust in the air can remarkably restrict the pilot’s visibility area. Consequently, a brownout can disorient the pilot and lead to the helicopter collision against the ground. Given its risks, brownout has become a high-priority problem for civil and military operations. Proper helicopter design is thus critical, as it has a strong influence over the shape and density of the cloud of dust that forms when brownout occurs. A way forward to improve aircraft design against brownout is the use of particle simulations. For simulations to be accurate and comparable to the real phenomenon, billions of particles are required. However, using a large number of particles, serial simulations can be slow and too computationally expensive to be performed. In this work, we investigate an message passing interface (MPI) + graphics processing unit (multi-GPU) approach to simulate brownout. In specific, we use a semi-implicit Euler method to consider the particle dynamics in a Lagrangian way, and we adopt a precomputed aerodynamic field. Here, we do not include particle–particle collisions in the model; this allows for independent trajectories and effective model parallelization. To support our methodology, we provide a speedup analysis of the parallelization concerning the serial and pure-MPI simulations. The results show (i) very high speedups of the MPI + multi-GPU implementation with respect to the serial and pure-MPI ones, (ii) excellent weak and strong scalability properties of the implemented time-integration algorithm, and (iii) the possibility to run realistic simulations of brownout with billions of particles at a relatively small computational cost. This work paves the way toward more realistic brownout simulations, and it highlights the potential of high-performance computing for aiding and advancing aircraft design for brownout mitigation.|The international journal of high performance computing applications|2020|10.1177/1094342020905971|N. Parolini, R. Porcù, M. Penati, N. Vergopolan, E. Miglio|0.4|0
2115|Simulation of Skeletal Muscles in Real-Time with Parallel Computing in GPU|Modeling and simulation of the skeletal muscles are usually solved using the Finite Element method (FEM) which, although accurate, commonly needs a complex mesh and the solution is not processed in real-time. In this work, a meshfree model that simulates skeletal muscles considering their functioning and control based on electrical activity, their structure based on biological tissue, and that computes in real-time, is presented. Meshfree methods were used because they are able to surpass most of the limitations that are present in mesh-based methods. The muscular belly was modelled as a particle-based viscoelastic fluid, which is controlled using the monodomain model and shape matching. The smoothed particle hydrodynamics (SPH) method was used to solve both the fluid dynamics and the electrophysiological model. To analyze the accuracy of the method, a similar model was implemented with FEM. Both FEM and SPH methods provide similar solutions of the models in terms of pressure and displacement, with an error of around 0.09, with up to a 10% difference between them. Through the use of General-purpose computing on graphics processing units (GPGPU), real-time simulations that offer a viable alternative to mesh-based models for interactive biological tissue simulations was achieved.|Applied Sciences|2020|10.3390/app10062099|O. Navarro-Hinojosa, M. Alencastre-Miranda|0.4|0
2158|Data-centric GPU-based adaptive mesh refinement|It has been demonstrated that explicit stencil computations of high-resolution scheme can highly benefit from GPUs. This includes Adaptive Mesh Refinement (AMR), which is a model for adapting the resolution of a stencil grid locally. Unlike uniform grid stencils, however, adapting the grid is typically done on the CPU side. This requires transferring the stencil data arrays to/from CPU every time the grid is adapted. We propose a data-centric approach to GPU-based AMR. That is, porting all the mesh adaptation operations touching the data arrays to the GPU. This would allow the stencil data arrays to reside on the GPU memory for the entirety of the simulation. Thus, the GPU code would specialize on the data residing on its memory while the CPU specializes on the AMR metadata residing on CPU memory. We compare the performance of the proposed method to a basic GPU implementation and an optimized GPU implementation that overlaps communication and computation. The performance of two GPU-based AMR applications is enhanced by 2.21x, and 2.83x compared to the basic implementation.|IA3@SC|2015|10.1145/2833179.2833181|M. Wahib, N. Maruyama|0.4|0
2187|Realistic Buoyancy Model for Real‐Time Applications|Following Archimedes' Principle, any object immersed in a fluid is subject to an upward buoyancy force equal to the weight of the fluid displaced by the object. This simple description is the origin of a set of effects that are ubiquitous in nature, and are becoming commonplace in games, simulators and interactive animations. Although there are solutions to the fluid‐to‐solid coupling problem in some particular cases, to the best of our knowledge, comprehensive and accurate computational buoyancy models adequate in general contexts are still lacking. We propose a real‐time Graphics Processing Unit (GPU) based algorithm for realistic computation of the fluid‐to‐solid coupling problem, which is adequate for a wide generality of cases (solid or hollow objects, with permeable or leak‐proof surfaces, and with variable masses). The method incorporates the behaviour of the fluid into which the object is immersed, and decouples the computation of the physical parameters involved in the buoyancy force of the empty object from the mass of contained liquid. The dynamics of this mass of liquid are also computed, in a way such that the relation between the centre of mass of the object and the buoyancy force may vary, leading to complex, realistic beha viours such as the ones arising for instance with a sinking boat.|Computer graphics forum (Print)|2020|10.1111/cgf.14013|C. Delrieux, G. Patow, J. M. Bajo|0.4|0
2197|A Study of Double-negative meta-Materials on Parallel GPU-FDTD Algorithm|In this paper,we studies a parallel method of DNG(double-negative)meta-materials on CUDA(Compute Unified Device Architecture)GPU(Graphics Processing Unit).Z-transform is applied to FDTD method to analysis propagation and scattering from DNG materials.As Z-FDTD algorithm has the characteristics of spatial parallelism,it is suitable for execution on GPU.The iterative equations of FDTD based on the Drude model are derived;and parallel GPU-FDTD is discussed.The Correctness and efficiency of this method are testified by simulated experiments.||2015|10.1002/cpe.3731|Shen Che|0.4|0
2201|FRED: a fast Monte Carlo code on GPU for quality control in Particle Therapy|Charged Particle Therapy is a non-invasive technique for radio-resistant tumor treatment performed with protons or light ions, aiming to deliver a high precision treatment. Compared to conventional radiotherapy, ions allow for a higher dose deposition in the tumor region while sparing the surrounding healthy tissue. To really exploit the potential benefits of this technique, the highest possible accuracy in the calculation of dose and its spatial distribution is required in treatment planning. Commonly used Treatment Planning Software solutions adopt a simplified beam-body interaction model. An alternative is the use of Monte Carlo simulations which explicitly take into account the interaction of charged particles with actual human tissues hence providing highly accurate results. However, Monte Carlo simulations are used in a restricted number of cases due to substantial computational resources required. The code FRED has been developed to allow a fast optimization of the treatment plans in Charged Particle Therapy while profiting from the dose release accuracy of a Monte Carlo tool. Currently, the most refined module is the transport of proton beams in water. A comparison with measurements shows that the lateral dose tails are reproduced within 2% in the field size factor test up to 20 cm. Models for the interaction of ion with the matter are currently under development in the FRED code. The status of new developments and the performance of FRED will be presented.|Journal of Physics: Conference Series|2020|10.1088/1742-6596/1548/1/012020|A. Sciubba, A. Sarti, V. Patera, G. Traini, A. Schiavi, M. D. Simoni, R. Mirabelli, M. Fischetti, E. Gioscio, M. Marafini|0.4|0
2309|Visually Realistic Rain Modeling Optimization for VR Application|Simulation purposes are to create an event in the world as real as possible, and by using Virtual Reality Technologies, it can achieve a higher level of realism and immersion. By using particle Systems from Unity can be used to producing a phenomenon that involving a small moving object on a big scale such as smoke, fire, or rain. However, when trying to use the particle system, sometimes the performance got reduced because there are too many particles working at the same time and result in a frame rate drop that causing discomfort to the player. For this reason, it's essential to optimize the performance by limiting objects that are needed to draw to lighten up the GPU process. In this paper, we discussed how to produces rain using the particle system in unity and find the best way to simulating rain with the best frame rate. We will show how the lifetime of the particle affecting the cycle of the particle system and the impact on the particle emission. We concluded that it's essential for developers to focus on the balance for creating realistic and detailed simulation and the stability of the simulation performance so the player can be more comfortable and immersive.|Asia Pacific Symposium on Intelligent and Evolutionary Systems|2020|10.1109/IES50839.2020.9231591|A. Basuki, T. Harsono, David Fahmi Abdillah|0.4|0
2313|Predicting the Energy Consumption of CUDA Kernels using SimGrid|Building a sustainable Exascale machine is a very promising target in High Performance Computing (HPC). To tackle the energy consumption challenge while continuing to provide tremendous performance, the HPC community have rapidly adopted GPU-based systems. Today, GPUs have became the most prevailing components in the massively parallel HPC landscape thanks to their high computational power and energy efficiency. Modeling the energy consumption of applications running on GPUs has gained a lot of attention for the last years. Alas, the HPC community lacks simple yet accurate simulators to predict the energy consumption of general purpose GPU applications. In this work, we address the prediction of the energy consumption of CUDA kernels via simulation. We propose in this paper a simple and lightweight energy model that we implemented using the open-source framework SimGrid. Our proposed model is validated across a diverse set of CUDA kernels and on two different NVIDIA GPUs (Tesla M2075 and Kepler K20Xm). As our modeling approach is not based on performance counters or detailed-architecture parameters, we believe that our model can be easily approved by users who take care of the energy consumption of their GPGPU applications.|Symposium on Computer Architecture and High Performance Computing|2020|10.1109/SBAC-PAD49847.2020.00035|Dorra Boughzala, L. Lefèvre, Anne-Cécile Orgerie|0.4|0
1691|Parallel replica dynamics method for bistable stochastic reaction networks: Simulation and sensitivity analysis.|Stochastic reaction networks that exhibit bistable behavior are common in systems biology, materials science, and catalysis. Sampling of stationary distributions is crucial for understanding and characterizing the long-time dynamics of bistable stochastic dynamical systems. However, simulations are often hindered by the insufficient sampling of rare transitions between the two metastable regions. In this paper, we apply the parallel replica method for a continuous time Markov chain in order to improve sampling of the stationary distribution in bistable stochastic reaction networks. The proposed method uses parallel computing to accelerate the sampling of rare transitions. Furthermore, it can be combined with the path-space information bounds for parametric sensitivity analysis. With the proposed methodology, we study three bistable biological networks: the Schlögl model, the genetic switch network, and the enzymatic futile cycle network. We demonstrate the algorithmic speedup achieved in these numerical benchmarks. More significant acceleration is expected when multi-core or graphics processing unit computer architectures and programming tools such as CUDA are employed.|Journal of Chemical Physics|2017|10.1063/1.5017955|Ting Wang, P. Plecháč|0.375|0
1808|An Application of GPU Acceleration in CFD Simulation for Insect Flight|The mobility and maneuverability of winged insects have been attracting attention, but the knowledge on the behavior of free-flying insects is still far from complete. This paper presents a computational study on the aerodynamics and kinematics of a free-flying model fruit-fly. An existing integrative computational fluid dynamics (CFD) framework was further developed using CUDA technology and adapted for the free flight simulation on heterogenous clusters. The application of general-purpose computing on graphics processing units (GPGPU) significantly accelerated the insect flight simulation and made it less computational expensive to find out the steady state of the flight using CFD approach. A variety of free flight scenarios has been simulated using the present numerical approach, including hovering, fast rectilinear flight, and complex maneuvers. The vortical flow surrounding the model fly in steady flight was visualized and analyzed. The present results showed good consistency with previous studies.|Supercomputing Frontiers and Innovations|2017|10.14529/JSFI170202|K. Yeo, Yang Yao|0.375|0
1857|Hybrid CPU-GPU computing for simulating calcium handling in the heart|Calcium plays a vital role in the normal functioning of a healthy human heart. Any disturbances in calcium handling can alter the excitation-contraction coupling and cellular properties, which lead to cardiac arrhythmias. Models of the electrophysiology and calcium handling in a cell can help in understanding the mechanisms of arrhythmias. Realistic simulations of the cellular and subcellular processes require a lot of computational power. The aim of this thesis is to investigate the heterogeneous CPU-GPU computing as an approach to increase the performance of a realistic 3D Tissue-Scale simulator. We study a multiscale cardiac ventricular myocyte model which reproduces local calcium release processes and electrical activity. The cardiac cell model consists of 10000 calcium release units which contain 100 ryanodine receptors and 15 L-type calcium channels. The most time consuming dyad-level computations are implemented on GPU using CUDA API. In order to achieve high efficiency of the simulator, we apply several optimizations to the code. Due to a large number of load and store operations, it is important to have a fast access to the memory. The completely optimized implementation demonstrates a significant speedup of the simulation. Numerical experiments showed that in order to fully utilize a single GPU, multiple cells must be involved in the computation. Due to a large number of cells required for the realistic simulation, we implement the 3D Tissue-Scale simulator on multiple GPUs. Simulations of multiple cardiac cells are performed using multiple compute nodes equipped with two GPUs each. A good scalability was indicated by weak and strong scaling tests. Scientific experiments demonstrated that physiological processes in a cell are correctly reproduced using the computational cell model. Thus, we are able to simulate arrhythmogenic patterns which arise from disturbances in calcium handling. This provides a possibility to understand the cause of cardiac ventricular arrhythmias and develop the preventive mechanisms.||2017|10.1186/s12872-017-0727-7|Neringa Altanaite|0.375|0
1995|Parallel acceleration on simulation of a 2D Takeuchi electrophysiology cardiac model using GPUs|Modeling and simulation of cardiac electrophysiology has the advantages of economic and security than anatomical experiments. However, the simulation of a cardiac model that matches a real one always demands very high computing ability that an ordinary CPU could hardly satisfy. In this paper, a parallelized solution of the simulation of a 2D electrophysiology cardiac model based on CUDA (Compute Unified Device Architecture) using GPUs is introduced. CUDA is a parallel computing platform and API (Application Programming Interface) model created and released by NVIDIA, which offers us a way of using GPUs as co-processors to implement parallel computation. Through our work, it is shown that the simulation of electrophysiology cardiac propagation model is an ideal candidate for GPU computing and achieves impressive speedup compared to ordinary CPUs.|International Conference on Innovative Computing and Cloud Computing|2017|10.1109/COMPCOMM.2017.8323017|Yanghua Shen, Yaopeng Hu, Feng Qiu, W. Shen, Baohua Liu, Xin Zhu|0.375|0
2255|Fast Simulation Method for Ocean Wave Base on Ocean Wave Spectrum and Improved Gerstner Model with GPU|For the randomness and complexity of ocean wave, and the simulation of large-scale ocean requires a great amount of computation, but the computational efficiency is low, the real-time ability is poor, a fast method of wave simulation is proposed based on the observation and research results of oceanography, it takes advantage of the grid which combined with the technique of LOD and projection, and use the height map of ocean which is formd by retrieval of ocean wave spectrum and directional spectrum to compute with FFT, and it uses the height map to cyclic mapping for the grid on GPU which combined with the technique of LOD and projection to get the dynamic height data and simulation of ocean. The experimental results show that the method is vivid and it conforms with randomness and complexity of ocean wave, it effectively improves the simulation speed of the wave and satisfied with the real-time ability and fidelity in simulation system of ocean.||2017|10.1088/1742-6596/787/1/012027|Jing Zhang, Wenqiao Zhang, Tianchi Zhang|0.375|0
1668|Algorithms for procedural generation and display of trees|Main goal of this paper is to explore in which situations should some procedural algorithm be used to generate a tree model. Each algorithm is modified so as to be able to generate a tree model whose shape resembles the required model. Space colonization algorithm, algorithm using particle flows and algorithm simulating a Lindenmayer system are compared depending on a time needed to generate a tree with similar complexity. The voxelization procedure of the model and its application in this context is explained. Method for generating a tree mesh on a graphics card using Bézier's curves is presented.|International Convention on Information and Communication Technology, Electronics and Microelectronics|2019|10.23919/MIPRO.2019.8757140|Z. Mihajlovic, Hrvoje Nuic|0.3333333333333333|0
1689|MoMaS: Mold Manifold Simulation for real‐time procedural texturing|The slime mold algorithm has recently been under the spotlight thanks to its compelling properties studied across many disciplines like biology, computation theory, and artificial intelligence. However, existing implementations act only on planar surfaces, and no adaptation to arbitrary surfaces is available. Inspired by this gap, we propose a novel characterization of the mold algorithm to work on arbitrary curved surfaces. Our algorithm is easily parallelizable on GPUs and allows to model the evolution of millions of agents in real‐time over surface meshes with several thousand triangles, while keeping the simplicity proper of the slime paradigm. We perform a comprehensive set of experiments, providing insights on stability, behavior, and sensibility to various design choices. We characterize a broad collection of behaviors with a limited set of controllable and interpretable parameters, enabling a novel family of heterogeneous and high‐quality procedural textures. The appearance and complexity of these patterns are well‐suited to diverse materials and scopes, and we add another layer of generalization by allowing different mold species to compete and interact in parallel.|Computer graphics forum (Print)|2022|10.1111/cgf.14697|E. Rodolà, R. Marín, S. Melzi, F Maggioli|0.3333333333333333|0
1701|GPU Acceleration : OpenACC for Radar Processing Simulation|This article gives a methodological approach to accelerating an environment of a RADAR (RAdio Detecting And Ranging) simulation, from a single-core CPU implementation to a multi-core GPU implementation. We focus our attention on the most common tools for GPU programming like CUDA [2], but more specifically on OpenACC [6], a directive based parallel programming language. One of its promises is, with minimal modifications, to transform a CPU code to take advantage of many-core architectures, CPUs or GPUs alike.Radar systems rely on many layers of testing, one of them being software validation. As technology moves forward, systems become increasingly complex, thus increasing the required processing power to simulate those systems. With CPU performance stalling, it is imperative to switch to alternative architectures. Our contribution is providing key steps for accelerating a software simulation of a radar algorithm on a GPU, with a particular focus on performance but also on the ease of programming. Maximum achieved execution time speedup on GPU architecture for our typical use case of radar processing is of 8.2 for CUDA and of 4.56 for OpenACC compared to the reference implementation on CPU.|2019 International Radar Conference (RADAR)|2019|10.1109/RADAR41533.2019.171296|N. Gac, A. Mérigot, M. Martelli, C. Enderli, Antoine Vermesse|0.3333333333333333|0
1714|Magnetic Induction Tomography with High Performance GPU Implementation|Magnetic induction tomography (MIT) is a non-invasive medical imaging technique with promising applications such as brain imaging and cryosurgery monitoring. Despite its potential, the realisation of medical MIT application is challenging. The computational complexity of both the forward and inverse problems, and specific MIT hardware design are the major limitations for the development of MIT research in medical imaging. The MIT forward modeling and linear system equations for large scale matrices are computationally expensive. This paper presents the implementation of GPU (graphics processing unit) for both forward and inverse problems in MIT research. For a given MIT mesh geometry composed of 167,488 tetrahedral elements, the GPU accelerated Biot-Savart Law for solving the free space magnetic field and magnetic vector potential is proved to be over 200 times faster compared to the time consumption of a CPU (central processing unit). The linear system equation arising from the forward and inverse problem, can also be accelerated using GPU. Both simulations and experimental results are presented based on a new GPU implementation. Laboratory experimental results are shown for a phantom study representing potential cryosurgery monitoring using an MIT system.||2016|10.2528/PIERB15101902|M. Soleimani, R. Banasiak, Lu Ma|0.3333333333333333|0
1878|3D-SiamMask: Vision-Based Multi-Rotor Aerial-Vehicle Tracking for a Moving Object|This paper aims to develop a multi-rotor-based visual tracker for a specified moving object. Visual object-tracking algorithms for multi-rotors are challenging due to multiple issues such as occlusion, quick camera motion, and out-of-view scenarios. Hence, algorithmic changes are required for dealing with images or video sequences obtained by multi-rotors. Therefore, we propose two approaches: a generic object tracker and a class-specific tracker. Both tracking settings require the object bounding box to be selected in the first frame. As part of the later steps, the object tracker uses the updated template set and the calibrated RGBD sensor data as inputs to track the target object using a Siamese network and a machine-learning model for depth estimation. The class-specific tracker is quite similar to the generic object tracker but has an additional auxiliary object classifier. The experimental study and validation were carried out in a robot simulation environment. The simulation environment was designed to serve multiple case scenarios using Gazebo. According to the experiment results, the class-specific object tracker performed better than the generic object tracker in terms of stability and accuracy. Experiments show that the proposed generic tracker achieves promising results on three challenging datasets. Our tracker runs at approximately 36 fps on GPU.|Remote Sensing|2022|10.3390/rs14225756|A. Klimchik, Geesara Kulathunga, Mohamad Al Al Mdfaa|0.3333333333333333|0
1883|A Parallelizable Framework for Segmenting Piecewise Signals|Piecewise signals appear in many application fields. Here, we propose a framework for segmenting such signals based on the modeling of each piece using a parametric probability distribution. The proposed framework first models the segmentation as an optimization problem with sparsity regularization. Then, an algorithm based on dynamic programming is utilized for finding the optimal solution. However, dynamic programming often suffers from a heavy computational burden. Therefore, we further show that the proposed framework is parallelizable and propose using GPU-based parallel computing to accelerate the computation. This approach is highly desirable for the analysis of large volumes of data that are ubiquitous. The experiments on both the simulated and real genomic datasets from the next-generation sequencing demonstrate an improved performance in terms of both segmentation quality and computational speed.|IEEE Access|2019|10.1109/ACCESS.2018.2890077|Yu-ping Wang, D. Brie, C. Soussen, J. Idier, M. Wan, Junbo Duan|0.3333333333333333|0
1948|Performance predictors for graphics processing units applied to dark‐silicon‐aware design space exploration|The limitations on the scalability of computer systems imposed by the dark‐silicon effects are so severe that they support the extensive use of heterogeneity such as the GP‐GPU for general purpose processing. Performance simulators of GP‐GPU heterogeneous systems aim to provide performance accuracy at the cost of execution time. In this work, we handle time‐consuming simulations of design space exploration systems based on GPUs. We have developed performance predictors based on machine learning (ML) algorithms and evaluated them in accuracy and throughput (number of predictions per second). We measure model accuracy through the mean absolute percentage error (MAPE) and the model efficiency through a throughput metric (millions of predictions per second). Our experiments revealed that decision trees predictors are the most promising regarding accuracy and efficiency. We applied the best predictors into the MultiExplorer, a dark silicon‐aware design space exploration tool that allows designers to explore the architecture and microarchitecture of multicore/manycore system design.|Concurrency and Computation|2022|10.1002/cpe.6877|Rhayssa Sonohata, Eraldo Rezende Fernandes, Liana Dessandre Duenha, D. C. A. Arigoni, Ricardo Ribeiro dos Santos|0.3333333333333333|0
2065|Simulation and performance analysis of quantum error correction with a rotated surface code under a realistic noise model|The demonstration of quantum error correction (QEC) is one of the most important milestones in the realization of fully-fledged quantum computers. Toward this, QEC experiments using the surface codes have recently been actively conducted. However, it has not yet been realized to protect logical quantum information beyond the physical coherence time. In this work, we performed a full simulation of QEC for the rotated surface codes with a code distance 5, which employs 49 qubits and is within reach of the current state-of-the-art quantum computers. In particular, we evaluate the logical error probability in a realistic noise model that incorporates not only stochastic Pauli errors but also coherent errors due to a systematic control error or unintended interactions. While a straightforward simulation of 49 qubits is not tractable within a reasonable computational time, we reduced the number of qubits required to 26 qubits by delaying the syndrome measurement in simulation. This and a fast quantum computer simulator, Qulacs, implemented on GPU allows us to simulate full QEC with an arbitrary local noise within reasonable simulation time. Based on the numerical results, we also construct and verify an effective model to incorporate the effect of the coherent error into a stochastic noise model. This allows us to understand what the effect coherent error has on the logical error probability on a large scale without full simulation based on the detailed full simulation of a small scale. The present simulation framework and effective model, which can handle arbitrary local noise, will play a vital role in clarifying the physical parameters that future experimental QEC should target.|Physical Review Research|2022|10.1103/PhysRevResearch.6.013024|Mitsuki Katsuda, K. Mitarai, K. Fujii|0.3333333333333333|0
2073|An optimized block forward‐elimination and backward‐substitution algorithm for GPU accelerated ILU preconditioner in evaluating the induced electric field during transcranial magnetic stimulation|Numerical simulation is the most popular noninvasive approach to characterize the induced electric field (E-field) in a human head during transcranial magnetic stimulation (TMS) [Deng et al., 2013]. Various computational methods have been proposed to solve equations for the E-field in anatomical head models [Gomez-Tames et al., 2018]. After discretization, solving the resulting system of linear equations is time-consuming, usually consisting of 95% of the total time of the numerical analysis [Chen et al., 2002]. To modify the spectrum of the coefficient matrix for faster convergence, preconditioning techniques, for example, incomplete lowerupper (ILU) factorization, are commonly applied [de Geeter et al., 2011]. However, the conventional element-wise forward-elimination and backsubstitution (FEBS) algorithm to solve the LU matrices is highly sequential, which is the bottleneck of parallelization. In this study, we sped up the iterations of the lower and upper (LU) matrix equations by an optimized block FEBS (B-FEBS). B-FEBS exploited the features of the derived coefficient matrix and enabled parallel processing for the unknowns. Compared with the routines provided by Compute Unified Device Architecture (CUDA), the parallel computing platform and programming model developed by NVIDIA (Santa Clara, CA) for general computing on graphical processing units (GPUs), B-FEBS yielded time reduction factors of 2.47–2.60 for the matrix solves and 1.47–1.91 for total calculation time when simulating TMS using a head model of adult size at 1mm. Seven models were used in the numerical studies, that is, the homogeneous spherical model (radius varies and dielectric conductivity was 0.33 S/ m at 2.44 kHz), head models from Chinese female and male adults [Wu et al., 2012], the upper-part head model of the corresponding author reconstructed using the method in our previous study [Li et al., 2015], and head models of Duke, Ella, and Billie from the virtual population [Gosselin et al., 2014]. They were selected to represent various races and age groups. In total, three kinds of TMS coils were implemented in the simulations, that is, loop coil (diameter of 90mm), figure of eight coil (FOE, diameter of 70mm for each loop), and slinky-7 coil (diameter of 40mm for each loop). The coil plane was parallel to the head transverse plane. A distance of 10mm was kept between the coil and head surface. The center of the coil was aligned to the head vertex.|Bioelectromagnetics|2019|10.1002/bem.22178|Congsheng Li, Yiwen Wei, Zhuolin Ye, Tongning Wu|0.3333333333333333|0
2077|GPU Parallelization of Realistic Purkinje Cells with Complex Morphology|High performance computing (HPC) is becoming mandatory for the simulation of complex and realistic neuronal models. The development of such realistic models will allow to discover innovative therapies and to study brain diseases without undertaking invasive experiments that are not always possible. However, the models complexity requires adopting suitable technologies in order to provide results in short times, hopefully in real-time. To address this issue, the authors decided to exploit Graphics Processing Units (GPUs) in order to develop a realistic and morphologically detailed Purkinje cell model. This paper describes the simulation of the Purkinje cell activity adopting both single and multi-GPU strategy, together with the exploitation of different NVIDIA architectures. Results shows that the simulation times of 10000 cells is reduced from 13 days and 18 hours to about 2 hours.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2019|10.1109/EMPDP.2019.8671581|Marta Ticli, E. Torti, F. Leporati, Stefano Masoli, Giordana Florimbi, E. D’Angelo|0.3333333333333333|0
2109|Complex Power Electronics Systems Modeling and Analysis|ACCURATE and reliable modeling of power converters and electrical systems is a key issue for their integration and development in modern applications. However, nonlinear factors in power electronics introduced by their switching operation, performance demand and complex environments among others, make conventional modeling and analysis methods unreliable and even often-unfeasible. Modern engineering application requirements for power electronics and the future grid development demand a constant evolution of modeling and analysis techniques. On the other hand, novel modeling and analysis methods for power converters and systems, such as improved small-signal modeling and stability criterion, large-signal modeling and analysis methods, describing function methods, and so on, have been developed and proposed by authors from academia and industry to deal with the challenges mentioned above. The aim of this Special Section is to share innovative research results and potentials in advanced modeling and analysis methods for power converters and systems proposed from academia to industry, and further promote their practical application in industrial electronics. We invited original manuscripts presenting recent advances in these fields with special reference to the following topics: 1) Modeling and analysis for power converters and systems. 2) Large-signal modeling and analysis. 3) Nonlinear modeling and analysis for power electronics. 4) Wide band gap devices modeling. 5) Modeling and analysis for HVdc systems. 6) Modeling and analysis for future grids. 7) Modeling and analysis for renewable energy systems. A total of 54 papers were received. Of these submissions, 19 were accepted and are published in this Special Section. These papers cover the various aspects complex systems modeling for modes and stability analysis [items 5)–9), 19) in the Appendix]; control [items 4)–6), 10)–15) in the Appendix]; power quality assessment [items 1), 17), and 18) in the Appendix]; and simulations [items 2), 3), and 16) in the Appendix]. In [item 1) in the Appendix], a generalized single-phase modular multilevel converter model is proposed, which eliminates the zero-sequence voltage coupling effect and is linearized based on harmonic state space theory to precisely characterize the internal harmonic features of modular multilevel converters. In [item 2) in the Appendix], fine-grained circuit partitioning is proposed for simulating high-order models of insulated-gate bipolar transistors (IGBTs) using GPUs.|IEEE transactions on industrial electronics (1982. Print)|2019|10.1109/TIE.2019.2901189|Hong Li, S. Bacha, Davis Montenegro-Martinez|0.3333333333333333|0
2118|Parallel Computing Framework for Optimizing Environmental and Economic Performances of Housing Units|AbstractDecision makers in the housing industry need to carefully analyze housing design and construction decisions to improve housing environmental and economic performances. Available energy optimization models are able to find minimum-cost housing design and construction decisions at different target energy-saving levels. The application of these models, however, is limited due to their time-intensive and often impractical computational requirements. This paper presents a scalable and expandable parallel computing framework to reduce the computational time that is required to optimize the trade-offs between the environmental performance of housing units and their initial cost. The framework is designed as a global parallel optimization algorithm to provide an efficient distribution of the multiobjective genetic algorithm computations over a number of parallel processors. The optimization algorithm is also coupled with an external building energy simulation engine to enable an accurate modeling of housi...|Journal of computing in civil engineering|2016|10.1061/(ASCE)CP.1943-5487.0000505|Aslihan Karatas, K. El-Rayes|0.3333333333333333|0
2134|GPU Acceleration of Smoothed Particle Hydrodynamics for the Navier-Stokes Equations|"Although there exist much work on GPU acceleration on the SPH method, the focus so far has been on the Euler equations in fluid mechanics. This paper presents GPU acceleration on the SPH method for the Navier-Stokes equations for both solid and fluid mechanics. We investigate and compare three CPU-GPU coupling models in terms of one large-scale parallel application code: (1) CPU?GPU (to only run hotspots on GPU), (2) GPU-alone (to run the whole of simulation on GPU), and (3) CPU||GPU (to treat CPU and GPU as equivalent processors). A common issue to the three models, ""easy code transplant onto GPU"", is emphasized. Optimizations on particle indexing and particle interaction on GPU, which are of unique importance to a SPH code, are addressed. Numerical experiments are finally performed and 4x, 10x, 16x speedups are observed for the three coupling models, respectively, with reference to single CPU core. Among the three, the fastest model -- Xthe ""CPU||GPU"" model -- Xfurther undergoes scalability tests on a cluster of 6 heterogeneous nodes and shows 90+% parallel efficiency."|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2016|10.1109/PDP.2016.28|Leisheng Li, Yingrui Wang, R. Tian, Jingtao Wang|0.3333333333333333|0
2183|Literature Survey on GPU Accelerated Circuit Simulation|Analysis, Testing and validation of electronic circuit is very crucial in industries of electronics and embedded systems. Instead of actual hardware testing, simulation software is used for this purpose. Very large circuit design like VLSI testing, affects speed and accuracy of such software tools. So there is the need to increase speed of simulation software without compromising quality. GPUs(Graphics Processing Unit) as a many core processors, are used for parallel processing. To get the advantage of parallel processing in circuit simulation, GPUs can be helpful. This paper explores various methods to accelerate electronic circuit simulation by using parallel processing on GPU. Event-driven, gate-level, model driven approaches are discussed here. Parallelism is induced in mathematical methods used for finding various parameters like voltage, current etc. Speed can be increased from 2 to 150 times for various designs.||2016|10.1109/icsc.2016.90|Shital V. Jagtap, Y. Rao|0.3333333333333333|0
2193|A GPU parallelization scheme for 3D agent-based simulation of in-stent restenosis|The collective biomechanical and mechanobiological effects of cell behaviors in in-stent restenosis (ISR) can be analyzed numerically using FEM (finite element method)-ABM (agent-based model) coupling approach. Traditional FEM-ABM frameworks, limited to simulations with a small amount of agent cells, are impossible for large-scale analysis, especially 3D simulations. A 3D parallelization scheme for FEM-ABM coupling simulations based on GPU acceleration is proposed to improve the computational efficiency of the coupling framework. Three-dimensional vascular restenosis simulations have been conducted to validate the algorithm and comparisons are completed to test the performance improvement as well as the effects of the GPU core number. The proposed approach is proved to be effective for large-scale FEM-ABM coupling simulations of restenosis and can be used to build real-scale virtual arteries for long-term restenosis prediction.|IEEE International Conference on Cyborg and Bionic Systems|2019|10.1109/CBS46900.2019.9114482|Yucheng He, Shibo Li, Long Lei, Ying Hu, Yu Sun, Yimin Zhou|0.3333333333333333|0
2227|The Efficient High‐Resolution SAR Reconstruction Imaging Algorithms for Three‐Dimensional Electrically Large‐Scale Targets in Clutter|In recent years, significant effort of both the scientific and industrial community is invested in research and development of multifrequency, multipolarization, and high‐resolution synthetic aperture radar (SAR) systems. One of the key issues is to realize efficient methods for imaging, reconstruction, and target detection of three‐dimensional (3‐D) electrically large‐scale objects. In this paper, an advanced multiple‐input multiple‐output (MIMO) SAR based on high‐resolution imaging algorithm was developed. Specifically, a reasonable sea surface model to quantify the influence of physical parameters in clutter was yielded as well. The SAR simulation system includes the following: (i) refined geometric‐electromagnetic modeling by Gmsh, (ii) a dynamic sea surface model, (iii) single‐input multiple‐output (SIMO) radar imaging formation by employing a fast backprojection algorithm, (iv) design of MIMO antenna arrays to achieve a good trade‐off between the main beam width and the side lobe level, (v) 3‐D SAR imaging algorithm on the basis of optimized physical optics for electromagnetic scattering computation, and (vi) hardware acceleration technique using graphics processing unit. Finally, numerical experiments indicate that 3‐D visualization of sphere, and examples of fighter aircraft and aircraft carrier are generated within 200 s, further proving correctness and effectiveness of the developed 3‐D MIMO SAR imaging algorithm.|Radio Science|2019|10.1029/2018RS006642|Xia Wu|0.3333333333333333|0
2289|An efficient ray-tracing method for determining terrain intercepts in EDL simulations|The calculation of a ray's intercept from an arbitrary point in space to a prescribed surface is a common task in computer simulations. The arbitrary point often represents an object that is moving according to the simulation, while the prescribed surface is fixed in a defined frame. For detailed simulations, this surface becomes complex, taking the form of real-world objects such as mountains, craters or valleys which require more advanced methods to accurately calculate a ray's intercept location. Incorporation of these complex surfaces has commonly been implemented in graphics systems that utilize highly optimized graphics processing units to analyze such features. This paper proposes a simplified method that does not require computationally intensive graphics solutions, but rather an optimized ray-tracing method for an assumed terrain dataset. This approach was developed for the Mars Science Laboratory mission which landed on the complex terrain of Gale Crater. First, this paper begins with a discussion of the simulation used to implement the model and the applicability of finding surface intercepts with respect to atmosphere modeling, altitude determination, radar modeling, and contact forces influencing vehicle dynamics. Next, the derivation and assumptions of the intercept finding method are presented. Key assumptions are noted making the routines specific to only certain types of surface data sets that are equidistantly spaced in longitude and latitude. The derivation of the method relies on ray-tracing, requiring discussion on the formulation of the ray with respect to the terrain datasets. Further discussion includes techniques for ray initialization in order to optimize the intercept search. Then, the model implementation for various new applications in the simulation are demonstrated. Finally, a validation of the accuracy is presented along with the corresponding data sets used in the validation. A performance summary of the method will be shown using the analysis from the Mars Science Laboratory's terminal descent sensing model. Alternate uses will also be shown for determining horizon maps and orbiter set times.|IEEE Aerospace Conference|2016|10.1109/AERO.2016.7500591|J. Shidner|0.3333333333333333|0
1638|Real Time Visualization of Crowd Dynamics Scenarios|This paper presents an approach to real-time simulation of crowd dynamics on GPU enabled computing architectures. We discuss challenges with parallelization of agent-based models, implementing parallel simulation algorithms, visualization and interaction with the simulated scene and, most importantly, ensuring communication and synchronization between all these processes. Our main objective is to provide interactive simulation of realistic models such as pedestrian dynamics, in which large crowds move and interact among themselves and with the environment. Simulation parameters like scene complexity, scene composition, as well as the number of agents are varied in order to simulate different scenarios and to assess the impact on performance.|Romanian Conference on Human-Computer Interaction|2015|10.1109/reconfig.2015.7393281|A. Sabou, Dan Razvan Ilies, D. Gorgan|0.3|0
1705|Analytical reconstructions of intensity modulated x-ray phase-contrast imaging of human scale phantoms.|This paper presents analytical approach to modeling of a full planar and volumetric acquisition system with image reconstructions originated from partial illumination x-ray phase-contrast imaging at a human scale using graphics processor units. The model is based on x-ray tracing and wave optics methods to develop a numerical framework for predicting the performance of a preclinical phase-contrast imaging system of a human-scaled phantom. In this study, experimental images of simple numerical phantoms and high resolution anthropomorphic phantoms of head and thorax based on non-uniform rational b-spline shapes (NURBS) prove the correctness of the model. Presented results can be used to simulate the performance of partial illumination x-ray phase-contrast imaging system on various preclinical applications.|Biomedical Optics Express|2015|10.1364/BOE.6.004255|Bartłomiej Włodarczyk, Jakub Pietrzak|0.3|0
1820|Accelerated molecular mechanical and solvation energetics on multicore CPUs and manycore GPUs|Motivation. Despite several reported acceleration successes of programmable GPUs (Graphics Processing Units) for molecular modeling and simulation tools, the general focus has been on fast computation with small molecules. This was primarily due to the limited memory size on the GPU. Moreover simultaneous use of CPU and GPU cores for a single kernel execution -- a necessity for achieving high parallelism -- has also not been fully considered. Results. We present fast computation methods for molecular mechanical (Lennard-Jones and Coulombic) and generalized Born solvation energetics which run on commodity multicore CPUs and manycore GPUs. The key idea is to trade off accuracy of pairwise, long-range atomistic energetics for higher speed of execution. A simple yet efficient CUDA kernel for GPU acceleration is presented which ensures high arithmetic intensity and memory efficiency. Our CUDA kernel uses a cache-friendly, recursive and linear-space octree data structure to handle very large molecular structures with up to several million atoms. Based on this CUDA kernel, we present a hybrid method which simultaneously exploits both CPU and GPU cores to provide the best performance based on selected parameters of the approximation scheme. Our CUDA kernels achieve more than two orders of magnitude speedup over serial computation for many of the molecular energetics terms. The hybrid method is shown to be able to achieve the best performance for all values of the approximation parameter. Availability. The source code and binaries are freely available as PMEOPA (Parallel Molecular Energetic using Octree Pairwise Approximation) and downloadable from http://cvcweb.ices.utexas.edu/software.|ACM International Conference on Bioinformatics, Computational Biology and Biomedicine|2015|10.1145/2808719.2808742|Alexander Rand, C. Bajaj, Qin Zhang, Jesmin Jahan Tithi, R. Chowdhury, Deukhyun Cha|0.3|0
2020|Analysis of hybrid parallelization strategies: simulation of Anderson localization and Kalman Filter for LHCb triggers|This thesis presents two experiences of hybrid programming applied to condensed matter and high energy physics. The two projects differ in various aspects, but both of them aim to analyse the benefits of using accelerated hardware to speedup the calculations in current science-research scenarios. The first project enables massively parallelism in a simulation of the Anderson localisation phenomenon in a disordered quantum system. The code represents a Hamiltonian in momentum space, then it executes a diagonalization of the corresponding matrix using linear algebra libraries, and finally it analyses the energy-levels spacing statistics averaged over several realisations of the disorder. The implementation combines different parallelization approaches in an hybrid scheme. The averaging over the ensemble of disorder realisations exploits massively parallelism with a master-slave configuration based on both multi-threading and message passing interface (MPI). This framework is designed and implemented to easily interface similar application commonly adopted in scientific research, for example in Monte Carlo simulations. The diagonalization uses multi-core and GPU hardware interfacing with MAGMA, PLASMA or MKL libraries. The access to the libraries is modular to guarantee portability, maintainability and the extension in a near future. The second project is the development of a Kalman Filter, including the porting on GPU architectures and autovectorization for online LHCb triggers. The developed codes provide information about the viability and advantages for the application of GPU technologies in the first triggering step for Large Hadron Collider beauty experiment (LHCb). The optimisation introduced on both codes for CPU and GPU delivered a relevant speedup on the Kalman Filter. The two GPU versions in CUDA R © and OpenCLTM have similar performances and are adequate to be considered in the upgrade and in the corresponding implementations of the Gaudi framework. In both projects we implement optimisation techniques in the CPU code. This report presents extensive benchmark analyses of the correctness and of the performances for both projects.||2015|10.5605/ieb.11.3|J. Mena|0.3|0
2108|A Performance and Scalability Analysis of the Tsunami Simulation EasyWave for Different Multi-Core Architectures and Programming Models|In this paper, the performance and scalability of different multi-core systems is experimentally evaluated for the Tsunami simulation EasyWave. The target platforms include a standard Ivy Bridge Xeon processor, an Intel Xeon Phi accelerator card, and also a GPU. OpenMP, MPI and CUDA were used to parallelize the program to these platforms. The absolute performance of the application on the different platforms is compared, and limiting factors are analyzed based on the application’s scaling behavior. Keywords-multi-core architectures, cache awareness, hybrid programming, scientific application, performance evaluation||2015|10.1109/spects.2015.7285285|Johannes Spazier, Steffen Christgau, Bettina Schnor|0.3|0
2312|GPU-based proximity query processing on unstructured triangular mesh model|This paper presents a novel proximity query (PQ) approach capable to detect the collision and calculate the minimal Euclidean distance between two non-convex objects in 3D, namely the robot and the environment. Such approaches are often considered as computationally demanding problems, but are of importance to many applications such as online simulation of haptic feedback and robot collision-free trajectory. Our approach enables to preserve the representation of unstructured environment in the form of triangular meshes. The proposed PQ algorithm is computationally parallel so that it can be effectively implemented on graphics processing units (GPUs). A GPU-based computation scheme is also developed and customized, which shows >200 times faster than an optimized CPU with single core. Comprehensive validation is also conducted on two simulated scenarios in order to demonstrate the practical values of its potential application in image-guided surgical robotics and humanoid robotic control.|IEEE International Conference on Robotics and Automation|2015|10.1109/ICRA.2015.7139808|W. Luk, Kit-Hang Lee, Yue Chen, G. C. Chow, K. Kwok, Ziyan Guo|0.3|0
1727|CFD Simulation and Optimization of the Cooling of Open Compute Machine Learning “Big Sur” Server|In recent years, there have been phenomenal increases in Artificial Intelligence and Machine Learning that require data collection, mining and using data sets to teach computers certain things to learn, analyze image and speech recognition. Machine Learning tasks require a lot of computing power to carry out numerous calculations. Therefore, most servers are powered by Graphics Processing Units (GPUs) instead of traditional CPUs. GPUs provide more computational throughput per dollar spent than traditional CPUs. Open Compute Servers forum has introduced the state-of-the-art machine learning servers “Big Sur” recently. Big Sur unit consists of 4OU (OpenU) chassis housing eight NVidia Tesla M40 GPUs and two CPUs along with SSD storage and hot-swappable fans at the rear. Management of the airflow is a critical requirement in the implementation of air cooling for rack mount servers to ensure that all components, especially critical devices such as CPUs and GPUs, receive adequate flow as per requirement. In addition, component locations within the chassis play a vital role in the passage of airflow and affect the overall system resistance. In this paper, sizeable improvement in chassis ducting is targeted to counteract effects of air diffusion at the rear of air flow duct in “Big Sur” Open Compute machine learning server wherein GPUs are located directly downstream from CPUs. A CFD simulation of the detailed server model is performed with the objective of understanding the effect of air flow bypass on GPU die temperatures and fan power consumption. The cumulative effect was studied by simulations to see improvements in fan power consumption by the server. The reduction in acoustics noise levels caused by server fans is also discussed.|Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems|2018|10.1109/ITHERM.2018.8419600|M. Dhadve, D. Agonafer, Jimil M. Shah|0.2857142857142857|0
1936|Particle–mesh coupling in the interaction of fluid and deformable bodies with screen space refraction rendering|On the basis of the smoothed particle hydrodynamics and finite element method (FEM) model, we propose a method integrating several improvements for the real‐time simulation of fluid interacting with deformable bodies. We improve the particle neighbor search in smoothed particle hydrodynamics, so that the predefined scene containers are no longer needed. This improvement can also be applied to the simulation of fluid interacting with other materials, such as rigid and soft bodies. We also propose a two‐way coupling method for fluid and deformable bodies, where the particle–mesh interaction is obtained by the ray‐traced collision detection method instead of the proxy/ghost particle generation. By using the forward ray‐tracing method for both velocity and position, we are able to calculate the coupling forces based on the conservation of momentum and kinetic energy in the particle–mesh interaction. We use the screen space fluid rendering for fluid, and on the basis of that, we introduce a screen space refraction rendering method to improve the refraction effect. We implement our method in NVIDIA CUDA and OptiX to make use of the full computational power of a graphics processing unit. The simulation results are analyzed and discussed to show the efficiency of our method.|Comput. Animat. Virtual Worlds|2018|10.1002/cav.1787|S. Im, Ka‐Hou Chan, W. Ke|0.2857142857142857|0
2019|Thread scheduling for GPU-based OPC simulation on multi-thread|As semiconductor product development based on shrinkage continues, the accuracy and difficulty required for the model based optical proximity correction (MBOPC) is increasing. OPC simulation time, which is the most timeconsuming part of MBOPC, is rapidly increasing due to high pattern density in a layout and complex OPC model. To reduce OPC simulation time, we attempt to apply graphic processing unit (GPU) to MBOPC because OPC process is good to be programmed in parallel. We address some issues that may typically happen during GPU-based OPC simulation in multi thread system, such as “out of memory” and “GPU idle time”. To overcome these problems, we propose a thread scheduling method, which manages OPC jobs in multiple threads in such a way that simulations jobs from multiple threads are alternatively executed on GPU while correction jobs are executed at the same time in each CPU cores. It was observed that the amount of GPU peak memory usage decreases by up to 35%, and MBOPC runtime also decreases by 4%. In cases where out of memory issues occur in a multi-threaded environment, the thread scheduler was used to improve MBOPC runtime up to 23%.|Advanced Lithography|2018|10.1117/12.2295696|Sooryong Lee, Heejun Lee, Sangwook Kim, Hwansoo Han, Jisuk Hong|0.2857142857142857|0
2042|Energy‐based dissolution simulation using SPH sampling|A novel unified particle‐based method is proposed for real‐time dissolution simulation that is fast, predictable, independent of sampling resolution, and visually plausible. The dissolution model is derived from collision theory and integrated into a smoothed particle hydrodynamics fluid solver. Dissolution occurs when a solute is submerged in solvent. Physical laws govern the local excitation of solute particles based on kinetic energy: when the local excitation energy exceeds a user‐specified threshold (activation energy), the particle will be dislodged from the solid. Solute separation during dissolution is handled using a new Graphics Processing Unit (GPU)‐based region growing method. The use of smoothed particle hydrodynamics sampling for both solute and solvent guarantees a predictable and smooth dissolution process and provides user control of the volume change during the phase transition. A mathematical relationship between the activation energy and dissolution time allows for intuitive artistic control over the global dissolution rate. We demonstrate this method using a number of practical examples, including antacid pills dissolving in water, hydraulic erosion of nonhomogeneous terrains, and melting.|Comput. Animat. Virtual Worlds|2018|10.1002/cav.1798|Richard Southern, J. Zhang, Min Jiang|0.2857142857142857|0
2129|GPU-based computational modeling of magnetic resonance imaging of vascular structures|Magnetic resonance imaging (MRI) is one of the most important diagnostic tools in modern medicine. Since it is a high-cost and highly-complex imaging modality, computational models are frequently built to enhance its understanding as well as to support further development. However, such models often have to be simplified to complete simulations in a reasonable time. Thus, the simulations with high spatial/temporal resolutions, with any motion consideration (like blood flow) and/or with 3D objects usually call for using parallel computing environments. In this paper, we propose to use graphics processing units (GPUs) for fast simulations of MRI of vascular structures. We apply a CUDA environment which supports general purpose computation on GPU (GPGPU). The data decomposition strategy is applied and thus the parts of each virtual object are spread over the GPU cores. The GPU cores are responsible for calculating the influence of blood flow behavior and MRI events after successive time steps. In the proposed approach, different data layouts, memory access patterns, and other memory improvements are applied to efficiently exploit GPU resources. Computational performance is thoroughly validated for various vascular structures and different NVIDIA GPUs. Results show that MRI simulations can be accelerated significantly thanks to GPGPU. The proposed GPU-based approach may be easily adopted in the modeling of other flow related phenomena like perfusion, diffusion or transport of contrast agents.|The international journal of high performance computing applications|2018|10.1177/1094342016677586|M. Kretowski, J. Bézy-Wendling, K. Jurczuk|0.2857142857142857|0
1637|Auto-Tuned Preconditioners for the Spectral Element Method on GPUs|The Poisson pressure solve resulting from the spectral element discretization of the incompressible Navier-Stokes equation requires fast, robust, and scalable preconditioning. In the current work, a parallel scaling study of Chebyshevaccelerated Schwarz and Jacobi preconditioning schemes is presented, with special focus on GPU architectures, such as OLCF’s Summit. Convergence properties of the Chebyshevaccelerated schemes are compared with alternative methods, such as low-order preconditioners combined with algebraic multigrid. Performance and scalability results are presented for a variety of preconditioner and solver settings. The authors demonstrate that Chebyshev-accelerated-Schwarz methods provide a robust and effective smoothing strategy when using p-multigrid as a preconditioner in a Krylovsubspace projector. At the same time, optimal preconditioning parameters can vary for different geometries, problem sizes, and processor counts. This variance motivates the development of an autotuner to optimize solver parameters on-line, during the course of production simulations.|arXiv.org|2021|10.2172/1823469|Malachi Phillips, S. Kerkemeier, P. Fischer|0.25|0
1652|Optimum Power-Performance GPU Configuration Prediction Based on Code Attributes|GPUs have been widely used in the past decade to speed up the execution of general purpose applications with high level of parallelism. The efficiency of running general purpose applications on GPUs depends on how well the processing and memory demands of the application is balanced with the hardware resources available on the target GPU and it can significantly affect the power and performance of the application execution. In this study, we are proposing a model that relates the architectural parameters of the GPU to the characteristics of the application running on it. The model is used to predict the GPU configuration that results in the best power-performance that the application can achieve running on the GPU. We compare the model produced Optimal configurations to actual optimal configurations obtained from simulations and show that the Optimal configurations obtained from the model is very close to the actual ones.|International Symposium on High Performance Computing Systems and Applications|2017|10.1109/HPCS.2017.69|A. Baniasadi, N. Dimopoulos, A. Jooya|0.25|0
1663|A multithreaded algorithm of UAV visual localization based on a 3D model of environment: implementation with CUDA technology and CNN filtering of minor importance objects|Visual based navigation plays an important role in localization and path planning, especially in GPS-denied environments. This paper presents a visual based localization algorithm for a UAV within an indoor environment. The algorithm uses multithreaded computing CUDA technology and CNN-preprocessing filtering, which is responsible for filtering out dynamic objects. The algorithm is simulated in ROS/Gazebo environment with two different approaches – one uses CPU only and the other uses CPU and GPU and their performance is compared.||2017|10.5954/ICAROB.2017.GS2-3|E. Magid, Alexander Buyval, Mikhail Gavrilenkov|0.25|0
1664|Simulation of large neuronal networks in cloud and grid with graphics processing units|Software for simulation of large networks of coupled non-linear oscillators in clusters, grids and clouds using graphical processing units (GPU) was designed, developed, tested and applied for scientific simulations. The software provides easy integration of new oscillators' models support, dynamic load distribution between hosts' central processing units (CPU) and several GPU devices. Different GPU devices provide speed-up of 12–50 compared to single core Intel Xeon, 2.4 GHz depending on GPU and job types. The software was efficiently applied for simulations of 3D networks with 107–108 oscillators described by Kuramoto-Sakaguchi model and new types of “chimera states” were discovered in such simulations.|International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications|2017|10.1109/IDAACS.2017.8095096|Andrii I. Cherederchuk, O. Sudakov, V. Maistrenko|0.25|0
1706|VISCOUS FLUID FLOW MODELING WITH THE LATTICE BOLTZMANN METHOD ON GRAPHICS PROCESSORS USING WebGL API|This work is dedicated to the modeling methodology of a viscous fluid flows with the lattice Boltzmann method on graphic processors based on the technology of images rendering in web browsers WebGL. A two-dimensional nine-velocity LBM model (D2Q9) with a collision integral in a Bhatnagar-Gross-Kruk approximation form is shown. The possibilities of calculation acceleration using WebGL technology is described, namely features of using textures to contain values of some physical quantities in numerical algorithms and using fremebuffers to storage the textures, influence of the texture parameters on the numerical algorithms, features of shaders programming. The questions of shader programs using for carrying out stages of physical modeling were considered. The proposed methodology was used to develop an original web program for modeling of classical test problems. Simulations of the Poiseuille flow in a plane channel and the flow around a circular cylinder in a plane channel were performed. The obtained results were compared with the results of calculations performed in the original verified modeling program based on the lattice Boltzmann method and in the Comsol Multiphysics package with the finite element method. Comparisons of the values of the velocity magnitude showed the consistency of the obtained results with the data of other numerical experiments. The analysis of computational speed in comparison with modeling using the optimized algorithm of a method with use of the technology of parallel calculations on CPU OpenMP in the original program is carried out. It is shown that the acceleration of calculations depends on the number of cells of the calculation grid. The results of the fluid flow modeling around a circular cylinder at Re = 1000 are demonstrated, which are obtained 30 times faster than with the calculations obtained with optimized lattice Boltzmann method and OpenMP technology.|Journal of Numerical and Applied Mathematics|2021|10.17721/2706-9699.2021.1.16|and Global, Kyiv Information Space, of Telecommunication|0.25|0
1792|A performance analysis of a mimetic finite difference scheme for acoustic wave propagation on GPU platforms|Realistic applications of numerical modeling of acoustic wave dynamics usually demand high‐performance computing because of the large size of study domains and demanding accuracy requirements on simulation results. Forward modeling of seismic motion on a given subsurface geological structure is by itself a good example of such applications, and when used as a component of seismic inversion tools or as a guide for the design of seismic surveys, its computational cost increases enormously. In the case of finite difference methods (or any other volumen‐discretization scheme), memory and computing demands rise with grid refinement, which may be necessary to reduce errors on numerical wave patterns and better capture target physical devices. In this work, we present several implementations of a mimetic finite difference method for the simulation of acoustic wave propagation on highly dense staggered grids. These implementations evolve as different optimization strategies are employed starting from appropriate setting of compilation flags, code vectorization by using streaming SIMD extensions Advanced Vector Extensions (AVX), CPU parallelization by exploiting the Open Multi‐Processing framework to the final code parallelization on graphics processing unit platforms. We present and discuss the increasing processing speed up of this mimetic scheme achieved by the gradual implementation and testing of all these performance optimizations. In terms of simulation times, the performance of our graphics processing unit parallel implementations is consistently better than the best CPU version. Copyright © 2016 John Wiley & Sons, Ltd.|Concurrency and Computation|2017|10.1002/cpe.3880|F. Solano-Feo, J. Francés, Beatriz Otero, J. Guevara-Jordan, R. Rodriguez, O. Rojas|0.25|0
1805|The Design and Development of a GPU-accelerated Radar Simulator for Space Debris Monitoring|The problem of space debris represents a major topic of concern in astronomy as the threat of space junk continues to grow, and the accuracy of its tracking is greatly restricted by the insufficiency and limitations of current surveillance sensors. This article presents the development of an open-source, high-performance, signal-level radar simulator to assist in modelling the detection and tracking of space debris from terrestrial radar stations, including multistatic installations where the transmitter and receiver may be separated by many kilometers. This tool is expected to aid astronomers and researchers in space situational awareness, supporting the modelling of radar interactions in this context and simulation-based exploration of radar designs for space surveillance. It makes use of an accelerated orbit propagation technique with measured two-line element datasets being used to define space debris objects. The software has been named the Space Object Astrodynamics and Radar Simulator – or SOARS – and both the transmitted and received signals generated by the application have been shown to agree with theoretical expectations. Additionally, SOARS is presently undergoing continued development, extension and optimization for heterogeneous computing platforms, enabling the use of the NVIDIA® Compute Unified Device Architecture (CUDA) interface. Results have demonstrated promising speed-ups in simulation runtimes when using the CUDA version of the application over the original sequential version, even on lower-end graphics processors. It is anticipated that the developed application will be used for the design and testing of radar sensors for space situational awareness applications, as well as for use in research, teaching and training environments.|High Performance Computing and Cluster Technologies Conference|2021|10.1145/3497737.3497741|M. Gaffar, Mogamat Yaaseen Martin, D. MacLeod, S. Winberg|0.25|0
1834|Accelerated solution of stiffness matrix for isoparametric elements based on CUDA|High precision results in structural with the shortest time consumption are expected when methods are introduced to solve FEM(Finite element method). Solving of stiffness matrix assembled by isoparametric elements and solving the assembled stiffness matrix are the most time-consuming. In the previous serial algorithms, there is always a time limitation for some applications and it is hard to achieve. However, break-through in programming and feasibility of general-purpose applications executed on GPU (Graph Processing Unit), such as the parallel computing platform and programming model CUDA (Compute Unified Device Architecture) released by NVIDIA corporation, make it possible for some large scale FEM explicit dynamic simulation in real time. Authors present an approach to accelerate calculation in element stiffness matrices, taking the three-dimensional hexahedral isoparametric element in different scale as an example. A speedup of about 15 times is achieved with respect to parallel algorithms using boost on CPU. The results show that the parallel algorithm based on CUDA can satisfy the fast simulation of finite element model of structural problems with certain computational scale.|International Conference on Signal Processing, Communications and Computing|2017|10.1109/ICSPCC.2017.8242497|Hu Binxing, Li Xinguo, Li Zenghao, Qiao Hao|0.25|0
1861|scAMACE: Model-based approach to the joint analysis of single-cell data on chromatin accessibility, gene expression and methylation|The advancement in technologies and the growth of available single-cell datasets motivate integrative analysis of multiple single-cell genomic datasets. Integrative analysis of multimodal single-cell datasets combines complementary information offered by single-omic datasets and can offer deeper insights on complex biological process. Clustering methods that identify the unknown cell types are among the first few steps in the analysis of single-cell datasets, and they are important for downstream analysis built upon the identified cell types. We propose scAMACE for the integrative analysis and clustering of single-cell data on chromatin accessibility, gene expression and methylation. We demonstrate that cell types are better identified and characterized through analyzing the three data types jointly. We develop an efficient expectation-maximization (EM) algorithm to perform statistical inference, and evaluate our methods on both simulation study and real data applications. We also provide the GPU implementation of scAMACE, making it scalable to large datasets. The software and datasets are available at https://github.com/cuhklinlab/scAMACE_py (python implementation) and https://github.com/cuhklinlab/scAMACE (R implementation).|bioRxiv|2021|10.1101/2021.03.29.437485|Zexuan Sun, Zhixiang Lin, Jiaxuan Wangwu|0.25|0
1866|Modeling the Dynamics of Heavy-Ion Collisions with a Hydrodynamic Model Using a Graphics Processor|Dense bulk matter is formed during heavy-ion collision and expands towards a vacuum. It behaves as a perfect fluid, described by relativistic hydrodynamics. In order to study initial condition fluctuation and properties of jet propagation in dense hot matter, we assume a Cartesian laboratory frame with several million cells in a stencil with high-accuracy data volume grids. Employing numerical algorithms to solve hydrodynamic equations in such an assumption requires a lot of computing power. Hydrodynamic simulations of nucleus + nucleus interactions in the range of energies of the Large Hadron Collider (LHC) are carried out using our program, which uses Graphics Processing Units (GPUs) and Compute Unified Device Architecture (CUDA). In this work, we focused on transforming hydrodynamic quantities into kinetic descriptions. We implemented the hypersurface freeze-out conditions using marching cubes techniques. We developed freeze-out procedures to obtain the momentum distributions of particles on the hypersurface. The final particle distributions, elliptic flow, and higher harmonics are comparable to the experimental LHC data.|Symmetry|2021|10.3390/sym13030507|P. Aszklar, M. Slodkowski, Dominik Setniewski, J. Porter-Sobieraj|0.25|0
1913|Acceleration of Gravitation Field Analysis for Asteroids by GPU Computation|Researches with explorations by space probes for asteroids have been performed actively to approach to the origin of the solar system and life. One of methods toward the goal is analyzing structure of solar system bodies by numerical simulation. GFandSlope is a code which calculates the gravitation field, slope, and attraction of given model data for small solar system bodies. When we use the existing sequential computation code, it is inevitable to take large time to analyze high resolution models with different initial conditions. This work achieved to compute several thousands faster than the previous by GPU implementation, which will also boost researches in the field of space science. This paper presents the evaluation of our GPU codes for fast gravitation field analysis and discusses numerical precision in floating point operations on the GPU for practical application.|International Symposium on Embedded Multicore/Many-core Systems-on-Chip|2021|10.1109/MCSoC51149.2021.00010|N. Nakasato, Fumiya Kono, K. Matsumoto, N. Hirata|0.25|0
2005|Spectral element method implementation on GPU for Lamb wave simulation|Parallel implementation of the time domain spectral element method on GPU (Graphics Processing Unit) is presented. The proposed spectral element method implementation is based on sparse matrix storage of local shape function derivatives calculated at Gauss–Lobatto–Legendre points. The algorithm utilizes two basic operations: multiplication of sparse matrix by vector and element-by-element vectors multiplication. Parallel processing is performed on the degree of freedom level. The assembly of resultant force is done by the aid of a mesh coloring algorithm. The implementation enables considerable computation speedup as well as a simulation of complex structural health monitoring systems based on anomalies of propagating Lamb waves. Hence, the complexity of various models can be tested and compared in order to be as close to reality as possible by using modern computers. A comparative example of a composite laminate modeling by using homogenization of material properties in one layer of 3D brick spectral elements with composite in which each ply is simulated by separate layer of 3D brick spectral elements is described. Consequences of application of each technique are explained. Further analysis is performed for composite laminate with delamination. In each case piezoelectric transducer as well as glue layer between actuator and host structure is modeled.|Smart Structures and Materials + Nondestructive Evaluation and Health Monitoring|2017|10.1117/12.2258600|W. Ostachowicz, M. Radzieński, P. Kudela, T. Wandowski|0.25|0
2006|Image formation simulation for computer-aided inspection planning of machine vision systems|In this work, a simulation toolset for Computer Aided Inspection Planning (CAIP) of systems for automated optical inspection (AOI) is presented along with a versatile two-robot-setup for verification of simulation and system planning results. The toolset helps to narrow down the large design space of optical inspection systems in interaction with a system expert. The image formation taking place in optical inspection systems is simulated using GPU-based real time graphics and high quality off-line-rendering. The simulation pipeline allows a stepwise optimization of the system, from fast evaluation of surface patch visibility based on real time graphics up to evaluation of image processing results based on off-line global illumination calculation. A focus of this work is on the dependency of simulation quality on measuring, modeling and parameterizing the optical surface properties of the object to be inspected. The applicability to real world problems is demonstrated by taking the example of planning a 3D laser scanner application. Qualitative and quantitative comparison results of synthetic and real images are presented.|Optical Metrology|2017|10.1117/12.2269166|Stephan Bergmann, C. Dachsbacher, Mahsa Mohammadikaji, Stephan Irgenfried, J. Beyerer, H. Wörn|0.25|0
2008|Novel fluid detail enhancement based on multi‐layer depth regression analysis and FLIP fluid simulation|In this paper, we propose a novel integrated method for effective modeling and realistic enhancement of scale‐sensitive fluid simulation details. The core of our method is the organic of multi‐layer depth image regression analysis and fluid implicit particle fluid simulation of which the regression analysis induces the criterion where the fluid details should be produced. First, we capture the depth buffer of the fluid surface dynamically from the top of scene. Second, we employ depth peeling technique to decompose the target fluid volume into multiple depth layers and conduct time‐space analysis over surface layers. Third, we propose a logistic regression‐based model to rigorously pinpoint the complex interacting regions, wherein multiple detail‐relevant factors are taken into account based on the captured multiple depth layers. Finally, details are enhanced by animating extra diffuse materials and augmenting the air‐fluid mixing phenomenon. It is evident that, with depth peeling technology, we can afford rigorous analysis not only across surface layers at different fluid depth but along the depth direction as well. After integrating the analysis results from these two sources, we are capable of performing detail enhancement both on the fluid surface and inside the fluid to obtain a great visual effect, even when large occlusion exists. Directly benefiting from the flexibility of image‐space‐dominant processing, our unified framework can be entirely implemented on graphics processing units and thus achieves interactive performance. For various fluid phenomena with different diffuse materials (e.g., spray, foam, and bubble), comprehensive experiments and evaluations have demonstrated its superiority in high‐fidelity fluid detail enhancement and its interaction with surrounding environment.|Comput. Animat. Virtual Worlds|2017|10.1002/cav.1741|A. Hao, Yuxing Qiu, Shuai Li, Qing Xia, Hong Qin, Lipeng Yang|0.25|0
2015|MEDINA: MECCA Development in Accelerators – KPP Fortran to CUDA source-to-source Pre-processor|The global climate model ECHAM/MESSy Atmospheric Chemistry (EMAC) is a modular global model that simulates climate change and air quality scenarios. The application includes different sub-models for the calculation of chemical species concentrations, their interaction with land and sea, and the human interaction. The paper presents a source-to-source parser that enables support for Graphics Processing Units (GPU) by the Kinetic Pre-Processor (KPP) general purpose open-source software tool. The requirements of the host system are also described. The source code of the source-to-source parser is available under the MIT License.||2017|10.5334/JORS.158|Michail Alvanos, Theodoros Christoudias|0.25|0
2094|Lattice Boltzmann simulations of bubble interactions on GPU cluster|ABSTRACT A three-dimensional two-phase lattice Boltzmann model is adopted to simulate bubble interactions on a multi-graphics processing unit (GPU) cluster. Both the level of the predicted spurious velocity and Laplace’s law’s satisfaction are used to validate the numerical implementation. The deformation and the terminal velocity of a single-rising bubble are further simulated, and the predicted terminal Reynolds number is contrasted with the available measurement and prediction. Dual-bubble rising at initial in-line and off-line states are also investigated with both uniform and differential bubble radii. For two bubbles of the same size, the trailing bubble suffers a dramatic deformation after entering the wake region generated by the leading bubble. However, this phenomenon is less significant if the larger bubble is placed beneath the smaller bubble. Finally, the present numerical procedure exhibits good scalability for multi-GPU computations.||2021|10.1080/02533839.2021.1919564|F. Lin, Ching-Wen Yang, Chao-An Lin, Chieng-yi Chang, T. Chiu|0.25|0
2149|Fly4Arts : Art numérique évolutionnaire avec l'Algorithme des mouches|The aim of this study is to generate artistic images, such as digital mosaics, as an optimisation problem without the introduction of any a priori knowledge or constraint other than an input image. The usual practice to produce digital mosaic images heavily relies on Centroidal Voronoi diagrams. We demonstrate here that it can be modelled as an optimisation problem solved using a cooperative co-evolution strategy based on the Parisian evolution approach, the Fly algorithm. An individual is called a fly. Its aim of the algorithm is to optimise the position of infinitely small 3-D points (the flies). The Fly algorithm has been initially used in real-time stereo vision for robotics. It has also demonstrated promising results in image reconstruction for tomography. In this new application, a much more complex representation has been study. A y is a tile. It has its own position, size, colour, and rotation angle. Our method takes advantage of graphics processing units (GPUs) to generate the images using the modern OpenGL Shading Language (GLSL) and Open Computing Language (OpenCL) to compute the difference between the input image and simulated image. Different types of tiles are implemented, some with transparency, to generate different visual effects, such as digital mosaic and spray paint. An online study with 41 participants has been conducted to compare some of our results with those generated using an open source software for image manipulation. It demonstrates that our method leads to more visually appealing images.||2017|10.21494/ISTE.OP.2017.0177|Zainab Ali Abbood, F. Vidal|0.25|0
2225|Online Audio-Visual Speech Separation with Generative Adversarial Training|Audio-visual speech separation has been demonstrated to be effective in solving the cocktail party problem. However, most of the models cannot meet online processing, which limits their application in video communication and human-robot interaction. Besides, SI-SNR, the most popular training loss function in speech separation, results in some artifacts in the separated audio, which would harm downstream applications, such as automatic speech recognition (ASR). In this paper, we propose an online audio-visual speech separation model with generative adversarial training to solve the two problems mentioned above. We build our generator (i.e., audio-visual speech separator) with causal temporal convolutional network block and propose a streaming inference strategy, which allows our model to do speech separation in an online manner. The discriminator is involved in optimizing the generator, which can reduce the negative effects of SI-SNR. Experiments on simulated 2-speaker mixtures based on challenging audio-visual dataset LRS2 show that our model outperforms the state-of-the-art audio-only model Conv-TasNet and audio-visual model advr-AVSS under the same model size. We test the running time of our model on GPU and CPU, and results show that our model meets online processing. The demo and code can be found at https://github.com/aispeech-lab/oavss.|International Conference on Computing and Artificial Intelligence|2021|10.1145/3467707.3467764|Jiaming Xu, Bo Xu, Yunzhe Hao, Peng Zhang|0.25|0
2229|Sound Field Simulation and Sound Field Rendering|Sound ﬁeld simulation is widely used because of the progress in computer environments and the spread of simple and easy simulation algorithms such as the ﬁnite-diﬀerence time domain (FDTD) method. In this paper, the theoretical basis of the FDTD method for sound ﬁeld analysis is ﬁrst described, then the implementation of the FDTD algorithm on a graphics processing unit is described. For the application of sound ﬁeld simulation to auralization, sound ﬁeld rendering is described. Sound ﬁeld rendering is a technique for auralization from three-dimensional numerical models constructed by a computer, with a similar concept to the graphics rendering technique. An example of sound ﬁeld rendering in collaboration with a head-mounted display is described.||2017|10.1587/ESSFR.10.3_206|T. Tsuchiya|0.25|0
2241|Medical Data and Mathematically Modeled Implicit Surface Real-Rime Visualization in Web Browsers|Raycasting can display volumetric medical data in fine details and reveal crucial inner imaging information, while implicit surface is able to effectively model complex objects with high flexibility, combining these two rendering modalities together will provide comprehensive information of the scene and has wide applications in surgical simulation, image-guided intervention, and medical training. However, medical data rendering is based on texture depth at every sampling point, while mathematically modeled implicit surfaces do not have geometric information in texture space. It is a challenging task to visualize both physical scalar data and virtual implicit surfaces simultaneously. To address this issue, in this paper, we present a new dual-casting ray-based double modality data rendering algorithm and web-based software platform to visualize volumetric medical data and implicit surface in the same browser. The algorithm runs on graphics processing unit and casts two virtual rays from camera to each pixel on the display panel, where one ray travels through the mathematically defined scene for implicit surface rendering and the other one passes the 3D texture space for volumetric data visualization. The proposed algorithm can detect voxel depth information and algebraic surface models along each casting ray and dynamically enhance the visualized dual-modality data with the improved lighting model and transparency adjustment function. Moreover, auxiliary innovative techniques are also presented to enhance the shading and rendering features of interest. Our software platform can seamlessly visualize volumetric medical data and implicit surfaces in the same web browser over Internet.|International Journal of Image and Graphics|2021|10.1142/S0219467822500279|Qi Zhang|0.25|0
2265|GPU-Accelerated Multiple Observer Siting|We present two fast parallel implementations of the FranklinVogt multiple observer siting algorithm, using either OpenMP or CUDA. In this problem, the objective is to site observers on a raster terrain such that the joint area visible by them is maximized. On a portion of terrain with 16,385×16,385 cells, assuming that observers can see out to a radius-of-interest of 100 cells, finding the approximate 15,000 observers that have the greatest coverage takes only 17s in CUDA. That is a factor of 70 speedup from the sequential version. The OpenMP version exhibits a factor of 17 speedup on a 16 core system. Applications for the multiple observer siting problem include radio transmission towers, environmental monitoring sites, and path planning for surveillance drones. The algorithm has four steps: finding the visibility indices of all points, selecting a candidate subset of potential top observers, finding each one’s viewshed, and greedily constructing the final solution. Introduction The purpose of multiple observer siting (Franklin, et al. 2002) is to place observers to cover the surface of a terrain or of targets above the terrain. It is useful in the placement of radio transmission towers, mobile ad hoc networks, and environmental monitoring sites. As described in (Magalhães, et al., 2010), an observer is a point in space from which we wish to see or communicate with other points, called targets. The usual notation for observer and target is O and T. Both the observer and the targets are considered to be at some given height above the ground (useful for modeling, e.g., towers). The base points of O and T are the points on the terrain directly below O and T, respectively. An observer covers (or views) a target that is within a given distance (called radius of interest), and that have a direct line of sight (LOS) from the observer. Figure 1 illustrates these concepts using a section of a terrain model. The viewshed V of an observer O is the set of all terrain points (base points) where targets positioned on these points are visible from O. The visible area of an observer is the size of its viewshed. The joint viewshed of a set of terrain points is the union of the individual viewsheds of observers sited above these points. Similarly, the joint visible area (or simply visible area) of a set of terrain points is the size of its joint viewshed. Given a terrain represented as a digital elevation matrix (DEM), there are many choices of what to optimize with the observer siting. For example, the objective may be to select a fixed number of observers whose visible area is maximized. Alternatively, the problem may be to select as few points as possible such that their visible area is at least a given threshold value. Other variations of the problem include: using observers that can cover targets with a quality or probability (Akbarzadeh, et al., 2013), adding constraints such as intervisibility, where observers are required to be visible from other observers, using mobile observers and targets (Efrat, et al., 2012), and having different costs for placing observers at different positions. The terrain may be either the traditional surface of the earth, or the tops of buildings in an urban terrain. The observers may be in remotely operated or autonomous airborne vehicles, whose operators desire to optimize the flight path. This may require repeatedly re-computing the siting problem with slightly varying parameters. A recent application requiring optimizing observer siting is Li-Fi, or light fidelity, which switches LEDs on and off at a high speed to effect high speed communication. Its main advantage is its immunity to electronic interference. Its visibility computation is complicated by its light’s ability to reflect from shiny surfaces. “Visibility” is also applicable to GHz radio signals that can be blocked by heavy objects such as reinforced concrete pillars. That is relevant to siting the thousands of beacons that may be required for indoor way-finding in large buildings such as airports. For example, Schipol airport’s way-finding system has 2000 beacons. Other notable related research includes the many lineof-sight issues in the Modeling and Simulation community discussed (with comparisons of various LOS algorithms) (LineOf-Sight Technical Group, 2004), the relation of visibility to topographic features (Lee, 1992), and the pioneering work of (Nagy, 1994). (Champion, et al., 2002) studied line-of-sight on natural terrain defined by an L1-spline. Multiple observer siting is a compute-intensive problem with considerable inherent parallelism. That, plus the recent Wenli Li and W. Randolph Franklin are with Rensselaer Polytechnic Institute, Troy, New York (mail@wrfranklin.org). Salles Viana Gomes de Magalhães is with Rensselaer Polytechnic Institute, Troy, New York, and the University Federal de Viçosa, Viçosa, MG, Brazil. Marcus V. A. Andrade is with Federal de Viçosa, Viçosa, MG, Brazil. Photogrammetric Engineering & Remote Sensing Vol. 83, No. 6, June 2017, pp. 439–446. 0099-1112/17/439–446 © 2017 American Society for Photogrammetry and Remote Sensing doi: 10.14358/PERS.83.6.439 Figure 1. Visibility of targets from observers (adapted from (Magalhães, S.V., et al., 2010)). PHOTOGRAMMETRIC ENGINEERING & REMOTE SENSING J une 2017 439 06-17 June Peer Reviewed Digital.indd 439 5/17/2017 3:10:04 PM availability of higher resolution geographical databases, makes it a good candidate for parallelization. In particular, the multiple observer siting algorithm of (Franklin and Vogt, 2004) and (Franklin and Vogt, 2006) can be optimized to reduce its time and space complexity. Parallelizing the algorithm would greatly increase its speed, so that the program would be very fast for small terrains and useful for large ones. Recently, the parallel computing capabilities of GPUs have attracted increasing interest of researchers. These devices contain thousands of processing units and, even though they were designed for computer graphics applications, they are fully programmable and optimized for SIMD vector operations. Using GPUs for scientific computing is called general-purpose computing on graphics processing units (GPGPU) (Luebke, et al., 2004). Although a CPU core is over ten times more powerful than a GPU core, a GPU may have over 2,000 cores, while a high-end CPU might have only 28 cores. Recent GPUs have up to a few hundred GB/s of memory bandwidth and a few TFLOPS of single-precision processing power, although achieving theoretical peak performance is nontrivial. The parallelization of line-of-sight and viewshed algorithms on terrains using GPGPU or multi-core CPUs is an active topic. (Strnad, 2011) parallelized the line-of-sight calculations between two sets of points (a source set and a destination set) on a GPU, and implemented it on a multi-core CPU for comparison. (Zhao, et al., 2013) parallelized the R3 algorithm (Franklin and Clark, 1994) to compute viewsheds on a GPU. The parallel algorithm combines coarse-scale and fine-scale domain decompositions to deal with memory limit and enhance memory access performance. (Osterman, 2012) parallelized the r.los module (R3 algorithm) of the open-source GRASS GIS on a GPU. (Osterman, et al., 2014) also parallelized the R2 algorithm (Franklin, and Clark, 1994). (Axell, and Mattias, 2015) parallelized and compared the R2 algorithm on a GPU and on a multi-core CPU. (Bravo, et al., 2015) parallelized the XDRAW algorithm (Franklin and Clark, 1994) to compute viewsheds on a multi-core CPU, after improving its IO efficiency and compatibility with SIMD instructions. (Ferreira, et al., 2014) and (Ferreira, et al., 2016) parallelized the sweepline algorithm of (Kreveld, 1996) to compute viewsheds on multi-core CPUs. In multiple observer siting, (Magalhães, et al., 2010) proposed a local search heuristic to increase the percentage of a terrain covered by a set of k observers. Given a set of candidate observers, each subset of k observers is a solution S, and each solution with one observer different from S is a neighbor of S. Starting from an initial solution, the heuristic repeatedly replaces the current solution with a better neighbor, until a local optimum is found. (Pena, et al., 2014) improved the performance of the heuristic by accelerating the overlay of viewsheds on a GPU using dynamic programming and a sparse-dense matrix multiplication algorithm. In this paper, we optimize and parallelize the multiple observer siting algoirthm of Franklin and Vogt for GPUs. We also implement it on multi-core CPUs for comparison. In the next section, the sequential multiple observer siting algorithm will be reviewed. Multiple Observer Siting (Franklin and Vogt, 2004) and (Franklin and Vogt, 2006) proposed an algorithm to select a set of observers to cover the surface of a terrain. Let the visibility index of a terrain point be the visible area of an observer sited on the point divided by the total number of terrain points within an observer radius of interest. The algorithm first computes an approximate visibility index for each terrain point, and selects a set of terrain points with high visibility indexes as candidate observer positions. The set of observers at the candidate positions are called tentative observers. In the next step, the viewshed of each tentative observer is computed and an iterative process is employed to greedily select observers from the set of tentative observers to cover the terrain surface. As an option, the algorithm can be configured to only select observers that are visible from one of the other selected observers (intervisibility). At the top level, this algorithm has four sequential steps: vix, findmax, viewshed, and site. vix, the first step, computes an approximate visibility index for each terrain point, which is normalized as an integer from 0 to 255 and stored in a byte. The parameters of vix include the number of rows or columns of the terrain (nrows), the radius||2017|10.14358/PERS.83.6.439|W. Randolph Franklin, M. Andrade, Wenli Li, S. V. G. Magalhães|0.25|0
2280|CUDA-enabled Programming for Accelerating Flood Simulation|Floods are the most frequent disasters, causing widespread damage resulting in loss of lives and properties. In this paper, we present Sinotech Engineering Consultants Hydrodynamic (SEC-HY21) simulation modeling to predicted floods and estimate their damage efficiently. However, SEC-HY21 still suffers from the slow simulation rate due to its data dependency structure which does not make the numerical model of SEC-HY21 parallelizable. In this research, a near real-time flood simulation has been reached by Compute Uniﬁed Device Architecture (CUDA) parallel implementation on NVIDIA Graphics Processing Unit (GPU) to accelerate the performance of the slowest module in the SEC-HY21 package, namely iFlux. The experimental results have shown that the CUDA-based parallel implementation has made the SEC-HY21 simulation modeling ∼ 14× faster than before.|International Conference on Graphics and Signal Processing|2021|10.1145/3474906.3474916|P. Chittem, Mohammad Alkhaleefah, Chiang-An Hsu, M. Huang, C. Ko, Yang-Lang Chang|0.25|0
2294|A New User Oriented Platform to Develop AI for the Estimation of Bio-Geophysical Parameters from EO Data|Machine learning can be considered as a very important area within artificial intelligence and it is characterized by algorithms and techniques that learn by examples. In the last decade, mainly due to the improvements obtained in the field of high performance computing, such as the enhanced exploitation of cloud technology and of graphics processing units (GPU), machine learning models have gained considerable progress as far as remote sensing and Earth Observation (EO) applications are concerned. However, the need of huge quantities of data necessary for the training phase, may be still a limiting factor especially in problems addressing the quantitative estimation of geo-physical parameters. In this paper, we report about the design and the development of a new platform capable of meeting the requirements of scientists and researchers who are attracted by the use of machine learning but meet difficulties in the generation of reliable data sets. The platforms relies on the implementation of radiative transfer models, plus a bunch of appropriate functionalities, in order that simulated data can be added to those available by ground-truth campaigns.|2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS|2021|10.1109/IGARSS47720.2021.9553913|Cesare Rossi, S. Loekken, G. Pace, Stefano Marra, D. D. Santis, Kevin Rossini, D. Latini, Leonardo De Laurentiis, G. Schiavon, F. Frate, Alessandro Marin|0.25|0
1937|Automatic Optimization for Large-Scale Real-Time Coastal Water Simulation|We introduce an automatic optimization approach for the simulation of large-scale coastal water. To solve the singular problem of water waves obtained with the traditional model, a hybrid deep-shallow-water model is estimated by using an automatic coupling algorithm. It can handle arbitrary water depth and different underwater terrain. As a certain feature of coastal terrain, coastline is detected with the collision detection technology. Then, unnecessary water grid cells are simplified by the automatic simplification algorithm according to the depth. Finally, the model is calculated on Central Processing Unit (CPU) and the simulation is implemented on Graphics Processing Unit (GPU). We show the effectiveness of our method with various results which achieve real-time rendering on consumer-level computer.||2016|10.1155/2016/9034649|Fengju Kang, Shun-Sheng Wang|0.2222222222222222|0
2032|Parallel Computation with Fast Algorithms for Micromagnetic Simulations on GPUs|"Author(s): Fu, Sidi | Advisor(s): Lomakin, Vitaliy | Abstract: Micromagnetics is a field of study considering the magnetization behavior in magnetic materials and devices accounting for a wide set of interactions and describing the magnetization phenomena from the atomistic scale to several hundreds of microns. Micromagnetic simulations are essential in understanding the behavior of many magnetic systems. Modeling complex structures can require a significant computational time and in some cases, the system complexity can make simulations prohibitively long or require a prohibitively large memory.In this thesis, we present a set of methods and their implementations that resulted in high-performance numerical micromagnetic tools for modeling highly complex magnetic materials and devices. The focus of the dissertation is on solving Landau-Lifshitz-Gilbert (LLG) equation efficiently, both with numerical methods and advanced hardware acceleration.To understand the numerical problem to be solved, the introduction Chapter 1 addresses the LLG equation and the governing interactions involved as well as numerical modeling basics on the Finite Difference Method (FDM) and the Finite Element Method (FEM). Chapter 1 also presents a versatile micromagnetic framework, referred to as FastMag, which implements some of these methods.Chapter 2 provides a detailed description of computing based on Graphics Processing Units (GPUs). The history of GPU programming model and the programming tips serve as the basis for understanding parallel computing on GPUs. It presents applications of GPUs on various platforms to demonstrate the current mainstream usage of GPUs and their promising future development direction. Chapter 2 also summarizes applications of GPUs in micromagnetics.Chapters 3 and 4 address two essential aspects of micromagnetic solvers: fast algorithms for computing the key interaction components and efficient time integration methods. Chapter 3 introduces a non-uniform Fourier transform (NUFFT) method, a scalar potential method, and sparse matrix-vector multiplication (SpMVM) algorithms implemented on GPUs to accelerate the magnetostatic and exchange interactions. Chapter 4 addresses basics of the time integration methods used in FastMag as well as a preconditioner to further accelerate the time integration process.Chapter 5 presents a numerical model for the current state-of-art magnetic recording system using advanced algorithms and GPU implementations described in Chapters 2-4."||2016|10.1109/bibm.2016.7822640|S. Fu|0.2222222222222222|0
2095|A Multi-GPU Program for Uncertainty-Aware Drainage Basin Delineation: Scalability benchmarking with country-wide data sets|Processing high-resolution digital elevation models (DEMs) can be tedious due to the large size of the data. In uncertainty-aware drainage basin delineation, we apply a Monte Carlo (MC) simulation that further increases the processing demand by two to three orders of magnitude. Utilizing graphics processing units (GPUs) can speed up the programs, but their on-chip random access memory (RAM) limits the size of the DEMs that can be processed efficiently on one GPU. Here, we present a parallel uncertainty-aware drainage basin delineation algorithm and a multinode GPU compute unified device architecture (CUDA) implementation along with scalability benchmarking. All of the computations are run on the GPUs, and the parallel processes communicate using a message-passing interface (MPI) via the host central processing units (CPUs). The implementation can utilize any number of nodes, with one or many GPUs per node. The performance and scalability of the program have been tested with a 10-m DEM covering 390,905 km2, i.e., the entire area of Finland. Performing the drainage basin delineation for the DEM with different numbers of GPUs shows a nearly linear strong scalability.|IEEE Geoscience and Remote Sensing Magazine|2016|10.1109/MGRS.2016.2561405|Ville Makinen, J. Westerholm, T. Sarjakoski, J. Oksanen|0.2222222222222222|0
1649|GPUAnimats—Simulating animats, an agent‐based, artificial life model on graphical processing units|A parallel implementation of an agent‐based, artificial life model for graphical processing units (GPUs) is presented. The model represents predator‐prey populations as a collection of agents (animats) who act independently and interact with their environment and each other. Individual animat interactions include avoidance, mating, and hunting which contribute to emergent behavior in the system as a whole such as herd formation and defensive spirals. Computational simulations of the model exploring the behavior of the model are limited in size due to the large populations that may result from certain environments and rule combinations. Parallel methods for computing the model on GPUs and a CUDA implementation are presented and are shown to provide a significant performance improvement over existing sequential implementations without simplifying the model behavior. The reduced simulation run‐time can allow both larger systems to be computed and a wider range of rule combinations that may produce larger populations to be investigated in a feasible amount of time.|Concurrency and Computation|2020|10.1002/cpe.5968|D. Playne, C. Scogings, D. Q. Quach|0.2|0
1754|Large-scale Cellular Automata on FPGAs|Cellular automata (CA) are discrete mathematical models discovered in the 1940s by John von Neumann and Stanislaw Ulam and have been used extensively in many scientific disciplines ever since. The present work evolved from a Field Programmable Gate Array– (FPGA) based design to simulate urban growth into a generic architecture that is automatically generated by a framework to efficiently compute complex cellular automata with large 29 × 29 neighborhoods in Cartesian or toroidal grids, with 16 or 256 states per cell. The new architecture and the framework are presented in detail, including results in terms of modeling capabilities and performance. Large neighborhoods greatly enhance CA modeling capabilities, such as the implementation of anisotropic rules. Performance-wise, the proposed architecture runs on a medium-size FPGA up to 51 times faster vs. a CPU running highly optimized C code. Compared to GPUs the speedup is harder to quantify, because CA results have been reported on GPU implementations with neighborhoods up to 11 × 11, in which case FPGA performance is roughly on par with GPU; however, based on published GPU trends, for 29 × 29 neighborhoods the proposed architecture is expected to have better performance vs. a GPU, at one-10th the energy requirements. The architecture and sample designs are open source available under the creative commons license.|ACM Transactions on Reconfigurable Technology and Systems|2020|10.1145/3423185|A. Dollas, Nikolaos Kyparissas|0.2|0
1757|Computing activities at the Spanish Tier-1 and Tier-2s for the ATLAS experiment towards the LHC Run3 and High-Luminosity periods|The ATLAS Spanish Tier-1 and Tier-2s have more than 15 years of experience in the deployment and development of LHC computing components and their successful operations. The sites are already actively participating in, and even coordinating, emerging R&D computing activities and developing new computing models needed for the Run3 and HighLuminosity LHC periods. In this contribution, we present details on the integration of new components, such as High Performance Computing resources to execute ATLAS simulation workflows. The development of new techniques to improve efficiency in a cost-effective way, such as storage and CPU federations is shown in this document. Improvements in data organization, management and access through storage consolidations (“data-lakes”), the use of data caches, and improving experiment data catalogs, like Event Index, are explained in this proceeding. The design and deployment of new analysis facilities using GPUs together with CPUs and techniques like Machine Learning will also be presented. Tier-1 and Tier-2 sites, are, and will be, contributing to significant R&D in computing, evaluating different models for improving performance of computing and data storage capacity in the High-Luminosity LHC era.|EPJ Web of Conferences|2020|10.1051/epjconf/202024507027|J. Peso, J. F. Molina, C. Acosta-Silva, J. L. Bahilo, A. Vedaee, E. F. Torregrosa, A. P. Pages, J. Cairols, Javier Sánchez Martínez, J. A. Pozo, Alvaro Fernandez Casani, S. G. Hoz, A. D. R. Montiel, C. G. Montoro|0.2|0
1761|GPU Acceleration of the Horizontal Diffusion Method in the Weather Research and Forecasting (WRF) Model|The Weather Research and Forecasting (WRF) is a next-generation mesoscale numerical weather prediction system. It is designed with a dual purpose, forecasting and research. The WRF software infrastructure consists of a number of components such as dynamic solvers and physical simulation modules. Dynamic solvers are intensive computational components of the WRF model. In this paper, the Horizontal Diffusion method, which is part of the ARW (Advanced Research WRF) dynamic solver, is accelerated using GPUs. The performance of the GPU-based method was compared to that one of a CPU-based single-threaded counterpart on a computational domain of 433x308 horizontal grid points with 35 vertical levels. Thus, the achieved speedup is 19x on a NVIDIA Tesla M2090, without considering data I/O.|2015 Asia-Pacific Conference on Computer Aided System Engineering|2015|10.1109/APCASE.2015.57|Ronald Gualan-Saavedra, Brett M. Bode, L. Solano-Quinde|0.2|0
1762|A novel integrated analysis-and-simulation approach for detail enhancement in FLIP fluid interaction|This paper advocates a novel integrated method to tightly couple simulation with analysis for the effective modeling and enhancement of scale-aware fluid details. It brings forth a suite of innovations in a unified framework, including depth-image-based space analysis for multi-scale detail detection, time-space analysis based on the logistic regression model that integrates both geometry and physics criteria, and depth-image-based sampling for quality-efficiency tradeoff. Our method contains an intertwined two-level processing architecture at its core. At the analysis level, we propose a rigorous time-space analysis model to pinpoint complex interacting regions, which can take into account multiple detail-relevant factors based on the depth-image sequence captured from FLIP-driven simulation sequence. At the simulation level, details are enhanced by animating extra diffuse materials, and augmenting the air-fluid mixing phenomenon. Directly benefitting from the flexibility of image-space-dominant processing, our unified framework can be entirely implemented on GPU, hence interactive performance could be guaranteed. Comprehensive experiments and evaluations on various diffuse phenomena (e.g., spray, foam, and bubble) have demonstrated its superiority in high-fidelity detail enhancement during fluid simulation and its interaction with surrounding environment for VR applications.|Virtual Reality Software and Technology|2015|10.1145/2821592.2821598|A. Hao, Shuai Li, Qing Xia, Hong Qin, Lipeng Yang|0.2|0
1771|A hybrid model for simulating grazing herds in real time|Computer simulations of animal groups are usually performed via individual‐based modelling, where simulated animals are designed on the level of individuals. With this approach, developers are able to capture behavioural nuances of real animals. However, modelling each individual as its own entity has the downside of having a high computational cost, meaning that individual‐based models are usually not suitable for real‐time simulations of very large groups. A common alternative approach is flow‐based modelling, where the dynamics of animal congregations are designed on the group level. This enables researchers to create real‐time simulations of massive phenomena at the cost of biological authenticity. A novel approach called hybrid modelling tries to mix the best of both worlds—precision of individual‐based models and speed of flow‐based ones. An unknown surrounding hybrid model is the question of their biological authenticity and relevance. In this study, we develop a hybrid model for the simulation of herds of grazing sheep. Through Bayesian data analysis, we show that such an approach can encompass several aspects of real‐world sheep behaviour. Our hybrid model is also extremely efficient, capable of simulating herds of more than 1,000 individuals in real time without resorting to graphics processing unit execution.|Comput. Animat. Virtual Worlds|2020|10.1002/cav.1914|I. L. Bajec, J. Demšar, W. Blewitt|0.2|0
1772|Real-Time Simulation of Contrast Media Diffusion Based on GPU|In this paper, a new contrast media diffusion simulation algorithm is proposed for minimally invasive vascular interventions. On the basis of smoothed particle hydrodynamics (SPH), the algorithm could be mainly divided into two parts: fluid-fluid interaction and fluid-solid interaction. In the fluid-fluid interaction, an adsorption model of iodine particles is built, and the adjacent blood flow particles around that could be got quickly by Eulerian grid and Space Sparse Hash. In the fluid-solid interaction, coulomb friction model is used to realize the frictional contact between the fluid particles and the inner walls of vessel. For improving the computational speed, the algorithm uses multi-thread parallel technology to address the complex diffusion problem of contrast media on computer unified device architecture (CUDA). The experimental results show that this algorithm is feasible, and could greatly enhance the development effect of blood vessels with real time performance, especially the capillaries.|International Conference on Virtual Reality and Visualization|2015|10.1109/ICVRV.2015.25|Wen Tang, P. Tang, Zhifeng Xie, Dongjin Huang, Yin Wang, Youdong Ding|0.2|0
1879|Hybrid OpenMP-CUDA parallel implementation of a deterministic solver for ultrashort DG-MOSFETs|The simulation of ultrashort two-dimensional double gate metal-oxide semiconductor field-effect transistors and similar semiconductor devices through a deterministic mesoscopic, hence accurate, model can be very useful for the industry: It can provide reference results for macroscopic solvers and properly describe weakly charged zones of the device. For the scope of this work, we use a Boltzmann–Schrödinger–Poisson model. Its drawback is being particularly costly from the computational point of view, and a purely sequential code may take weeks to simulate high voltages. In this article, we develop a hybrid parallel solver for a graphics processing unit (GPU)-based platform. In order to accelerate the simulations, the Boltzmann transport equations are solved on GPU using the CUDA programing model, while the Schrödinger–Poisson block is performed on multicore CPUs using OpenMP. We have adapted the costliest computing phases to the GPU in an efficient manner, achieving high performance and drastically reducing the simulation time. We give details about the parallel-design strategy and show the performance results.|The international journal of high performance computing applications|2020|10.1177/1094342019879985|J. M. Mantas, F. Vecil|0.2|0
1901|GPU Usage for Parallel Functions and Contacts in Modelica|This thesis investigates two ways of incorporating GPUs in Modelica. The first \nby automatically generating GPU code for Modelica functions, and the second \nby using GPU accelerated external code for a contact handling package. \nAutomatic parallelization of functions is desired, as it can potentially accelerate \nlarge simulations significantly. Special patterns of nested for-loops \nin Modelica code are recognized and translated into CUDA kernel functions. \nInline integration allows a broader spectrum of models to take advantage of \nthe parallelization, by reducing CPU-GPU transfers. The prototype has been \ntested and achieved a speed-up factor of up to five compared to the CPU. \nThe contact handling package is capable of handling both complex contact \nbehavior between arbitrarily shaped bodies and large DEM-like simulations, \nsomething which Modelica is currently lacking. Attempts to accelerate the \npackage with GPUs were made, with partial success for the broad phase. The \npackage uses Morton encoding for the broad phase, and the narrow phase is \nbased on CSG intersection with BSP trees. Contact response is calculated \nusing a volume dependent method, taking friction, damping and multiple contact \npoints into account. The capability of the package was demonstrated by \nthe fact that both complex contact behavior such as the inversion of the Tippe \nTop toy and tens of thousands of colliding spheres could be simulated.||2015|10.3384/ecp15118235|Vilhelm Roxling, Axel Goteman|0.2|0
1908|High-fidelity simulation of collective effects in electron beams using an innovative parallel method|Among the most challenging and heretofore unsolved problems in accelerator physics is accurate simulation of the collective effects in electron beams. Electron beam dynamics is crucial in understanding and the design of: (i) high-brightness synchrotron light sources --- powerful tools for cutting-edge research in physics, biology, medicine and other fields, and (ii) electron-ion particle colliders, which probe the nature of matter at unprecedented depths. Serial, or even naively parallel, implementation of the electron beam's self-interaction is prohibitively costly in terms of efficiency and memory requirements, necessitating simulation times on the order of months or years. In this paper, we present an innovative, high-performance, high-fidelity, scalable model for simulation of collective effects in electron beams using state-of-the-art multicore systems (GPUs, multicore CPUs, and hybrid CPU-GPU platform). Our parallel simulation algorithm implemented on different multicore systems outperforms the sequential simulation, achieving a performance gain of up to 7.7X and over 50X on the Intel Xeon E5630 CPU and GTX 480 GPU, respectively. It scales nearly linearly with the cluster size. Our simulation code is the first scalable parallel implementation on GPUs, multicore CPUs, and on hybrid CPU-GPU platform for simulating the collective dynamical effects of electron beams in accelerator physics.|Summer Simulation Multiconference|2015|10.1007/978-3-319-19048-8_6|M. Zubair, B. Terzić, D. Ranjan, K. Arumugam, A. Godunov|0.2|0
1959|Deformation of 3D Object of Human Body Internal Organs Using Finite Element Method Approach Accelerated by GPU|Few years ago, there are some research or publication about 3D simulation especially virtual surgery that still continuous growing, that is 3D deformable object. The 3D deformable object resembles or mimic some object. The purpose of the research is to continue or add previous research especially in term of deformable 3D object by using OpenGL and as well as startup especially in Indonesia in term of soft body simulation. To make object deform, one of the numeric approach methods would be used i.e. Finite Element Method. this approach is quite common or used by another researchers because it is stable, detail, but also quite heavy in modelling some objects. This approach would implement by using OpenGL GPU-based with eight kernels or functions. Result of this research show that this simulation of 3D of human body internal organs object produces the good FPS of simulation with 3.07 times faster rather than not using GPU.||2020|10.2991/aer.k.201124.003|W. Martiningsih, C. Wicaksana|0.2|0
2017|Efficient Executions of Community Earth System Model onto Accelerators Using GPUs|As the climate models become more and more complicated, we are facing an enormous challenge to run these models effectively. In this paper, we discuss the acceleration of the Community Earth System Model (CESM), which is a large-scaled model with MPI parallel, but still with low execution efficiency. We have conducted an efficient study on porting the Community Land Model (CLM) which an active component within CESM onto Graphics Processing Unit (GPU), and we focus on one major routine that occupies the most execution time, namely CanopyFluxes. To expedite computation, we have put tremendous effort into developing accelerated the CESM model using GPU to parallel computing. Specifically, we conducted CUDA kernel command to optimize some matrix computations in CanopyFluxes. For further optimization, GPU caches and compiler options are used. Running on a five computing nodes cluster with five GPUs, the CanopyFluxes routine achieves a speedup of 4.21x. While in the simulation on Tianhe-2 with NVIDIA Tesla K80 GPUs, the speedup of CanopyFluxes routine raises to 14.92x.|International Conference on Robotics and Artificial Intelligence|2020|10.1145/3449301.3449334|Bin Mu, Xiaodan Luo, Shijin Yuan, Cheng Wang|0.2|0
2146|Implementation and Optimization of a 1D2V PIC Method for Nonlinear Kinetic Models on GPUs|This paper considers the parallel numerical simulation of the time evolution of galaxies and globular clusters on GPUs. The model used is the Einstein–Vlasov system, which is designed, in particular, to study the formation of black holes and spacetime singularities in a general relativistic framework.First, a reference implementation is derived using NVIDIA CUDA as programming model, which is then optimized in several steps. Bottlenecks are identified by profiling, and different approaches, namely particle sort, improved treatment of atomic operations, and kernel fusion are investigated to overcome these bottlenecks. Each optimized variant is evaluated in relation to the other variants using detailed runtime experiments and profiling results. Using in the order of 107 to 108 particles, speedups between 1.84 and 2.38 w.r.t. the reference implementation have been observed.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2020|10.1109/PDP50117.2020.00012|Tim Werner, P. Raithel, Matthias Korch|0.2|0
2159|Automatic GPU Code Generation of Modelica Functions|Modelica users can and want to build more realistic and complex models. This typically means slower simulations. In the past, the speed of single CPUs has increased significantly to partly compensate, but more recently, there has been a shift to multi-core architectures. This is taken to the extreme in Graphics Processing Units (GPUs). \nThis paper discusses code generation for GPU cores. This is important when the model has regular structure, for example, discretization of PDEs. The behavior of each cell can then be partly described by a function call. The evaluation of such calls can then be made in parallel on the GPU cores. The same function is thus executed on every GPU core, but operates on different data; the data of its cell. \nOur GPU code generator automatically generates code for Modelica functions, i.e. no additional language constructs are needed. The function is just annotated as suitable for execution on a GPU.||2015|10.3384/ECP15118235|H. Olsson, Alexander Pollok, Vilhelm Roxling, H. Elmqvist, Axel Goteman, D. Zimmer|0.2|0
2166|A MUSTA-FORCE Algorithm for Solving Partial Differential Equations of Relativistic Hydrodynamics|Abstract Understanding event-by-event correlations and fluctuations is crucial for the comprehension of the dynamics of heavy ion collisions. Relativistic hydrodynamics is an elegant tool for modelling these phenomena; however, such simulations are time-consuming, and conventional CPU calculations are not suitable for event-by-event calculations. This work presents a feasibility study of a new hydrodynamic code that employs graphics processing units together with a general MUSTA-FORCE algorithm (Multi-Stage Riemann Algorithm – First-Order Centred Scheme) to deliver a high-performance yet universal tool for event-by-event hydrodynamic simulations. We also investigate the performance of selected slope limiters that reduce the amount of numeric oscillations and diffusion in the presence of strong discontinuities and shock waves. The numerical results are compared to the exact solutions to assess the code’s accuracy.||2015|10.1515/ijnsns-2016-0131|P. Aszklar, M. Slodkowski, J. Sikorski, D. Kikoła, J. Porter-Sobieraj|0.2|0
2248|Strategies for Efficient Executions of Irregular Message-Driven Parallel Applications on GPU Systems|Message-driven executions with over-decomposition of tasks constitute an important model for parallel programming and have been demonstrated for irregular applications. Supporting efficient execution of such message-driven irregular applications on GPU systems presents a number of challenges related to irregular data accesses and computations. In this work, we have developed strategies including coalescing irregular data accesses and combining with data reuse, and adaptive methods for hybrid executions to minimize idling. We have integrated these runtime strategies into our {\em G-Charm} framework for efficient execution of message-driven parallel applications on hybrid GPU systems. We demonstrate our strategies for irregular applications with an N-Body simulations and a molecular dynamics application and show that our dynamic strategies result in 8-38\% reduction in execution times for these irregular applications over the corresponding static strategies that are amenable for regular applications.|arXiv.org|2020|10.1007/978-3-030-50417-5_50|Vasudevan Rengasamy, Sathish S. Vadhiyar|0.2|0
2260|The Immersed Boundary-Lattice Boltzmann Method Parallel Model for Fluid-Structure Interaction on Heterogeneous Platforms|Immersed boundary-lattice Boltzmann method (IB-LBM) has become a popular method for studying fluid-structure interaction (FSI) problems. However, the performance issues of the IB-LBM have to be considered when simulating the practical problems. The Graphics Processing Units (GPUs) from NVIDIA offer a possible solution for the parallel computing, while the CPU is a multicore processor that can also improve the parallel performance. This paper proposes a parallel algorithm for IB-LBM on a CPU-GPU heterogeneous platform, in which the CPU not only controls the launch of the kernel function but also performs calculations. According to the relatively local calculation characteristics of IB-LBM and the features of the heterogeneous platform, the flow field is divided into two parts: GPU computing domain and CPU computing domain. CUDA and OpenMP are used for parallel computing on the two computing domains, respectively. Since the calculation time is less than the data transmission time, a buffer is set at the junction of two computational domains. The size of the buffer determines the number of the evolution of the flow field before the data exchange. Therefore, the number of communications can be reduced by increasing buffer size. The performance of the method was investigated and analyzed using the traditional metric MFLUPS. The new algorithm is applied to the computational simulation of red blood cells (RBCs) in Poiseuille flow and through a microchannel.||2020|10.1155/2020/3913968|H. Liu, Liping Zhou, Zhixiang Liu, Dongmei Huang|0.2|0
2299|Performance Improvement of Large-Scale Interconnection Network Simulator by Using GPU|In a parallel computing system that uses a lot of computing nodes, the performance of interconnection network is important since communication among computing nodes has a large effect on the performance of whole systems. Therefore performance evaluation of interconnection network prior to the implementation of the actual interconnection network is necessary. In order to evaluate the performance of interconnection network, a performance simulation by software is useful in terms of easily changeable conditions and at low cost. However, a software simulation of large-scale interconnection network requires enormous computation and takes a lot of time. This paper introduces a simulator which can simulate large-scale interconnection network, and we accelerate the simulator using GPU. To implement the parallelized simulator, this paper adopts cellular automata in modeling the behavior of interconnection networks and a table-based simulation method. The simulation principle naturally exploits parallelism and well matches the GPGPU acceleration. Furthermore, this paper describes some tuning techniques to exploit the maximum performance of GPU devices. For example, the tuning techniques include the modification of data structure for coalesced accesses and the reduction in the number of global memory access. The parallelized simulator that is applied all tuning techniques achieved the speed up ratio of maximum 37.8 times compared to a sequential simulator, and simulation speed achieved 7.0M[cycles*nodes/sec].|International Symposium on Computing and Networking - Across Practical Development and Theoretical Research|2015|10.1109/CANDAR.2015.98|T. Yokota, Yuki Suzuki, Takeshi Ohkawa, K. Ootsu|0.2|0
1645|A novel automated magnetic resonance image segmentation approach based on elliptical gamma mixture model for breast lumps detection|This article introduces a novel semisupervised automated segmentation approach for breast magnetic resonance (MR) image on multicore CPU‐GPU systems. The basic idea of the proposed method is clustering‐based semisupervised classifier devised by elliptical gamma mixture model (EGMM). Parameters of EGMM are identified by the iterative log‐expectation maximization (EM) algorithm. The suggested classifier labels the groups of voxels in an input image first and then classifies the image slices using the EGMM. Two different implementations of the proposed algorithm have been developed based on two different types of high‐performance computing architectures such as graphics processing units (GPUs) and multicore processors. To realize the real‐time segmentation performance of our algorithm with two distinctive architecture, we have tested a set of breast MR images collected from MedPix. Comparison between two architectures in terms of segmentation performance and computational cost is assessed by the analysis of simulation and experimental results.|International journal of imaging systems and technology (Print)|2019|10.1002/ima.22341|Swarup Kr Ghosh, Anupam Ghosh, Biswajit Biswas|0.16666666666666666|0
1695|Impossible Folding Font|We present a series of seemingly impossible objects made by cutting and folding a square piece of paper, in the style of hypercards. Each object is in the shape of a letter or numeral, thereby making an “impossible folding font”. Introduction Humans are fascinated by impossible objects — physical objects that obviously exist yet appear to be impossible. For extensive examples, see Sugihara’s award-winning collection [12]. The International Puzzle Party, an annual forum for serious puzzle collectors, has a session devoted to impossible objects, which can only be attended by those who have already created something “impossible”. Figure 1: How to make a hypercard. Figure 2: Magical hypercards. One elegant form of the impossible object is the hypercard [1, 5, 4, 9, 10, 11], as popularized by Martin Gardner in 1978 [6]. Hypercards are typically made from one complete playing card or index card, and yet seem to be impossible. Figure 1 shows an example: (1) cut along the black solid lines, (2) mountain fold along the red dash-dot line, and (3) valley fold along the blue dashed line. When we fold both crease lines by 90◦, we obtain the hypercard, which seems to be impossible at first glance. Using this type of trick, many different impossible cards, such as the one in Figure 2, have been designed and sold in magic shops [4]. Gardner describes hypercards as a kind of minimalist sculpture [6]. Why do these objects seem to be impossible? One reason is that the sheet has been twisted, and twisting is necessary to form the 3D shape, while the resulting folded state does not seem to be twisted. A key feature Figure 3: “IMPOSSIBLE” written in our impossible folding font. Figure 4: Folding of “W”, generated by Origami Simulator. The folding percentages are 0%, 20%, 40%, 60%, 80%, and 100%. Magenta and gray represent the two sides of the paper. of the folded state is a vertical flap extending from a horizontal plane, with the property that the vertical flap overlaps other material when rotated in either direction onto the horizontal plane, thus seeming to be impossible from a single sheet of material. We call such a folded state a one-flap impossible folding. In this paper, we design several new one-flap impossible foldings that together form an impossible folding font.1 From the viewpoint of origami design, the symbols of the impossible origami font require several nontrivial origami/paper craft techniques. From the viewpoint of computational origami [3], this framework can be seen as a natural extension from 2-dimensional origami and a natural intermediate model to general 3-dimensional origami. In our designs, we use a square sheet of paper with crease lines on horizontal, vertical, and 45◦ diagonal lines. Each folding angle is orthogonal, that is, ±90◦ or ±180◦. Figure 3 shows some text written in our font. Can you figure out how each letter can be made from a square of paper? For example, “W” can be folded as shown in Figure 4, which is one of the simplest patterns. Notation and System In each hypercard in our impossible folding font, we use a square sheet of paper and draw our cuts and creases using a 16 × 16 grid. For each crease line, as in classic origami diagrams, we assign mountain or valley to describe the direction of folding. However, in our context of 3D folding, to fix the folded state, we also have to assign a folding angle to each crease line. To represent such a crease pattern with mountain–valley assignment and folding angles, as well as the pattern of cuts, we follow the representation used by Origami Simulator developed by Amanda Ghassaei [7, 8]. This simulator not only adopts the FOLD file format [2], but also allows input to be in a particular form of SVG2. We use the latter format to draw our examples, which we briefly explain. In each cut/crease pattern, a red line indicates mountain fold, and a blue line indicates valley fold. A black line indicates the boundary of the paper, and a green line indicates a cut or slit. In the SVG format, the folding angle of each crease is set by its opacity. In particular, 1.0 (fully opaque) indicates 180◦ (fully folded), while 0.5 (half opaque) indicates 90◦ (which makes red and blue appear pink and light blue respectively). Some of the foldings can be decomposed into a sequence of folding steps that avoid self-intersection, but we 1You can play with the font in a web app: http://erikdemaine.org/fonts/impossible/ 2SVG stands for Scalable Vector Graphics, which is a data format for vector graphic data. do not know whether this is universally the case. In the real design process, we first design each symbol by cutting and folding a physical sheet of paper. Then we trace the pattern using a vector drawing program (Inkscape) and save it in SVG format. Origami Simulator then allows us to visualize the 3D folded letters, as well as animate a continuous folding process (as in Figure 4). These renderings are great in an interactive setting, but for easier-to-see still images, we drew the 3D folded states (again in Inkscape) using oblique projection (with out-of-plane lines represented by 15◦ rotation at 85% scale and gradient shadows). Yellow and orange represent the two sides of the paper. Impossible Folding Fonts Here we show our impossible folding typeface in two fonts: the 3D folded state and the cut/crease pattern (as described above). Each folded state is a puzzle: how can it be cut/folded from a square of paper? Each cut/crease patterns is also a (straightforward) puzzle: what letter/numeral does it fold into? To encourage the reader to play with these puzzles, we present each font separately. Figures 5, 6, and 7 show the folded font (drawn at a uniform scale), while Figures 8, 9, and 10 show the cut/crease patterns (drawn at a different uniform scale). Figure 11 shows real-world foldings of (an earlier version of) the uppercase letters. Acknowledgements T. Taniguchi and R. Uehara are partially supported by JSPS KAKENHI Grant Number JP17H06287 and 18H04091. We thank Karen Noiva for input on the figures. References [1] Jack Botermans. Paper Capers. Henry Holt, 1986. [2] Erik D. Demaine, Jason Ku, and Robert J. Lang. FOLD file format for origami models, crease patterns, etc. https://github.com/edemaine/fold. [3] Erik D. Demaine and Joseph O’Rourke. Geometric Folding Algorithms: Linkages, Origami, Polyhedra. Cambridge University Press, 2007. [4] Tom Frame. The Hypercard Project. Booklet, 2006. [5] Karl Fulves. Hypercard. In The Chronicles. 1978. Volume 1, numbers 1 and 12. [6] Martin Gardner. Minimal sculpture. In Hypercards and More Mathematical Recreations from Scientific American Magazine, chapter 8. W. H. Freeman and Company, New York, 1992. Original article in Scientific American, 1978. [7] Amanda Ghassaei. Origami Simulator. http://apps.amandaghassaei.com/OrigamiSimulator/. [8] Amanda Ghassaei, Erik D. Demaine, and Neil Gershenfeld. Fast, interactive origami simulation using GPU computation. In Proceedings of the 7th International Meeting on Origami in Science, Mathematics and Education, Oxford, England, September 2018. [9] Ben Harris. The Hypercard Experience. Ben Harris Magic, Queensland, Australia, 1965. [10] Gordon Jepperson. The Ultracard Principle. Privately issued magic trick, 1985. [11] Robert E. Neale. This is Not a Book, pages 227–271. Hermetic Press, Inc., Seattle, 2008. [12] Kokichi Sugihara. http://www.isc.meiji.ac.jp/~kokichis/Welcomee.html. Figure 5: Uppercase impossible folding font, folded into 3D. Figure 6: Lowercase impossible folding font, folded into 3D. Figure 7: Numeral impossible folding font, folded into 3D.||2019|10.2307/j.ctvd58spj.7|M. Demaine, E. Demaine, Ryuhei Uehara, T. Taniguchi|0.16666666666666666|0
1746|OpenACC Parallelization of Stochastic Simulations on GPUs|We present an OpenACC-based parallelization implementation of stochastic algorithms for simulating biochemical reaction networks on modern GPUs (graphics processing units). To investigate the effectiveness of using OpenACC for leveraging the massive hardware parallelism of the GPU architecture, we carefully apply OpenACC’s language constructs and mechanisms to implementing a parallel version of stochastic simulation algorithms on the GPU. Using our OpenACC implementation in comparison to both the NVidia CUDA and the CPU-based implementations, we report our initial experiences on OpenACC’s performance and programming productivity in the context of GPU-accelerated scientific computing. key words: GPU computing, OpenACC, parallel programming, stochastic simulation|IEICE Trans. Inf. Syst.|2019|10.1587/TRANSINF.2019EDL8032|Pilsung Kang|0.16666666666666666|0
1819|Constructive Modeling of Conservative DBMS|Commercial OLAP-systems are quite expensive and therefore inaccessible to organizations with limited financial capabilities. Analytical processing of significant amounts of data in these organizations can be carried out using open source software systems on low price cluster platform. Previously made Clusterix-like conservative-type DBMSs (with occasional data updates) using a regular query processing plan were not sufficiently effective. Therefore, research on such systems has been developed with a focus on the full load of processor cores and the use of dynamic segmentation of intermediate and temporary relations on the GPU (Clusterix-N, N - from New) up to the development of a system comparable in efficiency to the promising open Spark system. The development methodology was based on the constructive system modeling methodology. According to this methodology, the simulation of the synthesis process of the considered DBMS was carried out iteratively. Each iteration involved a complete software development of the corresponding state of the model and analysis of the results of testing on the GPU cluster platform. The required simulation quality was obtained at the fourth iteration.|2019 International Russian Automation Conference (RusAutoCon)|2019|10.1109/RUSAUTOCON.2019.8867678|R. K. Klassen, V. Raikhlin|0.16666666666666666|0
1827|Performance optimization of air shower simulations with CORSIKA|With the steady increase in accuracy, size and complexity of astroparticle physics experiments the need for an extensive amount of high precision Monte Carlo simulations is rapidly growing. Contrary to the increasing demand is the demise of ``Moore's law'' which leads to situations where the system structure of high-performance computing is fundamentally changing and large amounts of money are invested in new infrastructure. In the field of astroparticle physics CORSIKA 7 is currently the most commonly used simulation program, therefore the presented work is focused on this software. All methods provided are also currently transferred to the new CORSIKA 8 framework which will replace CORSIKA 7 in the near future. Due to various constraints on hardware, geometry and physics no experiment is able to observe the full air shower particle cascade developing in the atmosphere. The removal of the non-visible phase space of the cascade at an early stage of the simulation has immense potential to reduce the expense of calculations without changing the results of the simulation for the experiment. Fast machine learning models allow the identification and removal of particles from those regions to speed up the simulations. This works for example for neutrinos by orders of magnitude. First results are shown to demonstrate this technique. \n \nFurthermore, when showers are simulated with the IACT configuration around 75\% of the time is spent on the Cherenkov photon creation and propagation. We also show results from parallelizing this part of the simulation on GPUs and CPUs with OpenCL.|International Conference on Rebooting Computing|2019|10.22323/1.358.0181|D. Baack|0.16666666666666666|0
1870|Real-time construction on the GPU of adaptive terrestrial relief model based on the spheroid|The paper considers a task of real-time modelling of Earth relief based on detailed global height maps specified relative to the rotational ellipsoid (spheroid) WGS-84. The technology of adaptive tessellation of triangular patches on the GPU is proposed, which provides real-time construction of complex polygonal models of the Earth's relief. The technology includes the stage of dividing the visible ellipsoid into coarse patches (low level of detail) and the stage of their tessellation into triangles of the relief model, executed in parallel and independently on the GPU cores. The paper proposes new, distributed methods and algorithms to extract the patches needed for visualization of the current frame, to increase their level of detail in accordance with the height map and screen resolution, and to transform to relief polygons. The novelty of the work is that the tessellation of triangular patches of an ellipsoid is based on the original scheme, which allows relief areas that require high level detail to be effectively localized. This significantly reduces the time of relief model construction and improves the quality of the created images. The developed technology, methods and algorithms were implemented in program modules and tested in the visualization system of the Earth’s virtual surface. Modelling and visualization of the Earth's relief was carried out with a detailed texture of the underlying Earth's surface and the calculation of illumination taking into account the atmosphere. The approbation was carried out in a virtual space scene comprising a detailed polygonal model of the International Space Station (ISS). The obtained results confirmed the adequacy of the proposed solution to the task and its applicability for building of space video simulators, virtual environment systems, virtual laboratories, etc.||2019|10.30987/graphicon-2019-2-4-6|M. Mikhaylyuk, A. Maltsev, P. Timokhin|0.16666666666666666|0
1887|Compact GPU-based Visualization Method for High-resolution ResultingData of Unstable Oil Displacement Simulation|In the paper the task of real-time synthesis of quality images of resulting data obtained in simulation of unstable oil displacement from porous media is considered. A new, GPU-based method to construct and visualize on UltraHD screens a polygonal model of the isosurface of the saturation of displacing liquid was proposed. The method is based on distributing and parallelizing of «marching cubes» threads between GPU cores by means of programmable tessellation. As initial graphic primitives, quadrangular parametric patches are used, the processing of which on the GPU is high-performance and has low video memory overhead. The proposed method was implemented in visualization software and successfully tested. The proposed solution can be used in researches in oil and gas industry as well as in virtual environment systems, virtual laboratories, scientific and educational applications, etc.|GraphiCon'2019 Proceedings. Volume 2|2019|10.30987/graphicon-2019-2-4-6|Михаил Михайлюк, P. Timokhin, Петр Юрьевич Тимохин, M. Mikhaylyuk|0.16666666666666666|0
1966|Comparison of Road Traffic Simulation Speed on CPU and GPU|In this paper, we describe a fair comparison of the performance of a microscopic road traffic simulation performed on a GPU and on a CPU. The aim of our work is to determine the speedup, which can be achieved if the GPU is used for the same simulation instead of the (multi-core) CPU. A microscopic road traffic simulator capable of running on both platforms was created for this purpose with the aim to make the GPU-based and the CPU-based simulations as similar as possible. The performances of both the GPU-based and the CPU-based simulations were tested using two different road traffic models (a car-following model and a cellular automaton model), four road traffic networks (regular square grids of crossroads) of different sizes, and three different hardware configurations. The maximal achieved speedup using the GPU instead of the multi-core CPU for the cellular automaton model was 12.4. For the car-following model, the maximal achieved speedup was 10.7.|IEEE International Symposium on Distributed Simulation and Real-Time Applications|2019|10.1109/DS-RT47707.2019.8958702|Daniel Rajf, T. Potuzak|0.16666666666666666|0
2033|Hardware-Assisted Cross-Generation Prediction of GPUs Under Design|This paper introduces a predictive modeling framework for GPU performance. The key innovation underlying this approach is that performance statistics collected from representative workloads running on current generation GPUs can effectively predict the performance of next-generation GPUs. This is useful when simulators are available for the next-generation device, but simulation times are exorbitant, rendering early design space exploration of microarchitectural parameters and other features infeasible. When predicting performance across three Intel GPU generations (Haswell, Broadwell, Skylake), our models achieved impressively low out-of-sample-errors ranging from 7.45% to 8.91%, while running 29 481 to 44 214 times faster than cycle-accurate simulations. A detailed ranking of the most impactful features selected for these models provides an insight as to which microarchitectural subsystems have the greatest impact on performance from one generation to the next.|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|2019|10.1109/TCAD.2018.2834398|M. Kishinevsky, P. Brisk, Emily J. Shriver, Kenneth O&#x2019;Neal|0.16666666666666666|0
2068|Accuracy and Performance of Functional Parameter Estimation Using a Novel Numerical Optimization Approach for GPU-Based Kinetic Compartmental Modeling|Quantitative kinetic parameters derived from dynamic contrast-enhanced (DCE) data are dependent on signal measurement quality and choice of pharmacokinetic model. However, the fundamental optimization analysis method is equally important and its impact on pharmacokinetic parameters has been mostly overlooked. We examine the effects of those choices on accuracy and performance of parameter estimation using both computer processing unit and graphical processing unit (GPU) numerical optimization implementations and evaluate the improvements offered by a novel optimization approach. A test framework was developed where experimentally derived population-average arterial input function and randomly sampled parameter sets {Ktrans, Kep, Vb, τ} were used to generate known tissue curves. Five numerical optimization algorithms were evaluated: sequential quadratic programming, downhill simplex (Nelder–Mead), pattern search, simulated annealing, and differential evolution. This was combined with various objective function implementation details: delay approximation, discretization and varying sampling rates. Then, impact of noise and CPU/GPU implementation was tested for speed and accuracy. Finally, the optimal method was compared to conventional implementation as applied to clinical DCE computed tomography. Nelder–Mead, differential evolution and sequential quadratic programming produced good results on clean and noisy input data outperforming simulated annealing and pattern search in terms of speed and accuracy in the respective order of 10−8%, 10−7%, and ×10−6%). A novel approach for DCE numerical optimization (infinite impulse response with fractional delay approximation) was implemented on GPU for speed increase of at least 2 orders of magnitude. Applied to clinical data, the magnitude of overall parameter error was <10%.|Tomography|2019|10.18383/j.tom.2018.00048|B. Driscoll, Igor Svistoun, C. Coolens|0.16666666666666666|0
2093|Enhancing Monte Carlo proxy applications on GPUs|"In Monte Carlo neutron transport simulations, a computational routine commonly known as the ""cross-section lookup"" has been identified as being the most computationally expensive part of these applications. A tool which is commonly used as a proxy application for these routines, named ""XSBench"", was created to simulate popular algorithms used in these routines on CPUs. Currently, however, as GPU-based HPC resources have become more widely available, there has been significant interest and efforts invested in moving these traditionally CPU-based simulations to GPUs. Unfortunately, the algorithms commonly used in the cross-section lookup routine were originally devised and developed for CPU-based platforms, and have seen limited study on GPUs to date. Additionally, platforms such as XSBench implement approximations which may have a negligible effect on CPUs, but may be quite impactful to performance on GPUs given the more resource-limited nature of the latter. As a result, we have created VEXS, a new tool for modeling the cross-section lookup routine which removes or at least reduces the approximations made by XSBench in order to provide a more realistic prediction of algorithm performance on GPUs. In this paper, we detail our efforts to remove and reduce these approximations, show the resulting improvement in performance prediction in comparison to a reference production code, Shift, and provide some basic profiling analysis of the resulting application."|International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems|2019|10.1109/PMBS49563.2019.00009|Seyong Lee, F. Shriver, S. Hamilton, Justin Watson, J. Vetter|0.16666666666666666|0
2268|Field Programmable Gate Array Technology as an Enabling Tool Towards Large-Neighborhood Cellular Automata on Cells with Many States|Cellular Automata (CA) have been used for many decades to simulate physical processes. From the $3 \times 3$ and $5 \times 5$ neighborhoods of the 1950’s, and typically on binary images, as recently as the mid-2010’s the neighborhoods went up to $15 \times 15$ on images with a few states. Field Programmable Gate Array (FPGA) technology, already applicable to CA simulation since the early 1990’s, has reached such maturity levels that a small device can simulate large-neighborhood CA. In this work we present an architecture which we have fully implemented, that can simulate CA with up to $29 \times 29$ neighborhoods on 256-state cells for Full High Definition (FHD) image input/output with real-time 60 frames-per-second capability. Emphasis of the present work is on the game-changing opportunities that FPGA technology creates to the CA community. We present results from the Greenberg-Hastings and Hodgepodge models, as well as a large-neighborhood anisotropic model. Large neighborhoods either yield qualitatively different results vs. smaller neighborhoods, or lead to results which are merely impossible to produce with small neighborhoods. A comparison of FPGA technology for CA shows advantages vs. conventional Central Processing Units (CPUs) or Graphics Processor Units (GPUs).|International Symposium on High Performance Computing Systems and Applications|2019|10.1109/HPCS48598.2019.9188084|A. Dollas, Nikolaos Kyparissas|0.16666666666666666|0
1726|Effective GPU-Based Section Visualization in Isosurface of Saturation of Displacing Liquid in a Porous Medium|"One of the methods for analysis of data obtained in computer simulation of the processes of unstable oil displacement is to visual analyze the plane sections of level surface (isosurface) of the saturation of a displacing viscous liquid in a sample of a porous medium. This paper presents an efficient distributed method for plane section visualization on a graphics processing unit (GPU) in a polygonal model of considered level surface. A polygonal model of clipped isosurface is created using a modified ""marching cubes"" method by means of the developed geometric and fragment shader programs. Based on the solutions proposed, a visualization program complex with an ergonomic user interface was implemented, which can be used in scientific researches, virtual laboratories, virtual environment systems, educational applications, etc."|2018 International Conference on Engineering Technologies and Computer Science (EnT)|2018|10.1109/ENT.2018.00018|P. Timokhin, M. Mikhaylyuk|0.14285714285714285|0
1776|Pragma Based GPU Parallelizations for Cardiovascular Simulations|Directive based programming models as OpenACC, OpenMP 4.5 and CUDA provide a unified user interface for several hardware targets. We compare their performance on GPUs with respect to a solver for cardiovascular simulations using unstructured tetrahedral meshes and object oriented data structures in C++. Handling deep data copies appropriately in the constructors of the classes allows OpenACC to achieve the same performance as a well implemented CUDA solver. OpenMP 4.5 is still far away from reaching the same performance on GPUs. In contrast to OpenMP, OpenACC allows a user defined coalescing strategy and therefore OpenACC is already a realistic platform independent alternative to CUDA.|International Symposium on High Performance Computing Systems and Applications|2018|10.1109/HPCS.2018.00160|S. Rosenberger, G. Haase|0.14285714285714285|0
1811|Parallel, Optimized, Error Maxima-Agnostic, Pole Residue Equivalent System Solver|Fitting of sampled frequency response data to a rational function is often required for macromodeling of advanced microelectronic packages. The pole residue equivalent system solver (PRESS) method was recently proposed, as a novel fitting method that builds the global fit from local fits, computed from at most three data points at an error peak. To increase the fitting accuracy of PRESS, the error-maxima agnostic PRESS (EMPRESS) was proposed, but it is computationally intensive as the central processing unit needs to sequentially perform the local fits at all the frequency sample points before choosing the best among them. In this paper, we propose the parallel optimized EMPRESS (POEMPRESS) to reduce the computational complexity of EMPRESS while improving the fitting performance. POEMPRESS optimizes both the EMPRESS algorithm and its implementation through: 1) simplified local fit types, enabling the algorithm to rely on at most two data points only, and 2) an algorithm that can be implemented on graphics processing units for rapid parallel computation and evaluation of local fits at all the frequency points. The proposed algorithm is demonstrated on the interconnects of a microelectronic package. The results show that POEMPRESS is effective in fast macromodeling of complex interconnects for signal and power integrity simulations.|IEEE Transactions on Components, Packaging, and Manufacturing Technology|2018|10.1109/TCPMT.2017.2727059|P. Mahanta, Venkatesh Avula, A. Zadehgol|0.14285714285714285|0
1893|High Performance Computing (HPC) Implementation: A Survey|To maintain excellence in global hyper-competitive economies in the upcoming decades, manufacturers must improve the design, development, and distribution of subsequent product generations, production technologies, equipment, and processes. High-performance computing (HPC) solves the application of significant challenges using computer modeling, simulation, and analytical technology. By using HPC technology, products design will be faster, making and testing prototypes will save time, the production process can be simplified, the cost of innovation will decrease, and high-value innovation will be quickly developed. This study uses the PRISMA method, based on predetermined keywords and analyzes the algorithms, frameworks and programming languages and the processes that are most commonly used to optimize the Graphics Processing Unit (GPU). The results of this study can be a reference for future research on the implementation of High-Performance Computing (HPC) Application by combine modeling, simulation, and analysis.|2018 Indonesian Association for Pattern Recognition International Conference (INAPR)|2018|10.1109/INAPR.2018.8627040|Priati Assiroj, A. Hananto, H. L. Hendric Spits Warnars, Ahmad Fauzi|0.14285714285714285|0
2066|Development of Wireless Communication Algorithms on Multicore/Manycore Architectures|Wireless communication is one of the most rapid growing technology. To support new services demanding high data rate, the system design requires sophisticated signal processing at the transmitter and receiver. The signal processing techniques include Fast Fourier Transform (FFT), Inverse Fast Fourier Transform (IFFT) computation, Maximal Ratio Receive Combining (MRRC) scheme, Peak-to-Average Power Ratio (PAPR) reduction techniques, resource allocation algorithms, optimization algorithms, detection algorithms and assignment algorithms to name a few. These signal processing techniques makes wireless communication system computationally complex leading to performance limitation. \nIn this work we implemented some of the mentioned signal processing techniques under Graphics Processing Unit (GPU) environment to harness the parallel implementation. The huge computational capabilities of GPU have been used to reduce the execution time of complex systems. Modern wireless communication heavily relies on FFT and IFFT computation. This research work attempts to address implementation of FFT and IFFT through High Performance Computing (HPC) with GPU. PAPR is another area which requires high computational complexity. Here, it is also evaluated on GPU architecture. In Orthogonal Frequency Division Multiple Access (OFDMA) downlink system, allocation of resource to user at Base Station (BS) demand computational efficient algorithm to fulfill the user’s data rate requirement. Hence, this thesis proposes Resource Allocation and Subcarrier Assignment(RASA) algorithm for multi-core architectures with Open Multiprocessing(OpenMP) support. Further in this thesis, subcarrier assignment problem using well know Hungarian algorithm is analyzed and implemented on Compute Unified Device Architecture(CUDA). Strong computational advantages are observed in parallel implementation of above problems in the area of wireless communication applications. Hence GPU based HPC can provide the promising solution for complex wireless systems. \nThe findings have been extensively evaluated through computer simulation with GPU processor.||2018|10.1109/gucon.2018.8674888|S. Yadav|0.14285714285714285|0
2111|An Efficient Acceleration of Solving Heat and Mass Transfer Equations with the Second Kind Boundary Conditions in Capillary Porous Composite Cylinder Using Programmable Graphics Hardware|With the recent developments in computing technology, increased efforts have gone into simulation of various scientific methods and phenomenon in engineering fields. One such case is the simulation of heat and mass transfer in capillary porous media, which is becoming more and more important in analysing various scenarios in engineering applications. Analysing such heat and mass transfer phenomenon in a given environment requires us to simulate it. This entails simulation of coupled heat mass transfer equations. However, this process of numerical solution of heat and mass transfer equations is very much time consuming. Therefore, this paper aims at utilizing one of the acceleration techniques developed in the graphics community that exploits a graphics processing unit (GPU) which is applied to the numerical solutions of heat and mass transfer equations. The nVidia Compute Unified Device Architecture (CUDA) programming model caters a good method of applying parallel computing to program the graphical processing unit. This paper shows a good improvement in the performance while solving the heat and mass transfer equations for capillary porous composite cylinder with the second kind of boundary conditions numerically running on GPU. This heat and mass transfer simulation is implemented using CUDA platform on nVidia Quadro FX 4800 graphics card. Our experimental results depict the drastic performance improvement when GPU is used to perform heat and mass transfer simulation. GPU can significantly accelerate the performance with a maximum observed speedup of more than 7-fold times. Therefore, the GPU is a good approach to accelerate the heat and mass transfer simulation.||2018|10.4236/jcc.2018.69003|Fan Wu, H. Narang, Abdul Rafae Mohammed|0.14285714285714285|0
2233|Rapid Computation Of Permeability From Micro-CT Images On GPGPUs|Digital rock physics (DRP) is a rapidly evolving technology targeting fast turnaround times for repeatable core analysis and multi-physics simulation of rock properties. We develop a rapid and scalable distributed-parallel single-phase pore-scale flow simulator for permeability estimation on real 3D pore-scale micro-CT images using a novel variant of the lattice Boltzmann method (LBM). The LBM code implementation is designed to take maximum advantage of distributed computing on multiple general-purpose graphics processing units (GPGPUs). We describe and extensively test the distributed parallel implementation of an innovative LBM algorithm for simulating flow in pore-scale media based on the multiple-relaxation-time (MRT) model. The novel contributions of this work are (1) integration of mathematical and high-performance computing components together with a highly optimized implementation and (2) quantitative results with the resulting simulator in terms of robustness, accuracy, and computational efficiency for a variety of flow geometries including various types of real rock images. We report on extensive tests with the simulator in terms of accuracy and provide near-ideal distributed parallel scalability results on large pore-scale image volumes that were largely computationally inaccessible prior to our implementation. Permeability estimation results are provided on large 3D binary microstructures including real rocks from various sandstone and carbonate formations. We quantify the scalability behavior of the distributed parallel implementation of MRT-LBM as a function of model type/size and the number of utilized new-generation NVIDIA V100 GPGPUs for a panoply of permeability estimation problems.|ECMOR XVI - 16th European Conference on the Mathematics of Oil Recovery|2018|10.3997/2214-4609.201802184|M. Araya-Polo, F. Alpak|0.14285714285714285|0
2274|Compressed Sensing for Few-View Multi-Pinhole Spect with Applications to Preclinical Imaging|COMPRESSED SENSING FOR FEW-VIEW MULTI-PINHOLE SPECT WITH APPLICATIONS TO PRECLINICAL IMAGING by Benjamin Rizzo Doctor of Philosophy in Computational Sciences Marquette University, 2018 Single Photon Emission Computed Tomography (SPECT) can be used to identify and quantify changes in molecular and cellular targets involved in disease. A radiopharmaceutical that targets a specific metabolic function is administered to a subject and planar projections are formed by imaging emissions at different view angles around the subject. The reconstruction task is to determine the distribution of radioactivity within the subject from the projections. We present a reconstruction approach that utilizes only a few view angles, resulting in a highly underdetermined system, which could be used for dynamic imaging applications designed to quantify physiologic processes altered with disease. We developed an approach to solving the underdetermined problem that incorporates a fast matrix-based multi-pinhole projection model into a primal-dual algorithm (Chambolle-Pock), tailored to perform penalized data fidelity minimization using the reconstruction’s total variation as a sparse regularizer. The resulting algorithm was implemented on a Graphics Processing Unit (GPU), and validated by solving an idealized quadratic problem. Simulated noisy data from a digital rat thorax phantom was reconstructed using a range of regularizing parameters and primaldual scale factors to control the smoothness of the reconstruction and the step-size in the iterative algorithm, respectively. The approach was characterized by evaluating data fidelity, convergence, and noise properties. The proposed approach was then applied to few-view experimental data obtained in a preclinical SPECT study. 99mTc-labeled macroaggregated albumin (MAA) that accumulates in the lung was administered to a rat and multi-pinhole image data was acquired and reconstructed. The results demonstrate MAA uptake in the lungs is accurately quantified over a wide range of activity levels using as few as three view angles. In a pilot experiment, we also showed using 15 and 60 view angles that uptake of 99mTc-hexamethylpropyleneamineoxime in hyperoxia-exposed rats is higher than that in control rats, consistent with previous studies in our laboratory. Overall these experiments demonstrate the potential utility of the proposed method for few-view threedimensional reconstruction of SPECT data for dynamic preclinical studies.||2018|10.5935/1984-0063.20180014|B. Rizzo|0.14285714285714285|0
1711|Photodynamic therapies of high-grade gliomas: from theory to clinical perspectives. (Thérapies photodynamiques appliquées aux gliomes de haut grade : de la théorie à la réalité clinique)|Gliomas are the most common primary brain tumors in adults. Among them, glioblastoma (GBM) represents the most frequent primary brain tumor and have the most dismal prognosis. Its annual incidence is about 3 to 5 cases for 100,000 persons (about 3000 news cases each year in France). Median survival varies between 14 to 15 months according to the extent of tumor resection. \nThe standard of care includes surgery and is followed by radiation therapy and chemotherapy. Maximal resection is expected to delay recurrence. Despite of using intraoperative photodynamic diagnosis, or fluorescence guided resection (FGR), which improves the extent of resection, relapse still occurs in these resection margins in 85% of cases. \nAlternatives therapies have to be developed to enhance patients’ overall survival. In this context, Photodynamic Therapy (PDT) seems relevant. PDT is based on the synergy of three parameters: a photosensitizing molecule, the photosensitizer (PS) that concentrates preferentially into the tumor cells, laser light and oxygen. Laser light induces a reaction between the PS and the oxygen of the cell. This reaction produces highly cytotoxic molecules (including singlet oxygen) and leads to death of tumor cells. Two treatment modalities are investigated: interstitial PDT (iPDT) or intraoperative PDT. \nThe main goal of this thesis is to provide technological tools to develop the PDT for GBM treatment. Thus, the two treatment modalities have been investigated. \nWhen tumor resection is non-achievable (approximately 20% to 30% of cases), iPDT may be preferred. This modality aims to insert optical fibers directly into the target to illuminate tumor tissues. Thus, simulation of light propagation in brain tissues is required to plan the location of optical fibers. Considered as reference method, a Monte-Carlo model accelerated by graphics processing unit was developed. This model computes the light propagation emitted by a cylindrical diffusor inside heterogeneous media. Accuracy of the model was evaluated with experimental measurements. The acceleration provided by the parallelization allows its use in clinical routine. \nThe iPDT has to be planned using a Treatment Planning System (TPS). A proof of concept of a TPS dedicated to the stereotactic iPDT treatment of GBM was developed. This software provides basic tools to plan the stereotactic insertion of cylindrical diffusors in patient’s brain and to compute the associated dosimetry. The stereotactic registration and the dosimetry computation’s accuracy were evaluated with specific methodologies. \nWhen tumor resection is achievable, the intraoperative PDT may be applied early after the FGR. It takes advantage of the presence of the PS (protoporphyrin IX) used for FGR purpose and that is already concentrates into the tumor cells. Thus, the proposed treatment strategy fits into the current standard of care. A medical device was designed into fit to the resection cavity and illuminate homogeneously the cavity’s margins. The device is constituted of two parts: a trocar coupled to an inflatable balloon and a fiber guide developed in the ONCO-THAI laboratory allowing to insert the light source. Specific methodologies were developed to calibrate and assess the device in terms of mechanical properties and dosimetry. The calibration process leaded to a transfer function that provides fast, robust and easy treatment duration prescription to induce a PDT response in cavity margins. Furthermore, a comprehensive experimental design has been worked out prior to the clinical trial that evaluate the safety of the procedure.||2017|10.1117/12.2251351|C. Dupont|0.125|0
1835|A Machine Learning Approach for Efficient Parallel Simulation of Beam Dynamics on GPUs|Parallel computing architectures like GPUs have traditionally been used to accelerate applications with dense and highly-structured workloads; however, many important applications in science and engineering are irregular and dynamic in nature, making their effective parallel implementation a daunting task. Numerical simulation of charged particle beam dynamics is one such application where the distribution of work and data in the accurate computation of collective effects at each time step is irregular and exhibits control-flow and memory access patterns that are not readily amenable to GPU's architecture. Algorithms with these properties tend to present both significant branch and memory divergence on GPUs which leads to severe performance bottlenecks.We present a novel cache-aware algorithm that uses machine learning to address this problem. The algorithm presented here uses supervised learning to adaptively model and track irregular access patterns in the computation of collective effects at each time step of the simulation to anticipate the future control-flow and data access patterns. Access pattern forecast are then used to formulate runtime decisions that minimize branch and memory divergence on GPUs, thereby improving the performance of collective effects computation at a future time step based on the observations from earlier time steps. Experimental results on NVIDIA Tesla K40 GPU shows that our approach is effective in maximizing data reuse, ensuring workload balance among parallel threads, and in minimizing both branch and memory divergence. Further, the parallel implementation delivers up to 485 Gflops of double precision performance, which translates to a speedup of up to 2.5X compared to the fastest known GPU implementation.|International Conference on Parallel Processing|2017|10.1109/ICPP.2017.55|A. Godunov, M. Zubair, Tunazzina Islam, B. Terzić, D. Ranjan, K. Arumugam|0.125|0
2013|Molecular Dynamics Simulations of DNA-Functionalized Nanoparticle Building Blocks on GPUs|This thesis discusses massively parallel molecular dynamics simulations of nBLOCKs using graphical processing units. nBLOCKs are nanoscale building blocks composed of gold nanoparticles functionalized with single-stranded DNA molecules. To explore greater simulation time scales we implement our nBLOCK computational model as an extension to the coarse grain molecular simulator oxDNA. oxDNA is parameterized to match the thermodynamics of DNA strand hybridization as well as the mechanics of single stranded DNA and double stranded DNA. In addition to an indepth review of our implementation details we also provide results of the model validation and performance tests. These validation and performance tests are comprised of over a hundred separate simulations spanning in simulation length from one thousand to ten million times steps and with simulation sizes ranging from 16 to 27832 particles. Together these tests show the ability of our implementation to handle the full range of basic nBLOCK topologies in a diverse set of conditions. A selection of the utilities developed during the course of this thesis are also discussed. We provide descriptions of the scripting utilities which support nBLOCK assembly generation, simulation, and analysis.||2017|10.1002/9781119194033.ch2|Tyler Fochtman|0.125|0
2064|Direct acquisition method of long PN-code based on the GPU|In order to solve the acquisition problem of long PN-code spread spectrum signals, the partly correlation code phase-space acquisition model and implementation method of the FFT are analyzed. In order to segment data processing tasks and make full use of GPU to speed up FFT, a partly overlapped segmentation acquisition method of long PN-code is researched. Implementations of a direct global searching method and a partly overlapped segmentation searching method based on the GPU are then proposed, which parallel search for code phase using the FFT method, and accelerate FFT using the GPU. Simulation results indicate that, compared to a CPU implementation method, the search speed of the direct acquisition method of long PN-code based on the GPU is improved significantly.||2017|10.1360/N112016-00201|G. Ou, Weihua Mou, Chunjiang Ma, Feixue Wang, Xiaomei Tang|0.125|0
2085|POUMM: An R-package for Bayesian Inference of Phylogenetic Heritability|The Phylogenetic Ornstein-Uhlenbeck Mixed Model (POUMM) allows to estimate the phylogenetic heritability of continuous traits, to test hypotheses of neutral evolution versus stabilizing selection, to quantify the strength of stabilizing selection, to estimate measurement error and to make predictions about the evolution of a phenotype and phenotypic variation in a population. Despite this variety of applications, currently, there are no R-packages supporting POUMM inference on large non-ultrametric phylogenetic trees. Large phylogenies of that kind are becoming increasingly available, predominantly in epidemiology, where transmission trees are inferred from pathogen sequences during epidemic outbreaks, but also in some macroevolutionary studies incorporating fossil and contemporary data. In this article, we propose the R-package POUMM, providing Bayesian inference of the model parameters on large phylogenetic trees. We describe a novel breadth-first pruning algorithm for fast likelihood calculation, enabling highly parallelizable likelihood calculation on multi-core systems and graphics processing units (GPUs). We report simulation-based results proving the technical correctness and performance of the software.||2017|10.1101/115089|Venelin Mitov, T. Stadler|0.125|0
2106|Using graphics processing unit to accelerate simulation of membrane computing|This study presents models which can be used to describe active membrane systems using a matrix approach. Indisputably, matrix operations are very efficient over a graphic processing unit (GPU). With matrix operations, it is not necessary to assign one membrane to one thread block. Rather, in sharp contrast to earlier approaches, objects from different membranes can now be assigned to threads in one thread block, or objects from one membrane to threads in different thread blocks. With this approach, the number of active threads in each thread block can be balanced and processing speed enhanced. For previous approaches, when the number of objects in each membrane is equal to two the multiprocessor occupancy is low and the speedup is 0.6 times, whereas for the proposed approach the multiprocessor occupancy is high and achieves a 33.7-times speedup with respect to sequential implementation.|International Conference on Electrical Engineering and Informatics|2017|10.1109/ICEEI.2017.8312370|Elankovan A. Sundararajan, R. C. Muniyandi|0.125|0
2272|Efficient Simulation of Nested Hollow Sphere Intersections: for Dynamically Nested Compartmental Models in Cell Biology|In the particle-based simulation of cell-biological systems in continuous space, a key performance bottleneck is the computation of all possible intersections between particles. These typically rely for collision detection on solid sphere approaches. The behavior of cell biological systems is influenced by dynamic hierarchical nesting, such as the forming of, the transport within, and the merging of vesicles. Existing collision detection algorithms are found not to be designed for these types of spatial cell-biological models, because nearly all existing high performance parallel algorithms are focusing on solid sphere interactions. The known algorithms for solid sphere intersections return more intersections than actually occur with nested hollow spheres. Here we define a new problem of computing the intersections among arbitrarily nested hollow spheres of possibly different sizes, thicknesses, positions, and nesting levels. We describe a new algorithm designed to solve this nested hollow sphere intersection problem and implement it for parallel execution on graphical processing units (GPUs). We present first results about the runtime performance and scaling to hundreds of thousands of spheres, and compare the performance with that from a leading solid object intersection package also running on GPUs.|SIGSIM Principles of Advanced Discrete Simulation|2017|10.1145/3064911.3064920|K. Perumalla, A. Uhrmacher, Till Köster|0.125|0
1635|Numerical Solutions of Heat and Mass Transfer with the First Kind Boundary and Initial Conditions in Capillary Porous Cylinder Using Programmable Graphics Hardware|Recently, heat and mass transfer simulation is more and more important in various engineering fields. In order to analyze how heat and mass transfer in a thermal environment, heat and mass transfer simulation is needed. However, it is too much time-consuming to obtain numerical solutions to heat and mass transfer equations. Therefore, in this paper, one of acceleration techniques developed in the graphics community that exploits a graphics processing unit (GPU) is applied to the numerical solutions of heat and mass transfer equations. The nVidia Compute Unified Device Architecture (CUDA) programming model provides a straightforward means of describing inherently parallel computations. This paper improves the performance of solving heat and mass transfer equations over capillary porous cylinder with the first boundary and initial conditions numerically running on GPU. Heat and mass transfer simulation using the novel CUDA platform on nVidia Quadro FX 4800 is implemented. Our experimental results clearly show that GPU can accurately perform heat and mass transfer simulation. GPU can significantly accelerate the performance with the maximum observed speedups 10 times. Therefore, the GPU is a good approach to accelerate the heat and mass transfer simulation||2016|10.14569/IJACSA.2016.070607|Fan Wu, Abisoye Ogunniyan, H. Narang|0.1111111111111111|0
1661|Entropic lattice Boltzmann simulation of three-dimensional binary gas mixture flow in packed beds using graphics processors|The lattice Boltzmann method is employed for simulating the binary flow of oxygen/nitrogen mixture passing through a highly dense bed of spherical particles. Simulations are performed based on the latest proposed entropic lattice Boltzmann model for multi-component flows, using the D3Q27 lattice stencil. The curved solid boundary of the particles is accurately treated via a linear interpolation. To lower the total computational cost and time of the simulations, implementation on graphics processing units GPU is also presented. Since the workload associated with each iteration is relatively higher than that of conventional 3D LBM simulations, special emphasis is paid in order to obtain the best computational performance on GPUs. Performance gains of one order of magnitude over optimised multi-core CPUs are achieved for the complex flow of interest on Fermi generation GPUs. Moreover, the numerical results for a three-dimensional benchmark flow show excellent agreements with the available analytical data.|Int. J. Comput. Sci. Eng.|2016|10.1504/IJCSE.2016.076937|M. Ashrafizaadeh, Mohammad Amin Safi|0.1111111111111111|0
1737|Large Scale GPU Accelerated PPMLR-MHD Simulations for Space Weather Forecast|PPMLR-MHD is a new magnetohydrodynamics (MHD) model used to simulate the interactions of the solar wind with the magnetosphere, which has been proved to be the key element of the space weather cause-and-effect chain process from the Sun to Earth. Compared to existing MHD methods, PPMLR-MHD achieves the advantage of high order spatial accuracy and low numerical dissipation. However, the accuracy comes at a cost. On one hand, this method requires more intensive computation. On the other hand, more boundary data is subject to be transferred during the process of simulation. In this work, we present a parallel hybrid solution of the PPMLR-MHD model implemented using the computing capabilities of both CPUs and GPUs. We demonstrate that our optimized implementation alleviates the data transfer overhead by using GPU Direct technology and can scale up to 151 processes and achieve significant performance gains by distributing the workload among the CPUs and GPUs on Titan at Oak Ridge National Laboratory. The performance results show that our implementation is fast enough to carry out highly accurate MHD simulations in real time.|IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing|2016|10.1109/CCGrid.2016.68|Zhaohui Huang, Xiangyu Guo, Zhihui Du, Jian Tao, Binbin Tang|0.1111111111111111|0
1954|Comparative study of Krylov Subspace method implementations for a GPU cluster in elastoplastic problems|As a part of the finite element procedure, it is necessary to solve a system of simultaneous linear algebraic equations. A simulation with a sophisticated material model and a fine mesh requires using an iterative linear solver executed on a grid of computational accelerators. Most of modern iterative methods for linear systems are based on Krylov Subspaces. As there is no universal iterative method for linear equations available, numerous varieties of iterative linear solvers exist. The task of choosing a method best suited for a particular computational system and a physical model is arguably impossible to formalize due to both an unpredictable convergence of iterative methods and a sophisticated memory hierarchy of contemporary data processing systems. Computational experiments are virtually the sole approach to choosing a particular method. The paper presents computational experiment results for a cluster of graphic processor units (GPUs) and a linear system from the finite element analysis of an elas...||2016|10.1063/1.4967081|A. Konovalov, Yury Vladimirovich Khalevitsky, A. Partin, N. Burmasheva|0.1111111111111111|0
2173|Comparison of parallel implementations of controls on GPU for transient simulation of power system|Widely utilization of converter-based interfaces in AC-DC hybrid power system requires precise modeling and simulation for complicated and large-scale control system. However, this process can be accelerated by fine-grained parallel algorithm, and therefore improving the simulation efficiency. In this paper, two kind of fine-grained parallel control system solving methods are proposed and implemented on Graphics Processing Units (GPU) platform to accelerate a large-scale control system simulation. According to the test results, the simulation efficiency of the two methods can be effectively improved by using GPU.|Cybersecurity and Cyberforensics Conference|2016|10.1109/CHICC.2016.7554936|Shaowei Huang, Zhitong Yu, Chen-Ling Ying, Yankan Song|0.1111111111111111|0
2190|Towards the computation of aircraft engine fan noise generation with high order methods on GPUs|The present paper considers the computation of noise generation by aircraft engine fan with the in-house solver for graphic processing units called GHOST CFD. The solver is based on Dispersion Relation Preserving schemes which have high order of approximation and high resolution. An Optimized Low Dispersion and Dissipation Runge-Kutta scheme was utilized for the time integration. Large Eddy Simulation based on Relaxation Filtering was used for the turbulence modeling. The solver implements overset (“CHIMERA”) meshes which were used as rotor–stator interface treatment. The speedup gained from graphic processing units utilization was about 12–20 times compared to modern 8-core CPU, allowing to perform computations in a reasonable time period. The computations with GHOST CFD were performed in full annulus formulation with fan and outlet guide vane blades. The results were compared with the experimental data as well as the results of similar computations in the commercial ANSYS CFX solver some of which also included inlet guide vane blades.||2016|10.1177/1475472X16659386|E. V. Koromyslov, M. V. Usanin, T. Lyubimova, A. Siner, L. Gomzikov|0.1111111111111111|0
2199|Design and characterization of a dedicated cone-beam CT scanner for detection of acute intracranial hemorrhage|Purpose: Prompt and reliable detection of intracranial hemorrhage (ICH) has substantial clinical impact in diagnosis and treatment of stroke and traumatic brain injury. This paper describes the design, development, and preliminary performance characterization of a dedicated cone-beam CT (CBCT) head scanner prototype for imaging of acute ICH. Methods: A task-based image quality model was used to analyze the detectability index as a function of system configuration, and hardware design was guided by the results of this model-based optimization. A robust artifact correction pipeline was developed using GPU-accelerated Monte Carlo (MC) scatter simulation, beam hardening corrections, detector veiling glare, and lag deconvolution. An iterative penalized weighted least-squares (PWLS) reconstruction framework with weights adjusted for artifact-corrected projections was developed. Various bowtie filters were investigated for potential dose and image quality benefits, with a MC-based tool providing estimates of spatial dose distribution. Results: The initial prototype will feature a source-detector distance of 1000 mm and source-axis distance of 550 mm, a 43x43 cm2 flat panel detector, and a 15° rotating anode x-ray source with 15 kW power and 0.6 focal spot size. Artifact correction reduced image nonuniformity by ~250 HU, and PWLS reconstruction with modified weights improved the contrast to noise ratio by 20%. Inclusion of a bowtie filter can potentially reduce dose by 50% and improve CNR by 25%. Conclusions: A dedicated CBCT system capable of imaging millimeter-scale acute ICH was designed. Preliminary findings support feasibility of point-of-care applications in TBI and stroke imaging, with clinical studies beginning on a prototype.|SPIE Medical Imaging|2016|10.1117/12.2216544|A. Sisniega, J. Xu, J. Stayman, J. Siewerdsen, N. Aygun, X. Wang, D. Foos, W. Zbijewski, V. Koliatsos, H. Dang|0.1111111111111111|0
2239|Implementación de particle-in-cell sobre GPUs para la simulación de plasma producido por Láser|The Particle-in-Cell (PIC) method is used to study the phase space evolution on some systems made of charged particles. In this work, a bidimensional PIC model is presented to make the analysis of an electrostatic plasma that is result of an ablation laser process with an isotermic expansion. Some Cloud-In-Cell (CIC) techniques are used, the Fast Fourier Transform, Finite Difference Method and Euler-Cromer. In each iteration CIC is used to make interpolations between charge and fields; with FFT the electrostatic potential is calculated in nodes of the grid, the electric field was obtained using central finite differences, with Euler-Cromer Method the positions and velocities of superparticles are updated. Also, the high parallelism profile of this kind of algorithms (PIC) is advantageous for the implementation using Graphics Processing Units (GPUs). Some results of plasma expansion are showed until 7.5 emission time; the superparticles displacement is analyzed on electrons and ions. Some acceleration was achieved.|Cybersecurity and Cyberforensics Conference|2016|10.1109/COLUMBIANCC.2016.7750770|Oscar Henao, Y. Gomez, Alejandro Suárez, C. Alvarez, J. Osorio, H. Riascos|0.1111111111111111|0
1729|Harnessing aspect-oriented programming on GPU: application to warp-level parallelism|Stochastic simulations involve multiple replications in order to build confidence intervals for their results, and designs of experiments (DOEs) to explore their parameters set. In this paper, we propose warp–level parallelism (WLP), a GPU–enabled solution to compute multiple replications in parallel (MRIP) on GPUs (graphics processing units). GPUs are intrinsically tuned to process efficiently the same operation on several data, which is not suited to parallelise MRIP or DOEs. Our approach proposes to rely on small thread groups, called warps, to perform independent computations such as replications. This approach has proved to be efficient on three classical simulation models, but originally lacked the transparency users might expect. In this work, we enhance WLP using aspect oriented programming (AOP). Our work describes the way to combine CUDA and AOP, and brings forward the techniques available to exploit AOP in a CUDA–enabled development.|International Journal of Computer Aided Engineering and Technology|2015|10.1504/IJCAET.2015.068335|Pierre Schweitzer, D. Hill, C. Mazel, Jonathan Passerat-Palmbach, Jonathan Caux, P. Siregar|0.1|0
1876|Neumann-Neumann Waveform Relaxation methods for fractional RC circuits|The Waveform Relaxation (WR) methods are recognized as efficient solvers for large scale circuits and attract a lot of attention in recent years due to their favorable advantages where they are ideally suited for the use of multiple parallel processors for problems with multiple time scales. However, applying classical WR techniques to strongly coupled systems leads to non-uniform convergence. Therefore, more uniform WR methods have been developed. This paper is concerned to generalize the Neumann-Neumann waveform relaxation (NN-WR) method invented recently for time-dependent PDEs to time-fractional circuits which seems to be a promising method in circuit simulations. By choosing the RC circuit in infinite size as the model, we perform a convergence analysis for the NN-WR method and this corresponds to the analysis of this method for PDEs at the semi-discrete level. The NN-WR method contains a free parameter, namely β, which has a significant effect on the convergence rate. For PDEs, the analysis at the space-time continuous level shows β = 1 over 4, while the analysis in this paper shows that, at the semi-discrete level, i.e., for the circuit problem, we can have a better choice which leads to much faster convergence in practical computing. A comparison with the so-called Robin WR is also included.|International Conference on Information, Communications and Signal Processing|2015|10.1109/IACS.2015.7103205|Shulin Wu, M. Al-khaleel|0.1|0
1897|Design and Implementation of Massively Parallel Fine-Grained Processor Arrays|This thesis investigates the use of massively parallel fine-grained processor arrays to increase computational performance. As processors move towards multi-core processing, more energy-efficient processors can be designed by increasing the number of processor cores on a single chip rather than increasing the clock frequency of a single processor. This can be done by making processor cores less complex, but increasing the number of processor cores on a chip. Using this philosophy, a processor core can be reduced in complexity, area, and speed to form a very small processor which can still perform basic arithmetic operations. Due to the small area occupation this can be multiplied and scaled to form a large scale parallel processor array to offer a significant performance. Following this design methodology, two fine-grained parallel processor arrays are designed which aim to achieve a small area occupation with each individual processor so that a larger array can be implemented over a given area. To demonstrate scalability and performance, SIMD parallel processor array is designed for implementation on an FPGA where each processor can be implemented using four ?slices? of a Xilinx FPGA. With such small area utilization, a large fine-grained processor can be implemented on these FPGAs. A 32 � 32 processor array is implemented and fast processing demonstrated using image processing tasks.An event-driven MIMD parallel processor array is also designed which occupies a small amount of area and can be scaled up to form much larger arrays. The event-driven approach allows the processor to enter an idle mode when no events are occurring local to the processor, reducing power consumption. The processor can switch to operational mode when events are detected. The processor core is designed with a multi-bit data path and ALU and contains its own instruction memory making the array a multi-core processor array. With area occupation of primary concern, the processor is relatively simple and connects with its four nearest direct neighbours. A small 8 � 8 prototype chip is implemented in a 65 nm CMOS technology process which can operate at a clock frequency of 80 MHz and offer a peak performance of 5.12 GOPS which can be scaled up to larger arrays.An application of the event-driven processor array is demonstrated using a simulation model of the processor. An event-driven algorithm is demonstrated to perform distributed control of distributed manipulator simulator by separating objects based on their physical properties.||2015|10.1109/iscas.2015.7168891|Declan Walsh|0.1|0
1898|GPU Implementation of a Biological Electromagnetic Scattering Problem by FDTD|To ensure a high level of safety and reliability of electronic/electric systems EMC (electromagnetic compatibility) tests together with computational techniques are used. In this thesis, mathematical modeling and computational electromagnetics are applied to mainly two case studies. In the first case study, electromagnetic modeling of electric networks and antenna structures above, and buried in, the ground are studied. The ground has been modelled either as a perfectly conducting or as a dielectric surface. The second case study is focused on mathematical modeling and algorithms to solve the direct and inverse electromagnetic scattering problem for providing a model-based illustration technique. This electromagnetic scattering formulation is applied to describe a microwave imaging system called Breast Phantom. The final goal is to simulate and detect cancerous tissues in the human female breast by this microwave technique. The common issue in both case studies has been the long computational time required for solving large systems of equations numerically. This problem has been dealt with using approximation methods, numerical analysis, and also parallel processing of numerical data. For the first case study in this thesis, Maxwell’s equations are solved for antenna structures and electronic networks by approximation methods and parallelized algorithms implemented in a LAN (Local Area Network). In addition, PMM (Point-Matching Method) has been used for the cases where the ground is assumed to act like a dielectric surface. For the second case study, FDTD (Finite-Difference Time Domain) method is applied for solving the electromagnetic scattering problem in two dimensions. The parallelized numerical FDTD-algorithm is implemented in both Central Processing Units (CPUs) and Graphics Processing Units (GPUs).||2015|10.1109/ccece.2015.7129328|M. Otterskog, Linus Carlsson, S. Silvestrov, Farid Monsefi, M. Rančić, Simon Elgland|0.1|0
1910|Implementation of Thermochemistry and Chemical Kinetics in a GPU-based CFD Code|The implementation of multi-species thermochemistry and chemical kinetics in a GPUbased CFD code is described, focusing on issues which are specific to GPUs. The physical model and its numerical formulation are described in detail. Performance results are presented for two multidimensional test cases: non-reacting supersonic flow over a forward-facing step and the reacting flow of a cellular detonation in a low-pressure H2-O2Ar mixture. The performance results are analyzed to determine the performance when simulations are run on GPUs versus CPUs, the scalability of the solver on both types of compute devices, as well as the cost of the thermochemistry model relative to simpler thermodynamic models.||2015|10.2514/6.2015-0842|D. Schwer, Andrew Corrigan, B. Taylor|0.1|0
1931|Multi-scale Cardiac Electrophysiological Simulation: A GPU-based System by Computational Steering|A cardiac electrophysiological simulation tool is essential for the study of the modern virtual heart. However, there is not yet an ideal tool that can satisfy the requirements of rapidity, flexibility and interactivity. In this paper, we propose a simulation system to cope with this problem, which is (i) based on GPU for parallel computing, and very fast; (ii) flexible to embed various cell models and simulate multi-scale cardiac tissue; (iii) interactive and easy to use by computational steering. We designed a particular simulation case to show the convenience of computational steering and another test to compare the speed of our system to the traditional CPU-based method. These results suggest that our simulation system can be an ideal tool for the research of the virtual heart.||2015|10.2991/ISET-15.2015.40|Kuanquan Wang, Shanzhuo Zhang, Yongfeng Yuan, Songjun Xie|0.1|0
1968|Maintainability and Performance for LAMMPS.|Sandia National Laboratories is a multi-­‐program laboratory managed and operated by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation, for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-­‐AC04-­‐94AL85000. Over the last few years, molecular dynamics (MD) simulation codes have been at the forefront of supporting new hardware architectures. For example, most of the major older codes, including AMBER, Gromacs, NAMD, and LAMMPS, have provided significant support for GPUs for several years, and newer codes like HOOMD have been developed specifically for GPUs. We believe the older codes all initially approached this task by creating CUDA variants of their most important MD kernels specifically for the GPU. Some of these codes now support running completely on GPUs, with no regular data transfers to the CPU other than for MPI communication. Others offload only specific key computations to the GPU. Many also support hybrid execution where, for example, non-­‐bonded interactions are calculated on the GPU at the same time bonded interactions and long-­‐range Coulombics are calculated on the CPU. In general, performance improvements over many-­‐core CPU-­‐only clusters have been quite good. For LAMMPS, this approach of creating variants of key kernels tuned for each architecture (CPU/MPI-­‐only, OpenMP, GPU, Phi, etc), has become increasingly difficult to sustain over time as the code has grown and new architectures proliferate. As a general materials simulation code with models at the atomic to meso to continuum scales, LAMMPS currently has non-­‐bonded kernels for ~120 different functional forms. There are likewise ~40 different functional forms for bonded interactions (bonds, angles, dihedrals, impropers) and several variants of long-­‐range Coulombic models. The code also has 100+ options for add-­‐on calculations which affect the dynamics such as thermostats, barostats, different time integration schemes, force constraints, interactions with boundaries and other objects, external forces, etc., as well as several dozen optional diagnostics, which involve loops over atoms or more complex calculations. Overall, we estimate LAMMPS thus contains around 500 unique kernels, which if not optimized, could become bottlenecks in a particular simulation on specific hardware. The challenge this creates is reflected in the current GPU capabilities of LAMMPS, which only support a small fraction of all these LAMMPS features. The problem is compounded because LAMMPS input scripts allow users to combine these capabilities in flexible and unpredictable ways. As a consequence, just porting the most important kernels to a given architecture may still result in …||2015|10.2172/1177389|S. Moore, S. Plimpton, Tzu-Ray Shan, A. Thompson, C. Trott|0.1|0
1993|Rendering view dependent reflections using the graphics card|Real-time graphics applications are tending to get more realistic and approximate real world illumination gets more reasonable due to improvement of graphics hardware. Using a wide variation of algorithms and ideas, graphics processing units (GPU) can simulate complex lighting situations rendering computer generated imagery with complicated effects such as shadows, refraction and reflection of light. Particularly, reflections are an improvement of realism, because they make shiny materials, e.g. brushed metals, wet surfaces like puddles or polished floors, appear more realistic and reveal information of their properties such as roughness and reflectance. Moreover, reflections can get more complex, depending on the view: a wet surface like a street during rain for example will reflect lights depending on the distance of the viewer, resulting in more streaky reflection, which will look more stretched, if the viewer is locatedrnfarther away from the light source. This bachelor thesis aims to give an overview of the state-of-the-art in terms of rendering reflections. Understanding light is a basic need to understand reflections and therefore a physical model of light and its reflection will be covered in section 2, followed by the motivational section 2.2, that will give visual appealing examples for reflections from the real world and the media. Coming to rendering techniques, first, the main principle will be explained in section 3 followed by a short general view of a wide variety of approaches that try to generate correct reflections in section 4. This thesis will describe the implementation of three major algorithms, that produce plausible local reflections. Therefore, the developed framework is described in section 5, then three major algorithms will be covered, that are common methods in most current game and graphics engines: Screen space reflections (SSR), parallax-corrected cube mapping (PCCM) and billboard reflections (BBR). After describing their functional principle, they will be analysed of their visual quality and the possibilities of their real-time application. Finally they will be compared to each other to investigate the advantages and disadvantages over each other. In conclusion, the gained experiences will be described by summarizing advantages and disadvantages of each technique and giving suggestions for improvements. A short perspective will be given, trying to create a view of upcoming real-time rendering techniques for the creation of reflections as specular effects.||2015|10.1177/194849921500600001|G. Schmidt|0.1|0
1996|Shared-files parallel simulation framework for dynamic multi-domains networks using OMNeT++ (WIP)|Parallel discrete event simulation (PDES) has been recognized as a challenging research field bridging between modeling and simulation and high-performance computing. It tackles the problem of executing discrete event simulations on parallel processors. OMNeT++ [1] is a powerful and open-source simulation tool which is basically intended to model discrete-event systems. According to its authors, the OMNeT++ PDES implementation has a modular and extensible architecture, allowing new synchronization protocols and new communication mechanisms to be added easily, which makes it a particularly attractive platform for PDES research. Unfortunately, some constraints should be verified first in order that parallel simulation under OMNeT++ works properly. The most important one is that the topology of the network should be static. In this paper, we propose a new parallel simulation approach for OMNeT++ based on socket communication and shared files that allows dynamic models to work without any of the existing problems.|Summer Simulation Multiconference|2015|10.1109/irsec.2015.7455144|S. Albarrak, A. Hamzi|0.1|0
2037|coaster: Teaching Computer Graphics Incrementally (Abstract Only)|"coaster is a project that teaches a semester-long introductory computer graphics class by means of ten programming assignments. The assignments are incremental - each one building on the previous ones - and ultimately require implementation of most of the course content in the final one: a first-person rollercoaster simulation. Briefly described, the assignments (and their course contents) are: circles (""warmup"", 2D graphics, applying trigonometry), wire track (3D graphics, parametric curves), wire car (meshes), ""hedgehog"" car (face and vertex normals), shaded car (lighting models and vertex shaders), shaded track (extrusion, model transforms), surfaces (Bézier surfaces, height maps), first person (viewing transforms, animation, splines), dynamics (physics-based modeling), and textures (textures, pixel shaders). There is also an eleventh project of the student's own (approved) design. Students are provided with template code for the first ten programming assignments. The languages used are C++ on the CPU and GLSL on the GPU. Students are presumed to have access to OpenGL/GLSL 3.3/3.30 and the GLUT and GLEW libraries. Both undergraduate and graduate students take the class. and it has been presented twice at Washington State University, both times with about half of the students on a remote campus receiving it as a live telecourse. Student response has been very positive. The goal of this lightning talk is to elicit interest from the computer graphics teaching community in making coaster systematically available to other universities by providing source code and training to instructors."|Technical Symposium on Computer Science Education|2015|10.1145/2676723.2691880|R. Lewis|0.1|0
2112|A Software Component Approach for GPU Accelerated Physics-Based Blood Flow Simulation|High-fidelity patient specific model for blood flow simulation is important in medical applications. In this paper, a software component approach is proposed to perform physics based simulation of blood flow using GPU for general computing. The method comprises three independently developed software components, namely image processing component, object model component and GPU acceleration component. The fluid dynamics of blood flow is simulated using a mesh free modeler based on an improved Smoothed Particle Hydrodynamics method, and the interaction with arterial wall is simulated using a Finite Element modeler based on lumped element approach. Real-time simulation of blood flow has been demonstrated in an application on patients with abdominal aorta aneurysm. The internal fluidic structure and pressure distribution analysis have also been achieved using the proposed method.|IEEE International Conference on Systems, Man and Cybernetics|2015|10.1109/SMC.2015.431|C. Chui, Jichuan Wu, C. Teo|0.1|0
2185|Lagrangian particle‐based simulation of fluid–solid coupling on graphics processing units|Lagrangian particle method has been widely used in computer physics and graphics; however, numerically solving the partial differential physical equation on a great number of particles is a computationally complex task. In this paper, a unified particle method on graphics processing units is proposed to simulate fluid–solid interaction with large density ratio interactively. Motivated by microscopic molecular dynamics, we consider the solid object as a particular fluid limited to solid motions; therefore, fluid–solid interaction as well as solid–solid interaction could be solved directly using multiphase weakly compressible smoothed particle hydrodynamics solvers. And then, we present a momentum‐conserving particle collision handling scheme to prevent fluid penetrating into solid objects. In the simulation, a measure of particle densities is used to handle density discontinuities at fluid–solid interfaces, and consequently, new formulations for density‐weighted inter‐particle pressure and viscous forces are derived. Moreover, to realistically simulate various small‐scale interaction phenomena such as water droplets flowing on solids’ surfaces, a surface tension model that uses density‐weighted color gradient and can obtain a stable and accurate surface curvature is employed to capture the interfacial fluid–solid tensions. Because all of the computation is carried out on graphics processing unit and no CPU processing is needed, the proposed algorithm can exploit the massive computational power of graphics processing unit for interactive simulation with a higher particle resolution. The experiment results show that our method can simulate realistic fluid–solid couplings at interactive frame‐rates even for up to 126 k particles. Copyright © 2014 John Wiley & Sons, Ltd.||2015|10.1002/jnm.2003|Wei Wu, Jinsong Zhang, X. Shao, Zhong Zhou|0.1|0
2303|Ultra-Fast Scenario Analysis of Mortgage Prepayment Risk|Stochastic scenario analysis of mortgage hedging strategies using single-CPU core machines is often too time consuming. In order to achieve a large practical speedup, we present two methods implemented on a many-core system consisting of graphical processing units (GPUs). The first method is based on Monte Carlo simulations, which are widely used in risk management. The second method relies on a parallel implicit finite difference (FD) discretization of a forward Kolmogorov equation. To estimate the speedup that can be achieved in practice, we compared the performance of both methods with an existing serial trinomial tree implementation on a single CPU core currently in use in our department. For both methods, a large speedup of roughly two orders of magnitude is achieved for realistic workloads. We show that the FD method is approximately four times faster than the Monte Carlo method when implemented on GPUs. On the other hand we argue that the Monte Carlo method is more adaptable to accommodate generic models, while the FD method is typically suitable to low dimensional models, such as single-factor interest rate models. To our knowledge, the application of GPUs for mortgage hedge calculations is new, as is the implementation of the FD method on GPUs.||2015|10.21314/JOR.2015.323|Alexios Theiakos, Jurgen M.C Tas, D. Kandhai, Han van der Lem|0.1|0
1641|Memory-Efficient Parallel Simulation of Electron Beam Dynamics Using GPUs|Accurate simulation of collective effects in electron beams is one of the most challenging and computationally intractable problems in accelerator physics. More recently, researchers have developed a GPU-accelerated, high-fidelity simulation of electron beam dynamics that models the collective effects much more accurately. The simulation, however, is heavily data-intensive and memory-bound. In particular, data-dependent, irregular memory access patterns and control-flow in the collective effects computation phase of the simulation leads to a large number of non-coalesced memory accesses on the GPU. This significantly deteriorates the overall performance. Moreover, the parallel simulation exhibits poor data locality. This, together with non-coalesced memory accesses, leads to ineffective use of the memory hierarchy. We present a novel cache-aware algorithm that uses a locality heuristic to maximize data reuse by improving data locality. Additionally, the algorithm uses a control-flow heuristic to balance the workload among threads. The control-flow heuristic also minimizes threads divergence and enables reuse of partial results of previous iterations and thereby reducing the overall operation count. Experimental results on NVIDIA Tesla K40 GPU shows that our approach delivers up to 450 Gflops of double precision performance, which translates to a speedup of up to 16X compared with the current state-of-the-art GPU implementation.|International Conference on High Performance Computing|2016|10.1109/HiPC.2016.033|A. Godunov, M. Zubair, B. Terzić, D. Ranjan, K. Arumugam|0.0|0
1643|GPU-accelerated visualisation of ADS granular flow target model|This paper presents a discrete element method to handle particle collision detection and responses in transport simulation the simulation of transport of protons and neutrons in granular flow target geometric model based on GPUs. Discrete element method was adopted in the realisation of large-scale particle visualisation. The method simulates and solves edge detection, position judging, motion direction, calculation of the next collision point using GPU acceleration during the process of transport, and demonstrates the complete interaction process through OpenGL. Results show that the model presented exploits the acceleration of GPUs and has gained remarkable functional improvement compared with traditional method using solely CPUs. In addition, we used the MCNPX to calculate this model with high-speed proton bombardment. The distribution of power energies verifies that the granular flow target model is reliable and feasible.|International Journal of High Performance Computing and Networking|2015|10.1504/IJHPCN.2015.072824|Hongyu Sun, Kuan-Ching Li, Xun-Chao Zhang, Jiong Wu, Qingguo Zhou, Yan-Shan Tian|0.0|0
1648|A Performance Study of the 2D Ising Model on GPUs|The simulation of the two-dimensional Ising model is used as a benchmark to show the computational capabilities of Graphic Processing Units (GPUs). The rich programming environment now available on GPUs and flexible hardware capabilities allowed us to quickly experiment with several implementation ideas: a simple stencil-based algorithm, recasting the stencil operations into matrix multiplies to take advantage of Tensor Cores available on NVIDIA GPUs, and a highly optimized multi-spin coding approach. Using the managed memory API available in CUDA allows for simple and efficient distribution of these implementations across a multi-GPU NVIDIA DGX-2 server. We show that even a basic GPU implementation can outperform current results published on TPUs and that the optimized multi-GPU implementation can simulate very large lattices faster than custom FPGA solutions.|arXiv.org|2019|10.1145/3310273.3323918|J. Romero, M. Bernaschi, M. Fatica, M. Bisson|0.0|0
1653|Contention-Aware Selective Caching to Mitigate Intra-Warp Contention on GPUs|Modern GPUs embrace on-chip cache memory to exploit the locality present in applications. However, the behavior and effect of the cache on GPUs are different from those on conventional processors due to the Single Instruction Multiple Thread (SIMT) thread execution model and resulting memory access patterns. Previous studies report that caching data can hurt the performance due to increased memory traffic and thrashing on massively parallel GPUs. We found that the massive parallel thread execution of GPUs causes significant resource access contention among threads, especially within a warp. This is due to excessive demands for memory resources that are not sufficient to support massively parallel thread execution when memory access patterns are not hardware friendly.In this paper, we propose a locality and contention aware selective caching based on memory access divergence to mitigate intra-warp resource contention in L1 data (L1D) cache on GPUs. To determine when and what to cache we use the following heuristics: first, we detect memory divergence degree (i.e., how the memory requests from a warp are grouped) of the memory instruction to determine whether the selective caching is needed. Second, we use cache index calculation to handle congested cache sets. %the case where accesses are congested into certain cache sets. Finally, we calculate locality degree to find a better victim cache line. These algorithmic selective caching is developed based on our observation that 1) divergent memory access incurs severe contention for cache hardware resources and 2) accesses are mapped to certain sets when set associativity is relatively small compared with the memory divergence degree. Experimental results by GPU architectural simulator show that our proposed selective caching improves the average performance by 2.25x over baseline and reduces L1D cache accesses by 71%. It outperforms recently published state-of-the-art GPU cache bypassing schemes.|International Symposium on Parallel and Distributed Computing|2017|10.1109/ISPDC.2017.17|Esraa A. Gad, David Troendle, B. Jang, Kyoshin Choo|0.0|0
1654|Addition of flexible linkers to GPU-accelerated coarse-grained simulations of protein-protein docking|Multiprotein complexes are responsible for many vital cellular functions, and understanding their formation has many applications in medical research. Computer simulation has become a valuable tool in the study of biochemical processes, but simulation of large molecular structures such as proteins on a useful scale is computationally expensive. A compromise must be made between the level of detail at which a simulation can be performed, the size of the structures which can be modelled and the time scale of the simulation. Techniques which can be used to reduce the cost of such simulations include the use of coarse-grained models and parallelisation of the code. Parallelisation has recently been made more accessible by the advent of Graphics Processing Units (GPUs), a consumer technology which has become an affordable alternative to more specialised parallel hardware. We extend an existing implementation of a Monte Carlo protein-protein docking simulation using the Kim and Hummer coarse-grained protein model [1] on a heterogeneous GPU-CPU architecture [2]. This implementation has achieved a significant speed-up over previous serial implementations as a result of the efficient parallelisation of its expensive non-bonded potential energy calculation on the GPU. Our contribution is the addition of the optional capability for modelling flexible linkers between rigid domains of a single protein. We implement additional Monte Carlo mutations to allow for movement of residues within linkers, and for movement of domains connected by a linker with respect to each other. We also add potential terms for pseudo-bonds, pseudo-angles and pseudo-torsions between residues to the potential calculation, and include additional residue pairs in the non-bonded potential sum. Our flexible linker code has been tested, validated and benchmarked. We find that the implementation is correct, and that the addition of the linkers does not significantly impact the performance of the simulation. This modification may be used to enable fast simulation of the interaction between component proteins in a multiprotein complex, in configurations which are constrained to preserve particular linkages between the proteins. We demonstrate this utility with a series of simulations of diubiquitin chains, comparing the structure of chains formed through all known linkages between two ubiquitin monomers. We find reasonable agreement between our simulated structures and experimental data on the characteristics of diubiquitin chains in solution.||2019||A. Pinska|0.0|0
1657|Pillars of the Mantle: Imaging the Interior of the Earth with Adjoint Tomography|In this work, we investigate global seismic tomographic models obtained by spectral-element simulations of seismic wave propagation and adjoint methods. Global crustal and mantle models are obtained based on an iterative conjugate-gradient type of optimization scheme. Forward and adjoint seismic wave propagation simulations, which result in synthetic seismic data to make measurements and data sensitivity kernels to compute gradient for model updates, respectively, are performed by the SPECFEM3D_GLOBE package [1] [2] at the Oak Ridge Leadership Computing Facility (OLCF) to study the structure of the Earth at unprecedented levels. Using advances in solver techniques that run on the GPUs on Titan at the OLCF, scientists are able to perform large-scale seismic inverse modeling and imaging. Using seismic data from global and regional networks from global CMT earthquakes, scientists are using SPECFEM3D_GLOBE to understand the structure of the mantle layer of the Earth. Visualization of the generated data sets provide an effective way to understand the computed wave perturbations which define the structure of mantle in the Earth.|Practice and Experience in Advanced Research Computing|2017|10.1145/3093338.3104170|D. Pugmire, M. Lefebvre, D. Peter, N. Podhorszki, Judith C. Hill, E. Bozdağ, D. Komatitsch, J. Tromp|0.0|0
1658|Exact analytical algorithm for solvent accessible surface area and derivatives in implicit solvent molecular simulations on GPUs|In this paper, we present dSASA (differentiable SASA), an exact geometric method to calculate solvent accessible surface area (SASA) analytically along with atomic derivatives on GPUs. The atoms in a molecule are first assigned to tetrahedra in groups of four atoms by Delaunay tetrahedrization adapted for efficient GPU implementation and the SASA values for atoms and molecules are calculated based on the tetrahedrization information and inclusion-exclusion method. The SASA values from the numerical icosahedral-based method can be reproduced with more than 98% accuracy for both proteins and RNAs. Having been implemented on GPUs and incorporated into the software Amber, we can apply dSASA to implicit solvent molecular dynamics simulations with inclusion of this nonpolar term. The current GPU version of GB/SA simulations has been accelerated up to nearly 20-fold compared to the CPU version and it outperforms LCPO as the system size increases. The performance and importance of the nonpolar part in implicit solvent modeling are demonstrated in GB/SA simulations of proteins and accurate SASA calculation of nucleic acids.||2024|10.1021/acs.jctc.3c01366|E. A. Coutsias, Yuzhang Wang, M. H. Hummel, Xin Cao, C. Simmerling|0.0|0
1660|Implicit numerical multidimensional heat-conduction algorithm parallelization and acceleration on a graphics card|Analytical solutions are much less computationally intensive than numerical ones, and moreover, they are more accurate because they do not contain numerical errors; however, they can only describe a small group of simple heat-conduction problems. A numerical simulation of heat conduction is often used as it is able to describe complex problems, but its computational time is much longer, especially for unsteady multidimensional models with temperature-dependent material properties. After a discretization using the implicit scheme, the heat-conduction problem can be described with N non-linear equations, where N is the large number of the elements of the discretized model. This set of equations can be efficiently solved with an iteration of the line-by-line method, based on the heat-flux superposition, although the computational procedure is strictly serial. This means that no parallel computation can be done, which is strictly required when a graphics card is used to accelerate the computation. This paper describes a multidimensional numerical model of unsteady heat conduction solved with the line-by-line method and a modification of this method for a highly parallel computation. An enormous increase in the speed is demonstrated for the modified line-by-line method accelerated on the graphics card, and the durations of the computations for various mesh sizes are compared with the original line-by-line method.||2016|10.17222/MIT.2014.128|Jana Ondroušková, M. Pohanka|0.0|0
1666|Parallel XPBD Simulation of Modified Morse Potential - an Alternative Spring Model|In this paper, we introduce a modified Morse potential as an alternative to the existing spring models within a massively parallel extended Position Based Dynamics (XPBD) algorithm. To date, stretching is one of the most popular constraint types of XPBD frameworks due to its simplicity, robustness and efficiency. However, the underneath mathematical expression of stretching constraint does not fully represent a spring model and behaves too stiff over a certain iteration count or damping coefficient. On the other hand, Hookean spring potential behaves softer and viscoelastic within the XPBD algorithm under the same conditions as stretching constraint. Our modified Morse potential addresses this issue by keeping the simulation of deformable models in between Hooke’s law and stretching constraint. To demonstrate the benefits of modified Morse potential with higher frame rates, we develop an efficient Independent Edge Grouping algorithm for XPBD method which provides parallel processing on GPU. We compare the simulation results of cloth and volumetric models with stretching constraint, Hookean and St. Venant-Kirchhoff (STVK) spring potentials. We believe that our modified Morse potential is easy to implement and seamlessly fit into the existing XPBD frameworks. CCS Concepts • Computing methodologies → Physical simulation;|EGPGV@EuroVis|2019|10.2312/pgv.20191108|O. Cetinaslan|0.0|0
1667|TU-AB-BRC-09: Fast Dose-Averaged LET and Biological Dose Calculations for Proton Therapy Using Graphics Cards.|PURPOSE\nTo demonstrate fast and accurate Monte Carlo (MC) calculations of proton dose-averaged linear energy transfer (LETd) and biological dose (BD) on a Graphics Processing Unit (GPU) card.\n\n\nMETHODS\nA previously validated GPU-based MC simulation of proton transport was used to rapidly generate LETd distributions for proton treatment plans. Since this MC handles proton-nuclei interactions on an event-by-event using a Bertini intranuclear cascade-evaporation model, secondary protons were taken into account. The smaller contributions of secondary neutrons and recoil nuclei were ignored. Recent work has shown that LETd values are sensitive to the scoring method. The GPU-based LETd calculations were verified by comparing with a TOPAS custom scorer that uses tabulated stopping powers, following recommendations by other authors. Comparisons were made for prostate and head-and-neck patients. A python script is used to convert the MC-generated LETd distributions to BD using a variety of published linear quadratic models, and to export the BD in DICOM format for subsequent evaluation.\n\n\nRESULTS\nVery good agreement is obtained between TOPAS and our GPU MC. Given a complex head-and-neck plan with 1 mm voxel spacing, the physical dose, LETd and BD calculations for 108 proton histories can be completed in ∼5 minutes using a NVIDIA Titan X card. The rapid turnover means that MC feedback can be obtained on dosimetric plan accuracy as well as BD hotspot locations, particularly in regards to their proximity to critical structures. In our institution the GPU MC-generated dose, LETd and BD maps are used to assess plan quality for all patients undergoing treatment.\n\n\nCONCLUSION\nFast and accurate MC-based LETd calculations can be performed on the GPU. The resulting BD maps provide valuable feedback during treatment plan review. Partially funded by Varian Medical Systems.|Medical Physics (Lancaster)|2016|10.1118/1.4957403|Chan Tseung, H. Wan, C. Beltran|0.0|0
1671|Cell-to-cell Mathematical Modeling of Arrhythmia Phenomena in the Heart|With an aperiodic, self-similar distribution of two-dimensional arrangement of atrial cells, it is possible to simulate such phenomena as Fibrillation, Fluttering, and a sequence of Fibrillation-Fluttering. The topology of a network of cells may facilitate the initiation and development of arrhythmias such as Fluttering and Fibrillation. Using a GPU parallel architecture, two basic cell topologies were considered in this simulation, an aperiodic, fractal distribution of connections among 462 cells, and a chessboard-like geometry of 60×60 and 600×600 cells. With a complex set of initial conditions, it is possible to produce tissue behavior that may be identified with arrhythmias. Finally, we found several sets of initial conditions that show how a mesh of cells may exhibit Fibrillation that evolves into Fluttering.|bioRxiv|2020|10.1101/2020.07.28.225755|G. Garza, M. Castro-García, J. R. Godínez-Fernández, G. Román-Alonso, Aurelio Nicolas Mata|0.0|0
1675|SI2-SSI: JETSCAPE|The JETSCAPE Collaboration will develop a scalable and portable open \nsource software package to replace a variety of existing codes. The \nmodular integrated software framework will consist of interacting \ngenerators to simulate (i) wave functions of the incoming nuclei, (ii) \nviscous fluid dynamical evolution of the hot plasma, and (iii) transport\n and modification of jets in the plasma. Integrated advanced statistical\n analysis tools will provide non-expert users with quantitative methods \nto validate novel theoretical descriptions of jet modification, by \ncomparison with the complete set of current experimental data. To \nimprove the efficiency of this computationally intensive task, the \ncollaboration will develop trainable emulators that can accurately \npredict experimental observables by interpolation between full model \nruns, and employ accelerators such as Graphics Processing Units (GPUs) \nfor both the fluid dynamical simulations and the modification of jets. \nThe collaboration will create this framework with a user-friendly \nenvelope that allows for continuous modifications, updates and \nimprovements of each of its components. The effort will serve as a \ntemplate for other fields that involve complex dynamical modeling and \ncomparison with large data sets. It will open a new era for \nhigh-precision extraction of the internal structure of the Quark-Gluon \nPlasma with quantifiable uncertainties.||2017|10.6084/m9.figshare.4669030.v2|A. Majumder|0.0|0
1677|Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling|Sionna is a GPU-accelerated open-source library for link-level simulations based on TensorFlow. Since release v0.14 it integrates a differentiable ray tracer (RT) for the simulation of radio wave propagation. This unique feature allows for the computation of gradients of the channel impulse response and other related quantities with respect to many system and environment parameters, such as material properties, antenna patterns, array geometries, as well as transmitter and receiver orientations and positions. In this paper, we outline the key components of Sionna RT and showcase example applications such as learning radio materials and optimizing transmitter orientations by gradient descent. While classic ray tracing is a crucial tool for 6G research topics like reconfigurable intelligent surfaces, integrated sensing and communications, as well as user localization, differentiable ray tracing is a key enabler for many novel and exciting research directions, for example, digital twins.|arXiv.org|2023|10.48550/arXiv.2303.11103|J. Hoydis, Sebastian Cammerer, Alexander Keller, Fayccal Ait Aoudia, Guillermo Marcus, Nikolaus Binder, Merlin Nimier-David|0.0|0
1679|High performance ultrasound simulation using Monte-Carlo simulation: a GPU ray-tracing implementation|Medical ultrasound has gained relevance in medical diagnosis, procedure guidance and emergentology. It is a low cost, safe and non-invasive practice, while obtaining real-time results. Limitations on working hours and ultrasound equipment have made simulation the main alternative for training, since it eliminates the need for a real ultrasound machine and allows the modeling of a large number of case studies. Patient 11 from the 3D-IRCADb-01 CT-scans data-set was used to generate corresponding OBJ mesh files. A JSON file carries relevant information of the scene and tissues involved in it. Monte-Carlo simulation over GPU is used to improve ray tracing performance on a deterministic surface model. An improvement in ray tracing performance and throughput is achieved by the presented model. In execution cases presented, a minimum global acceleration of 30% is reached, while reducing ray tracing times by at least 80%. Monte-Carlo simulation allows a more realistic image generation than the previous deterministic model. Low memory usage gives room to implement the remaining stages of the process on GPU. A correct parametrization of tissues is required in order to achieve improved image quality.|Symposium on Medical Information Processing and Analysis|2020|10.1117/12.2576099|Tomás Pérez Cambet, I. Larrabide, Santiago Vitale|0.0|0
1682|Modeling and rendering of river in inland river ship handling simulator|In this paper, the algorithm of Tiled Directional Flow (TDF) is improved for real-time rendering river surface. To represent the reality of river flowing, the approach for building the river velocity field based on the procedural method is proposed. And the velocity field can be used to generate the flow texture of the TDF algorithm. With combining GPU, GLSL programming and LOD technique, the large scale river scene can be real-time rendered. At last, the results show that the solved velocity field can reflect the river flowing characters realistically. And the simulation results indicate that the proposed algorithm can be used to construct river scene with realistic virtual environment which also meet the real-time requirement in simulation.|International Conference on Informative and Cybernetics for Computational Social Systems|2017|10.1109/ICCSS.2017.8091456|Xiaoming Zhai, Helong Shen, Yong Yin|0.0|0
1683|Fast Digital Simulation of SAR Echoes Based on GPU|To deal with the fast simulation of Synthetic Aperture Radar (SAR) echoes, in this paper, we introduce an fast digital simulation method based on Graphic Processing Unit (GPU). Model of SAR echoes is established at first. Then, the digital simulation for SAR echo of single pulse is optimized by improved concentric circle algorithm. Moreover, the parallel computation based on GPU is adopted to further accelerate the whole digital simulation of all the SAR echoes. In the end, the validity of the above fast digital simulation method of SAR echoes has been verified and the experimental results show that the above method can accelerate the simulation speed dramatically compared with the traditional one.|International Conference on Information Communication and Management|2017|10.1145/3134383.3134396|T. Guo, Ming-bo Zhu, W. Dong, Qi Chen|0.0|0
1686|TU-AB-BRC-10: Modeling of Radiotherapy Linac Source Terms Using ARCHER Monte Carlo Code: Performance Comparison of GPU and MIC Computing Accelerators.|PURPOSE\n(1) To perform phase space (PS) based source modeling for Tomotherapy and Varian TrueBeam 6 MV Linacs, (2) to examine the accuracy and performance of the ARCHER Monte Carlo code on a heterogeneous computing platform with Many Integrated Core coprocessors (MIC, aka Xeon Phi) and GPUs, and (3) to explore the software micro-optimization methods.\n\n\nMETHODS\nThe patient-specific source of Tomotherapy and Varian TrueBeam Linacs was modeled using the PS approach. For the helical Tomotherapy case, the PS data were calculated in our previous study (Su et al. 2014 41(7) Medical Physics). For the single-view Varian TrueBeam case, we analytically derived them from the raw patient-independent PS data in IAEA's database, partial geometry information of the jaw and MLC as well as the fluence map. The phantom was generated from DICOM images. The Monte Carlo simulation was performed by ARCHER-MIC and GPU codes, which were benchmarked against a modified parallel DPM code. Software micro-optimization was systematically conducted, and was focused on SIMD vectorization of tight for-loops and data prefetch, with the ultimate goal of increasing 512-bit register utilization and reducing memory access latency.\n\n\nRESULTS\nDose calculation was performed for two clinical cases, a Tomotherapy-based prostate cancer treatment and a TrueBeam-based left breast treatment. ARCHER was verified against the DPM code. The statistical uncertainty of the dose to the PTV was less than 1%. Using double-precision, the total wall time of the multithreaded CPU code on a X5650 CPU was 339 seconds for the Tomotherapy case and 131 seconds for the TrueBeam, while on 3 5110P MICs it was reduced to 79 and 59 seconds, respectively. The single-precision GPU code on a K40 GPU took 45 seconds for the Tomotherapy dose calculation.\n\n\nCONCLUSION\nWe have extended ARCHER, the MIC and GPU-based Monte Carlo dose engine to Tomotherapy and Truebeam dose calculations.|Medical Physics (Lancaster)|2016|10.1118/1.4957404|L. Su, X. Xu, T. Liu, X. Tang, C. Shi, B. Bednarz, H. Lin|0.0|0
1688|Natural GPU-friendly dynamic animation of human hair|Natural-looking hair is a key component for presenting believable virtual humans, because the head and face form natural focal points of the human figure. In non-static scenes, hair behaviour is just as important as its looks. Principles of physics and dynamic simulation are often used for animating hair, because other traditional animation approaches—such as skeletal animation or motion capture—are difficult to apply to hair. Dynamic animation of hair is still an open problem without a known best solution, because hair has quite specific mechanical properties which, combined with the high number of hairs typically comprising a hairstyle, make realistic and efficient simulation challenging. In this work, we focus on dynamic hair animation methods capable of providing real-time or interactive performance while staying physically plausible. Basing on research and analysis of hair properties from the cosmetic industry, we have devised a novel hair animation method which provides more realistic results than existing comparable methods while at the same time offering better performance and stability. We have applied this method to two different approaches to hair animation in order to prove its independence on any particular representation of hair. In one of these approaches, our method allows us to replace an iterative function minimisation with a direct computation, giving a speed-up of about an order of magnitude in this one stage of the simulation, while at the same time increasing its robustness. Based on further observations natural behaviour of real hair, we have also proposed a novel organisation of simulated hair. This allows representing a larger number of hair strands by simulating fewer primitives without introducing artificial interpolation. Additionally, we have analysed behaviour of real hair when in mutual contact and based on this analysis, proposed an efficient model of collision response for collisions between hair strands. Our entire method is designed to be easily usable with current massively parallel computation architectures such as GPUs. To validate this design decision, we have also created a proof-of-concept implementation of the core parts of our simulation system on the GPU.||2018||P. Kmoch|0.0|0
1696|A fast mass spring model solver for high-resolution elastic objects|Real-time simulation of elastic objects is of great importance for computer graphics and virtual reality applications. The fast mass spring model solver can achieve visually realistic simulation in an efficient way. Unfortunately, this method suffers from resolution limitations and lack of mechanical realism for a surface geometry model, which greatly restricts its application. To tackle these problems, in this paper we propose a fast mass spring model solver for high-resolution elastic objects. First, we project the complex surface geometry model into a set of uniform grid cells as cages through *cages mean value coordinate method to reflect its internal structure and mechanics properties. Then, we replace the original Cholesky decomposition method in the fast mass spring model solver with a conjugate gradient method, which can make the fast mass spring model solver more efficient for detailed surface geometry models. Finally, we propose a graphics processing unit accelerated parallel algorithm for the conjugate gradient method. Experimental results show that our method can realize efficient deformation simulation of 3D elastic objects with visual reality and physical fidelity, which has a great potential for applications in computer animation.|International Conference on Advances in System Simulation|2017|10.1177/0037549717699073|Zhiyong Yuan, Weixu Zhu, Mianlun Zheng, Guian Zhang|0.0|0
1697|GPU-accelerated discrete element modeling of geological faults|In this paper, we present an original algorithm for the numerical simulation of tectonic movements and the related formation of geological faults. The approach is based on the use of Discrete Element Method, where the geological media is represented as an agglomeration of discrete particles which interact as elastic, visco-elastic, or elasto-plastic bodies. This approach naturally allows accounting for large deformations and discontinuities in the geological media; thus, allows simulation of faults formation. Implementation of the algorithm is based on the use of Graphical Processor Units. The Discrete Element Method requires a high number of floating point operations and logical operations per a single particle (degree of freedom) per time step; whereas the number of memory access operations is relatively low. Thus, the use of GPUs decreases the computational time substantially.|Journal of Physics: Conference Series|2019|10.1088/1742-6596/1392/1/012070|V. Tchebverda, V. Lisitsa, V. Volianskaia|0.0|0
1699|Automated, Accurate, and Scalable Relative Protein-Ligand Binding Free-Energy Calculations Using Lambda Dynamics.|Accurate predictions of changes to protein-ligand binding affinity in response to chemical modifications are of utility in small-molecule lead optimization. Relative free-energy perturbation (FEP) approaches are one of the most widely utilized for this goal but involve significant computational cost, thus limiting their application to small sets of compounds. Lambda dynamics, also rigorously based on the principles of statistical mechanics, provides a more efficient alternative. In this paper, we describe the development of a workflow to set up, execute, and analyze multisite lambda dynamics (MSLD) calculations run on GPUs with CHARMM implemented in BIOVIA Discovery Studio and Pipeline Pilot. The workflow establishes a framework for setting up simulation systems for exploratory screening of modifications to a lead compound, enabling the calculation of relative binding affinities of combinatorial libraries. To validate the workflow, a diverse data set of congeneric ligands for seven proteins with experimental binding affinity data is examined. A protocol to automatically tailor fit biasing potentials iteratively to flatten the free-energy landscape of any MSLD system is developed, which enhances sampling and allows for efficient estimation of free-energy differences. The protocol is first validated on a large number of ligand subsets that model diverse substituents, which shows accurate and reliable performance. The scalability of the workflow is also tested to screen more than 100 ligands modeled in a single system, which also resulted in accurate predictions. With a cumulative sampling time of 150 ns or less, the method results in average unsigned errors of under 1 kcal/mol in most cases for both small and large combinatorial libraries. For the multisite systems examined, the method is estimated to be more than an order of magnitude more efficient than contemporary FEP applications. The results thus demonstrate the utility of the presented MSLD workflow to efficiently screen combinatorial libraries and explore the chemical space around a lead compound and thus are of utility in lead optimization.|Journal of Chemical Theory and Computation|2020|10.26434/chemrxiv.12781310.v1|C. Brooks, Thomas J. Paul, R. L. Hayes, E. P. Raman, Dassault Systèmes|0.0|0
1702|Modelling Performance Loss due to Thread Imbalance in Stochastic Variable-Length SIMT Workloads|When designing algorithms for single-instruction multiple-thread (SIMT) devices such as general purpose graphics processing units (GPGPUs), thread imbalance is an important performance consideration. Thread imbalance can emerge in iterative applications where workloads are of variable length, because threads processing larger amounts of work will cause threads with less work to idle. This form of thread imbalance influences the design space of algorithms-particularly in terms of processing granularity-but we lack models to quantify its impact on application performance. In this paper, we present a statistical model for quantifying the performance loss due to thread imbalance for iterative SIMT applications with stochastic, variable-length workloads. Our model is designed to operate with minimal knowledge of the implementation details of the algorithm, relying solely on an understanding of the probability distribution of the lengths of the workloads. We validate our model against a synthetic benchmark based on a Monte Carlo simulation of matrix exponentiation, and show that our model achieves nearly perfect accuracy. Compared to empirical data extracted from real hardware, our model maintains a high degree of accuracy, predicting mean performance loss within a margin of 2%.|IEEE/ACM International Symposium on Modeling, Analysis, and Simulation On Computer and Telecommunication Systems|2022|10.1109/MASCOTS56607.2022.00026|Stephen Nicholas Swatman, A. Varbanescu, A. Pimentel, A. Krasznahorkay|0.0|0
1707|Rapid Exploration of Optimization Strategies on Advanced Architectures using TestSNAP and LAMMPS|The exascale race is at an end with the announcement of the Aurora and Frontier machines. This next generation of supercomputers utilize diverse hardware architectures to achieve their compute performance, providing an added onus on the performance portability of applications. An expanding fragmentation of programming models would provide a compounding optimization challenge were it not for the evolution of performance-portable frameworks, providing unified models for mapping abstract hierarchies of parallelism to diverse architectures. A solution to this challenge is the evolution of performance-portable frameworks, providing unified models for mapping abstract hierarchies of parallelism to diverse architectures. Kokkos is one such performance portable programming model for C++ applications, providing back-end implementations for each major HPC platform. Even with a performance portable framework, restructuring algorithms to expose higher degrees of parallelism is non-trivial. The Spectral Neighbor Analysis Potential (SNAP) is a machine-learned inter-atomic potential utilized in cutting-edge molecular dynamics simulations. Previous implementations of the SNAP calculation showed a downward trend in their performance relative to peak on newer-generation CPUs and low performance on GPUs. In this paper we describe the restructuring and optimization of SNAP as implemented in the Kokkos CUDA backend of the LAMMPS molecular dynamics package, benchmarked on NVIDIA GPUs. We identify novel patterns of hierarchical parallelism, facilitating a minimization of memory access overheads and pushing the implementation into a compute-saturated regime. Our implementation via Kokkos enables recompile-and-run efficiency on upcoming architectures. We find a $\sim$22x time-to-solution improvement relative to an existing implementation as measured on an NVIDIA Tesla V100-16GB for an important benchmark.|arXiv.org|2020|10.1201/9781003072164-13|S. Anderson, Rahulkumar Gayatri, A. Thompson, J. Deslippe, S. Moore, D. Perez, Evan Weinberg, N. Lubbers|0.0|0
1709|Accelerating earthquake simulations on general‐purpose graphics processors|Parallelization strategies are presented for Virtual Quake, a numerical simulation code for earthquakes based on topologically realistic systems of interacting earthquake faults. One of the demands placed upon the simulation is the accurate reproduction of the observed earthquake statistics over three to four decades. This requires the use of a high‐resolution fault model in computations, which demands computational power that is well beyond the scope of off‐the‐shelf multi‐core CPU computers. However, the recent advances in general‐purpose graphic processing units have the potential to address this problem at moderate cost increments. A functional decomposition of Virtual Quake is performed, and opportunities for parallelization are discussed in this work. Computationally intensive modules are identified, and these are implemented on graphics processing units, significantly speeding up earthquake simulations. In the current best case scenario, a computer with six graphics processing units can simulate 500 years of fault activity in California at 1.5 km × 1.5 km element resolution in less than 1 hour, whereas a single CPU requires more than 2 days to perform the same simulation. Copyright © 2015 John Wiley & Sons, Ltd.|Concurrency and Computation|2015|10.1002/cpe.3618|E. Heien, P. Sengupta, P. Menon, J. Nguyen, J. Kwan, J. Rundle|0.0|0
1716|N-body simulation of binary star mass transfer using NVDIA GPUs|Binary star systems are of particular interest to astronomers because they can be used as astrophysical laboratories to study the properties and processes of stars. Between 70% to 90% of the stars in our galaxy are part of a binary star system. Among the many types of binary systems observed, the dynamics of semi-detached and contact systems are the most interesting because they exhibit mass transfer, which changes the composition and life cycle of both stars. The time scales of the mass transfer process are extremely large which makes the process impossible to capture through physical observation. Computer simulations have proved invaluable in refining our understanding of the mass transfer processes. Here we introduce an intuitive, computationally efficient, gravity centered model that simulates the filling of the Roche lobe of an expanding star and its transfer of mass through the first Lagrangian point.||2023|10.1002/alz.080517|Mason McCallum, B. Wyatt, Taylor J. Hutyra, S. Goderya, Baylor G. Fain, E. Smith|0.0|0
1718|OpenCL-library-based implementation of SCLSU algorithm for remotely sensed hyperspectral data exploitation: clMAGMA versus viennaCL|In the last decade, hyperspectral spectral unmixing (HSU) analysis have been applied in many remote sensing applications. For this process, the linear mixture model (LMM) has been the most popular tool used to find pure spectral constituents or endmembers and their fractional abundance in each pixel of the data set. The unmixing process consists of three stages: (i) estimation of the number of pure spectral signatures or endmembers, (ii) automatic identification of the estimated endmembers, and (iii) estimation of the fractional abundance of each endmember in each pixel of the scene. However, unmixing algorithms can be very expensive computationally, a fact that compromises their use in applications under real-time constraints. This is, mainly, due to the last two stages in the unmixing process, which are the most consuming ones. In this work, we propose parallel opencl-library- based implementations of the sum-to-one constrained least squares unmixing (P-SCLSU) algorithm to estimate the per-pixel fractional abundances by using mathematical libraries such as clMAGMA or ViennaCL. To the best of our knowledge, this kind of analysis using OpenCL libraries have not been previously conducted in the hyperspectral imaging processing literature, and in our opinion it is very important in order to achieve efficient implementations using parallel routines. The efficacy of our proposed implementations is demonstrated through Monte Carlo simulations for real data experiments and using high performance computing (HPC) platforms such as commodity graphics processing units (GPUs).|Remote Sensing|2016|10.1117/12.2241524|S. Bernabé, Manuel Prieto-Matias, Jose M. R. Navarro, G. Botella, Carlos Orueta, A. Plaza|0.0|0
1725|Design and Implementation Of Cairo University Graphics Processing (CUGPU) Using HLS Approach|GPUs can be divided to fixed function GPUs and programmable GPUs. Fixed function GPUs are important for low power mobile applications. This paper addresses the design and implementation, on an FPGA, of a fixed function GPU fully compatible with OpenGL 1.1.12 standard. This GPU is code-named Cairo University GPU (CUGPU). We follow a systematic design approach to design our CUGPU. We developed MATLAB model, C-language model, RTL model and finally a logic model for CUGPU. A hybrid approach for RTL synthesis is followed where automatic and manual synthesis are judiciously combined to achieve small design time and acceptable performance. The final CUGPU is implemented on Xilinx Vertix-7 FPGA. The functionality of CUGPU is verified via direct comparison with the C model results, using hardware simulation and real time on-chip logic analysis.|Novel Intelligent and Leading Emerging Sciences Conference|2021|10.1109/NILES53778.2021.9600099|S. Habib, Abdulkareem Mohammed Ibrahim|0.0|0
1734|Distributed GPU Based Matrix Power Kernel for Geoscience Applications|\n High-performance computing is at the heart of digital technology which allows to simulate complex physical phenomena. The current trend for hardware architectures is toward heterogeneous systems with multi-core CPUs accelerated by GPUs to get high computing power. The demand for fast solution of Geoscience simulations coupled with new computing architectures drives the need for challenging parallel algorithms. Such applications based on partial differential equations, requires to solve large and sparse linear system of equations. This work makes a step further in Matrix Powers Kernel (MPK) which is a crucial kernel in solving sparse linear systems using communication-avoiding methods. This class of methods deals with the degradation of performances observed beyond several nodes by decreasing the gap between the time necessary to perform the computations and the time needed to communicate the results. The proposed work consists of a new formulation for distributed MPK kernels for the cluster of GPUs where the pipeline communications could be overlapped by the computation. Also, appropriate data reorganization decreases the memory traffic between processors and accelerators and improves performance. The proposed structure is based on the separation of local and external components with different layers of interface nodes-due to the MPK algorithm-. The data is restructured in a way where all the data required by the neighbor process comes contiguously at the end, after the local one. Thanks to an assembly step, the contents of the messages for each neighbor are determined. Such data structure has a major impact on the efficiency of the solution, since it permits to design an appropriate communication scheme where the computation with local data can occur on the GPUs and the external ones on the CPUs. Moreover, it permits more efficient inter-process communication by an effective overlap of the communication by the computation in the asynchronous pipeline way. We validate our design through the test cases with different block matrices obtained from different reservoir simulations : fractured reservoir dual-medium, black-oil two phase-flow, and three phase-flow models. The experimental results demonstrate the performance of the proposed approach compared to state of the art. The proposed MPK running on several nodes of the GPU cluster provides a significant performance gain over equivalent Sparse Matrix Vector product (SpMV) which is already optimized and provides better scalability.|Day 1 Tue, October 26, 2021|2021|10.2118/203947-ms|A. Sedrakian, T. Guignon|0.0|0
1735|Research on rough sea surface EM computation model based on parallel FDTD method|Technology of modern communication and channel estimation cannot be separated from the research on electromagnetic problems of the sea surface. Considering the cost and other factors, the electromagnetic simulation method is a very valuable research project. FDTD method can be applied to various electromagnetic problems very well, but when dealing with electrically large problems, there will be a large amount of computation, but also occupy a very large memory space, and the calculation is very difficult. To solve the above problems, this paper presents the parallel Moving-Window FDTD algorithm based on OpenCL and GPU, and has completed the wave propagation simulation above sea surface, and get the relevant information. The main content includes: rapid generation of sea surface based on OpenCL and GPU, the implementation of parallel three-dimensional FDTD algorithm and boundary conditions based on OpenCL, and the simulations on the typical parameters of the sea. The code of the above contents is written and debugged on AMD A8 6600K CPU and AMD R7 260X GPU, the computation speed is greatly improved, and the simulation results are in agreement with the theory.|Progress in Electromagnetics Research Symposium|2016|10.1109/PIERS.2016.7734908|T. Jiang, Hanlin Duan, Xiaowei Zhang|0.0|0
1740|Parallelization of the numerical simulation of motion of deformable objects within fluid domain on a GPU device|Computationally demanding numerical simulations can be significantly accelerated using GPU (Graphics Processing Unit) devices. This way, the results of the simulation can be observed in real time. In this paper, the principles of GPU programming are used to simulate the movement of deformable objects within fluid domain. Lattice Boltzmann (LB) method is used to simulate fluid flow. The solid-fluid interaction is modeled using the Immersed boundary method. The developed software was tested on a Tesla GPU device; the execution time of parallelized version and sequential version of the software are compared and significant speed-up is obtained. Fluid flow simulations in the field of biomedicine that needed up to several hours to be performed, can now be completed in just a few minutes.|EAI Endorsed Transactions on Pervasive Health and Technology|2018|10.4108/eai.28-2-2018.154143|N. Filipovic, T. Djukić|0.0|0
1743|Atom filtering algorithm and GPU-accelerated calculation of simulation atomic force microscopy images|Simulation atomic force microscopy computationally emulates experimental scanning of a biomolecular structure to produce topographic images that can be correlated with measured images. Its application to the enormous amount of available high-resolution structures, as well as to molecular dynamics modelling data, facilitates the quantitative interpretation of experimental observations by inferring atomistic information from resolution-limited measured topographies. The computation required to generate a simulated AFM image generally includes the calculation of contacts between the scanning tip and all atoms from the biomolecular structure. However, since only contacts with surface atoms are relevant, a filtering method shall highly improve the efficiency of simulation AFM computations. In this report we address this issue and present an elegant solution based on graphics processing unit (GPU) computations that significantly accelerates the computation of simulation AFM images. The method not only allows for visualization of biomolecular structures combined with ultra-fast synchronized calculation and graphical representation of corresponding simulated AFM images (live simulation AFM), but, as we demonstrate, can also reduce the computational effort during automatized fitting of atomistic structures into measured AFM topographies by orders of magnitude. Hence, the developed method will play an important role in post-experimental computational analysis involving simulation AFM. Implementation is realized in our BioAFMviewer software package for simulation AFM of biomolecular structures and dynamics.|bioRxiv|2023|10.1101/2023.11.15.567294|N. Kodera, R. Amyot, Holger Flechsig|0.0|0
1748|The technical art of sea of thieves|Sea of Thieves posed a unique challenge - developing a stylised, open world game in Unreal Engine 4, a demanding and contemporary game engine focused on photo-realistic rendering. Our game contains a large number of dynamic elements and is designed to run on hardware ranging from integrated GPUs on a laptop, to the most powerful modern gaming PCs. Over the course of development, we have come up with a number of innovative techniques focused both on keeping an open world game like ours performant and visually appealing. We introduced several techniques that we used to stylise and supplement the look of our FFT water implementation for the game's oceans. We also created a new cloud rendering and simulation system for this game, allowing for fast rendering of three-dimensional, art-directed cloudscapes without using expensive raymarching techniques. To bring the world to life, we also developed other graphical features, including a physically-based system of rendering rope-and-pulley systems, our use of baking simulation data to textures and real-time surface fluid simulations to model incidental water behaviour on the GPU.|SIGGRAPH Talks|2018|10.1145/3214745.3214820|Andy Catling, Nigel Ang, F. Ciardi, Valentine Kozin|0.0|0
1759|ФОРМИРОВАНИЕ РАДИОЛОКАЦИОННОЙ СЦЕНЫ ДЛЯ МОДЕЛИРОВАНИЯ РЛС С СИНТЕЗИРОВАННОЙ АПЕРТУРОЙ|This article discusses the creation of a radar scene for the subsequent modeling of a synthetic aperture radar. The complexity of the development of radar requires the creation of models of radar systems for testing signal processing algorithms. The main difficulty in simulating the reflection of a signal from the underlying surface is the high computational complexity associated with the large number of facets that are exposed to the radar. Reducing the cost of computation can be achieved by optimizing data processing, excluding non-illuminated facets from computation, while transferring computations to graphics processors. The purpose of the article is to create a method for generating variants of the radar scene, to create a method for generating target object simulators, to perform signal processing modeling, taking into account only illuminated facets in the carrier movement process and the implementation of radar image generation. In this work, optical satellite images of the terrain were used to create a radar scene, the brightness of each pixel of which is converted to the radar cross section (RCS) values. When calculating the reflected signal, the antenna pattern was taken into account, which made it possible to reduce the calculations time up to 26% (depending on the scene configuration). The developed model uses the MATLAB environment. This article shows a comparison of performance without exception and with the exclusion of illuminated facets for different radar scenes. Using the model with this radar scene allows you to assess the impact of system parameters to the output radar image.||2019|10.1134/s0234087919030055|Владимир|0.0|0
1765|Separable Self and Mixed Attention Transformers for Efficient Object Tracking|The deployment of transformers for visual object tracking has shown state-of-the-art results on several benchmarks. However, the transformer-based models are under-utilized for Siamese lightweight tracking due to the computational complexity of their attention blocks. This paper proposes an efficient self and mixed attention transformer-based architecture for lightweight tracking. The proposed backbone utilizes the separable mixed attention transformers to fuse the template and search regions during feature extraction to generate superior feature encoding. Our prediction head performs global contextual modeling of the encoded features by leveraging efficient self-attention blocks for robust target state estimation. With these contributions, the proposed lightweight tracker deploys a transformer-based backbone and head module concurrently for the first time. Our ablation study testifies to the effectiveness of the proposed combination of backbone and head modules. Simulations show that our Separable Self and Mixed Attention-based Tracker, SMAT, surpasses the performance of related lightweight trackers on GOT10k, TrackingNet, LaSOT, NfS30, UAV123, and AVisT datasets, while running at 37 fps on CPU, 158 fps on GPU, and having 3.8M parameters. For example, it significantly surpasses the closely related trackers E.T.Track and MixFormerV2-S on GOT10k-test by a margin of 7.9% and 5.8%, respectively, in the AO metric. The tracker code and model is available at https://github.com/goutamyg/SMAT|arXiv.org|2023|10.48550/arXiv.2309.03979|Goutam Yelluru Gopal, Maria A. Amer|0.0|0
1766|Guest editor’s note: Special issue on application performance optimization in the era of extreme heterogeneity|This special issue gathers revised and extended versions of selected papers presented at the 13th International Conference on Parallel Processing and Applied Mathematics, PPAM 2019, held September 8–11, 2019 in Bialystok, Poland (http://ppam.info). This conference is a continuation of a series of events started in 1994 when the first PPAM took place in Czestochowa. They have been held every 2 years in different universities in Poland. Thus, the event in Bialystok was an opportunity to celebrate the 25th anniversary of PPAM. The purpose of PPAM conferences is to exchange ideas between researchers involved in parallel and distributed computing, including theory and applications, as well as applied and computational mathematics. PPAM 2019 was organized by the Department of Computer Science of the Czestochowa University of Technology together with the Bialystok University of Technology, under the patronage of the Committee of Informatics of the Polish Academy of Sciences, in technical cooperation with the IEEE Computer Society and IEEE Computational Intelligence Society. The focus of PPAM 2019 was on models, algorithms, and software tools that facilitate efficient and convenient use of modern parallel and distributed computing systems, as well as on large-scale applications, including artificial intelligence and machine learning problems. This event gathered more than 170 participants from 26 countries. The accepted papers were presented at the regular tracks of the PPAM 2019 conference and during the workshops. With each submission evaluated by at least three reviewers, a strict reviewing process resulted in acceptance of 91 contributed papers for publication in the conference proceedings, while approximately 43% of the submissions were rejected. Forty-one papers were selected to be presented in the regular conference track, resulting in an acceptance rate of about 46%. Based on the results of the reviews, selected papers were recommended for a special journal issue. The authors were contacted after the conference and invited to submit revised and extended versions of their works. These new versions were reviewed independently again by at least three reviewers. Finally, three contributions were accepted for publication. They present research on the systematic evaluation and optimization of linear algebra kernels on GPUs (Graphics Processing Units), the efficient adaptation of machine learning and numerical simulation problems to HPC platforms with GPUs and FPGAs (fieldprogrammable gate arrays). The papers in this special issue address the following topics:|The international journal of high performance computing applications|2020|10.1177/1094342020978033|E. Deelman, R. Wyrzykowski|0.0|0
1769|Focused Ultrasound Therapy for Abdominal Organs During Respiratory Motion: Numerical Modeling and Simulation and In-Silico First-Stage Evaluation of a Novel Treatment System|Focused ultrasound (FUS) is a noninvasive method for tissue ablation that has the potential for complete and local tumor destruction with minimal side effects. Already being used for the treatment of static organs, compensating target motion is not yet clinically available due to the complexity of the treatment. We here propose a numerical model of the therapy effects during respiratory motion to study FUS for moving liver targets. A focus lies on incorporating the motion and the computational efficiency of the simulations. A temperature model is proposed predicting the temperature distributions efficiently on the graphics processing unit by mapping the problem from the moving physical world to a static motion reference state. We also investigate the accuracy of ultrasound modeling in the highly heterogeneous propagation domain including ribs. A novel angular spectrum approach for heterogeneous media is proposed as the widely used hybrid angular spectrum method is found to be ineffective. For real-time applications, we propose an approximate ultrasound propagation model. An integrated FUS model is developed combining these model with an abdominal motion model, tissue damage, and a parameter model. The patient anatomy is automatically derived from CT images. Two clinical use-cases of the integrated model are given: A simulation-driven planning tool allows a clinician to interactively explore treatment options. And a study is performed using the model to optimize the placement of the FUS device. The model is furthermore used to study a novel motion-compensation FUS treatment system by replacing hardware and patient by model predictions. We estimate the efficiency of the treatment system in combination with a clinically available FUS device and MR imaging device (6.67 Hz image rate, 20 Hz FUS control rate) to be above 80%. This estimated efficiency of the new treatment system is expected to be already suited for clinical applications.||2018|10.2337/db18-437-p|M. Schwenke|0.0|0
1770|Symplectic Multi-Particle Tracking Using Cuda|The symplectic tracking model can preserve phase space structure and reduce non-physical effects in long term simulation. Though this model is computationally expensive, it is very suitable for parallelization and can be accelerated significantly by using Graphic Processing Units (GPUs). Using a single GPU, the code achieves a speedup of more than 400 compared with the time on a single CPU core. It also shows good scalability on a GPU cluster at Oak Ridge Leadership Computing Facility. In this paper, we report on the GPU code implement, the performance test on both single-GPU andmulti-GPU cluster, and an application of beam dynamics simulation. INTRODUCTION Numerical simulation plays an important role in the beam dynamics study and design of high intensity accelerators, where space charge effects dominate. Most simulation codes at the accelerator community use Particle-In-Cell (PIC) method as the space charge solver [1–7]. The PIC method is an efficient algorithm to include self-consistent space charge effects. However, there are still some arguments that if the PIC algorithm could keep the symplectic condition during the particle tracking. Symplectic integrators were constructed for conserving the symplectic condition of Hamiltonian systems [8, 9]. Recently, a gridless symplectic particle tracking model was introduced and proved to be effective in serving as symplectic Poisson solver in long-term simulation [10]. It can effectively reduce the emittance growth associated with numerical grid heating compared with the PIC algorithm. However, this model is much slower compared with the PIC method. Fortunately, it is very suitable for parallelism and can achieve very good scalability, especially by using the Graphics Processing Unit (GPU). In contrast to CPU computer, one GPU contains several hundreds or even thousands of cores, as shown in Fig. 1. It uses high-bandwidth bus (∼200Gb/s) connecting the memory on chip to the computing cores and is optimized for parallel calculations, particularly for single instruction multiple data (SIMD) operations [11]. The Compute Unified Device Architecture (CUDA) library is a parallel computing platform and GPU programming model developed by NVIDIA [12]. It enables dramatic increase of computing performance by harnessing the power of GPUs. By using ∗ Work supported by the U.S. Department of Energy under Contract No. DE-AC02-05CH11231 and the Ministry of Science and Technology of China under Grant No.2014CB845501. † zhicongliu@lbl.gov ‡ jqiang@lbl.gov Figure 1: A schematic plot of CPU and GPU. the CUDA library, the gridless symplectic multi-particle tracking code can be speeded up significantly. In this paper, firstly, the symplectic tracking theory and the GPU implements are introduced in Section 2. Then, the performance of the tracking code using CUDA is presented in Section 3. After that, an application example using this code is presented in Section 4. Finally, conclusions are drawn in Section 5. CODE IMPLEMENTATION For a 3D bunch, the space charge transfer map in one dircction (X) direction can be expressed as [10]:  xi(τ) = xi(0) pxi(τ) = pxi(0) − τ 1 ε0 8 abc ωκγ0||2017|10.18429/JACoW-IPAC2017-THPAB027|J. Qiang, Z. Liu|0.0|0
1780|Dynamic Clustering and GPU Based Parallel Processing Approach to Accelerate Circuit Simulation|In this digital era, electronic circuit is the key component and its design is tested and validated through simulator. Simulator uses mathematical model to replicate circuit behavior. All electronic designs rely truly on simulation software. But even though simulation is cost-effective; large circuit simulation is relatively time consuming. Also various iterations in transient analysis may make simulation slower. Fast simulator is the basic requirement for large circuit simulation. In this paper, we have addressed parallel computing approach using Graphics Processing Unit(GPU) to accelerate simulation. As GPU is many core processor, compute intensive functions are redesigned to execute on GPU. Matrix operations, linear-nonlinear equations, integration, differential equations, numerical methods are some of the very basic operations required in circuit analysis. Mathematical operations are redesigned to get clusters of sufficient size. Forming clusters of circuit components and mathematical procedures proves to be crucial, for reliable mapping to graphics processor. Loop replacement, data-code partitioning, parallel data mapping, reductions, fast memory access are the strategies adopted for parallel processing on GPU. More than 40% speed gain is achieved on circuit having at least four components and transient analysis for more than thousand iterations.|2018 3rd International Conference for Convergence in Technology (I2CT)|2018|10.1109/I2CT.2018.8529531|Shital V. Jagtap, Y. Rao|0.0|0
1783|Large-Scale Simulation of Structural Dynamics Computing on GPU Clusters|Structural dynamics simulation plays an important role in research on reactor design and complex engineering. The Hybrid Total Finite Element Tearing and Interconnecting (HTFETI) method combined with Newmark method is an efficient way to solve large-scale structural dynamics problems. However, the sparse direct solver and the load imbalance caused by inconsistent density models are two critical issues limiting the performance and the scalability of structural dynamics computing. For the former, we propose an efficient variable-size batched method to accelerate SpMV on GPUs. For the latter, we establish an online performance prediction model, based on which we then design a novel inter-cluster subdomain fine-tuning algorithm to balance the workload of HTFETI parallel computing. We are the first to achieve the high-fidelity structural dynamics simulation of China Experimental Fast Reactor core assembly with up to 53.4 billion grids. The weak and strong scalability efficiencies reach 91.77% and 86.13% on 12,800 GPUs, respectively.|International Conference on Software Composition|2023|10.1145/3581784.3607082|Ningming Nie, Yumeng Shi, Shigang Li, Yangang Wang, Shunde Li, Kehan Yao, Fang Liu, Jue Wang, Yangde Feng, Yue Gao, Chunbao Zhou, Kehao Lin, Yan Zeng|0.0|0
1784|Taming next‐generation HPC systems: Run‐time system and algorithmic advancements|This special issue of Concurrency and Computation: Practice and Experience contains revised and extended versions of selected papers presented at the 13th International Conference on Parallel Processing and Applied Mathematics, PPAM 2019, which was held on September 8–11, 2019 in Bialystok, Poland. PPAM 2019 was organized by the Department of Computer and Information Science of the Czestochowa University of Technology together with the Bialystok University of Technology, under the patronage of the Committee of Informatics of the Polish Academy of Sciences, in technical cooperation with the IEEE Computer Society and IEEE Computational Intelligence Society. PPAM is a biennial series of international conferences dedicated to exchanging ideas between researchers involved in parallel and distributed computing, including theory and applications, as well as applied and computational mathematics. Twelve previous events have been held in different universities in Poland since 1994, when the first PPAM took place in Czestochowa. Thus, the event in Bialystok was an opportunity to celebrate the 25th anniversary of PPAM. The focus of PPAM 2019 was on models, algorithms, and software tools that facilitate efficient and convenient use of modern parallel and distributed computing systems, as well as on large-scale modern applications, including advances in machine learning and artificial intelligence. This meeting gathered more than 170 participants from 26 countries. The accepted papers were presented at the regular tracks of the PPAM 2019 conference and during the workshops. With each submission evaluated by at least three reviewers, a strict reviewing process resulted in the acceptance of 91 contributed papers for publication in the conference proceedings, while approximately 43% of the submissions were rejected. The Program Committee selected 41 papers for presentation in the regular conference track, resulting in an acceptance rate of about 46%. Based on the review results, 10 papers (11% of submissions) were selected for a special journal issue. Besides quality, another important criterion for selection was each paper’s contribution to the thematic consistency of the issue. The focus of this special issue is on algorithmic advancements in matching the software properties to parallel architecture, including GPU accelerators and clusters. These advancements are crucial for successfully parallelizing such complex applications as simulating geophysical flows, solving ordinary differential equations (ODEs), structural analysis of nuclear reactor containment buildings, solving generalized eigenvalue problems, modeling of material science phenomena, and others. A complementary topic of this issue is advances in run-time systems since increasing levels of parallelism in multiand many-core chips and the emerging heterogeneity of computational resources coupled with energy, resilience, and data movement constraints radically increase the importance of efficient run-time scheduling and execution control. After the conference, the Program Committee invited the authors of selected papers to submit revised and extended versions of their works. These new versions were reviewed independently again by at least three reviewers. Finally, nine contributions were accepted for publication. They are summarized below. Paper [1] focuses on the accurate assembly of the system matrix, which is an essential step in any code that solves partial differential equations on a mesh. This step can become costly in multigrid codes requiring cascades of matrices that depend upon each other, or dynamic adaptive mesh refinement. To reduce the time to solution, the authors propose that these constructions can be performed concurrently with the multigrid cycles. Furthermore, they desynchronize the assembly from the solution process. This non-trivial increase in the concurrency level improves the scalability. As assembly routines are notoriously memoryand bandwidth-demanding, the final algorithmic enhancement uses a hierarchical, lossy compression scheme that brings the memory footprint down aggressively even when the system matrix entries carry little information or are not yet available with high accuracy. An efficient algorithm for the parallel solution of indefinite saddle point systems with iterative solvers based on the Golub–Kahan bidiagonalization is presented in Reference [2]. Such systems arise in many application fields, for example, in structural mechanics. A scalability study of the generalized solver shows improved performance for the two-dimensional (2D) Stokes equations compared to previous works. Furthermore, the authors investigate the performance of different parallel inner solvers in the outer Golub–Kahan iteration for a three-dimensional (3D) Stokes problem. When the number of cores is increasing for a fixed problem size, the solver exhibits good speedups of up to 50% with the 1024 cores. For the tests in which the problem size grows while the workload in each core stays constant, the performance of the solver scales almost linearly with the increase in the number of cores. Paper [3] proposes a locality optimization technique for the parallel solution on GPUs of large systems of ODEs by explicit one-step methods. This technique is based on tiling across the stages of a one-step method and is enabled by a special structure of the class of ODE systems—with the limited access distance. The paper focuses on increasing the range of access distances for which the tiling technique can provide a speedup|Concurrency and Computation|2021|10.1002/cpe.6153|B. Szymanski, R. Wyrzykowski|0.0|0
1785|Use of FEM in traffic accident analyses|Since the 1950s, in traffic accident analysis, forensic experts have been using simple trigonometric methods of force triangles solved only with a pencil, a plastic ruler and a logarithmic ruler. Later, with the massive expansion of pocket calculators in the 1970s and 1980s, calculations accelerated, but procedures remained the same. It was only with the boom of desktop PCs that simple computer programs began to be created from the beginning of the 1990s, which were algorithmically based on the methods used so far. With the wide rise of Windows OS, faster processors and powerful graphics cards, several successful programs have gradually developed since the beginning of the new millennium. Some of these programs were probably based on algorithmic engines of racing car game simulators. These engines at that time were increasingly worked with the true vehicle dynamics based on mechanical multi-body-systems (MBS). Among such programs, which have achieved commercial use, it is necessary to mention especially PC-crash and later also Virtual-crash, with much easier and intuitive control. Gradual variants of this software from version 1.0, which solved ground plan tasks only in 2D, through version 2.0 and especially very successful version 2.2, which worked excellently since 2005 even in 3D space, further versions 3.0 and 4.0 focused mainly on improving real graphics even with rendering and also modeling realistic terrain using Google orthophotomaps, which make it easy to model a road elevation and realistic relief of the surrounding terrain. However, MBS algorithms generally work with non-deformable models defined only by the simple outer envelope of the modeled object. However, such models usually do not contain any deformation zone elements by demanding procedures tuned in the development of real vehicles or other elements hidden under the hood (engine, transmission, cooling system components and many others). Usually, the stiffness of the individual vehicle body parts is also not known. In the case of vehicles, only the outer contours of their shape and, in some cases, their interior contours are entered. The whole vehicle model is then composed of several hundred or several thousand usually triangular faces of various sizes so that the simplified shapes of the vehicle are sufficiently modeled. A range of vehicle damage in these models is implemented by simplifying special procedures working only with estimating the depth of overlap of vehicles at the moment of collision (in milliseconds), estimating the impact restitution coefficient and estimating the socalled equivalent energy speed (EES). Other simplifications envisaged in MBS programs include a linear relationship between vehicle deformation and contact force. In addition, movement after impact is extremely sensitive to contact parameters, which is typical for nonlinear systems. This cause differences between the results of the experiment and the simulation. Despite of these facts, the procedures described above give relatively convincing results for most traffic accidents. It should be noted, however, that the expert by the application of these procedures always obtains “some” result, which does not always mean a||2019||P. Pavláta|0.0|0
1786|JefiPIC: A 3-D Full Electromagnetic Particle-in-Cell Simulator Based on Jefimenko's Equations on GPU|This paper presents a novel 3-D full electromagnetic particle-in-cell (PIC) code called JefiPIC, which uses Jefimenko's equations as the electromagnetic (EM) field solver through a full-space integration method. Leveraging the power of state-of-the-art graphic processing units (GPUs), we have made the challenging integral task of PIC simulations achievable. Our proposed code offers several advantages by utilizing the integral method. Firstly, it offers a natural solution for modeling non-neutral plasmas without the need for pre-processing such as solving Poisson's equation. Secondly, it eliminates the requirement for designing elaborate boundary layers to absorb fields and particles. Thirdly, it maintains the stability of the plasma simulation regardless of the time step chosen. Lastly, it does not require strict charge-conservation particle-to-grid apportionment techniques or electric field divergence amendment algorithms, which are commonly used in finite-difference time-domain (FDTD)-based PIC simulations. To validate the accuracy and advantages of our code, we compared the evolutions of particles and fields in different plasma systems simulated by three other codes. Our results demonstrate that the combination of Jefimenko's equations and the PIC method can produce accurate particle distributions and EM fields in open-boundary plasma systems. Additionally, our code is able to accomplish these computations within an acceptable execution time. This study highlights the effectiveness and efficiency of JefiPIC, showing its potential for advancing plasma simulations.||2022|10.2139/ssrn.4231193|Xueming Li, Haifeng Qiao, Yongtao Zhao, Jian-Nan Chen, Jun-Jie Zhang|0.0|0
1787|GPU-Based Parallel Algorithm for Solving Multiphase, Multicomponent Fluid Filtration Problem|Summary Hydrodynamic modeling of oil reservoir processes is one of the complex problems of fluid mechanics, since underground reservoir processes can be very complex. It is necessary to take into account phase transitions, chemical transformations, temperature effects, and etc. When simulating multiphase, multicomponent liquid filtration, the properties of the phase can vary depending on the composition of temperature and pressure. The oil phase consists of hydrocarbon components that range from the lightest (methane) to the heaviest (bitumen). You can simulate the process of multicomponent filtration, knowing the physical parameters of the pseudo-components (molecular weight, critical pressure, critical temperature, compressibility, density, viscosity, thermal conductivity and specific heat). Parallel algorithms implemented on graphic processors (GPUs) than on traditional processors (CPUs) are excellently suited to speed up such demanding tasks. In various fields of research, there have been many successful implementations on the GPU, such as medical image analysis and computational fluid dynamics. The GPU achieves high performance by executing more than a thousand threads at the same time, and each of them processes different data sets. The purpose of this paper is to implement the parallel algorithm on modern graphic processors (GPUs) for numerically solving a multiphase multicomponent fluid filtration problem in porous media, taking into account the number of phases and components. For the numerical solution of the problem, the alternating direction implicit method (ADI) was chosen. ADI is a finite difference numerical method for solving parabolic, hyperbolic and elliptic equations, and it is widely used in the fields of science and technology. In the ADI method, each numerical step is divided into several sub-steps, depending on the spatial dimension of the problem, and systems of linear equations are solved implicitly in one direction, with an explicit scheme in the other direction. In addition, at each sub-stage, the equations have a tridiagonal structure. To solve the tridiagonal system of equations, several parallel algorithms were implemented: cyclic reduction (CR) and parallel cyclic reduction (PCR) methods. And to implement the sequential algorithm, the Thomas implicit method was used. In this paper, to implement parallel algorithms CUDA technology and the OpenCL framework were used. The results of the study showed that the OpenCL framework is promising to use on any GPUs of any devices and get comparable results in terms of calculation time with CUDA. And to calculate parallel algorithms on CUDA, only GPUs from Nvidia are needed.||2020|10.3997/2214-4609.202035205|T. Imankulov, B. Daribayev, D. Akhmed-Zaki, O. Turar|0.0|0
1789|Exploiting BSP Abstractions for Compiler Based Optimizations of GPU Applications on multi-GPU Systems|Graphics Processing Units (GPUs) are accelerators for computers and provide massive amounts of computational power and bandwidth for amenable applications. While effectively utilizing an individual GPU already requires a high level of skill, effectively utilizing multiple GPUs introduces completely new types of challenges. This work sets out to investigate how the hierarchical execution model of GPUs can be exploited to simplify the utilization of such multi-GPU systems. \nThe investigation starts with an analysis of the memory access patterns exhibited by applications from common GPU benchmark suites. Memory access patterns are collected using custom instrumentation and a simple simulation then analyzes the patterns and identifies implicit communication across the different levels of the execution hierarchy. The analysis reveals that for most GPU applications memory accesses are highly localized and there exists a way to partition the workload so that the communication volume grows slower than the aggregated bandwidth for growing numbers of GPUs. \nNext, an application model based on Z-polyhedra is derived that formalizes the distribution of work across multiple GPUs and allows the identification of data dependencies. The model is then used to implement a prototype compiler that consumes single-GPU programs and produces executables that distribute GPU workloads across all available GPUs in a system. It uses static analysis to identify memory access patterns and polyhedral code generation in combination with a dynamic tracking system to efficiently resolve data dependencies. The prototype is implemented as an extension to the LLVM/Clang compiler and published in full source. \nThe prototype compiler is then evaluated using a set of benchmark applications. While the prototype is limited in its applicability by technical issues, it provides impressive speedups of up to 12.4x on 16 GPUs for amenable applications. An in-depth analysis of the application runtime reveals that dependency resolution takes up less than 10% of the runtime, often significantly less. \nA discussion follows and puts the work into context by presenting and differentiating related work, reflecting critically on the work itself and an outlook of the aspects that could be explored as part of this research. The work concludes with a summary and a closing opinion.||2020|10.11588/HEIDOK.00029213|Alexander Matz|0.0|0
1790|Three Dimensional Pseudo-Spectral Compressible Magnetohydrodynamic GPU Code for Astrophysical Plasma Simulation|This paper presents the benchmarking and scaling studies of a GPU accelerated three dimensional compressible magnetohydrodynamic code. The code is developed keeping an eye to explain the large and intermediate scale magnetic field generation is cosmos as well as in nuclear fusion reactors in the light of the theory given by Eugene Newman Parker. The spatial derivatives of the code are pseudo-spectral method based and the time solvers are explicit. GPU acceleration is achieved with minimal code changes through OpenACC parallelization and use of NVIDIA CUDA Fast Fourier Transform library (cuFFT). NVIDIA's unified memory is leveraged to enable oversubscription of the GPU device memory for seamless out-of-core processing of large grids. Our experimental results indicate that the GPU accelerated code is able to achieve upto two orders of magnitude speedup over a corresponding OpenMP parallel, FFTW library based code, on a NVIDIA Tesla P100 GPU. For large grids that require out-of-core processing on the GPU, we see a 7x speedup over the OpenMP, FFTW based code, on the Tesla P100 GPU. We also present performance analysis of the GPU accelerated code on different GPU architectures - Kepler, Pascal and Volta.|2018 IEEE 25th International Conference on High Performance Computing Workshops (HiPCW)|2018|10.1109/HIPCW.2018.8634104|R. Mukherjee, Bharatkumar Sharma, U. Maurya, N. Vydyanathan, V. Saini, R. Ganesh|0.0|0
1797|A New Focus+Context Visualization Technique for Inspecting Black Oil Reservoir Models|In this paper, we propose a new visualization technique to inspect simulated black oil reservoir models. Numerical reservoir simulation is widely used in the oil & gas industry to predict and plan the exploration of petroleum fields. Of particular interest, it is crucial to understand the physical phenomena around injector and producer wells: how is the distribution of pressure, how oil/gas/water saturation varies over time, and others. Traditional visualization techniques only provide local insights, being hard to understand the three-dimension physical behavior. Another challenge is to handle today’s massive models. We propose an efficient and effective focus+context visualization technique employing a cutaway approach, having the wells as objects of interest. We explore cone-shaped and box-shaped view-dependent cutting surfaces. We also allow the user to control the cutting surface aperture freely and to freeze the cut to gain motion parallax depth cues during model exploration. The proposed rendering algorithm runs on GPU, delivering real-time frame rate even for large reservoir models.|SIBGRAPI Conference on Graphics, Patterns and Images|2021|10.1109/sibgrapi54419.2021.00013|Luiz Felipe Netto, W. Celes|0.0|0
1809|Derivatives Sensitivities Computation under Heston Model on GPU|This report investigates the computation of option Greeks for European and Asian options under the Heston stochastic volatility model on GPU. We first implemented the exact simulation method proposed by Broadie and Kaya and used it as a baseline for precision and speed. We then proposed a novel method for computing Greeks using the Milstein discretisation method on GPU. Our results show that the proposed method provides a speed-up up to 200x compared to the exact simulation implementation and that it can be used for both European and Asian options. However, the accuracy of the GPU method for estimating Rho is inferior to the CPU method. Overall, our study demonstrates the potential of GPU for computing derivatives sensitivies with numerical methods.|Social Science Research Network|2023|10.2139/ssrn.4575924|Paul Bilokon, Pierre-Antoine Arsaguet|0.0|0
1818|Updating and Rendering Content on Objects in a three Dimensional Virtual Environment|The virtual reality applications are increasingly popular in social life from culture, education, health to entertainment. In a three-dimensional virtual environment, the features of objects such as shape, color, movement, etc. are digitized and simulated using basic computer graphics techniques. In addition to the above characteristics, objects can contain content which changes in the form of images or text, with diverse information. The problem is that these content should be rendered on the virtual object, so that both the content and the real time of the application can be satisfied. In this paper we present an approach for updating and rendering content on objects in a three-dimensional virtual environment based on GPU architecture.||2017|10.22161/IJAERS.4.12.12|Le Viet Anh, N. Kien|0.0|0
1821|Biomaker CA: a Biome Maker project using Cellular Automata|We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA). In Biomaker CA, morphogenesis is a first class citizen and small seeds need to grow into plant-like organisms to survive in a nutrient starved environment and eventually reproduce with variation so that a biome survives for long timelines. We simulate complex biomes by means of CA rules in 2D grids and parallelize all of its computation on GPUs through the Python JAX framework. We show how this project allows for several different kinds of environments and laws of 'physics', alongside different model architectures and mutation strategies. We further analyze some configurations to show how plant agents can grow, survive, reproduce, and evolve, forming stable and unstable biomes. We then demonstrate how one can meta-evolve models to survive in a harsh environment either through end-to-end meta-evolution or by a more surgical and efficient approach, called Petri dish meta-evolution. Finally, we show how to perform interactive evolution, where the user decides how to evolve a plant model interactively and then deploys it in a larger environment. We open source Biomaker CA at: https://tinyurl.com/2x8yu34s .|arXiv.org|2023|10.48550/arXiv.2307.09320|A. Mordvintsev, E. Randazzo|0.0|0
1826|Simulation Test of Sensor Response Speed of EMG Signal Based on Finite Element Analysis|In order to test the attention distribution training on the effect of Sanshou athletes training, this paper presents a new signal acquisition and processing technology in the process of action reaction speed and attack accuracy analysis, the technology uses electrical tomography technology (ET) to carry on sensor arrangement for the muscle EMG signal, and the obtained image carries out the finite element reconstruction. In order to meet the demand of large data calculation, using GPU as a computing processor optimizes the computing model using clustering algorithm, and using VC++ software carries out programming design of the algorithm. Through the image reconstruction of Sanshou technology action, we can get the significant analysis results of attention distribution training on Sanshou technology action reaction speed and attack accuracy, which provides a new computer method for the Sanshou athletes training.||2015|10.2991/AMCCE-15.2015.172|Nai Wang|0.0|0
1832|Anisotropic numerical potentials for coarse‐grained modeling from high‐speed multidimensional lookup table and interpolation algorithms|A high‐speed numerical potential delivering computational performance comparable with complex coarse‐grained analytic potentials makes available models that have greater degrees of physical and chemical accuracy. This opens the possibility of increased accuracy in classical molecular dynamics simulations of anisotropic systems. In this work, we report the development of a high‐speed lookup table (LUT) of four‐dimensional gridded data, that uses cubic B‐spline interpolations to derive off grid values and their associated partial derivatives that are located between the known grid data points. The accuracy of the coarse‐grained numerical potential using a LUT from uniaxial Gay–Berne (GB) potential produced array of values is within a 3% and a 5% margin of error respectively for the interpolation of the uniaxial GB potential and its partial derivatives. The numerical potential model and partial derivatives speedup is made competitive with the analytical potential by exploiting graphics processing units on board functionality. The capability of the numerical potential is demonstrated by comparing minimizations of a box of 500 naphthalene molecules. The minimizations using a full atomistic (NAMD/CHARMM force field), a biaxial GB and a numerical potential from a LUT using data from the CHARMM pair potential was done. The numerical potential model is significantly more accurate in its approximation of the atomistic local minimum configuration than is the biaxial GB analytical potential function. This demonstrates that using a numerical potential founded on a direct lookup of the atomistic potential landscape significantly improves coarse grain (CG) modeling of complex molecules, possibly paving the way for accurate anisotropic system CG modeling.|Journal of Computational Chemistry|2021|10.1002/jcc.26487|S. Winberg, K. Naidoo, Ananya Gangopadhyay|0.0|0
1837|DISCUSSING THE PROPRIETIES OF THE CONVENTIONAL ASSESSMENT OF FLOOD CONTROL INVESTMENT FOCUSED ON THE UNDEVELOPED AREA|Large rivers running through arid regions but originating in tropical rain forests are considered as precious water resources along their riverbanks. However, upstream inundation due to flooding during the rainy season is a constraint for development. When flood control projects are proposed for such areas, technical hazards exist. To introduce a flood control project, Cost-Benefit Analysis (CBA) is used to evaluate the project’s effectiveness based on computational fluid dynamics (CFD) simulation that is widely adopted for decision-making. Numerous studies applying shallow water models have focused on small urbanized areas. These studies employed fine-gridded digital elevation model (DEM) in the CFD simulation for precise evaluation. In contrast, for simulating large riverbanks of undeveloped areas, coarse-gridded DEM must be used to reduce the computational time. However, this does not consider the micromorphology. Resultantly, reproducibility of the simulation is degraded. The bank of the Senegal River was selected as our study site. Conventional CFD simulation was carried out. A coarse-gridded DEM was applied to reduce the computational time, but did not show enough reproducibility. We tried to employ General Purpose Computing on Graphics Processing Unit (GPGPU), a parallel computing method, with a fine-gridded DEM. It improved the reproducibility. In addition, the preferred conventional CBA method of the Japan International Cooperative Agency was applied to the study site. Conventional CBA was developed to assess the flood control investments mostly in urbanized areas. When applying it to undeveloped areas that have lower asset values than urbanized areas, as expected, the evaluated benefit was lower than the cost of the project. However, agricultural productivity contributes to improved food security and trade balance of the country. The flood control investment related with agricultural development should consider those externalities as benefits. This motivates us to develop an appraisal method in future research.||2018|10.2495/FRIAR180151|B. Ahmed, Yukihiro Maruyama, Takashi Nakamura, Cherif O. Ahmed, K. Ujiie, M. Irie|0.0|0
1841|Protocols for Multi-Scale Molecular Dynamics Simulations: A Comparative Study for Intrinsically Disordered Amyloid Beta in Amber & Gromacs on CPU & GPU|Intrinsically disordered proteins (IDPs) present challenges to conventional experimental techniques due to their large-scale conformational fluctuations and the transient occurrence of structural elements. This work illustrates computational methods for studying IDPs at various levels of resolution. The included simulation protocol offers a step-by-step guide on how to conduct molecular dynamics (MD) simulations and analyze the results using the Amber and Gromacs packages, employing both all-atom and coarse-grained approaches. This protocol can be easily adapted to study other biomacromolecules, including folded and disordered proteins and peptides. Furthermore, it is discussed in this work how to perform standard molecular modeling operations, such as amino-acid substitutions (mutagenesis) and insertions of residues missing in a protein structure, as well as how to incorporate post-translational modifications into the simulations, such as disulfide bonds, which are often crucial for proteins to attain their physiologically functional structure. In conventional MD studies, disulfide bonds are typically fixed at the preparation step and remain unchanged throughout the simulations, unable to break or reform. Here, in contrast, a dynamic approach is presented. It involves adequate distance restraints applied to the sulfur atoms of selected cysteine residues, allowing disulfide bonds to break and reform during the simulation. The effectiveness of these methodologies is demonstrated by examining a model IDP, the monomeric form of 1-42 amyloid-β (Aβ42), both with and without disulfide bonds, at different levels of resolution. This study not only contributes to our understanding of the role of disulfide bonds but also provides detailed simulation protocols that can serve as a foundation for future investigations. SUMMARY Given the challenges of experimental studies on intrinsically disordered proteins, this manuscript demonstrates step-by-step protocols for conducting all-atom and coarse-grained molecular dynamics simulations using two widespread packages, Amber and Gromacs. The monomeric form of 1-42 amyloid-β (Aβ42) is used as an example, from which insights into the structure, dynamics and physicochemical properties of this protein can be obtained.|bioRxiv|2024|10.1101/2023.10.24.563575|Pawel Krupa, Pamela Smardz, B. Różycki, Pawel Rogowski, Mai Suan Li, Midhun Mohan Anila|0.0|0
1847|Contributions to Simulation of Modelica Models on Data-Parallel Multi-Core Architectures|Modelica is an object-oriented, equation-based modeling and simulation language being developed through an international effort by the Modelica Association. With Modelica it is possible to build computationally demanding models; however, simulating such models might take a considerable amount of time. Therefore techniques of utilizing parallel multi-core architectures for faster simulations are desirable. In this thesis the topic of simulation of Modelica on parallel architectures in general and on graphics processing units (GPUs) in particular is explored. GPUs support code that can be executed in a data-parallel fashion. It is also possible to connect and run several GPUs together which opens opportunities for even more parallelism. In this thesis several approaches regarding simulation of Modelica models on GPUs and multi-core architectures are explored. In this thesis the topic of expressing and solving partial differential equations (PDEs) in the context of Modelica is also explored, since such models usually give rise to equation systems with a regular structure, which can be suitable for efficient solution on GPUs. Constructs for PDE-based modeling are currently not part of the standard Modelica language specification. Several approaches on modeling and simulation with PDEs in the context of Modelica have been developed over the years. In this thesis we present selected earlier work, ongoing work and planned work on PDEs in the context of Modelica. Some approaches detailed in this thesis are: extending the language specification with PDE handling; using a software with support for PDEs and automatic discretization of PDEs; and connecting an external C++ PDE library via the functional mockup interface (FMI). Finally the topic of parallel skeletons in the context of Modelica is explored. A skeleton is a predefined, generic component that implements a common specific pattern of computation and data dependence. Skeletons provide a high degree of abstraction and portability and a skeleton can be customized with user code. Using skeletons with Modelica opens up the possibility of executing heavy Modelica-based matrix and vector computations on multi-core architectures. A working Modelica-SkePU library with some minor necessary compiler extensions is presented. This work has been supported by the European ITEA2 OPENPROD project (Open Model-Driven Whole-Product Development and Simulation Environment), the European ITEA3 MODRIO project (Model Driven Physical Systems Operation) and by the National Graduate School of Computer Science (CUGS)||2015||K. Stav|0.0|0
1848|GPU-accelerated high-fidelity simulation of beam-beam effects in particle colliders|Particle colliders are essential tools for understanding the fundamental structure of matter in physical science. In circular colliders, two counter rotating beams, each containing hundreds of millions of particles moving at nearly the speed of light are forced to collide at each turn. Numerical simulation of this beam-beam effects at each turn relies on matrix-based, arbitrary-order symplectic particle tracking for beam transport and the generalized Basetti-Erskine approximation for beam-beam interaction. Serial, or even naively parallel implementation of this simulation is prohibitively costly in terms of efficiency and computational requirements, necessitating simulation times on the order of months. In this paper, we present a high-performance, high-fidelity model for simulation of beam-beam effects in particle colliders using GPUs. The parallel simulation algorithm implemented on NVIDIA Tesla K40 GPU delivers two to three orders-of-magnitude speedup when compared to a non-optimized sequential simulation. With the parallel simulation model resulting in orders-of-magnitude reduction in the computation time, previously computationally prohibitive long-term simulations become tractable.|Summer Simulation Multiconference|2017|10.25777/GAM8-E879|A. Godunov, R. Majeti, M. Zubair, B. Terzić, D. Ranjan, K. Arumugam|0.0|0
1849|Simulated Diffusion in Realistic Imaging Features of Tissue (Sim-DRIFT)|Summary This library, simDRIFT , provides for rapid and flexible Monte-Carlo simulations of Pulsed Gradient Spin Echo (PGSE) Diffusion-Weighted Magnetic Resonance Imaging (DWI) experiments, which we expect to be useful for DWI signal processing model development and validation purposes. The primary focus of this library is forward simulations of molecular self-diffusion processes within an ensemble of nuclear magnetic resonance (NMR) active nuclei (“spins”) residing in complex, biophysical tissue systems. To achieve a large variety of tissue configurations, simDRIFT provides support for 𝑛 fiber bundles (with user-defined radii, intrinsic diffusivities, orientation angles, and densities) and 𝑚 cells (with user-defined radii and volume fractions). simDrift is written in Python (Python Software Foundation (VanRossum & Drake, 2010)) and supported by a Numba (Lam et al., 2015) backend. Thus, simDRIFT benefits from Numba’s CUDA API, allowing the simulation of individual spin trajectories to be performed in parallel on single Graphics Processing Unit (GPU) threads. The resulting performance gains support simDRIFT ’s aim to provide a customizable tool for the rapid prototyping of diffusion models, ground-truth model validation, and in silico phantom production.|Journal of Open Source Software|2023|10.21105/joss.05621|Jacob Blum, K. Utt|0.0|0
1853|Experimental Study of the Model for Predicting the Performance of a Heterogeneous Computer System in Telecommunications|The issue of creating high-performance computing systems based on heterogeneous computer systems is relevant, since the volume of processed information, calculations and studies with large data sets is constantly increasing. The aim of the work is to develop a model for predicting the performance of heterogeneous computing systems and its experimental evaluation in the simulation of access to the memory and in modeling fundamental parallel algorithms. As a result, the use of the developed model allows making an adequate estimate of the time of the parallelized task execution using heterogeneous computer systems based on graphics processors.|2018 Dynamics of Systems, Mechanisms and Machines (Dynamics)|2018|10.1109/DYNAMICS.2018.8601478|Y. Kropotov, A. A. Kolpakov|0.0|0
1855|Computational models of morphology's effects on cellular dynamics|Spatial effects such as cell shape, internal cellular organisation and cellular plasticity have very often been considered negligible in models of cellular pathways, and many existing simulation infrastructures do not take such effects into consideration. However, recent experimental results and systems level theories suggest that even small variations in shape can make a large difference to the fate of the cell. This is particularly the case when considering eukaryotic cells, which have a complex physical structure and many subtle control mechanisms. Bacteria are also interesting for their variation in shape, both between species and in different states of adaptation. \n \nIn this thesis we perform simulations that quantify the effect of three aspects of morphology - external cellular shape, internal cellular organisation and processes that change the shape of the cell - on the behaviour of model cellular pathways. To perform these simulations we develop Reaction-Diffusion Cell (ReDi-Cell), a highly scalable General Purpose Graphics Processing Unit Computing (GPGPU) cell simulation infrastructure for the modelling of cellular pathways in spatially detailed environments. ReDi-Cell is validated against known-good simulations, prior to its use in new work. \n \nBy measuring reaction trajectories and concentration gradients we quantify the responses of simulated cellular pathways to these three spatial aspects. Our results show that model cell behaviour is the composite of cellular morphology and reaction system. Different reaction systems display different dynamics even when placed in identical environments. \n \nTraditionally, computational approaches to cell biology have been focussed upon investigating how changes to reaction dynamics alter cellular behaviour. This thesis, on the other hand, demonstrates another way in which reaction dynamics can be altered, by changing the morphology of the cell.||2016|10.1016/j.biosystems.2016.05.012|Faiz Sayyid|0.0|0
1863|PARALLEL IMPLEMENTATION OF THE METHOD OF GRADIENT BOOSTING|The issue of machine learning has been paying more attention in all areas of information technology in recent times. On the one hand, this is due to the rapid growth of requirements for future specialists, and on the other with the very rapid development of information technology and Internet communications. One of the main tasks of e-learning is the task of classification. For this type of task, the method of machine learning called gradient boost is very well suited. Grading boosting is a family of powerful machine learning algorithms that have proven significant success in solving practical problems. These algorithms are very flexible and easily customized for the specific needs of the program, for example, they are studied in relation to different loss functions. The idea of boosting is the iterative process of sequential building of private models. Each new model learns based on information about errors made in the previous stage, and the resulting function is a linear combination of the whole ensemble of models, taking into account minimization of any penalty function. The mathematical apparatus of gradient boosting is well adapted for the solution of the classification problem. However, as the number of input data increases, the issue of reducing the construction time of the ensemble of decision trees becomes relevant. Using parallel computing systems and parallel programming technologies can produce positive results, but requires the development of new methods for constructing gradient boosting. The article reveals the main stages of the method of parallel construction of gradient boosting for solving the classification problem in e-learning. Unlike existing ones, the method allows to take into account the features of architecture and the organization of parallel processes in computing systems with shared and distributed memory. The method takes into account the possibility of evaluating the efficiency of building an ensemble of decision trees and parallel algorithms. Obtaining performance indicators for each iteration of the method helps to select the rational number of parallel processors in the computing system. This allows for a further reduction of the completion time of the gradient boosting. The simulation with the use of MPI parallel programming technology, the Python programming language for the architecture of the DM-MIMD system, confirms the reliability of the results. Here is an example of the organization of input data. Presented by Python is a program for constructing gradient boosting. The developed visualization of the obtained estimates of performance indicators allows the user to select the necessary configuration of the computing system.|Advanced Information Systems|2018|10.20998/2522-9052.2018.3.03|O. Tolstoluzka, Bogdan Parshencev, O. Moroz|0.0|0
1865|Analysis on the Active/Inactive Status of Computational Resources for Improving the Performance of the GPU|In recent high performance computing system, GPGPU has been widely used to process general-purpose applications as well as graphics applications, since GPU can provide optimized computational resources for massive parallel processing. Unfortunately, GPGPU doesn’t exploit computational resources on GPU in executing general-purpose applications fully, because the applications cannot be optimized to GPU architecture. Therefore, we provide GPU research guideline to improve the performance of computing systems using GPGPU. To accomplish this, we analyze the negative factors on GPU performance. In this paper, in order to clearly classify the cause of the negative factors on GPU performance, GPU core status are defined into 5 status: fully active status, partial active status, idle status, memory stall status and GPU core stall status. All status except fully active status cause performance degradation. We evaluate the ratio of each GPU core status depending on the characteristics of benchmarks to find specific reasons which degrade the performance of GPU. According to our simulation results, partial active status, idle status, memory stall status and GPU core stall status are induced by computational resource underutilization problem, low parallelism, high memory requests, and structural hazard, respectively.||2015|10.5392/JKCA.2015.15.07.001|Jong-Myon Kim, D. Son, H. Choi, C. Kim|0.0|0
1868|On GPU optimizations of stencil codes for highly parallel simulations|Stencil codes are valuable methods to solve partial differential equations of models in a wide range of applications in science and engineering. Graphics processing units (GPUs) provide a highly parallel architecture with fast directly accessible memory that is desirable to run stencil codes. They enable larger and more complex simulations that are solved faster compared to simulating on CPUs. We provide a solution for users to run highly parallel stencil codes on GPUs, that can also be efficiently used on a distributed GPU cluster.In this work, we present an extension to the multi-disciplinary framework NAStJA originally designed to efficiently run stencil codes on the CPUs in current high-performance computing (HPC) systems, to also be able to run on GPUs. We describe different methods to increase the performance of stencil codes on GPUs like a border exchange method which is transparent to the user, as well as a buffer for gradient values that are needed multiple times. We show their performance using the phase-field method as a stencil code example.With this GPU extension and optimizations implemented into the NAStJA framework, we can show highly improved performance compared to the CPU implementation, and an efficiency of 92% (weak scaling) for a large-scale example simulation on 64 GPUs on the ForHLR II HPC system.|International Euromicro Conference on Parallel, Distributed and Network-Based Processing|2021|10.1109/PDP52278.2021.00043|Nikolai Pfisterer, A. Streit, M. Berghoff|0.0|0
1869|Innovations in Computer Technologies Have Impacted Radiation Dosimetry Through Anatomically Realistic Phantoms and Fast Monte Carlo Simulations.|Radiological physics principles have not changed in the past 60 y when computer technologies advanced exponentially. The research field of anatomical modeling for the purpose of radiation dose calculations has experienced an explosion in activity in the past two decades. Such an exciting advancement is due to the feasibility of creating three-dimensional geometric details of the human anatomy from tomographic imaging and of performing Monte Carlo radiation transport simulations on increasingly fast and cheap personal computers. The advent of a new type of high-performance computing hardware in recent years-graphics processing units-has made it feasible to carry out time-consuming Monte Carlo calculations at near real-time speeds. This paper introduces the history of three generations of computational human phantoms (the stylized medical internal radiation dosimetry-type phantoms, the voxelized tomographic phantoms, and the boundary representation deformable phantoms) and new development of the graphics processing unit-based Monte Carlo radiation dose calculations. Examples are given for research projects performed by my students in applying computational phantoms and a new Monte Carlo code, ARCHER, to problems in radiation protection, imaging, and radiotherapy. Finally, the paper discusses challenges and future opportunities for research.|Health Physics|2019|10.1097/HP.0000000000001007|X. George Xu|0.0|0
1873|Implementation of the QUBE Force Field in SOMD for High-Throughput Alchemical Free-Energy Calculations|The quantum mechanical bespoke (QUBE) force-field approach has been developed to facilitate the automated derivation of potential energy function parameters for modeling protein-ligand binding. To date, the approach has been validated in the context of Monte Carlo simulations of protein-ligand complexes. We describe here the implementation of the QUBE force field in the alchemical free-energy calculation molecular dynamics simulation package SOMD. The implementation is validated by demonstrating the reproducibility of absolute hydration free energies computed with the QUBE force field across the SOMD and GROMACS software packages. We further demonstrate, by way of a case study involving two series of non-nucleoside inhibitors of HIV-1 reverse transcriptase, that the availability of QUBE in a modern simulation package that makes efficient use of graphics processing unit acceleration will facilitate high-throughput alchemical free-energy calculations.|Journal of Chemical Information and Modeling|2021|10.26434/chemrxiv.13116878|D. Cole, A. Mey, Sofia Bariami, Joshua T. Horton, C. Ringrose, J. Michel, Vadiraj Kurdekar, Lauren Nelson|0.0|0
1877|A Performance Study of Moving Particle Semi-Implicit Method for Incompressible Fluid Flow on GPU|The aim of moving particle semi-implicit (MPS) is to simulate the incompressible flow of fluids in free surface. MPS, when implemented, consumes a lot of time and thus, needs a very powerful computing system. Instead of using parallel computing system, the performance level of the MPS model can be improved by using graphics processing units (GPUs). The aim is to have a computing system that is capable of performing at high levels thereby enhancing the speed of processing the numerical computations required in MPS. The primary aim of the study is to build a GPU-accelerated MPS model using CUDA aimed at reducing the time taken to perform the search for neighboring particles. In order to increase the GPU processing speed, specific consideration is given towards the optimization of a neighboring particle search process. The numerical model of MPS is performed using the governing equations, notably the Navier-Stokes equation. The simulation model indicates that using GPU based MPS produce better performance compared to the traditional arrangement of using CPUs.|Int. J. Distributed Syst. Technol.|2020|10.4018/ijdst.2020010107|Kirankumar V. Kataraki, Satyadhyan Chickerur|0.0|0
1885|Beyond ExaBricks: GPU Volume Path Tracing of AMR Data|Adaptive Mesh Refinement (AMR) is becoming a prevalent data representation for scientific visualization. Resulting from large fluid mechanics simulations, the data is usually cell centric, imposing a number of challenges for high quality reconstruction at sample positions. While recent work has concentrated on real-time volume and isosurface rendering on GPUs, the rendering methods used still focus on simple lighting models without scattering events and global illumination. As in other areas of rendering, key to real-time performance are acceleration data structures; in this work we analyze the major bottlenecks of data structures that were originally optimized for camera/primary ray traversal when used with the incoherent ray tracing workload of a volumetric path tracer, and propose strategies to overcome the challenges coming with this.|arXiv.org|2022|10.48550/arXiv.2211.09997|Alper Sahistan, Kwan-Liu Ma, Stefan Zellmann, Qi Wu, I. Wald|0.0|0
1888|Continental-Scale Convection-Resolving Climate Simulations on Heterogeneous Supercomputers|With global climate change, the hydrological cycle of Planet Earth will likely undergo dramatic changes. The increase in atmospheric water vapor due to global warming is expected to lead to globally increasing precipitation amounts and, as basic physical principles suggest, to increases in precipitation extremes with potentially serious implications. Understanding and describing the involved processes, estimating potential future changes, and assessing the underlying uncertainties has proven to be difficult and complex. In this effort, numerical models are useful tools. However, in state-of-the-art climate models, the representation of clouds and moist convection remains a major challenge in particular the representation of the scale interactions between large-scale synoptic weather systems at scales of many 1000 km, and smallscale turbulent and convective processes, acting at scales around and below 1 km. For instance, difficulties arise from representing the scales between O(10 km) and O(100 km) where individual convective cells organize into meso-scale weather systems. Currently global and regional climate models typically operate at grid spacings on the order of 10-300 km, and thus many of these processes and their interactions are not explicitly represented. Refining the grid spacing to the kilometer-scale allows explicitly resolving deep convection. However, performing multi-year simulations at this ”convection-resolving” resolution is computationally still extremely demanding, and thus climate simulations at kilometer-scale resolutions have so far largely been limited to sub-continental computational domains, and/or to very small integration periods. In recent years, developments in the supercomputing domain have lead to compute node designs that mix multi-core CPUs and accelerators, such as graphics processing units (GPUs). These new supercomputer architectures possess properties beneficial for weather and climate models. However to fully make use of these innovations, the model codes have to be adapted and in some cases largely be rewritten. In this study a new version of the COSMO weather and climate model (Consortium for Small-Scale Modeling) is used, which is capable of using GPU accelerators. This thesis has contributed to the development of this model, has established its climate version, is validating its results, and is assessing its computational performance. Altogether the thesis is demonstrating the high potential of these new hardware platforms and codes for climate simulations. In chapter 2, a set of week to season-long simulations are conducted, using the new COSMO version. They include intermediate-resolution simulations, with a grid spacing of 12 km, on a mesh with 355×355×60 grid points, and nested convection-resolving simulations with a grid spacing of 2.2 km, employing 1536×1536×60 grid points. A case study, including winter storm Kyrill, shows that a convection-resolving simulation displays a high level of agreement with a corresponding intermediate-resolution simulation, which employs a parameterization scheme for deep convection. However, the agreement is limited to the synoptic and mesoalpha-scale development. Substantial differences are found in the representation of meso-scale atmospheric circulations and for their interactions with the synoptic-scale flow. In particular it displays narrow cold frontal rainbands embedded into the cold front of the Kyrill storm, and a more realistic representation of small-scale vortices over the ocean. A three-month long||2016|10.3929/ETHZ-A-010877641|D. Leutwyler|0.0|0
1891|Accelerating Stochastic Simulations on GPUs Using OpenCL|Since first introduced in 2008 with the 1.0 specification, OpenCL has steadily evolved over the decade to increase its support for heterogeneous parallel systems. In this paper, we accelerate stochastic simulation of biochemical reaction networks on modern GPUs (graphics processing units) by means of the OpenCL programming language. In implementing the OpenCL version of the stochastic simulation algorithm, we carefully apply its data-parallel execution model to optimize the performance provided by the underlying hardware parallelism of the modern GPUs. To evaluate our OpenCL implementation of the stochastic simulation algorithm, we perform a comparative analysis in terms of the performance using the CPU-based cluster implementation and the NVidia CUDA implementation. In addition to the initial report on the performance of OpenCL on GPUs, we also discuss applicability and programmability of OpenCL in the context of GPU-based scientific computing. key words: GPU computing, OpenCL, parallel programming, stochastic simulation|IEICE Trans. Inf. Syst.|2019|10.1587/transinf.2019edl8030|Pilsung Kang|0.0|0
1892|A New Simulation Method for UAV Communication Channels Based on GPUs|In this paper, an unmanned aerial vehicle (UAV) communication channel model is established by considering the propagation path loss, shadowing, and multi-path fading. Moreover, an efficient generation method for Gaussian random processes based on sum of sinusoids (SoS) theory is presented and is easy to realize by a graphics processing unit (GPU). Based on the proposed method, a new real-time generation method for multi-path shadowing composite fading is designed and implemented. The implementation results show that the proposed approach enables the easy generation of multi-path shadowing composite fading while reducing the processing time. Meanwhile, the impacts of flight altitude and communication scenarios on the performance of the UAV communication system are discussed. The results of this paper should have significant application value in UAV communication channel simulation.|International Journal of Modeling and Optimization|2018|10.7763/ijmo.2018.v8.641|Weizhi Zhong, Qiuming Zhu, Xujun Hu, N. Astronautics, Bin Chen, Xiaomin Chen|0.0|0
1894|Powderworld: A Platform for Understanding Generalization via Rich Task Distributions|One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating challenges distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models and certain reinforcement learning agents, yet may inhibit learning in high-variance environments. Powderworld aims to support the study of generalization by providing a source of diverse tasks arising from the same core rules.|International Conference on Learning Representations|2022|10.48550/arXiv.2211.13051|Kevin Frans, Phillip Isola|0.0|0
1899|Research and Simulate of Real-Time Particle Systems Based on GPU-Acceleration|Study on realistic simulation of natural scenery has always been one of the difficult and focused problems in the field of computer graphics. In order to solve the problem of time-consuming and inaccurate simulation of particle system, this paper proposed a particle system simulation based on GPU acceleration. This method used the geometric modeling of the particle system to extract the external characteristics of the flame and analyzed the influence of two external forces of gravity and wind force. It also introduced the simplex noise to control the particle behavior, and combined texture mapping, particle collision and other technologies to make sure better realistic quality and high efficiency. Experiments testify that the method can satisfy the requirement of the reality and the real-time of fire simulation, which got more realistic simulation effect. And further improvements had been made compared with the traditional flame simulation method.|2019 International Conference on Computer Network, Electronic and Automation (ICCNEA)|2019|10.1109/ICCNEA.2019.00035|Junnan Liu, Peng Yin, Jie Li|0.0|0
1900|Analysis of Cache Memories in Highly Parallel Systems|Though advances in VLSI technology will soon make it practical to construct parallel processors consisting of thousands of processing elements (PEs) sharing a central memory, the performance of these parallel processors is limited by the high memory access time due to interconnect network latency. This thesis is a study of how the performance of a parallel processor is affected by associating a cache memory with each PE of the system. Cache parameters and policies are varied and the performance of the resulting cache configurations are compared. The cache coherence problem is discussed and a solution that is compatible with the philosophy of parallel systems is adopted. \nPerformance is analyzed by analytic and simulation models. Due to time and space limitations the simulation modeling is done in a hierarchical fashion: a primary level simulates a single cache and a secondary level simulates a parallel machine. The simulators can run in a trace-driven and self-driven mode. The trace data used to drive the simulators was collected by tracing the reference patterns of actual parallel programs. An approximate analytic model is developed that predicts the queue waiting times of various components of a parallel system, enabling the comparison of a water range of cache parameters than is possible with the simulators.||2015|10.1002/9781118785317.weom080132|K. McAuliffe|0.0|0
1904|Acceleration of large-scale multi-physics simulation for biomedical EMC with manycore architecture based computing|Recently, computer simulation scale for biomedical EMC becomes extremely huge, because numerical models, which represent shape, structure, configuration, etc., of biological bodies, increase these preciseness. Parallelizing simulation code is one of the effective solutions to deal with such a high definition and massive scale problem. Finite difference methods, frequently used for solving biomedical EMC problem, are classified as the stencil scheme which has the nature of memory-bound feature. Hence, it is desirable to employ many-core architecture with the high-speed memory bandwidth (approximately 200-300GB/s) such as the graphics processing unit (GPU) or the many integrated core (MIC) hardware accelerators. The purpose of this study is to accelerate the multiphysics simulation system for numerical dosimetries by the many-core architecture based computing technique.||2015|10.1109/URSI-AT-RASC.2015.7302949|K. Wake, Masayo Takamura, Soichi Watanabe, C-Y. Tsai, H. Sasaki, M. Sasaki, R. Imai, M. Kojima, J. Chakarothai, M. Taki, Y. Suzuki, S. Onishi, K. Sasaki|0.0|0
1905|Batched Small Tensor-Matrix Multiplications on GPUs|We present a fine-tuned library, ZTMM, for batched small tensor-matrix multiplication on GPU architectures. Libraries performing optimized matrix-matrix multiplications involving large matrices are available for many architectures, including a GPU. However, these libraries do not provide optimal performance for applications requiring efficient multiplication of a matrix with a batch of small matrices or tensors. There has been recent interest in developing fine-tuned libraries for batched small matrix-matrix multiplication - these efforts are limited to square matrices. ZTMM supports both square and rectangular matrices. We experimentally demonstrate that our library has significantly higher performance than cuBLAS and Magma libraries. We demonstrate our library's use on a spectral element-based solver called CMT-nek that performs high-fidelity predictive simulations using compressible Navier-Stokes equations. CMT-nek involves three-dimensional tensors, but it is possible to apply the same techniques to higher dimensional tensors.|International Conference on High Performance Computing|2020|10.1109/HiPC50609.2020.00044|Tania Banerjee-Mishra, A. Wijayasiri, Keke Zhai, S. Ranka|0.0|0
1906|Code optimization by using GPU applied to a Dataflow numerical simulation model|This work aims to study techniques for parallel computing using GPU (Graphics Processing Unit) in order to optimize the performance of a fragment of computational code, implemented as a Dataflow system, which is part of a meteorological numerical model responsible for calculating the advection transportation phenomena. The possible algorithm limitations for GPU efficiency will also be addressed through an extensive code instrumentation. Considering the difficulties found on the original algorithm which implies a GPU accelerated code dealing with flow dependencies and coarse grain parallelism, the performance gain with GPU may be considered fair.||2015|10.1088/1742-6596/649/1/012003|Á. Fazenda, Luiz Eduardo Souza Evangelista, Vincius V de Melo|0.0|0
1907|Study of Platform Passenger Evacuation Simulation Based on GPU|Pedestrian Evacuation Simulation can be used to support developing the evacuation solutions for subway stations. Although the pedestrian evacuation based on general purpose processors acquire good effect when the simulation scale is very large. We investigated the effect of the layout of subway stations on the pedestrian evacuation time through simulation. Based on the cellular automaton model, we partitioned the platform space into a two-dimensional grid structure. The walking path of pedestrians can be represented as movement on the grid according to a specified rule. Although the weighted shortest path algorithm is an appropriate routing rule for pedestrians, it puts pressure on the CPU simulation performance. To solve this problem, we introduced GPGPU as the parallel computing hardware to accelerate the simulation process. Experimental results show that the computation time of each time-step of the simulation on GPU is 20 times faster than on CPU. The real time simulation can support rapid decision-making and department response to emergencies.||2016|10.2991/NCEECE-15.2016.13|Zhiyong Cai, Qianni Deng|0.0|0
1915|Implementation of Massive FDTD Simulation Computing Model Based on MPI Cluster for Semi-conductor Process|반도체 공정에서는 소자 내부의 물리량 계산을 통해 불순물의 움직임을 해석하여 결점을 검출하는 시뮬 레이션을 수행하게 된다. 이를 위해 유한 차분 시간 영역 알고리즘(Finite-Difference Time-Domain, 이하 FDTD)과 같은 수치해석 기법이 사용된다. 반도체 칩의 집적도 향상으로 인하여 소자의 크기는 나노스케 일 시대로 접어들었으며, 시뮬레이션 사이즈 또한 커지고 있는 추세이다. 이에 따라 CPU와 GPU 같은 하 나의 연산 장치에서 수행할 수 없는 문제와 다중의 연산 장치로 구성된 한 대의 컴퓨터에서 수행할 수 없는 문제가 발생하기도 한다. 이러한 문제로 인해 분산 병렬처리를 통한 FDTD 알고리즘 연구가 진행되고 있 다. 하지만 기존의 연구들은 단일 연산장치만을 이용하기 때문에 GPU를 사용하는 경우 연산 속도는 빠르 나 메모리의 제한이 있으며 CPU의 경우 GPU에 비해 연산 속도가 느린 단점이 존재한다. 이를 해결하기 위해 본 논문에서는 CPU, GPU의 이기종 연산 장치를 포함하는 컴퓨터로 구축된 클러스터 상에서 작업 사이즈에 제한되지 않고 시뮬레이션 수행이 가능한 컴퓨팅 모델을 구현하였다. 점대점 통신 기반의 MPI 라이브러리를 이용하여 연산 장치 간 통신을 통한 시뮬레이션을 테스트 하였고 사용하는 연산 장치의 종류 와 수에 상관없이 시뮬레이션이 정상 동작함을 확인하였다.||2015|10.5392/JKCA.2015.15.09.021|Cheol-Hoon Lee, Sang-Gil Lee, Yeon-Il Kim, Seung-Il Lee|0.0|0
1916|Parallel Simulation of a Multi-Span DWDM System Limited by FWM Using OpenMP and Dynamic Parallelism in CUDA|One of the non-linear phenomena that affect high bandwidth and long reach communication systems is the non-linear phenomenon called four-wave mixing (FWM). Unfortunately, the simulation of such systems aiming to obtain their design parameter limitations require more time as the number of channels increases. In this paper, we propose a new high-performance computational model to obtain optimal design parameters in a multi san Dense Wavelength Division Multiplexing (DWDM) system, limited by FWM and the intrinsic Amplified Spontaneous Emission (ASE) noise of optical amplifiers employed in each segment. The simulation in this work provides a complete optical design characterization and compares the efficiency and speed improvement of the proposed parallelization model versus a previous sequential model. Additionally, an analysis of the computational complexity of parallel model is presented, where two parallel implementations are used. First, Open Multi−Processing (OpenMP), based on the use of a central, multi-core processing unit is used and secondly the Compute Unified Device Arquitecture (CUDA), which is based on the use of graphics processing unit. Results show that parallelism improves to up to 40 times the performance of the simulation when nested parallelization with CUDA is used, over de sequential method and up to 6 times compared with the implementation with OpenMP using 12 processors. Within our parallel implementation, it is possible to simulate with an increased number of channels, that was unpractical in the sequential simulation.||2021|10.20944/PREPRINTS202103.0558.V1|J. López-Martínez, J. Trejo-Sánchez, R. Sanchez-Lara, J. Álvarez-Chávez, H. Offerhaus|0.0|0
1917|An optimized solver for unsteady transonic aerodynamics and aeroacoustics around wing profiles|The extensive optimisation for HPC of Fluid Dynamics software is possible under a variety of aspects on GPU clusters within GPUDirect/C/CUDA/Thrust programming paradigms. In particular, our algorithm could be made more modular to adapt to the CUDA register usage limit, Thrust libraries provide highly efficient solutions on global memory computations and warp collaboration through shared memory proves crucial. Large Eddy Simulation, based on basic principles of field mechanics, despite its very high computing requirements, complements Reynolds-Averaged Navier-Stokes models, which lack versatility. In the field of aeronautical flows around wing profiles in steady or off-design configurations, our solver provides efficient solutions on 128-TESLA clusters for adequate 2-billion cell grids.||2016||J. L. Gouez, J. Etancelin|0.0|0
1920|Improving the Simulation Performance of Colored Hybrid Petri Nets by the Graphics Processing Units|The most substantial aim in systems biology is studying and understanding biological phenomena at the system level. Toward achieving this goal, it is imperative to construct and execute accurate models to predict the behaviour of the underlying system and get more insights about the interactions between the different model components. Coloured Petri nets are a promising tool to model such biological systems, which extend the power of Petri nets by assigned colours to places. However, the sequential implementation of coloured Petri net execution is prohibitively slow. Particularly, when complex models are considered that may contain reactions or species at different scales. Here, we are more interested in an extended class Petri nets class called coloured hybrid Petri nets$\left( {\mathcal{H}\mathcal{P}{\mathcal{N}^{\mathcal{C}}}} \right)$, which can combine different components at the same model. However, speeding up the simulation of $\mathcal{H}\mathcal{P}{\mathcal{N}^{\mathcal{C}}}$ models is of a paramount importance. One direction to release this goal is to resort to parallel processing. In this paper, we use the Graphics Processing Units (GPU) to increase the efficiency of simulating coloured hybrid models, whereby the time-extensive part (the stochastic regime) is simulated on the GPU, while the deterministic simulation is kept running on the CPU. Besides, the performance of our parallel approach is compared with the sequential one.|International Conference on Communication and Electronics Systems|2018|10.1109/ICCES.2018.8639191|A. Hefnawy, I. Elansary, M. Herajy, Nasser Sewilam|0.0|0
1921|Impact of memory bottleneck on the performance of graphics processing units|Recent graphics processing units (GPUs) can process general-purpose applications as well as graphics applications with the help of various user-friendly application programming interfaces (APIs) supported by GPU vendors. Unfortunately, utilizing the hardware resource in the GPU efficiently is a challenging problem, since the GPU architecture is totally different to the traditional CPU architecture. To solve this problem, many studies have focused on the techniques for improving the system performance using GPUs. In this work, we analyze the GPU performance varying GPU parameters such as the number of cores and clock frequency. According to our simulations, the GPU performance can be improved by 125.8% and 16.2% on average as the number of cores and clock frequency increase, respectively. However, the performance is saturated when memory bottleneck problems incur due to huge data requests to the memory. The performance of GPUs can be improved as the memory bottleneck is reduced by changing GPU parameters dynamically.|International Conference on Graphic and Image Processing|2015|10.1117/12.2228592|C. Kim, Jong – Myon Kim, D. Son, H. Choi|0.0|0
1922|Enforcing Energy Preservation in Microfacet Models|Microfacet models suffer from a signiﬁcant limitation: they only simulate a single interaction between light and surface, ignoring the subsequent scattering across the microfacets. As a consequence, the BSDF is not energy preserving, resulting in an unexpected darkening of rough specular surfaces. Energy compensation methods face this limitation by adding to the BSDF a secondary component accounting for multiple scattering contributions. While these methods are fast, robust and can be added to a renderer with relatively minor modiﬁcations, they involve the computation of the directional albedo. This quantity is expressed as an integral that does not have a closed-form solution, but it needs to be precomputed and stored in tables. These look-up tables are notoriously cumbersome to use, in particular on GPUs. This work obviates the need of look-up tables by ﬁtting an analytic approximation of the directional albedo, which is a more practical solution. We propose a 2D rational polynomial of degree three to ﬁt conductors and a 3D rational polynomial of degree three to ﬁt dielectrics and materials composed of a specular layer on top of a diffuse one, such as plastics. We enforce energy preservation by rescaling the specular albedo, thus maintaining the same lobe shape. We validated our results via the furnace test, highlighting that materials rendered using our analytic approximations match almost exactly the behaviour of the ones rendered with the use of look-up tables, resulting in an energy-preserving model even at maximum roughness. The software we use to ﬁt coefﬁcients is open-source and can be used to ﬁt other BSDF models as well.|Smart Tools and Applications in Graphics|2022|10.2312/stag.20221258|D. Sforza, F. Pellacini|0.0|0
1923|Variable Precision Computing|This report summarizes the activities and major accomplishments of the Variable Precision Computing Strategic Initiative project. The overarching goal of this project was to initiate and promote a new paradigm in High Performance Computing (HPC) that would fundamentally change how we represent and make use of representations of real numbers in finite precision and that would establish Lawrence Livermore National Laboratory (LLNL) as a leader in precision-related research for HPC. We pursued three integrated and concurrent research thrusts: new, more dynamic data representations to address data motion and capacity limitations; improved mixed precision algorithms to accelerate computation and to leverage new hardware; and new tools to help developers reason about precision and to automate code transformations. Within these thrust areas, we made significant advances in the use of compressed array data types in numerical calculations, in the theoretical justification, in improved performance, and in the prospects of hardware implementation; in the development of metrics to significantly reduce the storage needs of molecular dynamics simulations; in the development and analysis of efficient mixed-precision algorithms for time-dependent partial differential equations and for orthogonal factorization; and in the capability of tools that can analyze sensitivity of computed results to choices in precision and that can make automated code transformations based on such analysis. We established that there are definite opportunities to realize significant improvements in HPC above and beyond the 2x limits of traditional mixed precision using the technologies we have developed. The project produced numerous papers, presentations, and posters in important venues as well as open-source software contributions to the community, and many of the new capabilities have initiated or are being leveraged in ongoing projects. Background and Research Objectives Within a computer, real numbers are represented using a finite number of 0’s and 1’s, i.e., finite precision, which means there are a finite number of exactly representable numbers depending on the number of bits used. Decades ago, when memory was a scarce resource, computational scientists routinely worked in 32-bit (single) precision and were more sophisticated dealing with the pitfalls of finite-precision arithmetic. Today, we routinely compute and store results in 64-bit (double) precision by default, even when very few significant digits are required of a calculation. More precision is often used as a simple guard against corruption from finite-precision roundoff error instead of making the effort to ensure algorithms are robust to roundoff. In other applications, only isolated calculations, like tangential intersections, require extended precision. As indicated in Figure 1, many of the 64 bits represent errors – truncation, iteration, and roundoff – instead of useful information about the solution. This over-allocation of resources wastes power, bandwidth, storage, and Floating-Point Operations Per Second (FLOPS): many meaningless bits are stored, communicated, and computed. Today, we are at a crossroads where increases in compute power based on traditional technologies are dwindling due to physical limits, so to continue to increase performance, we need to reconsider inefficiencies in our approaches. Indeed, many physical simulation codes obtain only a few percent of peak FLOPS performance because data motion is too slow to keep processors busy with useful work. Because of the growing disparity of FLOPS to memory bandwidth and capacity (Lucas et al. 2014), the rise of General-Purpose Graphics Processing Unit (GPGPU) computing, and the introduction of special-purpose, high-performance, low-precision hardware like NVIDIA’s Tensor Core Unit (NVIDIA 2019), there has been renewed interest in mixed precision computing, where tasks are identified that can be accomplished by using reduced precision (half or single) in conjunction with double precision. Such static optimizations reduce data movement and FLOPS, but their implementations are time consuming and difficult to maintain, particularly across computing platforms. Task-based mixed-precision would be more common if there were tools to simplify development, maintenance, and debugging, but it inherently is limited by not being able to adapt to the precision needs of a calculation. We often dynamically adapt discretization parameters (mesh size and approximation order) and models in physics simulations to focus the greatest effort only where needed. In the Variable Precision Computing (VPC) project, our ambitious goal was to do the same with precision: to develop technologies that would allow simulations to adjust dynamically precision at a per-bit level depending on the needs of the task at hand. Just as adaptive mesh refinement (AMR) adapts spatial grid resolution to the underlying solution, our system can locally provide more or less precision as needed. An overarching goal was to establish Lawrence Livermore National Laboratory (LLNL) in a leadership role in what we perceived to be an important growing direction in High Performance Computing (HPC) research. Acceptance from the community necessitates that we address three concerns: that we can ensure accuracy, ensure efficiency, and ensure ease of use in development, in debugging, and in application. To achieve our vision, we pursued three integrated and concurrent thrusts that pulled together recent advances from multiple areas: new, efficient data representations to address the data motion and storage problems; improved mixed precision algorithms to reduce unnecessary computation and to leverage specialized hardware; and new tools to help developers reason about precision and automate data type transformations. Underlying these thrusts was a significant effort in numerical analysis to provide the necessary theoretical justification for our work. Our expectation was that VPC would allow for 4-100x less data storage and would increase computational throughput by factors of 2-10x for bandwidth-limited applications. Specifically, our original intent was to consider a breadth of high-level research objectives. Building on the ZFP compressed floating point array representation developed at LLNL, we sought to couple this format with a hierarchical multiresolution data format to provide a “data optimal” representation that could refine or coarsen mesh resolution and precision as needed to minimize data under an error budget for visualization and data analysis purposes. We also planned to develop a rigorous analysis of the roundoff error accumulation for calculations in which the underlying data was represented in the ZFP compressed array format and to extend the ZFP representation to be locally adaptive to further increase the effective gains in data reduction. To reduce computation, we sought to extend the error transport approach for a posteriori error estimation from truncation error to roundoff error estimation and to consider combining it with a local, patch-based approach related to AMR. Furthermore, we sought to investigate the utility of reduced (mixed) precision algorithms for eigensolver problems frequent in molecular dynamics and graph clustering and to demonstrate these algorithms on a new ARM/GPU cluster to be procured by Livermore Computing (LC). On the tools front, our major research objective was to use compiler-based, source-to-source transformation techniques to develop tools to automatically convert types consistently in codes to ease the burden of software maintainability of switching types or developing mixed precision implementations. Throughout the project, we sought to demonstrate our techniques on LLNLrelevant application codes, with a splash app of demonstrating the adaptive ZFP capability in the radiation transport code Ardra. We successfully completed the majority of these objectives, even as the scope and direction of the research naturally evolved. Notably, we did not complete a demonstration of mixed precision algorithms on an ARM/GPU cluster nor did we fully complete a demonstration of ZFP compressed arrays on the full Ardra application problem, both of which were end goals of lengthy research paths. In both cases, aligning tasks to expertise within the team proved to be a limiting factor, in addition to the unexpected challenges that arise in any research project. However, in addition to the original objectives, we pursued promising new research directions as they arose. We produced additional analyses of (and corrections for) the bias in ZFP lossy compression, demonstrated linear equivalence of error transport with iterative refinement and identified additional opportunities for parallelization, and extended traditional roundoff error analysis methods to account for mixed precision. In our pursuit of efficient means of reducing data from unstructured particle methods, we investigated a number of reordering techniques in ZFP as well as the concept of channel capacity as a metric for decimation of molecular dynamics simulation data. Our work on tools also grew to accommodate algorithmic differentiation techniques to identify candidate variables for lower precision, and ultimately, we developed a more sophisticated analysis tool chain than originally planned. Scientific Approach and Accomplishments Within each of the three thrusts of our project (new data representations, improved mixed precision algorithms, and enabling tools), there were multiple research threads. We here summarize the approaches and accomplishments of each major effort within the thrusts. New Data Representations One thrust of our project dealt with new, more efficient ways of representing data. For data analysis, we investigated data optimal representations that could locally adapt both the precision of the data and/or the granularity of the underlying grid based on the needs of||2019|10.2172/1573151|Harshitha Menon, J. Hittinger, D. Hoang, H. Bhatia, D. Osei-Kuffuor, G. Sanders, P. Bremer, Valerio Pascucci, T. Vanderbruggen, G. Morrison, A. Metere, Alyson Fox, L. Yang, N. Pinnow, K. Chand, G. S. Lloyd, M. Schordan, L. G. Moody, Peter Lindstrom, Pavol Klacansky, D. M. Copeland, D. Quinlan, James Diffenderfer, W. Usher, Michael O. Lam|0.0|0
1928|A cartoon-style rendering for physical dynamic hair animation|Hair rendering is one of the major research topics in computer graphics. A lot of papers on hair modeling have focused on photo-realistic rendering. This paper proposes a stylized cartoon hair rendering flow diagram including animated hair strand creation, stroke path generation, brush stroke line simulation, and expressive hair rendering. A reordering mechanism addresses depth and stroke line rendering for a suitable performance and temporal coherence. The proposed method creates a delicate expression similar to an artist’s drawing. To validate real-time performance, all stroke effects are rendered with different shaders on a graphics processing unit. The method is flexible enough to allow users to select shader effects and control the parameters of each activated shader to create esthetically pleasing results. Finally, the proposed method produces excellent experimental results. These results are compared to the results of previous studies.||2015|10.1080/02533839.2014.955970|Yu-Sheng Chang, Der-Lor Way|0.0|0
1932|Foreword to the special issue of the International Conference on Innovative Network Systems and Applications held under the Federated Conference on Computer Science and Information Systems|The purpose of this special issue is to collate a selection of representative research articles that were primarily presented at the 3rd International Conference on Innovative Network Systems and Applications (iNetSApp’15), held in conjunction with Federated Conference on Computer Science and Information Systems (FedCSIS’15). The mission of the conference is to provide a highly acclaimed forum for the area of modern network systems which encompass a wide range of solutions and technologies, including wireless and wired networks, network systems, services, and applications. The forum is also considered as a valuable experience-sharing platform for scientific researchers and experts from research institutes, SMEs, and companies who work in this domain for exchanging relevant skills and experiences as well as discuss upcoming trends and new ideas from different fields of network-related research. The scope of this special issue is broad, aiming at different results in numerous active research areas oriented towards various technical, scientific, and social aspects of network systems and applications. With regard to the FedCSIS policy, 23% acceptance rate was kept within all regular paper submissions with the help of the well-structured and experienced conference program committee. For this special issue, only the papers with best review score were selected; thus, the quality of the CPE series can be preserved. The problematic of indoor positioning which currently lacks novel services based on location of users is discussed in [1] where authors focus on application of optimization algorithms on performance of hybrid indoor positioning system, which utilizes radio signals from both Global System for Mobile communication and Wi-Fi networks simultaneously. In [2], authors show that a naive brute force algorithm, enhanced by a simple heuristics, in an average case, can be faster than comprehensive solutions based on parallel implementation of an asymptotically optimal sequential algorithm when constructing the cell graph on a single instruction, multiple data-like graphics processing unit processor. Very interesting practical implementation of a novel hybrid security scheme suitable for maritime coastal environment-based wireless sensor networks is presented in [3] in line with the comparison of its performance together with the existing cryptographic schemes. The critical issue of barrier-coverage in wireless sensor networks with the proposal of four different approaches to construct reinforced barriers from a given layout of sensors is presented in [4]. Special wireless sensor network for the power metering at places with shared power sources developed and implemented in a scenario of multi-store parking garage is described in [5]. In [6], author introduces a novel evaluation of wireless sensor networks emissions metering using the Exposure Index as well as a method of reducing exposure by intelligent network scheduling. Still actual issue of wireless sensor network energy consumption is targeted in [7]. Authors present an activity model of the network and evaluate how the remaining energy behaves to predict the energy consumption of the nodes. The ratio between the network security and energy consumption is discussed in [8]. Three energy-aware security methods for hierarchically clustered wireless sensor network were presented and evaluated through simulation scenarios as a basis to establish recommendations regarding the use cases of those methods. The problematic of Machineto-Machine communication is handled in [9] where the algorithm and protocol devised for task allocation to nodes of Machine-to-Machine architecture which consists of battery powered devices are presented. Two novel scheduling procedures for energy consumption optimization for wireless|Concurrency and Computation|2017|10.1002/cpe.4356|H. Fouchal, M. Hodoň|0.0|0
1934|Rapid GPU-based simulation of x-ray transmission, scatter, and phase measurements for threat detection systems|To support the statistical analysis of x-ray threat detection, we developed a very high-throughput x-ray modeling framework based upon GPU technologies and have created three different versions focusing on transmission, scatter, and phase. The simulation of transmission imaging is based on a deterministic photo-absorption approach. This initial transmission approach is then extended to include scatter effects that are computed via the Born approximation. For phase, we modify the transmission framework to propagate complex ray amplitudes rather than radiometric quantities. The highly-optimized NVIDIA OptiX API is used to implement the required ray-tracing in all frameworks, greatly speeding up code execution. In addition, we address volumetric modeling of objects via a hierarchical representation structure of triangle-mesh-based surface descriptions. We show that the x-ray transmission and phase images of complex 3D models can be simulated within seconds on a desktop computer, while scatter images take approximately 30-60 minutes as a result of the significantly greater computational complexity.|SPIE Defense + Security|2016|10.1117/12.2223244|J. Greenberg, Razvan-Ionut Stoian, David Coccarelli, E. Vera, Qian Gong, M. Gehm|0.0|0
1935|A Flexible Predictive Density Combination Model for Large Financial Data Sets in Regular and Crisis Periods|A flexible predictive density combination is introduced for large financial data sets which allows for model set incompleteness. Dimension reduction procedures that include learning allocate the large sets of predictive densities and combination weights to relatively small subsets. Given the representation of the probability model in extended nonlinear state-space form, efficient simulation-based Bayesian inference is proposed using parallel dynamic clustering as well as nonlinear filtering, implemented on graphics processing units. The approach is applied to combine predictive densities based on a large number of individual US stock returns of daily observations over a period that includes the Covid-19 crisis period. Evidence on dynamic cluster composition, weight patterns and model set incompleteness gives valuable signals for improved modelling. This enables higher predictive accuracy and better assessment of uncertainty and risk for investment fund management.|Social Science Research Network|2023|10.2139/ssrn.4034901|S. Grassi, Francesco Ravazzollo, H. K. van Dijk, R. Casarin|0.0|0
1940|Error-aware Quantization through Noise Tempering|Quantization has become a predominant approach for model compression, enabling deployment of large models trained on GPUs onto smaller form-factor devices for inference. Quantization-aware training (QAT) optimizes model parameters with respect to the end task while simulating quantization error, leading to better performance than post-training quantization. Approximation of gradients through the non-differentiable quantization operator is typically achieved using the straight-through estimator (STE) or additive noise. However, STE-based methods suffer from instability due to biased gradients, whereas existing noise-based methods cannot reduce the resulting variance. In this work, we incorporate exponentially decaying quantization-error-aware noise together with a learnable scale of task loss gradient to approximate the effect of a quantization operator. We show this method combines gradient scale and quantization noise in a better optimized way, providing ﬁner-grained estimation of gradients at each weight and activation layer’s quantizer bin size. Our controlled noise also contains an implicit curvature term that could encourage ﬂatter minima, which we show is indeed the case in our experiments. Experiments training ResNet architectures on the CIFAR-10, CIFAR-100 and ImageNet benchmarks show that our method obtains state-of-the-art top-1 classiﬁcation accuracy for uniform (non mixed-precision) quantization, out-performing previous methods by 0.5-1.2% absolute.|arXiv.org|2022|10.48550/arXiv.2212.05603|Shuhui Qu, Florian Metze, Zheng Wang, Juncheng Billy Li, Emma Strubell|0.0|0
1941|Accelerating Action Potential Generation Using GPU Implementation of a Resonant Model of a Cell|Computational modelling of bioelectric phenomena in cardiac tissue has made significant contributions to our understanding of the electrical activity of the human heart. However, extending these simulations over large temporal and spatial ranges is an inherently computationally expensive task. Motivated by the apparent advantages of GPU over CPU in many areas of science, we have developed a new Resonant Model (RM) of an AP of a biological cell amenable to parallel computing. In this study, we have investigated the potential performance benefits of the implementation of the RM on a GPU over the sequential execution on a CPU. In the proposed implementation, all threads in the same GPU warp share data using a warp-shuffle operation, which eliminates the access of global or shared memory. By exploiting the memory model and computational strengths of GPU, the obtained performance of the RM on the GPU is far superior to that of the CPU. On the GPU, the RM demonstrate linear or sub-linear growth in the execution times with the increase in the number of cells. The speedup of the computations achieved on the GPU may, in the near future, facilitate the patient-specific electrophysiological studies and treatment planning.|2019 Computing in Cardiology (CinC)|2019|10.23919/CinC49843.2019.9005852|Sucheta Sehgal, Saif Charania, N. Patel, Jonathan Reshef, M. Trew|0.0|0
1942|Whole-cortex simulation reveals spatiotemporal patterns emerging from the interplay of network connectivity and intracellular dynamics|Recent advances in Graphics Processing Unit (GPU) computing have allowed for computational models of whole-brain activity at unprecedented scales. In this work, we use desktop computers to build and simulate a whole-cortex mouse brain model using Hodgkin-Huxley type models for all the most active neurons in the mouse cortex. We compare the model dynamics over different types of connectivity, ranging from uniform random to realistic connectivity derived from experimental data on cell positions and the Allen Brain Atlas. By changing the external drive and coupling strength of neurons in the network, we can produce a wide range of oscillations in the gamma through delta bands. While the global mean-field behaviors of different connectivities share some similarities, an experimentally determined hierarchical connectivity allows for complex, heterogeneous behaviors typically seen in EEG recordings that are not observed in networks with nearest neighbors or uniform coupling. Moreover, our simulations reveal a wide range of spatiotemporal patterns, such as rotational or planar traveling waves, that are observed in experiments. Different traveling waves are observed with different connectivity and coupling strengths on the same connectivity. Our simulations show that many cortical behaviors emerge at scale with the full complexity of the network structure and ionic dynamics. We also provide a computational framework to explore these cortex- wide behaviors further.|bioRxiv|2024|10.1101/2024.01.10.574958|James Hazelden, Guanhua Sun, Ruby Kim, Daniel Forger|0.0|0
1946|Exact Error Backpropagation Through Spikes for Precise Training of Spiking Neural Networks|—Event-based simulations of Spiking Neural Net- works (SNNs) are fast and accurate. However, they are rarely used in the context of event-based gradient descent because their implementations on GPUs are difﬁcult. Discretization with the forward Euler method is instead often used with gradient descent techniques but has the disadvantage of being computationally expensive. Moreover, the lack of precision of discretized simula- tions can create mismatches between the simulated models and analog neuromorphic hardware. In this work, we propose a new exact error-backpropagation through spikes method for SNNs, extending Fast & Deep to multiple spikes per neuron. We show that our method can be efﬁciently implemented on GPUs in a fully event-based manner, making it fast to compute and precise enough for analog neuromorphic hardware. Compared to the original Fast & Deep and the current state-of-the-art event-based gradient-descent algorithms, we demonstrate increased performance on several benchmark datasets with both feedforward and convolutional SNNs. In particular, we show that multi-spike SNNs can have advantages over single-spike networks in terms of convergence, sparsity, classiﬁcation latency and sensitivity to the dead neuron problem.|arXiv.org|2022|10.48550/arXiv.2212.09500|Dominique F. Chu, Florian Bacho|0.0|0
1947|A GPU Based 3D Particle Tracking Code for Multipacting Simulation|A new GPU based 3D electron tracking code is developed at BNL and benchmarked with both popular existing parallel tracking code and experimental results. The code takes advantage of massive concurrency of GPU cards to track electrons under RF field in 3D Tetrahedron meshed structures. Approximately ten times more FLOPS can be achieved by utilizing GPUs compare to CPUs with same level of power consumption. Different boundary materials can be specified and the 3D EM field can be imported from the result of Omega3P calculation. CUDA_OpenGL interop was implemented so that the emerging of multipactors can be monitored in real time while the simulation is undergoing. Code also has GPU farm version that can run on multiple GPUs to further increase the turnover of multipacting simulation. INTRODUCTION Electron multipacting (MP) study in an SRF cavity and power coupler is of great importance in both designing and operating phase of the device. There are several 2D codes that can handle structures with cylindrical symmetry such as Multipac and Fishpact. To deal with 3D structures we have Track3P solver in the ACE3P package and Particle Studio in the CST suite. For 2D codes the limitation is obvious, especially when we are facing a power coupler problem where the structures are usually lack azimuthal symmetry. The Track3P code is extremely powerful in terms of the range of problems it can handle but it also requires a cluster such as NERSC to fully harness this power. Therefore we developed this GPU based 3D tracking code to increase the turnover of the multipacting simulation in SRF structures with only several GPU cards. This code can run on either PC or workstation as long as a GPU that support Nvidia CUDA computing capability 1.3 and above is available. STRUCTURE OF THE CODE The idea of this code is to take the advantage of high concurrency of the GPU to run a large scale Monte Carlo process to simulate the multipacting phenomenon. There are three primary parts in the code. Main (Master) Function The main function is a host function that runs on CPU and controls the work flow of the program. All the kernels running on GPU are launched from the main host code. First, the input parameters are read into the main function from an input file. Then the geometry model of an RF structure and the field distribution from Omega3P eignesolver are read in. The mesh model will be preprocessed before it is sent to the GPU so that the particles can be more easily located when it is going through the tracking process. Then the main function calls the sequence of the core kernels in the display call back function of the OpenGL so that the tracking process is synchronized with the rendering process. The core tracking kernels will be discussed below.||2015|10.18429/JACOW-SRF2015-MOPB060|I. Pinayev, V. Litvinenko, J. Skaritka, Qiong Wu, Bin Xiao, S. Belomestnykh, J. C. Brutus, I. Ben-Zvi, T. Xin|0.0|0
1949|Research on fast hyperspectral atmospheric radiation transfer imaging modeling based on .NET environment|Atmospheric radiation transmission is one of the most complex and variable parts of hyperspectral remote sensing systems. Aimed at the abstraction and complexity of the influence of atmospheric radiation on the quality of hyperspectral imaging, the design of simulation software for hyperspectral atmospheric radiation transmission imaging in visible light is proposed. Firstly,this paper analyzes the radiation transmission process including the surface reflectivity, the adjacent pixel reflectivity and the atmospheric transmission factor, and describes the calculation method of the radiance at-sensor for the hyperspectral image in the visible light bands. Then the multi-core CPU based on the .Net environment is constructed. The adjacent pixel point diffusion function parallel computing module and the GPU-based on-satellite reflectivity parallel computing module; the experimental part takes the hyperspectral surface reflectance image as input data, and degenerates the output into the hyperspectral radiance simulation data in different scenarios. At the same time, test of the time of individual modules and the overall algorithm in the simulation process is tested. The experimental results of real-time performance show that the parallel algorithm has significantly improved.|Applied Optics and Photonics China|2019|10.1117/12.2547669|Tian Lan, Xiaomei Chen, Yunqiao Xi|0.0|0
1951|A GPU-ACCELERATED MODELING OF SCALAR TRANSPORT BASED ON BOUSSINESQ-TYPE EQUATIONS|This paper describes a two-dimensional scalar transport model solving advection-diffusion equation based on GPU-accelerated Boussinesq model called Celeris. Celeris is the firstly-developed Boussinesq-type model that is equipped with an interactive system between user and computing unit. Celeris provides greatly advantageous user-interface that one can change not only water level, topography but also model parameters while the simulation is running. In this study, an advection-diffusion equation for scalar transport was coupled with extended Boussinesq equations to simulate scalar transport in the nearshore.Recorded Presentation from the vICCE (YouTube Link): https://youtu.be/aHvMmdz3wps|Coastal Engineering Proceedings|2020|10.9753/icce.v36v.waves.11|P. Lynett, S. Son, Sooncheol Hwang|0.0|0
1953|Evaluation of Portable Programming Models to Accelerate LArTPC Detector Simulations|The Liquid Argon Time Projection Chamber (LArTPC) technology is widely used in high energy physics experiments, including the upcoming Deep Underground Neutrino Experiment (DUNE). Accurately simulating LArTPC detector responses is essential for analysis algorithm development and physics model interpretations. Accurate LArTPC detector response simulations are computationally demanding, and can become a bottleneck in the analysis workflow. Compute devices such as General-Purpose Graphics Processing Units (GPGPUs) have the potential to substantially accelerate simulations compared to traditional CPU-only processing. The software development for these compute accelerators often carries the cost of specialized code refactorization and porting to match the target hardware architecture. With the rapid evolution and increased diversity of the computer architecture landscape, it is highly desirable to have a portable solution that also maintains reasonable performance. We report our ongoing effort in evaluating Kokkos as a basis for this portable programming model using LArTPC simulations in the context of the Wire-Cell Toolkit, a C++ library for LArTPC simulations, data analysis, reconstruction and visualization.|Journal of Physics: Conference Series|2022|10.1088/1742-6596/2438/1/012036|B. Viren, Meifeng Lin, Haiwang Yu, Zhihua Dong, K. Knoepfel|0.0|0
1956|GPU Implementation and Optimization of 3D FDTD Algorithm|FDTD algorithm is a very extensive numerical method for the electromagnetic field,which has good accuracy and flexibility,and has become a powerful tool for solving various electromagnetic problems.The rapid development of semiconductor technology makes the computational performance of the CPU has made progress in leaps and bounds,but until now on the CPU computing time of FDTD method is still very time consuming,which greatly limits the FDTD method in various engineering fields of applications.The FDTD algorithm on the GPU is realized and optimized,so as to improve the calculation efficiency of the FDTD method,save the simulation time.Experimental results show that the serial program is executed on the relative Xeon Intel processor,and the maximum of GPU can get 166 times speedup.According to the roofline model,the performance of the GPU reaches 89% of the theoretical value.||2015|10.1117/12.2176910|Song Qingzen|0.0|0
1960|Peer Reviewed Title: Increasing Biological Realism in Models of Sequence Evolution for Improved Statistical and Computational Performance Author:|Models of molecular sequence evolution have been a pivotal source of insight into the biological mechanisms and forces of evolution. Model development and improvements in computational capacity have allowed for increasingly sophisticated analyses in the last four decades, however, many studies still use twenty year old GTR+[Gamma] models for convenience and computational tractability. Here we present a new model in the branch-site random effects likelihood framework, Adaptive Branch-Site Random Effects Likelihood (aBSREL), that is more sensitive and an order of magnitude faster than previous branch-site models. We demonstrate the effectiveness of aBSREL at detecting episodic diversifying selection using simulated sequences and previously studied empirical alignments. As part of an ongoing investigation into the importance of modeling natural selection and especially selection heterogeneity in molecular dating analyses, we undertake a detailed study of ten potentially ancient viral lineages using aBSREL, comparing our estimates with previously published molecular dating, historical, and fossil estimates. We also present two special purpose models within the branch- site REL framework and the addition of massively parallel accelerators such as Graphics Processing Units to the high performance computing resources addressable by HyPhy. The first of these two models, RELAX, is purpose built to detect relaxed selection while the BUSTED model uses a priori partitioning and sublinear complexity scaling to further improve computational tractability. Both models are demonstrated on simulated and empirical datasets and compared to current state of the art alternatives from the literature. These models represent a substantial expansion to the branch-site REL framework and are available for use in the HyPhy software package and on the Datamonkey.org webserver||2015||M. Dégrange|0.0|0
1962|High Geometric Fidelity Solar Radiation Pressure Modeling via Graphics Processing Unit|Solar radiation pressure (SRP), the force imparted on a spacecraft due to impinging solar photons, becomes a dominant dynamic perturbation for both interplanetary and above 1000 km Earth orbit altitude spacecraft missions. This thesis presents a method for the fast computation of spacecraft force and torque due to SRP considering a geometrically complex spacecraft model. The method uses the highly parallel execution capabilities of commodity Graphics Processing Unit (GPU) and the Open Graphics Library (OpenGL) vector graphics software library to render a Computer Aided Design (CAD) generated spacecraft model on the GPU. The SRP forces and torques are resolved per model facet in the custom-developed render pipeline. Using common commercial and open-source 3D mesh modeling tools the material properties are encoded with the CAD model to provide realistic specular, diffuse and absorption surface optical properties. A first order validation is carried out by comparing the method’s force result to that provided by the analytic cannonball SRP model. Validation of more complex spacecraft geometry is achieved by comparison of the OpenGL method’s computed force value with the force value computed from flight data for the same spacecraft. The method is successfully implemented as a modular component of the Autonomous Vehicle Systems Laboratory’s (AVS Lab) spacecraft simulation framework to demonstrate the methods faster than real time online simulation capability. Finally the OpenGL method’s online simulation capability is used to demonstrate the methods rapid simulation capability in aid of spacecraft maneuver design.||2016||P. Kenneally|0.0|0
1964|Cross-comparison of state of the art neuromorphological simulators on modern CPUs and GPUs using the Brain Scaffold Builder|A variety of software simulators exist for neuronal networks, and a subset of these tools allow the scientist to model neurons in high morphological detail. The scalability of such simulation tools over a wide range in neuronal networks sizes and cell complexities is predominantly limited by effective allocation of components of such simulations over computational nodes, and the overhead in communication between them. In order to have more scalable simulation software, it is therefore important to develop a robust benchmarking strategy that allows insight into specific computational bottlenecks for models of realistic size and complexity. In this study, we demonstrate the use of the Brain Scaffold Builder (BSB; De Schepper et al., 2021) as a framework for performing such benchmarks. We perform a comparison between the well-known neuromorphological simulator NEURON (Carnevale and Hines, 2006), and Arbor (Abi Akar et al., 2019), a new simulation library developed within the framework of the Human Brain Project. The BSB can construct identical neuromorphological and network setups of highly spatially and biophysically detailed networks for each simulator. This ensures good coverage of feature support in each simulator, and realistic workloads. After validating the outputs of the BSB generated models, we execute the simulations on a variety of hardware configurations consisting of two types of nodes (GPU and CPU). We investigate performance of two different network models, one suited for a single machine, and one for distributed simulation. We investigate performance across different mechanisms, mechanism classes, mechanism combinations, and cell types. Our benchmarks show that, depending on the distribution scheme deployed by Arbor, a speed-up with respect to NEURON of between 60 and 400 can be achieved. Additionally Arbor can be up to two orders of magnitude more energy efficient.|bioRxiv|2022|10.1101/2022.03.02.482285|E. D’Angelo, A. Morrison, C. Casellato, B. F. B. Huisman, R. De Schepper, N. A. Akar, T. Hater|0.0|0
1965|Final Report for Award #DE-SC3956 Separating Algorithm and Implementation via programming Model Injection (SAIMI)|"Programming parallel machines is fraught with difficulties: the obfuscation of algorithms due to implementation details such as communication and synchronization, the need for transparency between language constructs and performance, the difficulty of performing program analysis to enable automatic parallelization techniques, and the existence of important ""dusty deck"" codes. The SAIMI project developed abstractions that enable the orthogonal specification of algorithms and implementation details within the context of existing DOE applications. The main idea is to enable the injection of small programming models such as expressions involving transcendental functions, polyhedral iteration spaces with sparse constraints, and task graphs into full programs through the use of pragmas. These smaller, more restricted programming models enable orthogonal specification of many implementation details such as how to map the computation on to parallel processors, how to schedule the computation, and how to allocation storage for the computation. At the same time, these small programming models enable the expression of the most computationally intense and communication heavy portions in many scientific simulations. The ability to orthogonally manipulate the implementation for such computations will significantly ease performance programming efforts and expose transformation possibilities and parameter to automated approaches such as autotuning. At Colorado State University, the SAIMImore » project was supported through DOE grant DE-SC3956 from April 2010 through August 2015. The SAIMI project has contributed a number of important results to programming abstractions that enable the orthogonal specification of implementation details in scientific codes. This final report summarizes the research that was funded by the SAIMI project.« less"||2015|10.2172/1210028|M. Strout|0.0|0
1970|GPU Parallelization Nested Decomposition Method for Solving Large Linear Systems in Reservoir Numerical Simulation|This paper designs a highly parallel Nested Factorization (NF) to solve large linear equations generated in reservoir numerical simulation problems. The NF method is a traditional linear solution preprocessing method for reservoir numerical simulation problems, and has regained attention in recent years due to its potential to extend to parallel architectures such as GPUs (Graphics Processor Units). The parallel algorithm of this paper is based on the MPNF (Massively Parallel Nested Factorization) framework proposed by Appleya (Appleyard, Appleyard, Wakefield, & Desitter, 2011). The MPNF algorithm designed in this paper focuses on its efficient implementation on the GPU parallel architecture. Its features include: using a custom matrix structure to achieve merge access, improving access bottlenecks and improving the efficiency of the SpMV algorithm. It is also applicable to the two-stage preprocessing method CPR. (Constrain Pressure Residual) pressure solution and global preprocessing stage; the MPNF method is extended to the solution of 2.5-dimensional unstructured grid problem. The parallel algorithm in this paper has been integrated into the reservoir numerical simulator. For the SPE10 (million grid, highly heterogeneous) standard example, the GPU-based parallel NF algorithm is in the structured grid model and the equivalent 2.5-dimensional non- On the structured grid model, compared with the serial version of the NF method, the acceleration ratios of 19.8 and 17.0 times were obtained respectively; compared with the mainstream serial solution method, the efficiency was also improved by 2 to 3 times.|Earth Sciences Research Journal|2019|10.15446/esrj.v23n3.81669|Yuan Di, Xin Shi|0.0|0
1972|Editorial Issue 29.1|This issue contains six papers. In the first paper, Kim J. L. Nevelsteen from Stockholm University proposes to sample technologies using grounded theory and obtained a definition for a “virtual world” that is directly applicable to technology. The obtained definition is compared with related work and used to classify advanced technologies, such as a pseudo-persistent video game, a MANet, a virtual and mixed reality, and the Metaverse. The results of this article include: a breakdown of properties that set apart various technologies; a definition that is validated by comparing it with other definitions; an ontology showing the relation of different complimentary terms and acronyms; and, the usage of pseudo-persistence to categories of those technologies that only mimic persistence. In the second paper, Tsung-Yu Tsai, Sai-Keung Wong, Yi-Hung Chou, and Guan-Wen Lin, from National Chiao Tung University in Hsinchu, Taiwan, present a method to solve the problem of congestion in navigation fields for crowd simulation. In their paper, they propose to place crowd monitors at the corners to collect the data such as the movement direction of crowds and crowd densities. Then, the navigation field is adjusted dynamically so that the crowds are led to move away from the congested regions. They also propose a simple data structure for speeding up the collision detection process between agents and objects. Experimental results show that their approach successfully alleviates the congestion problem at the corners. In the third paper, Gang Feng and Shiguang Liu, from Tianjin University Computer Science and Technology in China describe a new method to drive particle-based SPH fluid to match target shape and deforming fluid shape between different models smoothly, especially when the natural fluid motion must be preserved. To achieve the desired behavior, they first generate control particles by sampling the target shapes and then apply a deformation constraint to each control particle, with its neighboring fluid particles keeping details within its influence region. For the generation of control particles, they divide models into source object and target object, then separately sample them by voxelization method, and generate source control particles and target control particles, respectively. In the fourth paper, Soonhyeon Kwon, Younguk Kim, Kihyuk Kim, and Sungkil Lee, from Sungkyunkwan University, Gyeonggi-do Suwon, Korea, propose a novel heterogeneous volume deformation technique and an intuitive volume animation authoring framework. Their volume deformation extends the previous technique based on moving least squares with a density-aware weighting metric for data-driven importance control and efficient upsampling-based volume synthesis. For user interaction, they present an intuitive visual metaphor and interaction schemes to support effective spatiotemporal editing of volume deformation animation. Their framework is implemented fully on graphics processors and thus suitable for quick-and-easy prototyping of volume deformation with improved controllability. In the fifth paper, Ka-Hou Chan, Wei Ke, and Sio-Kei Im, from Macao Polytechnic, China, propose a method integrating several improvements for the real-time simulation of fluid interacting with deformable bodies. They improve the particle neighbor search in SPH so that the pre-defined scene containers are no longer needed. This improvement can also be applied to the simulation of fluid interacting with other materials, such as rigid and soft bodies. They also propose a two-way coupling method for fluid and deformable bodies, where the particle–mesh interaction is obtained by the ray-traced collision detection method instead of the proxy/ghost particles generation. By using the forward ray-tracing method for both velocity and position, they are able to calculate the coupling forces based on the conservation of momentum and kinetic energy in the particle–mesh interaction. They use Screen Space Fluid Rendering (SSFR) for fluid, and based on that we introduce a screen space refraction rendering method to improve the refraction effect.|Comput. Animat. Virtual Worlds|2023|10.1002/cav.1803|D. Thalmann, N. Magnenat-Thalmann|0.0|0
1974|A GPU Accelerated Discontinuous Galerkin Conservative Level Set Method for Simulating Atomization|This dissertation describes a process for interface capturing via an arbitrary-order, nearly quadrature free, discontinuous Galerkin (DG) scheme for the conservative level set method (Olsson et al., 2005, 2008). The DG numerical method is utilized to solve both advection and reinitialization, and executed on a refined level set grid (Herrmann, 2008) for effective use of processing power. Computation is executed in parallel utilizing both CPU and GPU architectures to make the method feasible at high order. Finally, a sparse data structure is implemented to take full advantage of parallelism on the GPU, where performance relies on well-managed memory operations. With solution variables projected into a kth order polynomial basis, a k + 1 order convergence rate is found for both advection and reinitialization tests using the method of manufactured solutions. Other standard test cases, such as Zalesak’s disk and deformation of columns and spheres in periodic vortices are also performed, showing several orders of magnitude improvement over traditional WENO level set methods. These tests also show the impact of reinitialization, which often increases shape and volume errors as a result of level set scalar trapping by normal vectors calculated from the local level set field. Accelerating advection via GPU hardware is found to provide a 30x speedup factor comparing a 2.0GHz Intel Xeon E5-2620 CPU in serial vs. a Nvidia Tesla K20 GPU, with speedup factors increasing with polynomial degree until shared memory is filled. A similar algorithm is implemented for reinitialization, which relies on heavier use of shared and global memory and as a result fills them more quickly and produces smaller speedups of 18x.||2015||Z. Jibben|0.0|0
1976|GPU-initiated Fine-grained Overlap of Collective Communication with Computation|In order to satisfy their ever increasing capacity and compute requirements, many machine learning models are distributed across multiple nodes using space-efficient parallelism strategies. As a result, collective communications are often on the critical path, and hiding their latency by overlapping kernel-granular communication and computation is difficult due to the absence of independent computation. In this work, we propose fusing computation with communication using GPU-initiated networking, and leverage GPUs' massive parallelism to enable fine-grained overlap of the fused operations. We have developed a single, self-contained GPU kernel where workgroups (WGs) immediately communicate their results to remote GPUs when they complete their computation. Meanwhile, other WGs within the same kernel perform overlapping computation, maintaining high ALU utilization. Furthermore, we propose zero-copy optimizations for peer-to-peer GPU communication where the data computed by one GPU is directly written to the destination buffers within the peer GPUs, eliminating intermediate stores and extra buffering. Our approach leverages the emerging multi-node GPU system trend where GPUs are physically close to network with direct GPU-NIC interconnects. We demonstrate our approach by creating an embedding + All-to-All fused kernel which overlaps embedding operations and the dependent all-to-all collective in DLRM models. We evaluate our approach both using simulation and real hardware. Our evaluations show that our approach can effectively overlap All-to-All communication with embedding computations, subsequently reducing their combined execution time by 31% on average (up to 58%) for inter-node and by 25% (up to 35%) for intra-node configurations. Scale-out simulations indicate that our approach reduces DLRM execution time by ~10% for 128 node system.|arXiv.org|2023|10.48550/arXiv.2305.06942|Kishore Punniyamurthy, Khaled Hamidouche, Bradford M. Beckmann|0.0|0
1977|Multi-scale modeling using the dual domain material point method combined with molecular dynamics|For problems involving large material deformation rate, the material deformation time scale can be shorter than the material takes to reach a thermodynamical equilibrium. For such problems, it is difficult to obtain a constitutive relation. Furthermore, history dependency are usually important in these problems because of the thermodynamic non-equilibrium. A numerical method capable of tracking material deformation history is needed in a numerical simulation effort. A multi-scale numerical method consider ing non-equilibrium thermodynamics is developed based on dual domain material point method (DDMP). The DDMP method uses Lagrangian material points to track the history of the material whereas Eulerian grids are used to calculate the gradients in continuum level. Molecular dynamics (MD) calculations are performed in the material points and to calculate the closure quantities such as stress, under the condition of non-equilibrium thermodynamics, bypassing the need for a constitutive relation. Since the material points only communicate with mesh nodes and do not communicate among themselves , the MD calculations can be done in embarrassingly parallel. An efficient CUDA code is developed to accelerate MD calculations in GPU. Examples of shock wave propagation in Cerium and Copper single crystal are presented.||2016|10.2172/1261793|Tilak R. Dhakal|0.0|0
1980|Application of the Lagrangian Meshfree Approach to Modelling of Batch Crystallisation: Part II—An Efficient Solution of Integrated CFD and Population Balance Equations|"The second article in the series presents the application of the Smoothed Particle Hydrodynamics (SPH) method to modelling of batch crystallisation in stirred tanks. A methodology to integrate the population balance equations (PBE) in parallel and independently from the Navier-Stokes equations is demonstrated. The benefits of the proposed methodology in terms of computational requirements, accuracy and availability of the crystal size distribution are discussed. The specific formulation of the SPH equations where the resulting system of ordinary differential equations is solved using the weighted contributions rather than numerically by solving a linear system of equations allows for massive parallelisation and a very loose coupling of the population balance and the fluid dynamics. It has been demonstrated, that the population balance equations can be solved on a Shared Memory Architecture (SMA) system using the OpenMP interface while the fluid dynamics equations being computed independently on a General Purpose Graphics Processing Unit (GPGPU) using the NVidia CUDA technology. This way, a significant portion of the computational overhead due to the large number of additional transport equations resulting from the discretisation of the population balance was removed: the SPH simulation coupled with 200 population balance equations was only 40% slower compared to SPH-only simulation. Two methods for the solution of population balance equations that preserve full crystal size distribution were implemented: discretised population balance (DPB) and method of characteristics (MOCH). The DPB equations are solved using the high-resolution finite-volume method with flux limiter and the effect of a large number of different flux limiters have been investigated. Both methods were validated using the case studies from the literature where an analytical solution can be derived. The developed models were applied to a numerical solution of coupled computational fluid dynamics and population balance equations to model a batch crystallization process. The effect of the hydrodynamics on the local temperature/supersaturation and the resulting crystal size Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 2 November 2016 doi:10.20944/preprints201611.0012.v1"||2016|10.20944/PREPRINTS201611.0012.V1|Dragan D. Nikolic, P. Frawley|0.0|0
1981|On The Temporal Parallelisation of The Viterbi Algorithm|This paper presents an algorithm to parallelise the Viterbi algorithm along the temporal dimension to compute the maximum a posteriori (MAP) trajectory estimate of a hidden Markov model. We reformulate the MAP estimation problem as an optimal control problem. The proposed algorithm uses a parallelisation algorithm developed for optimal control problems that first performs a backward value function pass and then a forward trajectory recovery pass. The parallel Viterbi algorithm then corresponds to a specialised backward optimal control problem with a forward value function pass and backward MAP-trajectory recovery pass. The algorithm is empirically tested by running numerical simulations on a multi-core central processing unit (CPU) and a graphics processing unit (GPU).|European Signal Processing Conference|2023|10.23919/EUSIPCO58844.2023.10289998|S. Särkkä, Á. F. García-Fernández|0.0|0
1982|Predictive Maintenance Using GPU-Accelerated Partially Observable Markov Decision Process|The Industrial IoT era has seen an outburst of areas benefiting from collecting more data. This includes Industry 4.0 and predictive maintenance, which have benefited from advancements in edge and fog computing. Predictive maintenance aims to minimize the downtime due to maintenance of machinery, while simultaneously minimizing the risk of unforeseen failures. This paper proposes a method to aid industries to make maintenance scheduling decisions that can be adopted in a distributed factory environment. The Partially Observable Markov Decision Process (POMDP) approach is used to determine the optimal time for maintenance for a machine. We first put forward an offline method for learning the Markov model parameters using historical sensor data. To allow for continual learning, an algorithm based on particle filters is proposed to provide online estimation of parameters of a Partially Observable MDP model. The particle filter algorithm allows the framework to adapt uniquely to each machine. The relative benefits of the POMDP model over a standard MDP model in the presence of noisy sensor data are evaluated through simulations which show significant improvements in revenue and reduced downtime. The POMDP and particle filter computations are executed on GPU-accelerated edge devices which achieve a speed-up of around 4 times compared to the CPU implementation.|International Conference on Parallel and Distributed Systems|2019|10.1109/ICPADS47876.2019.00113|Naman Sharma, C. Tham|0.0|0
1984|A Hybrid Climate Modeling System Using AI-assisted Process Emulators|Focal Area This white paper addresses Focus Area II. We advocate developing a hybrid modeling system to improve the understanding of decadaland longer-scale predictability of high impact water cycle components. This hybrid model combines a partial differential equation (PDE)-based dynamic core with AI/ML based emulators to represent many of the computationally expensive processes in Earth’s climate models. The hybrid modeling system has the potential to exploit emerging graphics processing unit (GPU)-accelerated architectures and allows for the generation of large ensemble (~1000’s) simulations to better characterize the model uncertainty and understand predictability.||2021|10.2172/1769645|P. Xue, W. Pringle, J. Bessac, V. Ghate, Won Chang, J. Wozniak, Jiali Wang, Prasanna Balaprakash, Bethany Lusch, Xingqiu Yuan, R. Kotamarthi|0.0|0
1987|Supernode transformation on GPGPUs|ABSTRACT Supernode transformation, or tiling, is a technique that partitions algorithms to improve data locality and parallelism to achieve shortest running time. It groups multiple iterations of nested loops into supernodes to be assigned to processors for processing in parallel. A supernode transformation can be described by supernode size and shape. This paper focuses on supernode transformation on General Purpose Graphic Processing Units (GPGPUs), including supernode scheduling, supernode mapping to GPGPU blocks, and the finding of the optimal supernode size, for achieving the shortest total running time. The algorithms considered are two nested loops with regular data dependencies. The Longest Common Subsequence problem is used as an illustration. A novel mathematical model for the total running time is established as a function of the supernode size, algorithm parameters such as the problem size and the data dependence, the computation time of each loop iteration, architecture parameters such as the number of GPGPU blocks, and the communication cost. The optimal supernode size is derived from this closed form model. The model and the optimal supernode size provide better results than previous research and are verified by simulations on GPGPUs. Iterations in a two-dimensional uniform dependence algorithm iteration space of , shown as the intersections in the picture, can be grouped into rectangles of known as a tile or a supernode. This process is called supernode transformation or tiling. It reduces the inter-iteration communication cost thus improves the total execution time. The supernodes on the same wavefront may be scheduled on GPU to be processed at the same time, each by a GPU block. The size of the tile, , plays an important role in this transformation. The optimal size can lead to minimal total execution time. Graphical Abstract|Int. J. Parallel Emergent Distributed Syst.|2019|10.1080/17445760.2017.1296147|Weijia Shang, Yong Chen|0.0|0
1988|Simulation of Gas Dynamics of Hypersonic Aircrafts with the Use of Model of High-Temperature Air and Graphics Processor Units|Проводится численное моделирование обтекания гиперзвукового летательного аппарата с использованием модели высокотемпературного воздуха и гибридной архитектуры на основе высокопроизводительных графических процессорных устройств. Расчеты проводятся на основе уравнений Эйлера, для дискретизации которых применяется метод конечных объемов на неструктурированных сетках. Приводятся результаты исследования эффективности расчета гиперзвуковых течений газа на графических процессорах. Обсуждается время счета, достигнутое при использовании моделей совершенного и реального газа.\n Numerical simulation of the flow around a hypersonic aircraft is carried out using a high-temperature air model and a hybrid architecture based on high-performance graphics processing units. The calculations are performed with the Euler equations discretized by the finite volume method on unstructured meshes. The scalability of the developed implementations of the model is studied and the results of the study of the efficiency of calculating hypersonic gas flows on graphics processors are analyzed. The computational time spent with the perfect and real gas models is discussed.|Numerical Methods and Programming (Vychislitel'nye Metody i Programmirovanie)|2021|10.26089/NUMMET.V22R103|A. A. Sorokin, Y. Dobrov, K. N. Volkov, A. Karpenko, S. Malkovsky|0.0|0
1991|A Front Tracking Method Accelerated by Graphics Processing Units for Phase Change Modelling in Latent Heat Thermal Energy Storage: A Comparison with Interface Capturing Methods|Latent heat thermal energy storage (LHTES) has recently evolved into a promising approach for energy savings and pollution reduction. Phase change materials (PCMs) and the latent heat accompanying the phase change can be utilized to accumulate, store, are release the energy. Computer simulation tools are usually applied in the optimal design of LHTES devices as the simulations are fast, relatively easy to perform and not expensive. The paper presents a performance and accuracy comparison between a front tracking algorithm and well-known interface capturing methods – the enthalpy and apparent heat capacity methods. Acceleration by means of graphics processing unit (GPU) is used to enhance the computational efficiency of the presented front tracking algorithm. A container filled with a commercial PCM, which has been utilized in a numerous LHTES applications, is considered in the comparison. Simulation results are also compared to experimental data. The evaluation of results shows that the front tracking algorithm allows for a significantly higher accuracy than in case of interface capturing methods. The higher accuracy of simulation tools then contributes to a more accurate design of LHTES devices and allows for their higher performance and efficiency.||2017|10.3303/CET1761171|T. Mauder, J. Stetina, L. Klimeš, P. Charvát|0.0|0
1992|CUDA implementation of a fuzzy collecitve behaviour model|This thesis consists of an implementation of a fuzzy model using the graphics card and the CUDA environment. Our fuzzy model consists of entities, which are all the same type. Entity interaction occurs only between neighbouring entities, where the probability of an interaction is inversely proportional to the distance between the neighbouring entities. For this reason an optimisation of the process of gathering neighbouring entities has been made by dividing the simulation area into bins. The algorithm that drives the entity interaction is implemented with multiple kernels, which are described in detail in this thesis. For the purpose of ensuring the correctness of our results, JUnit tests were used along with the jFuzzyLogic library, which serves as a reference fuzzy logic system. For each entity we tested if the choice of the neighbouring entity was valid. We also tested the individual membership functions and the final crisp output. We conclude this thesis by comparing the execution times of the individual implementations and measuring the speed up value of our graphic card implementation.||2016||Anže Čuk|0.0|0
1994|Global climate simulations at 2.8 km on GPU with the ICON model|\n <p>The ICON modelling framework is a unified numerical weather and climate model used for applications ranging from operational numerical weather prediction to low and high resolution climate projection. In view of further pushing the frontier of possible applications and to make use of the latest evolution in hardware technologies, parts of the model were recently adapted to run on heterogeneous GPU system. This initial GPU port focus on components required for high-resolution climate application, and allow considering multi-years simulations at 2.8 km on the Piz Daint heterogeneous supercomputer. These simulations are planned as part of the QUIBICC project &#8220;The Quasi-Biennial Oscillation (QBO) in a changing climate&#8221;, which propose to investigate effects of climate change on the dynamics of the QBO.</p><p>Because of the low compute intensity of atmospheric model the cost of data transfer between CPU and GPU at every step of the time integration would be prohibitive if only some components would be ported to the accelerator. We therefore present a full port strategy where all components required for the simulations are running on the GPU. For the dynamics, most of the physical parameterizations and infrastructure code the OpenACC compiler directives are used. For the soil parameterization, a Fortran based domain specific language (DSL) the CLAW-DSL has been considered. We discuss the challenges associated to port a large community code, about 1 million lines of code, as well as to run simulations on large-scale system at 2.8 km horizontal resolution in terms of run time and I/O constraints. We show performance comparison of the full model on CPU and GPU, achieving a speed up factor of approximately 5x, as well as scaling results on up to 2000 GPU nodes. Finally we discuss challenges and planned development regarding performance portability and high level DSL which will be used with the ICON model in the near future.</p>\n||2020|10.5194/egusphere-egu2020-10306|R. Pincus, S. Rast, P. Marti, Valentin Clement, R. Schnur, L. Kornblueh, D. Alexeev, X. Lapillonne, M. Esch, R. Dietlicher, M. Giorgetta, W. Sawyer|0.0|0
1997|From CPU to GPU in Two Days: 3D Elastic Orthorhombic Modeling with OpenAcc.|Wavefield modeling is necessary in modern seismic imaging applications such as reverse time migration and full-waveform inversion. When the medium has complex structures such as salt bodies or carbonate reservoirs finite-difference methods (FDM) for wavefield simulation (extrapolation) are typically used to handle those cases. FDM allows us to simulate a multitude of realistic wave phenomena, but in some cases it makes our applications computationally intensive. When large numbers of sources and receivers are considered, a large number of wavefield extrapolations in the process of inversion is executed. To accelerate the 3-D wavefield simulation in elastic orthorhombic anisotropic media we rely on GPU technology. With the OpenAcc PGI compiler we create a pool of automatically managed memory that is shared between the CPU and GPU, thus achieving data management with minimal code modifications. We collapse the tightly nested loops used for velocity and stress updates which allows us to improve the execution time of the whole code by about ten percent. We report a performance speedup as we compare to a 16 core dual socket Haswell server of 1.15X on a K80 GPU and 2.32X when using the Pascal Tesla P100 GPU.||2017|10.3997/2214-4609.201702323|T. Alkhalifah, J-W. Oh, N. Masmoudi, V. Kazei, C. Tzivanakis|0.0|0
2002|Collision-based Particle Simulations on GPUs|As part of the Institute of System Dynamics and Control (SR) of the German Aerospace Center \n(DLR), the Department of Space System Dynamics (SR-RFS) is involved in developing locomotion \nsystems of planetary exploration missions. To reduce development time and costs, virtual prototypes \nare simulated to verify new designs and concepts e.g. for rover wheels before first physical \nprototypes are constructed. A common way of modeling soil in these simulations is the discrete \nelement method (DEM) where the soil volume is discretized into particles. Pasimodo, the engine \ncurrently used at DLR, uses the message passing interface (MPI) to distribute force computations to \nmultiple CPU cores or computers. The DEM however could be parallelized even further to profit \nfrom the highly concurrent execution on graphics processing units (GPUs). \nThis thesis therefore presents a particle engine capable of DEM to simulate soil and granular material \nsolely on GPU with the CPU acting as coordinator only. The engine is provided as NodeKit for \nthe OpenSceneGraph (OSG) library to ensure interoperability with existing software, e.g. the visualization \ntool DLR SimViz. Built upon the abstraction layer of OSG, OpenGL 4.3 and its compute \nshaders are the foundation for GPU computing performed by the engine. The simulation loop is \ndefined by object-oriented simulation steps. These are added to a particle system which also provides \nan API to define and manage particle attributes. Besides the basic classes, the software package \nships with built-in particle-particle and particle-mesh contact models for simulations of soil and \ngranular matter. An effcient render component visualizes the current state during a simulation. \nAdditionally, serializers can output the state in humand-readable form or via the H5Part format for \npost-processing. \nThree example applications have been implemented, qualitatively and quantitatively verifying that \nthe engine and its built-in contact models are correct and that it can be applied to the problem \ndomains at DLR. Benchmarks showed furthermore that, for up to 10000 (Nvidia GTX 970) and \n22000 (Nvidia GTX 1080), the proposed engine is faster than Pasimodo on an Intel Xeon E5-1620 \nand E5-2697 CPU. Detailed profiling revealed that the brute force contact detection with O(n^2) is \nthe main bottleneck and that an optimized algorithm will increase the speed-up compared to Pasimodo \nalso for larger number of particles.||2017|10.1007/978-3-319-58880-3_11|Simon Kerler|0.0|0
2003|Full Stokes finite-element modeling of ice sheets using a graphics processing unit|Thermo-mechanical simulation of ice sheets is an important approach to understand and predict their evolution in a changing climate. For that purpose, higher order (e.g., ISSM, BISICLES) and full Stokes (e.g., Elmer/Ice, http://elmerice.elmerfem.org) models are increasingly used to more accurately model the flow of entire ice sheets. In parallel to this development, the rapidly improving performance and capabilities of Graphics Processing Units (GPUs) allows to efficiently offload more calculations of complex and computationally demanding problems on those devices (Cecka and others, 2011; Markall and others, 2013). Thus, in order to continue the trend of using full Stokes models with greater resolutions, using GPUs should be considered for the implementation of ice sheet models. We developed the GPU-accelerated ice-sheet model Sainō. Sainō is an Elmer (http:// www.csc.fi/english/pages/elmer) derivative implemented in Objective-C which solves the full Stokes equations with the finite element method. It uses the standard OpenCL language (http://www.khronos.org/opencl/) to offload the assembly of the finite element matrix on the GPU. A mesh-coloring scheme (Komatitsch and others, 2010) is used so that elements with the same color (non-sharing nodes) are assembled in parallel on the GPU without the need for synchronization primitives. The current implementation shows that, for the ISMIP-HOM experiment A, during the matrix assembly in double precision with 8000, 87,500, 252,000 and 391,500 brick elements, Sainō is respectively 7x, 17x, 23x and 25x faster than Elmer/Ice (when both models are run on a single processing unit). In single precision, Sainō is even 9x, 38x, 45x and 49x faster than Elmer/Ice. A detailed description of the comparative results between Sainō and Elmer/Ice will be presented, and further perspectives in optimization and the limitations of the current implementation.||2016|10.5194/tc-2016-243|Seddik Hakime, G. Ralf|0.0|0
2007|Porting the MPI-parallelised LES model PALM to multi-GPU systems and many integrated core processors - an experience report|The computational power and availability of graphics processing units (GPUs) and many integrated core (MIC) processors on high performance computing (HPC) systems is rapidly evolving. However, HPC applications need to be ported to take advantage of such hardware. This paper is a report on our experience of porting the MPI+OpenMP parallelised large-eddy simulation model (PALM) to multi-GPU as well as to MIC processor environments using OpenACC and OpenMP. PALM is written in Fortran, entails 140 kLOC and runs on HPC farms of up to 43,200 cores. The main porting challenges are the size and complexity of PALM, its inconsistent modularisation and no unit-tests. We report the methods used to identify performance issues as well as our experiences with state-of-the-art profiling tools. Moreover, we outline the required porting steps, describe the problems and bottlenecks we encountered and present separate performance tests for both architectures. We however, do not provide benchmark information.|Int. J. Comput. Sci. Eng.|2018|10.1504/IJCSE.2017.10011396|C. Knigge, K. Ketelsen, T. Gronemeier, M. Noack, M. Sühring, T. Steinke, S. Raasch, P. Steinbach, Florian Wende, Helge Knoop|0.0|0
2009|Disease Simulation in Airport Scenario Based on Individual Mobility Model|As the rapid-spreading disease COVID-19 occupies the world, most governments adopt strict control policies to alleviate the impact of the virus. These policies successfully reduced the prevalence and delayed the epidemic peak, while they are also associated with high economic and social costs. To bridge the microscopic epidemic transmission patterns and control policies, simulation systems play an important role. In this work, we propose an agent-based disease simulator for indoor public spaces, which contribute to most of the transmission in cities. As an example, we study Guangzhou Baiyun International Airport, which is one of the most bustling aviation hubs in China. Specifically, we design a high-efficiency mobility generation module to reconstruct the individual trajectories considering both lingering behavior and crowd mobility, which greatly enhances the credibility of the simulated mobility and ensures real-time performance. Based on the individual trajectories, we propose a multi-path disease transmission module optimized for indoor public spaces, which includes three main transmission paths as close contact transmission, aerosol transmission, and object surface transmission. We design a novel convolution-based algorithm to mimic the diffusion process, which can leverage the high concurrent capability of the graphics processing unit to accelerate the simulation process. Leveraging our simulation paradigm, the effectiveness of common policy interventions can be quantitatively evaluated. For mobility interventions, we find that lingering control is the most effective mobility intervention with 32.35% fewer infections, while increasing social distance and increasing walking speed have a similar effect with 15.15% and 18.02% fewer infections. It demonstrates the importance of introducing crowd mobility into disease transmission simulation. For transmission processes, we find the aerosol transmission involves in 99.99% of transmission, which highlights the importance of ventilation in indoor public spaces. Our simulation also demonstrates that without strict entrance detection to identify the input infections, only performing frequent disinfection cannot achieve desirable epidemic outcomes. Based on our simulation paradigm, we can shed light on better policy designs that achieve a good balance between disease spreading control and social costs.|ACM Transactions on Intelligent Systems and Technology|2023|10.1145/3593589|Yong Li, Lu Geng, Yang Zhang, Changzheng Gao, Erzhuo Shao, Yulai Xie, Siran Ma, Zhenyu Han|0.0|0
2011|AdaSplats: Adaptative Splats from Semantic Point Cloud for Fast and High-Fidelity LiDAR Simulation|LiDAR sensors provide rich 3D information about sur-rounding scenes and are becoming increasingly important for autonomous vehicles’ tasks, such as semantic segmentation, object detection, and tracking. The ability to simulate a LiDAR sensor will accelerate the testing, validation, and deployment of autonomous vehicles while reducing the cost and eliminating the risks of testing in real-world sce-narios. To tackle the issue of simulating LiDAR data with high ﬁdelity, we present a pipeline that leverages real-world point clouds acquired by mobile mapping systems. Point-based geometry representations, more speciﬁcally splats, have proven their ability to accurately model the underlying surface in very large point clouds. We introduce an adaptive splats generation method that accurately models the underlying 3D geometry, especially for thin structures. We have also developed a faster-than-real-time LiDAR simulation by ray casting on GPU while focusing on efﬁciently handling large point clouds. We test our LiDAR simulation in real-world conditions, showing qualitative and quantitative results compared to basic splatting and meshing techniques, demonstrating the superiority of our modeling technique.|arXiv.org|2022|10.48550/arXiv.2203.09155|Jean-Emmanuel Deschaud, Jean Pierre Richa, Nicolas Dalmasso, Franccois Goulette|0.0|0
2014|CUDA GPU libraries and novel sparse matrix-vector multiplication - implementation and performance enhancement in unstructured finite element computations|The efficient solution to systems of linear and nonlinear equations arising from sparse matrix operations is a ubiquitous challenge for computing applications that can be exacerbated by the employment of heterogeneous architectures such as CPU-GPU computing systems. This paper presents our implementation of a novel sparse matrix-vector multiplication (a significant compute load operation in the iterative solution via pre-conditioned conjugate gradient based methods) employing LightSpMV with compressed sparse row (CSR) format, and the resulting performance characteristics using an unstructured finite element-based computational simulation. Computational performance analysed indicates that LightSpMV can provide an asset to boost performance for these computational modelling applications. This work also investigates potential improvements in the LightSpMV algorithm using CUDA 35 intrinsic, which results in an additional performance boost by 1%. While this may not be significant, it supports the idea that LightSpMV can potentially be used for other full-solution finite element-based computational implementations.|Int. J. Comput. Sci. Eng.|2019|10.1504/IJCSE.2017.10011618|R. Haney, R. Mohan|0.0|0
2025|Future Processor Hardware Architectures for the Benefit of Precise Particle Accelerator Modeling|Emerging processor architectures such as graphical processing units (GPUs) and Intel Many Integrated Cores (MICs) provide a huge performance potential for high performance computing. However developing software that uses these hardware accelerators introduces additional challenges for the developer. These challenges may include exposing increased parallelism, handling different hardware designs, and using multiple development frameworks in order to utilize devices from different vendors. During this work the Dynamic Kernel Scheduler (DKS) was developed, to provide a software layer between the host application and hardware accelerators. DKS handles the communication between the host and the device, schedules task execution, and provides a library of built-in algorithms. Algorithms available in the DKS library will be written in CUDA, OpenCL, and OpenMP. Depending on the available hardware, the DKS can select the appropriate implementation of the algorithm. The author used DKS to enable co-processor usage in applications such as OPAL (Object-oriented Particle Accelerator Library), musrfit and PET (Positron Emission Tomography) Image reconstruction application. These applications are developed at Paul Scherrer Institut, and ETH Zurich for particle accelerator modeling and experimental data analysis, and used by the world wide user community. The achieved results show that substantial speedups in application execution times can be achieved using co-processors compared to CPUs and with the help of DKS the process of integrating new processors in existing applications is simplified and more maintainable. The potential of the new hardware architectures is further demonstrated by porting to CUDA application for multibunch tracking (mbtrack) developed at SOLEIL (French national synchrotron facility). This application is used at PSI for the detailed study of coupled bunch instabilities and transient beam-loading. By using the computational power of GPUs the necessary simulations can be executed on the GPU instead of a larger computing cluster that would be required otherwise.||2017||U. Locans|0.0|0
2026|FPGA-BASED HYBRID COMPUTING FOR ESS LINAC SIMULATOR.|The thesis explores efficient implementation strategies for the European Spallation Source (ESS) linear accelerator simulator. The target simulator needs to run at real time, requires high computation accuracy, and should be scalable for high density beam scenarios. The high data processing, communication, and storage requirements due to a large data set, along with a strict accuracy requirement, poses a critical implementation challenge for traditional computing platforms. \n \nTo tackle these issues, this thesis uses a scalable platform with hybrid computing capabilities and the OpenCL framework for an unified programming model. The hybrid computing platform allows for mapping tasks to the most suitable hardware and explores heterogeneous memory hierarchy to fast data shuffling. The OpenCL framework allows functional portability and scalability across different target devices such as CPU, GPU and OpenCL accelerator devices like with FPGA and DSP arrays. The computational intensive tasks of the simulator can be conveniently mapped to the accelerators, where computational parallelism is explored. \n \nThe targeted simulator is implemented in a Xilinx hybrid computing platform, consisting of an Intel i7 CPU, an Nvidia 960 GPU, and a Xilinx Kintex Ultra-scale FPGA. Comparing to the benchmark (a C++ based implementation), we are able to accelerate the ESS simulator by more than 80x on the GPU and 25x with FPGA, with the same simulation accuracy (double precision floating point). We identified the implementation bottleneck on the specific platform, which is the memory bandwidth. This leads to our future work. One important future task is to investigate different hybrid computing platforms of different vendors, considering computation capability, memory bandwidth, as well as design software. Moreover, different data types will be examined, including fixed-point, double/single-precision floating point, or custom floating point.||2017||A. Jeevaraj|0.0|0
2027|MIMO Array InISAR 3-D Imaging with GPU Acceleration|In InISAR 3-D imaging, the separation of scatters which have same Range-Doppler location is a key problem. In this paper, a MIMO radar array, which is located along the z-axis, is used measure the positions of scatterters of a synthesis scatterters. The x-and ycoordinates of scatters are obtained from ISAR image directly. The z-coordinates are reconstructed by the spatial frequencies of scatterters in each synthesis scatterters based on the MIMO radar array. To accelerate the algorithm, a parallel programming model based on GPU is applied. The simulation results show that the proposed method is effective and precisionist for 3-D image reconstruction.||2016|10.1109/apemc.2016.7523020|Xiaochun Xie|0.0|0
2028|An Investigation of Parallel Programming Techniques Applied to Monte Carlo Simulations for Post-Flight Reconstruction of Spacecraft Trajectory|Parallelizing software to execute on multi-core central processing units (CPUs) and graphics processing units (GPUs) can be challenging. For some fields outside of Computer Science, this transition comes with new issues. For example, memory limitations can require modifications to code not initially developed to run on GPUs. This work applies the Open Multi-Processing (OpenMP) and Open Accelerators (OpenACC) directive-based parallelization strategies on a Monte Carlo simulation approach for trajectory reconstruction enabling it to run on multi-core CPUs and GPUs. Large matrix operations are the most common use of GPUs, which are not present in this algorithm; however, the natural parallelism of independent trajectories in Monte Carlo simulations is exploited. Benchmarking data are presented comparing execution times of the software for single-thread CPUs, multi-thread CPUs with OpenMP, and multi-thread GPUs using OpenACC. These data were collected using nodes with Intel ® Xeon ® E5-2670 (Sandy Bridge) CPUs enhanced with NVIDIA ® Tesla ® K40 GPUs on the Pleiades Supercomputer cluster at the National Aeronautics and Space Administration (NASA) Ames Research Center (ARC) and a local Intel ® Xeon Phi ™ node at NASA Langley Research Center (LaRC). and orientation), and integrates the inertial measurement unit (IMU) data to determine the vehicle states throughout its flight. Lugo et al. 1 developed a Monte Carlo based approach for trajectory reconstruction that incorporated the vehicle’s final state information and introduces statistics. This method decreases uncertainties in the reconstruction results, which improves model validations and post-flight analysis. However, this Monte Carlo approach requires the integration of several thousand trajectories. These calculations are time consuming when executed serially, but the execution time can be decreased by utilizing concurrent computation. This paper examines the use of parallel programming techniques on an algorithm that applies inertial navigation to trajectory reconstruction in a Monte Carlo dispersion process. The two parallel programming techniques being utilized are OpenMP and OpenACC, which are used on multi-core CPUs and GPUs, respectively. Two studies are conducted to determine optimal performance based on thread count with OpenMP and register per thread for OpenACC. Additionally, comparisons are shown between three different compilers and three different types of hardware. or V100, will tested in future work.|2018 Modeling and Simulation Technologies Conference|2018|10.2514/6.2018-3431|Robert A. Williams, Justin S. Green|0.0|0
2029|A Distribution Network State Estimation Method With Non-Gaussian Noise Based on Parallel Particle Filter|The particle filter (PF) algorithm is a powerful method for tackling non-Gaussian noise interference in distribution network state measurement. However, this algorithm suffers from slow solving speed and lengthy calculation time. To overcome this, a state estimation method based on parallel particle filter (PPF) is proposed, which leverages the independent computation features of each particle in the PF model to improve computational efficiency. This study utilizes the parallel architecture of Compute Unified Device Architecture (CUDA) and General Purpose Graphics Processing Units (GPGPU) to establish a one-to-one correspondence between particles and computing threads. An improved rejecting-resampling method is introduced to solve the problem of low execution efficiency caused by unmerged access to GPGPU memory. In addition, according to the relationship between the particle number and estimation accuracy of state variable of the PPF, the optimal particle number suitable for parallel computation is solved. Ultimately, the simulation results indicate that the proposed method can be used to effectively filter the non-Gaussian-colored noises from the collected data, which meets the requirements of the distribution network state estimation for the accuracy and real-time performance.|IEEE Access|2023|10.1109/ACCESS.2023.3335598|Wanxing Sheng, Ke-yan Liu, Haotian Ma|0.0|0
2031|Application of the graphics processor unit to simulate a near field diffraction|For many years, computer modeling program used for lecture demonstrations. Most of the existing commercial software, such as Virtual Lab, LightTrans GmbH company are quite expensive and have a surplus capabilities for educational tasks. The complexity of the diffraction demonstrations in the near zone, due to the large amount of calculations required to obtain the two-dimensional distribution of the amplitude and phase. At this day, there are no demonstrations, allowing to show the resulting distribution of amplitude and phase without much time delay. Even when using Fast Fourier Transform (FFT) algorithms diffraction calculation speed in the near zone for the input complex amplitude distributions with size more than 2000 × 2000 pixels is tens of seconds. Our program selects the appropriate propagation operator from a prescribed set of operators including Spectrum of Plane Waves propagation and Rayleigh-Sommerfeld propagation (using convolution). After implementation, we make a comparison between the calculation time for the near field diffraction: calculations made on GPU and CPU, showing that using GPU for calculations diffraction pattern in near zone does increase the overall speed of algorithm for an image of size 2048×2048 sampling points and more. The modules are implemented as separate dynamic-link libraries and can be used for lecture demonstrations, workshops, selfstudy and students in solving various problems such as the phase retrieval task.|Optical Metrology|2017|10.1117/12.2270293|Oleg K. Topalov, A. A. Zinchik, Yana B. Muzychenko|0.0|0
2035|Graphical Processing Unit Based Time-Parallel Numerical Method for Ordinary Differential Equations|On-line transient stability analysis of a power grid is crucial in determining whether the power grid will traverse to a steady state stable operating point after a disturbance. The transient stability analysis involves computing the solutions of the algebraic equations modeling the grid network and the ordinary differential equations modeling the dynamics of the electrical components like synchronous generators, exciters, governors, etc., of the grid in near real-time. In this research, we investigate the use of time-parallel approach in particular the Parareal algorithm implementation on Graphical Processing Unit using Compute Unified Device Architecture to compute solutions of ordinary differential equations. The numerical solution accuracy and computation time of the Parareal algorithm executing on the GPU are demonstrated on the single machine infinite bus test system. Two types of dynamic model of the single synchronous generator namely the classical and detailed models are studied. The numerical solutions of the ordinary differential equations computed by the Parareal algorithm are compared to that computed using the modified Euler’s method demonstrating the accuracy of the Parareal algorithm executing on GPU. Simulations are performed with varying numerical integration time steps, and the suitability of Parareal algorithm in computing near real-time solutions of ordinary different equations is presented. A speedup of 25× and 31× is achieved with the Parareal algorithm for classical and detailed dynamic models of the synchronous generator respectively compared to the sequential modified Euler’s method. The weak scaling efficiency of the Parareal algorithm when required to solve a large number of ordinary differential equations at each time step due to the increase in sequential computations and associated memory transfer latency between the CPU and GPU is discussed.|Journal of Computer and Communications|2020|10.4236/jcc.2020.82004|Sumathi Lakshmiranganatha, S. Muknahallipatna|0.0|0
2039|Massively Parallel Algorithm for Solving the Eikonal Equation on Multiple Accelerator Platforms|The research presented in this thesis investigates parallel implementations of the Fast Sweeping Method (FSM) for Graphics Processing Unit (GPU)-based computational platforms and proposes a new parallel algorithm for distributed computing platforms with accelerators. Hardware accelerators such as GPUs and co-processors have emerged as general-purpose processors in today’s high performance computing (HPC) platforms, thereby increasing platforms’ performance capabilities. This trend has allowed greater parallelism and substantial acceleration of scientific simulation software. In order to leverage the power of new HPC platforms, scientific applications must be written in specific lower-level programming languages, which used to be platform specific. Newer programming models such as OpenACC simplifies implementation and assures portability of applications to run across GPUs from different vendors and multi-core processors. The distance field is a representation of a surface geometry or shape required by many algorithms within the areas of computer graphics, visualization, computational fluid dynamics and more. It can be calculated by solving the eikonal equation using the FSM. The parallel FSMs explored in this thesis have not been implemented on GPU platforms and do not scale to a large problem size. This thesis addresses this problem by designing a parallel algorithm that utilizes a domain decomposition strategy for multi-accelerated distributed platforms. The proposed algorithm applies first coarse grain parallelism using MPI to distribute subdomains across multiple nodes and then fine grain parallelism to optimize performance by utilizing accelerators. The results of the parallel implementations of FSM for GPU-based platforms showed speedup greater than 20× compared to the serial version for some problems and the||2016|10.1142/s021812661650153x|Anup Shrestha|0.0|0
2040|Scalable High-Resolution Seismic Tomography|Summary Traditional ray-based tomography aims at recovering the long wavelength of the velocity model, The continuous progress in computing hardware and algorithms allows now for very dense simulation grids. Efficiently targeting finer simulations grids requires strong parallel scalability. High-resolution tomography seems a good candidate to this challenge, either on CPU or on GPU. Tomography workflow is composed of three main steps. The ray-based shooting and the Frechet derivatives computations steps are embarrassingly parallel although highly imbalanced. The minimization problem step is approximated by solving a linear system with a very sparse and highly rectangular matrix. A suitable parallel implementation on CPU is a client/server paradigm upon dynamic scheduled OpenMP and the modern pipe-l-cg iterative method. Benchmarks performed on Pangea2 supercomputer demonstrated the very well strong scalability behaviour. Furthermore, the move towards GPU is under investigation using streams upon OpenACC for shooting and derivatives processes, and the PETSc-GPU version for the solving step. Ray-based tomography is now capable to preserve and improve the high-frequency content of input velocity model. Moreover, its reasonable computational cost makes it competitive against more intensive computational methods like Full Waveform Inversion.|Fourth EAGE Workshop on High Performance Computing for Upstream 2019|2019|10.3997/2214-4609.201903286|P. Basini, L. Boillot|0.0|0
2047|Implementation of Radio Wave Propagation using RT Cores and Consideration of Programming Models|With the NVIDIA Turing architecture generation, several NVIDIA graphics processing units (GPUs) have introduced ray tracing acceleration hardware (RT cores). Ray tracing processing can be regarded as a simulation of wave and particle propagation, collision, and reflection. Therefore, it is expected to be applied to computational science and high-performance computing. However, few studies have been conducted using RT cores. The purpose of this research is to demonstrate the use of RT cores in the scientific and technical computing fields. We implemented a radio wave propagation loss calculation with the programmable ray tracing application framework OptiX and evaluated its performance. Furthermore, we investigated the challenges of reducing the description of framework-specific settings and the needs of hardware allocation. In the simple two spheres experiment, the RT core implementation showed the highest performance. Moreover, the acceleration was super linear scaling, between (10000, 5000) and (20000, 10000). In the experiment with a sphere and planes, the performance achieved by the RT cores was up to approximately 390 times higher than the parallel execution of the BVH search algorithm. We also proved that a large number of RT cores yielded higher performance. In the open data problem space experiment, we evaluated various GPUs and revealed that a larger number of RT cores is effective. These results show that RT cores are sufficiently effective for radio propagation calculations with an adequate number of ray projections. Through this research, we contributed to the RT core use in computational science by proposing an implementation method for ray tracing applications and revealing the effects of RT cores in radio wave propagation loss calculations.|IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum|2023|10.1109/IPDPSW59300.2023.00115|S. Ohshima, Toru Nagai, Tetsuya Hoshino, T. Katagiri, Shinya Hashinoki|0.0|0
2049|Методы визуализации протяженных ландшафтов в тренажерно-обучающих системах|One of the tasks that arise during the development of training (simulation) systems is the creation of a subsystem for displaying the earth's surface. Subsystems of this class provide modeling and visualization of the landscape, underlying surface (rivers, roads, forests, etc.), geometric objects that simulate cities or airports and so on. There are two main approaches to the visualization of extended landscapes --- continuous level of detail algorithms that use clustered triangulation and geometry clipmapbased algorithms. The method of generation and visualization of the Earth's surface presented in the work is based on the second approach. It is possible to perform calculations on the GPU using single precision, which allows faster calculations compared to using data types with double precision. In addition, this approach may be used on mobile graphics processors that do not support double precision. The proposed new method of dynamic resource management reduces the occupied video memory, which allows more detailed texture data to be loaded for a larger number of objects simultaneously.||2019|10.32603/2071-2340-2019-2-31-42|A. V. Roditelev, A. M. Giatsintov, V. N. Reshetnikov|0.0|0
2051|A Model of Vietnamese Optical Character Recognition|Optical Character Recognition (OCR) is a method to transform images in words into digital documents in the computer vision field. This helps digital or hand-written words and characters in images to be recognized by reading a document file with a monitor. However, a computer can only understand a picture as pixels or a tree-dimension array with values from 0 to 255. OCR applications can help translate nearby pixels into characters, words, and sentences. In this paper, we propose a transformer model to solve the Vietnamese OCR problem which is optimized or shortened to fit on the GPU while still producing solid results and loaded pre-trained from the HuggingFace Hub [1]. The proposed model achieves the maximum level of accuracy, 96.2%, with a CER of 0.8% in case of training on a labeled dataset that could not discriminate between single and compound words. The simulations result also proves that the training and assessment losses are reduced quickly in the first half and steadily in the second half. Due to the complexity of the Vietnamese and a few studies related to identifying Vietnamese through images, our study can be considered as an effective and supportive model for optical character recognition and a basis for related research.|Conference on Research, Innovation and Vision for the Future in Computing & Communication Technologies|2022|10.1109/RIVF55975.2022.10013905|Cong Tran, Huu-Sy Le, Kha-Tu Huynh|0.0|0
2052|On sampling determinantal and Pfaffian point processes on a quantum computer|\n DPPs were introduced by Macchi as a model in quantum optics the 1970s. Since then, they have been widely used as models and subsampling tools in statistics and computer science. Most applications require sampling from a DPP, and given their quantum origin, it is natural to wonder whether sampling a DPP on a quantum computer is easier than on a classical one. We focus here on DPPs over a finite state space, which are distributions over the subsets of {1, . . . ,N} parametrized by an N×N Hermitian kernel matrix. Vanilla sampling consists in two steps, of respective costs O(N3) and O(Nr2) operations on a classical computer, where r is the rank of the kernel matrix. A large first part of the current paper consists in explaining why the state-of-the-art in quantum simulation of fermionic systems already yields quantum DPP sampling algorithms. We then modify existing quantum circuits, and discuss their insertion in a full DPP sampling pipeline that starts from practical kernel specifications. The bottom line is that, with P (classical) parallel processors, we can divide the preprocessing cost by P and build a quantum circuit with O(Nr) gates that sample a given DPP, with depth varying from O(N) to O(r logN) depending on qubit-communication constraints on the target machine. We also connect existing work on the simulation of superconductors to Pfaffian point processes, which generalize DPPs and would be a natural addition to the machine learner’s toolbox. In particular, we describe ``projective'' Pfaffian point processes, the cardinality of which has constant parity, almost surely. Finally, the circuits are empirically validated on a classical simulator and on 5-qubit machines.|Journal of Physics A: Mathematical and Theoretical|2023|10.48550/arXiv.2305.15851|M. Fanuel, R. Bardenet, A. Feller|0.0|0
2053|Multi­-Scattering: Computational light transport in turbid media|This thesis presents and describes the development of an online freely accessible software called Multi-Scattering for the computational modeling of light propagation in scattering and absorbing media. The model is based on the use of the Monte Carlo method, where billions of photon packets are being launched and tracked through simulated cubic volumes. The software also includes features for modeling image formation by inserting a virtual collecting lens and a detection matrix which simulate a camera objective and a sensor array respectively. In addition, the Lorenz-Mie theory is integrated to generate the scattering phase functions from spherical particles. The model has been accelerated by means of general-purpose computing on graphics processing units, reducing the computation time by a factor up to 200x in comparison with a single CPU thread. By using four graphic cards on a single computer, the simulation speed increases by a factor of 800x. With an anisotropy factor g= 0.86, the transport path of one billion photons can be computed in 10 seconds for optical depth OD=10 and in 20 minutes for OD=500.The simulations are running from a computer server at Lund University, allowing researchers to login and use it freely without any need for programming skills or specific software/hardware installations. There are countless types of scattering media in which this model can be used to predict photon transport, including medical tissues, blood samples, clouds, smoke, fog, turbid liquids, spray systems, etc. In this thesis, the software has been used for a variety of scattering situations and to simulate photon transport: 1) inside a portion of a human head, 2) within atomizing spray systems, 3) in controlled aqueous dispersion of polystyren spheres, 4) for time-of-flight measurements in intralipid solutions and 5) for Diffuse Correlation Spectroscopy applications.Finally, the numerical results have been validated by rigorously comparing the simulated results with experimental data. The user interface for both setting-up a simulation and displaying the corresponding results is found at: https://multi-scattering.com (Less)||2021||J. Jönsson|0.0|0
2054|Energy calculation of MMFF94 force field on GPU (Special session on array computing systems and applications)|One important field of computer simulations is material science, the examination of the structure and interaction of molecules, for which various models and algorithms have been developed. One of these is the MMFF94 force field, which describes potential energies. Gradually modifying the coordinates of the atoms in the molecule and calculating the energy, states with favorable energies can be found. These could describe the real structure. In this work the implementation of the energy calculation of MMFF94 force field on GPU is described and analyzed.|European Conference on Circuit Theory and Design|2017|10.1109/ECCTD.2017.8093267|Á. Rák, Ádám Jász, G. Cserey|0.0|0
2055|Dynamic Grid Sea Surface Simulation Using Tessellation|"In order to reduce the resource consumption and enhance the effect in the simulation of large-scale ocean scenes, a dynamic grid algorithm based on viewpoint is proposed to remove unnecessary vertices outside the field of view. At the same time, the GPU hardware accelerated Tessellation is used to adaptively subdivide the sea surface grid inside the GPU. Refine the sea surface grid to meet the needs of high-precision grids near the field of view and low-precision grids far away from the field of view. Finally, using Perlin noise texture to simulate the natural sea surface. Experiments show that the algorithm improves the efficiency of generating the sea surface grid and improves the rendering frame rate while exhibiting realistic ocean visual effects. Introduction Natural phenomena like fluid motion and water surface are ubiquitous in our daily environment. Presenting natural phenomena more realistically and naturally is one of the most important aspects of improving virtual reality scenarios, especially in various fields related to computer graphics applications. However, the appearance of many natural phenomena like fluid motion and water surface is still a problem in computer graphics. The research on sea surface simulation has been carried out a lot, in the direction of grid model, wave waveform, calculation acceleration and so on. For the sea surface grid model, the concentric circle grid model [1-3] is similar to the common Clipmap algorithm, its grid regular, the drawing frame rate is high and constant, but because the detail control is simple, when the viewpoint moves up or down, it will cause the sea surface drawing details to be too low or too high. Bo Li [4] and Vladimr Belyaev [5] both give implementation of camera-based grid, and determine discrete LOD by horizontal distance. An adaptive near-surface mesh optimization method [6] is proposed to simulate a large-scale near-surface scene. Varela [7] developed a discretization of the wave spectrum that obtained a sea state statistically more equivalent to the original. Su [8] used CUDA parallel computing to improve the calculation speed, and applied GPU programming technology to achieve rapid rendering of the marine environment. Maheshya at el[9] introduces a spectral wave model based on the Phillips spectrum and render it by a hybrid method focusing on user perception. In previous studies, GPU-based Tessellation technology was still not widely used. In this paper, a dynamic mesh model based on viewpoint is used to effectively update the entire grid in the movement of the viewpoint and draw the basic grid of the sea surface. At the same time, using GPU Tessellation technology, the sea surface gird is subjected to secondary processing according to the LOD strategy, and the fineness of the sea surface grid is increased, thereby enhancing the effect of sea surface drawing. Dynamic Sea Surface Grid Algorithms Water surface rendering requires real-time, realistic reproduction of the three-dimensional surface of the waterscape. According to the actual situation, the close-up details are clear and the foreground details are blurred. At the same time, due to the limitation of the range of the angle of view, the scene behind the angle of view cannot be observed. Therefore, this article will use the 515 dynamic grid algorithm to generate a dynamic grid based on viewpoints, eliminating vertices outside the camera's perspective. Define the XOZ plane as the standard sea surface. To achieve a view-dependent grid, this article creates a camera and a camera-based frustum. The frustum will track the camera movement. The intersection of the frustum and the XOZ plane is used to constrain the grid of the ocean surface so that only a portion of the visible water is present. Figure 1. Camera and frustum. When the camera's viewing angle is down, the angle between the lower surface of the frustum and the sea surface is less than 90°, and the boundary of the sea surface grid is behind the near clipping plane of the window, causing the grid to not completely cover the window. As shown in Figure 1, the shaded area is lost. At this point, the waves generated based on the sea surface grid will create a blank near the viewpoint, as shown in Figure 2. In order to ensure a sufficient safety margin, the angle of view of the frustum can be dynamically increased, as follows: Fru = Camera(1 + σ|N · DirF|) (1) Where σ is the base magnification factor, N is the normal vector of the XOZ plane, and DirF is the direction vector of the camera. Figure 2. Missing area of sea surface. Figure 3. Dynamic grid. The projection grid idea is used to calculate the four boundary points of the intersection of the frustum and the XOZ plane. The LOD method is used in calculating the vertices of the grid. The position of the vertex Vij(0 < i ≤ P, 0 < j ≤ Q) in the grid is obtained by the interpolation of the positions of the four boundary points based on the weights, the weights and the vertices and the viewpoints. The distance is related, as shown in Figure 3. Vij = (1 − (i − 1)di)(1 − (j − 1)dj)wAVA +(1 − (i − 1)di)(j − 1)djwBVB +(i − 1)di(j − 1)djwCVC +(i − 1)di(1 − (j − 1)dj)wDVD (2)"|DEStech Transactions on Engineering and Technology Research|2018|10.12783/dtetr/ecar2018/26400|Xin Yuan, Qingzhou Mao, Yao-jie Chen|0.0|0
2058|Multiscale Approximation with Graphical Processing Units for Multiplicative Speedup in Molecular Dynamics|The timescales and structure sizes accessible via simulations of atomistic molecular dynamics (MD) can be advanced substantially by two independent techniques: (1) many-core parallelization with graphics processing units (GPUs) and (2) multiscale approximation with hierarchical charge partitioning (HCP). Achieving efficient many-core parallelization on the GPU generally requires highly synchronized and regular computation across the GPU. However, multiscale methods can result in highly asynchronous and irregular processing. Thus, one might expect that realizing such multiscale algorithms on the GPU would result in an overall loss of performance and that the total speedup obtained would be less than the product of the individual speedups for the two techniques separately, i.e., less than multiplicative speedup. To test this expectation in the context of atomistic MD, we designed and implemented our HCP multiscale method on NVIDIA GPU platforms. The HCP code was implemented in NAB, short for nucleic acid builder, and tested using the distance-dependent-dielectric, implicit solvent model. (NAB is the molecular dynamics module in the open-source Amber-Tools v1.4.) We show that for the HCP multiscale approximation and the common MD simulation model considered here, the degradation in performance due to asynchronous and irregular processing is mostly offset by a corresponding reduction in other asynchronous operations and slow global memory accesses. As a result, we realize near multiplicative speedups. For example, for a 475,000-atom virus capsid we were able to achieve a 11,071-fold combined speedup, only slightly less than the 11,706-fold multiplicative limit speedup -- 48.0-fold from the parallelization on the GPU times 243.9-fold from the multiscale approximation. The overall speedup depends on structure size, with smaller structures having lower speedups. An additional benefit of the HCP implementation on the GPU is the reduced memory requirement, which allows the processing of much larger structures that would otherwise be impossible on the limited memory GPU platform.|ACM International Conference on Bioinformatics, Computational Biology and Biomedicine|2016|10.1145/2975167.2975214|Wu-chun Feng, R. Anandakrishnan, A. Onufriev, Mayank Daga|0.0|0
2061|Real-time texturing and visualization of a 2.5D terrain model from live LiDAR and RGB data streaming in a remote sensing workflow|2.5D terrain model generation from a data stream provides high quality data, which can be used for assisting situational awareness, conducting operations and training in simulated environments. The objective of our research is to design and implement a real-time texturing and visualization of a 2.5D terrain model from live LiDAR and a RGB data streaming in a high performance remote sensing workflow. To achieve real-time processing, the incoming data streams are evaluated in small patches. In addition, the calculation time per patch must be lower than the recording/sampling time to ensure a real-time processing. Data meshing and projection of the images onto the mesh cannot be implemented in real-time using an off-the-shelf CPU. However, most of these steps are highly vectorizable (e.g., the projection of each LiDAR point into the camera images). In fact, modern graphics cards are highly specialized in computing such data types. Therefore, all computationally intensive steps were performed in the graphics card. Most of the steps for the terrain model generation have been implemented in CUDA and OpenCL. We compare both technologies regarding calculation times and memory management. The fastest technology was selected for each calculation step. Since the model generation is faster than the data acquisition time, the implemented software is real-time. Our approach has been embedded and tested in a real-time system consisting of a modern reconnaissance system connected to a ground control station via a radio link. During a flight, a human operator in the ground control station is able to observe a texturized terrain model, which was recently generated. The user is able to zoom in an interesting area.|Remote Sensing|2018|10.1117/12.2325401|Jonas Mispelhorn, Merlin Becker, W. Middelmann|0.0|0
2062|Numerical simulation of 2D real large scale floods on GPU: the Ebro River|Modern flood risk management and mitigation plans incorporate the presence of numerical models that are able to assess the response of the system and to help in the decision-making processes. The shallow water system of equations (SWE) is widely used to model free surface flow evolution in river flooding. Although 1D models are usually adopted when simulating long rivers due to their computational efficiency, 2D models approximate better the behaviour in floodplains of meandering rivers using a fine mesh which implies unaffordable computations in real-world applications. However, the advances on parallelization methods accelerate computation making 2D models competitive. In particular, GPU technology offers important speed-ups which allow fast simulations of large scale scenarios. In this work, an example of the scope of this technology is presented. Several past flood events have been modelled using GPU. The physical domain (middle part of the Ebro River in Spain) has a extent of 477 km 2 , which gives rise to a large computational grid. The steps followed to carry out the numerical simulation are detailed, as well as the comparison between numerical results and observed flooded areas reaching coincidences up to 87.25 % and speed enhancements of 1-h of simulation time for 1-day flood event. These results lead to the feasible application of this numerical model in real-time simulation tools with accurate and fast predictions useful for flood management.||2018|10.1051/E3SCONF/20184006007|M. Morales-Hernández, P. Brufau, P. García-Navarro, I. Echeverribar|0.0|0
2063|Registration of 3D Triangular Models to 2D X-ray Projections Using Black-box Optimisation and X-ray Simulation|Registration has been studied extensively for the past few decades. In this paper we propose to solve the registration of 3D triangular models onto 2D X-ray projections. Our approach relies extensively on global optimisation methods and fast X-ray simulation on GPU. To evaluate our pipeline, each optimisation is repeated 15 times to gather statistically meaningful results, in particular to assess the reproducibility of the outputs. We demonstrate the validity of our approach on two registration problems: i) 3D kinematic configuration of a 3D hand model, i.e. the recovery of the original hand pose from a postero-anterior (PA) view radiograph. The performance is measured by Mean Absolute Error (MAE). ii) Automatic estimation of the position and rigid transformation of geometric shapes (cube and cylinders) to match an actual metallic sample made of Ti/SiC fibre composite with tungsten (W) cores. In this case the performance is measured in term of F-score (86%), accuracy (95%), precision (75%), recall (100%), and true negative rate (94%). Our registration framework is successful for both test-cases when using a suitable optimisation algorithm.|Computer Graphics and Visual Computing|2019|10.2312/CGVC.20191265|S. Al-Maliki, R. Mihail, F. Vidal, Tianci Wen, J. Létang|0.0|0
2067|Working with incremental spatial data during parallel (GPU) computation|Central to many complex systems, spatial actors require an awareness of their local environment to enable behaviours such as communication and navigation. Complex system simulations represent this behaviour with Fixed Radius Near Neighbours (FRNN) search. This algorithm allows actors to store data at spatial locations and then query the data structure to find all data stored within a fixed radius of the search origin. \nThe work within this thesis answers the question: What techniques can be used for improving the performance of FRNN searches during complex system simulations on Graphics Processing Units (GPUs)? \nIt is generally agreed that Uniform Spatial Partitioning (USP) is the most suitable data structure for providing FRNN search on GPUs. However, due to the architectural complexities of GPUs, the performance is constrained such that FRNN search remains one of the most expensive common stages between complex systems models. \nExisting innovations to USP highlight a need to take advantage of recent GPU advances, reducing the levels of divergence and limiting redundant memory accesses as viable routes to improve the performance of FRNN search. This thesis addresses these with three separate optimisations that can be used simultaneously. \nExperiments have assessed the impact of optimisations to the general case of FRNN search found within complex system simulations and demonstrated their impact in practice when applied to full complex system models. Results presented show the performance of the construction and query stages of FRNN search can be improved by over 2x and 1.3x respectively. These improvements allow complex system simulations to be executed faster, enabling increases in scale and model complexity.||2019|10.1007/978-3-642-36203-3_71-1|Robert Chisholm|0.0|0
2070|APPLICATION OF R-FUNCTIONS METHOD AND SMOOTHED PARTICLE HYDRODYNAMICS FOR FLUID SIMULATION|Context. Existing fluid simulation methods have several disadvantages and can be improved with the help of new approaches to the solution of problems of computational fluid dynamics, which confirms the relevance of the work. Objective. The goal of the work is to improve existing methods of mathematical modeling of fluid based on smoothed particle hydrodynamics and R-functions method. Method. A new approach of joint use of smoothed particle hydrodynamics, marching cubes and R-functions method is proposed. Smoothed particle hydrodynamics helps to simulate fluid movement in real time. The method considers fluid as a discrete number of sample points (particles), which have mass, velocity, position and physical field quantities (pressure, temperature, mass-density, etc.). The R-functions method allows to solve the inverse problem of analytic geometry: finding an analytical equation of a 2D (3D) object based on its geometrical representation. Using the obtained equation, one can simply detect a particle collision with the object boundary and plot the object surface with the help of marching cubes algorithm. The suggested method allows to achieve good simulation quality and to perform all needed calculations and rendering in real time. Results. Computational experiments for the problem of fluid simulation were carried out. Various numbers of particles were used. Different kinds of objects were put into the considered region in order to investigate the fluid behavior. Conclusions. The results of visual simulations allow us to say that the obtained approach works as expected. Therefore, this method can be applied to several problems of fluid simulation where the collision detection with arbitrary objects is considered. Further research may be devoted to the optimization of neighbor-search algorithm, to performing all calculations in graphics processing unit or to taking into account other physical quantities.|Radio Electronics, Computer Science, Control|2019|10.15588/1607-3274-2019-3-3|O. O. Shabalin, A. V. Artiukh, I. Lytvyn, S. Kozyrenko|0.0|0
2072|Modeling and simulation of wind-farm flows|The subject of this chapter is the modeling and simulation of the flow through full wind farms, also known as wind plants, which are collections of many wind turbines within a region working together to supply power to the grid like any other type of power plant. This topic is interesting and exciting because of the increasing variety of and uses for wind plant flow modeling and simulation. For example, Figure 6.1 shows the computed flow field from a high-fidelity simulation of the Lillgrund offshore wind farm using computational fl uid dynamics (CFD) with large-eddy simulation (LES) turbulence modeling and actuator line rotor aerodynamics models. There are many ways to model this type of flow, ranging from very simple models of wind turbine wakes and their interactions that can run in a matter of seconds on a laptop to high-fidelity, turbulence -resolving LES of the atmospheric flow and wakes that require days of use of thousands of cores on a high-performance computing cluster. The uses of wind plant flow simulations are equally varied. Wind plant layout designers run hundreds or thousands of simulations over all possible wind directions and speeds using simple models to predict a future wind plant's annual energy production. Researchers use LESs of the wind plant to study phenomena such as wind turbine wake interactions with each other and with the atmospheric boundary layer (ABL). As computational resources become more readily available, it is even becoming common for turbine manufacturers to use CFD of some form to model wind plants in complex terrain to help in the wind turbine siting and selection process. We are beginning to see the expensive, high-fidelity tools being used to create data and knowledge that improves or enables the creation of new cheaper, lower fidelity tools. We are also beginning to see the low- and high-fidelity tools used together to perform uncertainty quantification of energy estimates and optimization of wind plant layouts. As new computing technologies mature, such as graphics processing unit (GPU)-based computing, the future of full wind plant flow modeling is exciting.|Volume 1|2019|10.1049/pbpo125f_ch6|P. Moriarty, M. Churchfield|0.0|0
2075|"ya||a: GPU-powered Spheroid Models for Mesenchyme and Epithelium"|"ya||a is yet another parallel agent-based model for morphogenesis. It is several orders of magnitude faster than onventional models, because it runs on GPUs and because it has been designed for performance: Previously only complex and therefore computationally expensive models could simulate both mesenchyme and epithelium. We chose o extend the simple spheroid model by the addition of spin-like polarities to simulate epithelial sheets and tissue polarity. We also incorporate recently developed models for protrusions and migration. ya||a is written in concise, plain UDA/C++ and available at github.com/germannp/yalla under the MIT license."|bioRxiv|2019|10.1101/525352|Philipp Germann, J. Sharpe, Miquel Marin-Riera|0.0|0
2081|Data Aware Simulation of Complex Systems on GPUs|GPUs have been demonstrated to be highly effective at improving the performance of Multi-Agent Systems (MAS). One of the major limitations of further performance improvements is in the memory bandwidth required to move agent data through the GPU’s memory hierarchy. This paper presents a formal model for data aware simulation and an empirical study into the impact of minimising data movement on performance. This study proposes a method that can be applied to the simulation of complex systems on GPUs to extract required data from agent behaviour during simulation time and how this information can be used to reduce data movement. The FLAME GPU software has been extended to demonstrate this technique. Three benchmark experiments have been applied to evaluate the overall reduction in simulation execution time under specific criteria. The results of the comparison between the current and new system show that reducing data movement within a simulation improves overall performance with up to 4.8x speedup reported.|International Symposium on High Performance Computing Systems and Applications|2019|10.1109/HPCS48598.2019.9188054|P. Richmond, Eidah J. Alzahrani, A. Simons|0.0|0
2082|Towards real-time simulation of human skin appearance (Conference Presentation)|In the current report, further developments of a biophysically-based optical model of human skin and a novel method for real-time realistic simulation of human skin are presented. The model utilizes voxelized representation of the tissue and considers spatial/volumetric variations in both structural e.g. surface roughness and chromophore concentration changes in skin layers such as distribution of blood, melanin, collagen, index of blood oxygen saturation, water, pigment content, etc. A Monte-Carlo based approach for simulation of spatially-varying Bidirectional Scattering-Surface Reflectance Distribution Function (BSSRDF) and subsequent physically based computer rendering has been developed. Computer modelling is accelerated by parallel computing on Graphics Processing Units (GPUs) using OpenCL (Open Computing Language). The results of simulation of BSSRDFs, reflectance spectra of human tissues, corresponding colours and 3D rendering examples of human skin appearance are presented and compared with in vivo experimental data obtained during clinical studies.|Dynamics and Fluctuations in Biomedical Photonics XVI|2019|10.1117/12.2509175|Natallia Trayan, A. Doronin|0.0|0
2083|FedHC: A Scalable Federated Learning Framework for Heterogeneous and Resource-Constrained Clients|Federated Learning (FL) is a distributed learning paradigm that empowers edge devices to collaboratively learn a global model leveraging local data. Simulating FL on GPU is essential to expedite FL algorithm prototyping and evaluations. However, current FL frameworks overlook the disparity between algorithm simulation and real-world deployment, which arises from heterogeneous computing capabilities and imbalanced workloads, thus misleading evaluations of new algorithms. Additionally, they lack flexibility and scalability to accommodate resource-constrained clients. In this paper, we present FedHC, a scalable federated learning framework for heterogeneous and resource-constrained clients. FedHC realizes system heterogeneity by allocating a dedicated and constrained GPU resource budget to each client, and also simulates workload heterogeneity in terms of framework-provided runtime. Furthermore, we enhance GPU resource utilization for scalable clients by introducing a dynamic client scheduler, process manager, and resource-sharing mechanism. Our experiments demonstrate that FedHC has the capability to capture the influence of various factors on client execution time. Moreover, despite resource constraints for each client, FedHC achieves state-of-the-art efficiency compared to existing frameworks without limits. When subjecting existing frameworks to the same resource constraints, FedHC achieves a 2.75x speedup. Code has been released on https://github.com/if-lab-repository/FedHC.|arXiv.org|2023|10.48550/arXiv.2305.15668|Yongbo Yu, Fuxun Yu, M. Zhang, Minjia Zhang, Ang Li, Xiang Chen|0.0|0
2088|Tutorial on finite-difference time-domain (FDTD) methods for room acoustics simulation|Wave-based simulation models for acoustics have remained an active area of research and development for the past three decades. Wave-based methods aim to solve the 3D wave equation directly and therefore have large computational costs relative to conventional ray-based methods, which tend to simplify wave-diffraction effects. However, wave-based methods offer the potential of complete numerical solutions, including all wave-scattering and diffraction effects over the full audible bandwidth. Additionally, wave-based methods are highly parallelisable, making them amenable to parallel computing architectures such as graphics processing units, which can greatly cut down lengthy simulation times. This tutorial will give an introduction to wave-based methods for room acoustics with a focus on the finite-difference time-domain (FDTD) method. The basic concepts behind FDTD methods, along with practical implementation issues, will be discussed and illustrated with examples. The relationship of FDTD methods to other wave-based methods, along with differences to ray-based methods, will be explained. Also, the use of graphics processing units (GPUs) with freely available FDTD software will be discussed.||2021|10.1121/10.0004614|B. Hamilton|0.0|0
2089|Mathematical modelling of burn injuries|Burn injuries can lead to serious complications that have a large influence on someone’s quality of life. In order to help patients, we need to gain insight into the wound healing process and in the development of complications that come with serious burn injuries. The final goal would be to prevent or at least reduce these complications. With a mathematical model, we simulate the time-evolution of the skin after a burn injury. The objective is to be able to quantify the impact of (patient-specific) parameters on the evolution of the skin properties. In this thesis, a cell-based model for the cell migration during wound healing is presented. Two types of cells, macrophages and fibroblasts, migrate by incentives of the strain energy density and concentration fields. Two concentration fields are implemented in the model: Platelet Derived Growth Factor (PDGF) and Transforming Growth Factor β (TGF-β). PDGF is a chemical that occurs in the wound bed and TGF-β is secreted by the macrophages. The processes such as cell division and death are modelled through stochastic processes. In this way, the data-consuming cell history does not have to be taken into account. The computational work of the corresponding simulations increases rapidly for larger numbers of cells. This holds in particular for the part that computes the strain energy density for every cell pair. This is tackled by employing the Graphics Processing Unit (GPU) for the largest bottlenecks. The CUDA framework is used to program the GPU, where certain parts of the computations can be run in parallel to make the computations more efficient. The GPU implementations are described in this report, alongside with the improved computation times. For a 2D simulation of one day, the speed-up in computation time from the CPU to the GPU implementation was a factor 58. To assess the accuracy of the computation of the concentration fields, Richardson’s Extrapolation is used to estimate the order of the error. More research is required on this part to achieve reasonable outcomes for the order. Moreover, an alternate approach for determining the TGF-β field by Green’s function was studied. The computational work for this method increased rapidly and was therefore unfit to be implemented in the model. Lastly, the influences of parameters on the model outcomes were investigated. Monte Carlo simulations are needed for the interpretation of the stochastic model. The influence of the time step and choice of normalization of the gradients were investigated. For the tested scenarios, the hypothesis that they behave similarly was not rejected. The single-precision implementation of the model did not lead to an overall speed-up. Especially, the increase in computation time of the solver for the concentration fields is unexpected. Using the GPU for efficiently modelling cell migration seems a good idea. The current model can be used as a basis for more sophisticated models in the future.||2018||E. Kleimann|0.0|0
2090|Statistical Analysis of High-Dimensional Genetic Data in Complex Traits|With the recent development of high-throughput DNA microarray and next-generation sequencing techniques for detecting various genomic variants (SNVs, CNVs, INDELs, etc.), genome-wide association studies (GWASs) have become a popular strategy to discover genetic factors affecting common complex diseases. Many GWASs have successfully identified genetic risk factors associated with common diseases and have achieved substantial success in unveiling genomic regions responsible for the various aspects of phenotypes. \n \nHowever, identifying the underlying mechanism of disease susceptible loci has proven to be difficult due to the complex genetic architecture of common diseases. The previously associated variants through GWASs only explain a small portion of the genetic factors in complex diseases. This rather limited finding is partly ascribed to the lack of intensive analysis on undiscovered genetic determinants such as rare variants and gene-gene interactions. Unfortunately, standard methods used to test for association with single common genetic variants are underpowered for detection of rare variants and genetic interactions. \n \nThis special issue is dedicated to presenting state-of-the-art statistical and computational methods for finding missing heritability underlying complex traits with massive genetic data including GWAS, next-generation sequencing, and DNA microarray data. The main focus of this special issue is on data mining and machine learning for advanced GWAS analysis. The advanced GWAS analysis includes multi-SNP analysis, gene-gene and gene-environment interaction analysis, estimation of missing heritability, and analysis of population heterogeneity. This special issue provides a platform to the researchers with expertise in data mining to discuss recent advancements in analytic approach of post-GWAS association analysis in field of statistics and bioinformatics. \n \nThe paper by W. Lee et al. proposes an approach to identifying clinically interesting subgroups in a heterogeneous population. The identification step uses a clustering algorithm and proposes an improved false discovery rate- (FDR-) based measure to remedy the overestimation of the ordinary FDR-based approach. The paper by Y. Kim et al. performs heritability estimation by using population- and family-based samples. The main idea lies in utilizing genetic relationship matrix to parameterize the variance of a polygenic effect for population-based samples. \n \nThree other papers consider gene-gene and gene-environment analysis. First, J. Yee et al. proposed interaction analysis for quantitative traits using entropy. Although there have been several methods proposed for gene-gene interaction using entropy, this is a robust entropy-based gene-gene interaction analysis that does not necessarily require an assumption on the distribution of trait for quantitative traits. Second, S. Y. Lee et al. focused on identifying multi-SNP effects or gene-gene interactions for survival phenotypes. In the framework of the multifactor dimensionality reduction (MDR) method, several extensions for the survival phenotype are considered and compared to the earlier MDR method through comprehensive simulation studies. Third, the paper by H. Xu et al. proposes a new GWAS strategy for detecting gene-gene and gene-environment analysis by combining the generalized multifactor dimensionality reduction-graphics processing unit (GMDR-GPU) algorithm with mixed linear model approach. It was further employed to investigate the genetic architecture of important quality traits in rice. The reliability and efficiency of the model and analytical methods were verified through Monte Carlo simulations. \n \nThe next two papers discuss multi-SNP analysis. Y. J. Yoo et al. propose a new multi-bin linear combination (MLC) test for multiple SNP analysis. It first performs clustering analysis to find cliques, complete subnetworks of SNPs with all pairwise correlations above a threshold, and then performs MLC test. Through simulation studies, the clique-based algorithm was shown to produce smaller clusters with stronger positive correlation than other MLC tests. The paper by S. Won et al. focuses on comparing penalized and nonpenalized methods for disease prediction with large-scale genetic data. It was shown that penalized regressions are usually robust and provide better accuracy than nonpenalized methods for disease prediction. \n \nNext, the work of J. Joo et al. considers robust genetic association tests for GWAS. How these robust tests can be applied to the replication study of GWAS and how the overall statistical significance can be evaluated using the combined test formed by p values of the discovery and replication studies were demonstrated. \n \nFinally, the paper by L. Li and M. Xiong proposes a dynamic model for RNA-seq data analysis. To extract biologically useful transcription process from the RNA-seq data, the ordinary differential equation (ODE) model was proposed for modeling the RNA-seq data. Differential principal analysis was developed for estimation of location-varying coefficients of the ODE. \n \nThis special issue discusses the most challenging issues in multiple SNPs approaches including gene-gene interaction and introduces statistical and computational methods for data mining and machine learning for revealing hidden association network of genotype-phenotype relationship. The nine papers in this special issue provide scientists with an overview on the recent advancements in multiple SNP analysis for GWASs. We hope the papers can encourage researchers towards a more extensive use of statistical genetics and bioinformatics techniques for research in biology and medical sciences.|BioMed Research International|2015|10.1155/2015/564273|T. Park, M. Xiong, X. Lou, K. van Steen|0.0|0
2092|Multiple GPU Parallel Strategy of Beam Tracing for Propagation Prediction in Large-Scale Scenes|Beam tracing (BT) can be used to simulate electromagnetic environment of urban area for its high efficiency and acceptable accuracy. As is known to all, implementing beam tracing on GPU will increase efficiency. However, when the scene scale is very large, one GPU is not sufficient to calculate all sites, besides, signal coverage is also a problem for conventional BT method. This paper provides a strategy of BT on multiple GPUs using OpenMP, and implements an optimized diffraction as well as multiple transmission on GPU to promote the coverage of our model. Numerical results demonstrate the high efficiency and good accuracy of proposed radio propagation model.|International Conference on Advances in Cybersecurity|2019|10.23919/ACES48530.2019.9060640|Chunbei Luo, Hai Lin, Kaiwei Lin, Xiangyang He|0.0|0
2099|An Experimental Fast Approach of Self-collision Handling in Cloth Simulation Using GPU|This study describes a fast\napproach using GPU to process self-collision in cloth animation without\nsignificant compromise in physical accuracy. The proposed fast approach is\nbuilt and works effectively on a modification of Mass Spring Model which is\nseen in a variety of cloth simulation study. Instead of using hierarchical data\nstructure which needs to be updated each frame, this fast approach adopts a\nspatial hashing technique which virtually partitions the space where the cloth\nobject locates into small cubes and stores the information of the particles\nbeing held in the cells with an integer array. With the data of the particles\nand the cells holding information of the particles, self-collision detection\ncan be processed in a very limited cost in each thread launched in GPU\nregardless of the increase in the amount of particles. This method is capable\nof visualizing self-collision detection and response in real time with limited\ncost in accessing memory on the GPU. \n\nThe idea of the proposed fast\napproach is extremely straightforward, however, the amount of memory which is\nneeded to be consumed by this method is its weakness. Also, this method\nsacrifices physical accuracy in exchange for the performance.||2021|10.25394/PGS.14504817.V1|Jichun Zheng|0.0|0
2102|Simian integrated framework for parallel discrete event simulation on GPUS|Discrete Event Simulation (DES) allows the modelling of ever more complex systems in a variety of domains ranging from biological systems to road networks. The increasing need to model larger systems stresses the demand for efficient parallel implementations of DES engines. Recently, Graphics Processing Units have emerged as an efficient alternative to Central Processing Units for the computation of some problems. Although substantial speedups can be achieved by using GPUs, writing an efficient implementations of given suitable problems often requires in-depth knowledge of the architecture. We present a new framework integrated in the Simian engine, which allows to make efficient use of GPUs for computationally intense sections of code. This framework allows modellers to offset some or all handlers to the GPU by efficiently grouping and scheduling these handlers. As a case-study, we implement a population activity simulation that takes into account evolving traffic conditions in a simulated urban area.|Online World Conference on Soft Computing in Industrial Applications|2015|10.5555/2888619.2888742|S. Eidenbenz, N. Santhi, Eunjung Park, Guillaume Chapuis|0.0|0
2103|Parallelization of the streamline simulation based on CUDA|To accelerate the streamline simulation and satisfy the real-time demands, in this paper, we proposed a method based on GPUs to parallelize the streamline simulation. CUDA architecture was used to implement the parallel algorithm on a single GPU and a multi-GPU computer. In our method, a grid is organized into a 2D array of blocks, and all threads in a block are organized into a 1D array, such that each thread in a block computes one streamline. To implement the method on multiple GPUs, the physical cell model is divided into sub-models to make the number of sub-models equal to the number of GPUs. The algorithm is applied to a Tóthian basin as an example. The experimental analysis shows that the parallel algorithm based on different numbers of GPUs has different accelerations. For a single GPU, the speedup reaches 170 times; and for five GPUs, it is 808 times, for a physical model with 40×106 cells. The conclusion is that GPUs can greatly accelerate the streamline simulation.|HP3C|2019|10.1145/3318265.3318269|Xiaohui Ji, Mulan Luo, Xu-sheng Wang|0.0|0
2104|Real-time Digital Simulation of CIGRE HVDC System using Full Spectrum Simulator|Analysing the behaviour of power system under various conditions of operation is essential in view of its importance. Analysis of HVDC system in particular, considering its increased popularity and complexity, is of importance. Although various software are available for analysing HVDC system, there exists lack of a precise tool for the purpose of real-time analysis. In this paper, full spectrum simulator (FSS) is used for real-time digital simulation of CIGRE HVDC system, after modelling it with mathematical equations. The computer language C is used for coding the system and individual sections of HVDC model are coded separately. Making use of parallel processors, the separate codes are combined to form the complete HVDC system and run on FSS for real-time analysis. The complete program for the model is analysed in off-line simulation and found to be similar to that obtained from FSS for real-time simulation.|2020 IEEE International Conference on Power Electronics, Smart Grid and Renewable Energy (PESGRE2020)|2020|10.1109/PESGRE45664.2020.9070761|A. Ajeesh, R. M. Shereef, Nevin Samuel, Charisma Maria, P. Jayan|0.0|0
2105|Efficient LBM on GPUs for dense moving objects using immersed boundary condition|There exists an increasing interest for using immersed boundary methods (IBMs) (Peskin 2000) to model moving objects in computational fluid dynamics. Indeed, this approach is particularly efficient, because the fluid mesh does not require to be body-fitted or to adjust dynamically to the motion of the body. Frequently, IBMs are implemented in combination with the lattice Boltzmann methods (LBM) (Kruger 2016). They fit elegantly into the framework of this method, and yield impressive parallel performances. It has also become quite common to accelerate LBM simulations with the use of Graphics Processing Units (GPUs) (Tolke 2010), as the underlying algorithm adjusts naturally to the architecture of such platforms. It is not uncommon that speedups of an order of magnitude, or more, at equal financial cost or energy consumption are observed, as compared to classical CPUs. IBM algorithms are however more difficult to adapt to GPUs, because their complex memory access pattern conflicts with a GPU's strategy of broadcasting data to a large number of GPU cores in single memory accesses. In the existing literature, GPU implementations of LBM-IBM codes are therefore restricted to situations in which the immersed surfaces are very small compared to the total number of fluid cells (Valero-Lara 2014), as is often the case in exterior flow simulations around an obstacle. This assumption is however not valid in many other cases of interest. \nWe propose a new method for the implementation of a LBM-IBM on GPUs in the CUDA language, which allows to handle a substantially larger immersed surfaces with acceptable performance than previous implementations.|arXiv.org|2019|10.1016/j.difgeo.2019.07.001|Joël Bény, J. Latt|0.0|0
2114|Increasing Biological Realism in Models of Sequence Evolution for Improved Statistical and Computational Performance|Models of molecular sequence evolution have been a pivotal source of insight into the biological mechanisms and forces of evolution. Model development and improvements in computational capacity have allowed for increasingly sophisticated analyses in the last four decades, however, many studies still use twenty year old GTR+[Gamma] models for convenience and computational tractability. Here we present a new model in the branch-site random effects likelihood framework, Adaptive Branch-Site Random Effects Likelihood (aBSREL), that is more sensitive and an order of magnitude faster than previous branch-site models. We demonstrate the effectiveness of aBSREL at detecting episodic diversifying selection using simulated sequences and previously studied empirical alignments. As part of an ongoing investigation into the importance of modeling natural selection and especially selection heterogeneity in molecular dating analyses, we undertake a detailed study of ten potentially ancient viral lineages using aBSREL, comparing our estimates with previously published molecular dating, historical, and fossil estimates. We also present two special purpose models within the branch- site REL framework and the addition of massively parallel accelerators such as Graphics Processing Units to the high performance computing resources addressable by HyPhy. The first of these two models, RELAX, is purpose built to detect relaxed selection while the BUSTED model uses a priori partitioning and sublinear complexity scaling to further improve computational tractability. Both models are demonstrated on simulated and empirical datasets and compared to current state of the art alternatives from the literature. These models represent a substantial expansion to the branch-site REL framework and are available for use in the HyPhy software package and on the Datamonkey.org webserver||2015|10.1016/b978-0-08-097086-8.43010-2|Martin D. Smith|0.0|0
2117|Parallel approaches of genetic algorithm in the MIC architecture of the Intel Xeon Phi|Today, genetic algorithms are widely used in many fields such as bioinformatics, computer science, artificial intelligence, finance ... Genetic algorithms are applied to create high quality solutions for complex optimization problems in the above industries. There have been many studies based on the proposed new hardware architecture that aims to speed up the execution of genetic algorithms as quickly as possible. Some studies suggest parallel genetic algorithms on systems with multicore CPUs and / or graphics processing units (GPUs). However, very few solutions propose a genetic algorithm that can be run on systems that use the new Intel Xeon Phi co-processor (Intel Many-Integrated Core (MIC) architecture). For that reason, we propose and develop the study of the genetic algorithm on high-performance computing systems with Intel Xeon Phi co-processors. This study will present the results of parallel approaches of genetic algorithm on one and more Intel Xeon Phi co-processors by the following methods: (i) Intel Xeon Phi programming model Offload and Native; and (ii) a combined model of MPI and OpenMP. The proposed genetic algorithm can find the optimal schedule for the energy-efficient scheduling problem of virtual machines on physical machines with the goal of minimization total energy consumption. The results of the simulations show the feasibility of implementing a genetic algorithm on one or many Intel Xeon Phi. Genetic algorithm on one or more distributed Intel Xeon Phi always results in faster algorithm execution time than sequential genetic algorithm and the ability to find better solutions using more Intel Xeon Phi. This research result can be applied to other meta-heuristic like TABU search, Ant Colony Optimization.||2020|10.32508/stdjet.v2i4.612|Tuan Nguyen, Nguyen Quang Hung, N. Thoai|0.0|0
2119|GPU‐accelerated parallel forward method to establish parametric scattering center models for complex targets|Forward method to establish parametric scattering center models is highly efficient for radar feature extraction of target. Nevertheless, scattering center extraction especially for multiple reflections is still time‐consuming due to path tracing, ray clustering, and parameter determination from electrically large and complex targets. As the multiple reflection items add, the time consumption of forward parametric scattering center modeling method increases dramatically, which can hardly meet the practical requirements. In this work, we propose a graphics processing unit‐accelerated forward parametric scattering center modeling method together with stackless KD‐Tree traversal technique that accelerates path tracing, clustering, and parameter determination to extract parametric scattering centers. Concurrently, the parameters of the scattering centers arising from triple (usually the maximum number of reflections) or above reflection items are forward determined for obtaining feature information as much as possible from complex target. Finally, the reconstructed radar characteristics from parametric models have been verified by comparison with exactly simulated results. Meanwhile, those examples using this improved method have achieved great speedup.|Microwave and optical technology letters (Print)|2020|10.1002/mop.32355|G. Zhu, H. Yin, Liang-Liang Zhang, Siyuan He, Kai Huang, Yunhua Zhang|0.0|0
2120|GPU-Accelerated Post-Processing and Animated Volume Rendering of Isogeometric Analysis Results|. Isogeometric analysis (IGA) has enabled better CAD integration by using the same spline representations for modeling and analysis. Traditionally, the (cid:28)nite element analysis re-sults are visualized by creating a texture map of the property of interest and superimposing them over the boundary representation (B-rep) model or the mesh. This technique cannot be directly used to render internal quantities of interest without computationally intensive sectioning and remapping of the textures. In this paper, we present a GPU accelerated algorithm that produces a time-varying voxelized representation of the property of interest. We then render the voxelized models generated using this approach at an interactive frame rate. To voxelize the models, we perform a modi(cid:28)ed ray intersection test with BØzier elements using a localized ray grid. We then generate a variable density voxel model representing the analysis results using the intersection data, which is repeated for the di(cid:27)erent time frames of the analysis. The complete time-series data is stored in GPU memory using a (cid:29)at data structure, which is then rendered using GPU-accelerated ray casting. This direct voxelization technique enables a detailed analysis of the simulation using interactive slicing techniques without computationally intensive post-processing. We demonstrate this approach for two biomechanics simulations(cid:22)a cardiac solid mechanics model and an aorta (cid:29)uid dynamics model. These models were tested at di(cid:27)erent resolutions using single or batch frame processing. Our method can render the results of a complete isogeometric analysis at an interactive frame rate of over 30 frames per second for all test cases.|CAD'21 Proceedings|2021|10.14733/CADCONFP.2021.177-181|M. Rajanna, Harshil S. Shah, A. Krishnamurthy, O. Bingol, Xin Huang|0.0|0
2121|Monte Carlo Simulation of Kinetic Phenomena in Solids Using OpenACC Parallel Computing Technology|The development of a software package using the OpenACC parallel computing technology for Monte Carlo simulation of the kinetic coefficients of homogeneous semiconductor materials is presented. The package is a set of interconnected classes, the parameters of the material and external fields are redefined in the child classes available to the user, which makes it possible to model a wide range of materials. The package allows us to use models of elastic (acoustic phonons, charged impurities) and inelastic (polar and nonpolar optical phonons) electron scattering in the single-band approximation. The use of OpenACC technology makes it possible to use both shared memory systems and hybrid systems equipped with graphics processors as a computing platform. The possibility of saving data about each particle at each time step of the simulation is provided. It allowed, in particular, to trace the dependence of the average collision frequency of the energy of charge carriers and strength of the DC electric field applied to the sample, in the beta-modification of gallium oxide, to assess the applicability of the conductivity models offered by other research groups. It is shown that the greatest contribution to the conductivity of the beta modification of gallium oxide at room temperatures is made by the scattering of electrons on polar optical phonons, and the average collision frequency, as well as the percentage of collisions of an electron with various types of inhomogeneities of the crystal lattice, weakly depend on the strength of the constant electric field. At a temperature of about 100 K, with an increase in the constant electric field applied to the sample, firstly, the proportion of scattering with the emission of polar optical phonons increases significantly and the proportion of scattering on charged impurities decreases, and secondly, the total frequency of collisions increases. This is due, on the one hand, to the heating of the electron gas by an electric field and the activation of scattering channels with the emission of a phonon at a given temperature, on the other hand, to an insufficiently rapid increase in the concentration of current carriers due to the ionization of impurities. Thanks to the Monte Carlo simulation, it was possible to directly evaluate the validity of the use of the Farvaque correction for an approximate description of the processes of inelastic electron scattering on polar optical phonons by introducing some effective relaxation time.||2021|10.15688/MPCM.JVOLSU.2021.1.3|D. Abdrakhmanov, V. Konchenkov, D. Zav’yalov, V. Abdrakhmanov|0.0|0
2122|RAPIDS: Reconciling Availability, Accuracy, and Performance in Managing Geo-Distributed Scientific Data|In modern science, big data plays an increasingly important role. Many scientific applications, such as running simulations on supercomputers or conducting experiments on advanced instruments, produce huge amount of data at unprecedented speed. Analyzing and understanding such big data is the key for scientists to make scientific breakthroughs. However, data might become unavailable for scientists to access when outages or maintenance of the storage system occur, which severely hinders scientific discovery. To improve the data availability, data duplication and erasure coding (EC) are often used. But as the scientific data gets larger, using these two methods can cause considerable storage and network overhead. In this paper, we propose RAPIDS, a hybrid approach that combines the multigrid-based error-bounded lossy compression with erasure coding, to significantly reduce the storage and network overhead required for maintaining high data availability. Our experiments show that RAPIDS reduces the storage overhead by up to 7.5x and network overhead by up to 3x to achieve the same level of availability compared to the regular EC method. We improve RAPIDS by building two models to optimize the fault tolerance configurations and data gathering strategy. We demonstrate that RAPIDS significantly improves performance when running on many CPU cores in parallel or on GPUs.|IEEE International Symposium on High-Performance Parallel Distributed Computing|2023|10.1145/3588195.3592983|Ben Whitney, Qing Liu, Ana Gainaru, S. Klasky, Zhengchun Liu, Qian Gong, Ian T. Foster, Joy Arulraj, Lipeng Wan, Jieyang Chen, Xin Liang|0.0|0
2126|A GPU-accelerated high-order multiphase computational tool for asteroid fragmentation and pulverization modeling|The impact threat of asteroids or comets, referred to as near-Earth objects (NEOs), is a growing concern to the global community. A NEO collision could have severe consequences, especially in highly populated regions. To combat this threat, several asteroid deflection strategies have been introduced, but computational modeling is needed to investigate the feasibility of such missions. To this end, an asteroid disruption software tool was built to handle the simulation of multiple disruption techniques, namely high-energy explosives and kinetic-energy impactors, and to investigate the feasibility and effectiveness of these approaches. In addition, the software is intended to use graphics processing units (GPUs) as the primary computational resource rather than central processing units (CPUs). While GPUs are quickly becoming an alternative computing platform for numerical simulations, it is not clear which numerical schemes provide the highest computational efficiency for different problem types. The numerical accuracies and computational work of several numerical methods are compared using GPU computing implementation. The Correction Procedure via Reconstruction (CPR), Discontinuous Galerkin (DG), Nodal Discontinuous Galerkin (NDG), Spectral Difference (SD), and Finite Volume (FV) methods are investigated for smooth and discontinuous problems to determine the most efficient method to apply towards asteroid disruption simulations. The computational time to reach a set error criteria and total time to compute solutions are compared across the methods. It is shown that while FV methods can produce solutions with the lowest computation time for discontinuous problems, they produce larger errors for smooth problems at the same order of accuracy. The SD method illustrates an excellent trade-off in terms of error and total work, computing both smooth and discontinuous problems faster than most other methods while providing low error norms. From the aforementioned study, the SD method is applied to multifluid modeling for asteroid disruption applications. In order to model the multiple material phases associated with the problem, a Diffused-Interface Method (DIM) approach is integrated into the SD method (SD-DIM). This allows high-order solution reconstructions for problems containing multi-material interactions, where different||2016|10.31274/ETD-180810-4784|B. Zimmerman|0.0|0
2130|sPEGG: high throughput eco-evolutionary simulations on commodity graphics processors|Integrating population genetics into community ecology theory is a major goal in ecology and evolution, but analyzing the resulting models is computationally daunting. Here we describe sPEGG ($\underline{\textrm{s}}\textrm{imulating}$ $\underline{\textrm{P}}\textrm{henotypic}$ $\underline{\textrm{E}}\textrm{volution}$ on $\underline{\textrm{G}}\textrm{eneral Purpose}$ $\underline{\textrm{G}}\textrm{raphics Processing Units}$ (GPGPUs)), an open-source, multi-species forward-time population genetics simulator. Using a single commodity GPGPU instead of a single central processor, we find sPEGG can accelerate eco-evolutionary simulations by a factor of over 200, comparable to performance on a small-to-medium sized computer cluster.||2016|10.1007/978-4-431-55840-8_9|K. Okamoto, P. Amarasekare|0.0|0
2132|RIMurban – A generalized GPU-based model for urban pluvial flood risk modelling and forecasting|<p>Urban flash floods caused by heavy convective precipitation pose an increasing threat to communes world-wide due to the increasing intensity and frequency of convective precipitation caused by a warming atmosphere. Thus, flood risk management plans adapted to the current flood risk but also capable of managing future risks are of high importance. These plans necessarily need model based pluvial flood risk simulations. In an urban environment these simulations have to have a high spatial and temporal resolution in order to site-specific management solutions. Moreover, the effect of the sewer systems needs to be included to achieve realistic inundation simulations, but also to assess the effectiveness of the sewer system and its fitness to future changes in the pluvial hazard. The setup of these models, however, typically requires a large amount of input data, a high degree of modelling expertise, a long time for setting up the model setup and to finally run the simulations. Therefor most communes cannot perform this task.</p><p>&#160;In order to provide model-based pluvial urban flood hazard and finally risk assessments for a large number of communes, the model system RIM<em>urban</em> was developed. The core of the system consists of a simplified raster-based 2D hydraulic model simulating the urban surface inundation in high spatial resolution. The model is implemented on GPUs for massive parallelization. The specific urban hydrology is considered by a capacity-based simulation of the sewer system and infiltration on non-sealed surfaces, and flow routing around buildings. The model thus considers the specific urban hydrological features, but with simplified approaches. Due to these simplifications the model setup can be performed with comparatively low data requirements, which can be covered with open data in most cases. The core data required are a high-resolution DEM, a layer of showing the buildings, and a land use map.</p><p>The spatially distributed rainfall input can be derived local precipitation records, or from an analysis of weather radar records of heavy precipitation events. A catalogue of heavy rain storms all over Germany is derived based on radar observations of the past 19 years. This catalogue serves as input for pluvial risk simulations for individual communes in Germany, as well as a catalogue of possible extreme events for the current climate. Future changes in these extreme events will be estimated based on regional climate simulations of a &#916;T (1.5&#176;C, 2&#176;C) warmer world.</p><p>RIM<em>urban</em> simulates the urban inundation caused by these events, as well as the stress on the sewer system. Based on the inundation maps the damage to residential buildings will be estimated and further developed to a pluvial urban flood risk assessment. Because of the comparatively simple model structure and low data demand, the model setup can be easily automatized and transferred to most small to medium sized communes in Europe and even beyond, if the damage estimation is modified. RIM<em>urban</em> is thus seen as a generally app&#246;licable screening tool for urban pluvial flood risk and a starting point for adapted risk management plans.</p>||2021|10.5194/EGUSPHERE-EGU21-2985|H. Apel, B. Merz, S. Vorogushyn, M. Farrag, H. Kreibich, N. Dung, M. Karremann|0.0|0
2136|Data-scarce surrogate modeling of shock-induced pore collapse process|Understanding the mechanisms of shock-induced pore collapse is of great interest in various disciplines in sciences and engineering, including materials science, biological sciences, and geophysics. However, numerical modeling of the complex pore collapse processes can be costly. To this end, a strong need exists to develop surrogate models for generating economic predictions of pore collapse processes. In this work, we study the use of a data-driven reduced order model, namely dynamic mode decomposition, and a deep generative model, namely conditional generative adversarial networks, to resemble the numerical simulations of the pore collapse process at representative training shock pressures. Since the simulations are expensive, the training data are scarce, which makes training an accurate surrogate model challenging. To overcome the difficulties posed by the complex physics phenomena, we make several crucial treatments to the plain original form of the methods to increase the capability of approximating and predicting the dynamics. In particular, physics information is used as indicators or conditional inputs to guide the prediction. In realizing these methods, the training of each dynamic mode composition model takes only around 30 seconds on CPU. In contrast, training a generative adversarial network model takes 8 hours on GPU. Moreover, using dynamic mode decomposition, the final-time relative error is around 0.3% in the reproductive cases. We also demonstrate the predictive power of the methods at unseen testing shock pressures, where the error ranges from 1.3% to 5% in the interpolatory cases and 8% to 9% in extrapolatory cases.|arXiv.org|2023|10.48550/arXiv.2306.00184|Siu Wun Cheung, H. Springer, Youngsoo Choi, T. Kadeethum|0.0|0
2138|Simulating Peer to Peer Networks using GPU High Perfomance Support|Peer-to-Peer networks are used by many applications to share resources between nodes. We have proposed a parallel version of a simulator for some aspects of a peerto-peer network performing file sharing. Being this analysis computationally expensive for contemporary CPUs, the computing power of Graphic Processing Units allows a great gain in performance during simulation. Specifically, we have used the NVIDIA Computer Unified Device Architecture programming model to simulate the behaviour of a peer-to-peer network performing data transfers, and compute communication delays as well as data updates.|System (Linköping)|2015|10.1119/perc.2014.pr.015|G. Susinni, G. Greco|0.0|0
2139|Algorithmic advances in parallel architectures and energy‐efficient computing|This special issue of Concurrency and Computation: Practice and Experience contains revised and extended versions of selected papers presented at the 12th International Conference on Parallel Processing and Applied Mathematics (PPAM 2017), which was held on September 10-13, 2017, in Lublin, Poland. PPAM is a biennial series of international conferences dedicated to exchanging ideas between researchers involved in parallel and distributed computing, including theory and applications, as well as applied and computational mathematics. The focus of PPAM 2017 was on models, algorithms, and software tools that facilitate efficient and convenient use of modern parallel and distributed computing systems, as well as on large-scale applications, including data-intensive and machine learning problems. PPAM 2017 was organized by the Department of Computer and Information Science of Czestochowa University of Technology in Czestochowa, Poland, together with Maria Curie-Sklodowska University in Lublin, Poland, under the patronage of the Committee of Informatics of the Polish Academy of Sciences, in cooperation with the ICT COST Action IC1305 ‘‘Network for Sustainable Ultrascale Computing (NESUS)’’. This meeting gathered more than 170 participants from 25 countries. The accepted papers were presented at the regular tracks of the PPAM 2017 conference, as well as during the workshops. A strict reviewing process, with each submission evaluated by at least three reviewers, resulted in acceptance of 100 contributed papers for publication in the conference proceedings, while approximately 42% of the submissions were rejected. For regular tracks, 49 papers were selected from 98 submissions, resulting in an acceptance rate of 50%. Based on the results of the reviews, selected papers were recommended for a special journal issue. Besides quality, another important criterion for selection was each paper contribution to thematic consistency of the issue. The main focus of this special issue is on algorithmic advances in matching the software properties to the targeted parallel architecture, including graphics processing unit (GPU) accelerators and clusters. These advances are crucial for parallelizing successfully such complex applications as simulating granular flows, solving nonsingular systems, electronic transport simulations, solving three-dimensional fractional power diffusion problems, dynamic programming, computer graphics, parallel event-driven simulation, and others. A complementary topic of this issue is energy-efficient computing, since the energy consumption has become a limiting factor for high-performance computing (HPC) applications in recent years. The authors of selected papers were contacted after the conference and invited to submit revised and extended versions of their works. These new versions were reviewed independently again by at least three reviewers. Finally, ten contributions were accepted for publication. They are summarized below. The work of Krestenitis and Weinzierl1 focuses on simulating granular flows using discrete element method models. This problem is computationally challenging— a bottleneck arises when identifying all particle contact points per time steps. To introduce concurrency to particle comparisons, while keeping their number low, the authors propose a tree-based multilevel metadata structure to manage the particles, as well as a novel scheme of identifying the contact points. Furthermore, a novel adaptivity criterion allows an explicit time stepping technique to work with comparably large time steps. The fusion of the proposed developments yields promising speedups for maximally asynchronous task-based realizations. This work shows that new computer architectures can push the boundary of such many-particle simulations by choosing the right data structures and data processing schemes. An efficient algorithm for the parallel robust solution of triangular linear systems is presented in the paper by Mikkelsen et al2. Such systems are central to the solution of general linear systems and computation of eigenvectors, using either forward or backward substitution. However, there are well-conditioned systems for which substitution fails due to overflow. This paper presents novel algorithms that are blocked and parallel, while dynamically scaling the solution and right-hand side values to avoid overflows. A new task-based parallel robust solver Kiya is developed and compared against LAPACK solvers. When there are many complex right-hand sides, Kiya performs significantly better than the robust solver DLATRS and is not significantly slower than the nonrobust solver DTRSM. The algorithm developed in the work of Spellacy et al3 extends previous work on inversion of block tridiagonal matrices from the Hermitian/symmetric case to the general case, with variable sub-block sizes. The presented investigation is motivated by the requirements of atomic and molecular–scale electronic transport simulations, in particular, the SMEAGOL electronic transport code. A parallel divide-and-conquer approach is used to develop a novel algorithm, which is then implemented in Fortran with message passing interface. Its benefits in terms of runtimes and memory footprint are examined when compared against inverses obtained using the well-known libraries ScaLAPACK and MUMPS.|Concurrency and Computation|2019|10.1002/cpe.5260|B. Szymanski, R. Wyrzykowski|0.0|0
2142|[A Method for Fluorescent Diffuse Optical Tomography Based on Lattice Boltzmann Forward Model on GPU Parallelization].|Fluorescent Diffuse Optical Tomography (FDOT) is an emerging imaging method with great prospects in fields of biology and medicine. However, the current solutions to the forward problem in FDOT are time consuming, which greatly limit the application. We proposed a method for FDOT based on Lattice Boltzmann forward model on GPU to greatly improve the computational efficiency. The Lattice Boltzmann Method (LBM) was used to construct the optical transmission model. This method separated the LBM into collision, streaming and boundary processing processes on GPUs to perform the LBM efficiently, which were local computational and inefficient on CPU. The feasibility of the proposed method was verified by the numerical phantom and the physical phantom experiments. The experimental results showed that the proposed method achieved the best performance of a 118-fold speed up under the precondition of simulation accuracy, comparing to the diffusion equation implemented by Finite Element Method (FEM) on CPU. Thus, the LBM on the GPU may efficiently solve the forward problem in FDOT.|Zhongguo yi liao qi xie za zhi = Chinese journal of medical instrumentation|2020|10.3969/j.issn.1671-7104.2020.02.001|Huandi Wu, Xingxing Cen, Zhuangzhi Yan|0.0|0
2145|Fluid-film lubrication computing with many-core processors and graphics processing units|The advancement of modern processors with many-core and large-cache may have little computational advantages if only serial computing is employed. In this study, several parallel computing approaches, using devices with multiple or many processor cores, and graphics processing units are applied and compared to illustrate the potential applications in fluid-film lubrication study. Two Reynolds equations and an air bearing optimum design are solved using three parallel computing paradigms, OpenMP, Compute Unified Device Architecture, and OpenACC, on standalone shared-memory computers. The newly developed processors with many-integrated-core are also using OpenMP to release the computing potential. The results show that the OpenACC computing can have a better performance than the OpenMP computing for the discretized Reynolds equation with a large gridwork. This is mainly due to larger sizes of available cache in the tested graphics processing units. The bearing design can benefit most when the system with many-integrated-core processor is being used. This is due to the many-integrated-core system can perform computation in the optimization-algorithm-level and using the many processor cores effectively. A proper combination of parallel computing devices and programming models can complement efficient numerical methods or optimization algorithms to accelerate many tribological simulations or engineering designs.|Advances in Mechanical Engineering|2018|10.1177/1687814018804719|Nenzi Wang, Hsin-Yi Chen, Yu-Wen Chen|0.0|0
2147|Comparison between pystan and numpyro in Bayesian item response theory: evaluation of agreement of estimated latent parameters and sampling performance|Purpose The purpose of this study is to compare two libraries dedicated to the Markov chain Monte Carlo method: pystan and numpyro. In the comparison, we mainly focused on the agreement of estimated latent parameters and the performance of sampling using the Markov chain Monte Carlo method in Bayesian item response theory (IRT). Materials and methods Bayesian 1PL-IRT and 2PL-IRT were implemented with pystan and numpyro. Then, the Bayesian 1PL-IRT and 2PL-IRT were applied to two types of medical data obtained from a published article. The same prior distributions of latent parameters were used in both pystan and numpyro. Estimation results of latent parameters of 1PL-IRT and 2PL-IRT were compared between pystan and numpyro. Additionally, the computational cost of the Markov chain Monte Carlo method was compared between the two libraries. To evaluate the computational cost of IRT models, simulation data were generated from the medical data and numpyro. Results For all the combinations of IRT types (1PL-IRT or 2PL-IRT) and medical data types, the mean and standard deviation of the estimated latent parameters were in good agreement between pystan and numpyro. In most cases, the sampling time using the Markov chain Monte Carlo method was shorter in numpyro than that in pystan. When the large-sized simulation data were used, numpyro with a graphics processing unit was useful for reducing the sampling time. Conclusion Numpyro and pystan were useful for applying the Bayesian 1PL-IRT and 2PL-IRT. Our results show that the two libraries yielded similar estimation result and that regarding to sampling time, the fastest libraries differed based on the dataset size.|PeerJ Computer Science|2023|10.7717/peerj-cs.1620|Takamichi Murakami, Eiji Ota, Hidetoshi Matsuo, Mizuho Nishio, Takaaki Matsunaga, Aki Miyazaki|0.0|0
2148|Enhancing Seismic Performance of Tall Buildings by Optimal Design of Supplemental Energy-Dissipation Devices|"Author(s): Wang, Shanshan | Advisor(s): Mahin, Stephen A | Abstract: This dissertation focuses on the use of supplemental energy-dissipation devices to improve the seismic performance of tall steel buildings. It is divided into two parts. Part 1 focuses on exploring cost-effective retrofit strategies to improve the seismic performance of an existing tall building. The selected building is a 35-story steel moment-resisting frame, with representative details from the early 1970s. Detailed seismic evaluations were conducted in the framework of Performance Based Earthquake Engineering (PBEE), using the scenario-based performance assessment methods. A three-dimensional numerical model capturing the mechanical properties of the most critical structural elements was generated using the program: Open System for Earthquake Engineering Simulation (OpenSees). Seismic evaluation of the selected building was done following ASCE 41-13, FEMA 351 and FEMA P-58, and two hazard levels: basic safety earthquake levels 1 and 2 (BSE-1E and BSE-2E) prescribed by ASCE 41 were used for the assessment. Results predicted that this building failed to meet the recommended performance objectives and had a variety of seismic vulnerabilities, and possible retrofits were needed. Therefore, a two-level retrofit approach was examined that focused on achieving the collapse prevention limit state under the BSE-2E hazard level. In Level-1, the brittle column splices were fixed everywhere in the building, and the massive concrete cladding was replaced with lightweight substitute in the exterior of the building. Level-2 strategies augmented the Level-1 methods by adding different supplemental energy-dissipation devices. Devices investigated include: fluid viscous dampers (FVDs), viscous wall dampers (VWDs) and buckling restrained braces (BRBs). Among these, the scheme that used FVDs was expected to be the most promising to upgrade the seismic performance of the case-study steel moment frame, and thus was examined first. In this approach, feasible damper locations and overall effective damping ratios were evaluated through a series of preliminary studies, and then a two-phase manual design method was used to refine the distribution and mechanical properties of the dampers. Thorough assessments of the refined design were carried out and the results indicated that the proposed retrofit method of using FVDs could achieve the retrofit goal and provide a cost-effective means of improving the structural behavior and reducing economic losses in a major seismic event for this case-study building. The study was extended to examine alternative measures to upgrade the case-study building by using either VWDs or BRBs, and compared their relative effectiveness and economy with the scheme using FVDs. The locations and effective damping ratios were kept the same for all three schemes to insure a valid comparison. Results indicated that the proposed schemes of VWDs and BRBs both failed to achieve the targeted performance goal for this structure under a BSE-2E event, and special design considerations were required.Part 2 of the dissertation focuses on developing an automated tool to streamline the design of FVDs in tall buildings. Aided by the high-performance computers and parallel processors, a large amount of complicated nonlinear response history analysis was conducted to facilitate the automate design procedure. The optimization problem was devised in a simplified PBEE framework under one hazard level each time. Basic optimization ingredients were selected to reflect the target performance goal, and several cases using different objective functions were evaluated.Two tall buildings: the existing steel moment frame examined before and a newly-designed mega-brace steel frame were selected to rely on the automated procedure to optimally design FVDs. In both cases, the automated procedure turned to be very efficient, help identify design parameters of dampers in selected locations and reduce a great amount of engineering efforts. With only limited number of iterations, optimal design patterns of FVDs in a tall building could be found, which were able to improve the structural performance under different hazard events. The suggested optimal design could meet retrofit goal for the existing tall building, as well as achieve enhanced performance goal for both existing and new tall buildings."||2017|10.1142/9789813225237_0003|Shanshan Wang|0.0|0
2153|SU‐E‐T‐558: Monte Carlo Photon Transport Simulations On GPU with Quadric Geometry|Purpose: Monte Carlo simulation on GPU has experienced rapid advancements over the past a few years and tremendous accelerations have been achieved. Yet existing packages were developed only in voxelized geometry. In some applications, e.g. radioactive seed modeling, simulations in more complicated geometry are needed. This abstract reports our initial efforts towards developing a quadric geometry module aiming at expanding the application scope of GPU-based MC simulations. Methods: We defined the simulation geometry consisting of a number of homogeneous bodies, each specified by its material composition and limiting surfaces characterized by quadric functions. A tree data structure was utilized to define geometric relationship between different bodies. We modified our GPU-based photon MC transport package to incorporate this geometry. Specifically, geometry parameters were loaded into GPU’s shared memory for fast access. Geometry functions were rewritten to enable the identification of the body that contains the current particle location via a fast searching algorithm based on the tree data structure. Results: We tested our package in an example problem of HDR-brachytherapy dose calculation for shielded cylinder. The dose under the quadric geometry and that under the voxelized geometry agreed in 94.2% of total voxels within 20% isodose line based on a statisticalmore » t-test (95% confidence level), where the reference dose was defined to be the one at 0.5cm away from the cylinder surface. It took 243sec to transport 100million source photons under this quadric geometry on an NVidia Titan GPU card. Compared with simulation time of 99.6sec in the voxelized geometry, including quadric geometry reduced efficiency due to the complicated geometry-related computations. Conclusion: Our GPU-based MC package has been extended to support photon transport simulation in quadric geometry. Satisfactory accuracy was observed with a reduced efficiency. Developments for charged particle transport in this geometry are currently in progress.« less||2015|10.1118/1.4924920|Y. Chi, X. Jia, Steve B. Jiang, Z. Tian|0.0|0
2155|PyBONDEM-GPU: A discrete element bonded particle Python research framework – Development and examples|Discrete element modelling (DEM) is widely used to simulate granular systems, nowadays routinely on graphical processing units. Graphics processing units (GPUs) are inherently designed for parallel computation, and recent advances in the architecture, compiler design and language development are allowing general-purpose computation to be computed on multiple GPUs. Application of DEM to bonded particle systems are much less common, with a number of open research questions remaining. This study outlines a Bonded-Particle Research DEM Framework, PyBONDEM-GPU, written in Python. This framework leverages the parallel nature of GPUs for computational speed-up and the rapid prototype flexibility of Python. Python is faster and easier to learn than classical compiled languages, making computational simulation development accessible to undergraduate and graduate engineers. PyBONDEMGPU leverages the Numba-CUDA module to compile Python syntax for execution on GPUs. The framework enables research of fibre pull-out from fibre-matrix embeddings. Bonds are simulated between all interacting particles. The performance of PyBONDEM-GPU is compared against Python CPU implementations of PyBONDEM using the Numpy and Numba-CPU Python modules. PyBONDEM-GPU was found to be 1000 times faster than the Numpy implementation and 4 times faster than the Numba-CPU implementation to resolve forces and to integrate the equations of motion.|EPJ Web of Conferences|2021|10.1051/EPJCONF/202124914009|Sven Dressler, D. Wilke|0.0|0
2156|An Efficient Acceleration of Solving Heat and Mass Transfer Equations with the Second Kind Boundary Conditions in Capillary Porous Radially Composite Cylinder Using Programmable Graphics Hardware|With the recent developments in computing technology, increased efforts have gone into the simulation of various scientific methods and phenomenon in engineering fields. One such case is the simulation of heat and mass transfer equations which is becoming more and more important in analyzing various scenarios in engineering applications. Analysing the heat and mass transfer phenomenon under various environmental conditions require us to simulate it. However, this process of numerical solution of heat and mass transfer equations is very time consuming. Therefore, this paper aims at utilizing one of the acceleration techniques developed in the graphics community that exploits a graphics processing unit (GPU) which is applied to the numerical solutions of heat and mass transfer equations. The nVidia Compute Unified Device Architecture (CUDA) programming model can be a good method of applying parallel computing to program the graphical processing unit. This paper shows a good improvement in the performance, while solving the heat and mass transfer equations for a capillary porous radially composite cylinder with the second kind of boundary conditions, numerically running on GPU. This heat and mass transfer simulation is implemented using CUDA platform on nVidia Quadro FX 4800 graphics card. Our experimental results depict the drastic performance improvement when GPU is used to perform heat and mass transfer simulation. GPU can significantly accelerate the performance with a maximum observed speedup of more than 8 fold times. Therefore, the GPU is a good approach to accelerate the heat and mass transfer simulation.||2020|10.5539/cis.v13n2p75|Fan Wu, H. Narang, Abdul Rafae Mohammed|0.0|0
2157|Coupled earth system modeling on heterogeneous HPC architectures with ParFlow in the Terrestrial Systems Modeling Platform|<p>Rapidly changing heterogeneous supercomputer architectures pose a great challenge to many scientific communities trying to leverage the latest technology in high-performance computing. Implementations that simultaneously result in a good performance and developer productivity while keeping the codebase adaptable and well maintainable in the long-term are of high importance. ParFlow, a widely used hydrologic model, achieves these attributes by hiding the architecture-dependent code in preprocessor macros (ParFlow embedded Domain Specific Language, eDSL) and leveraging NVIDIA's Unified Memory technology for memory management. The implementation results in very good weak scaling with up to 26x speedup when using four NVIDIA A100 GPUs per node compared to using the available 48 CPU cores. Good weak scaling is observed using hundreds of nodes on the new JUWELS Booster system at the J&#252;lich Supercomputing Centre, Germany. Furthermore, it is possible to couple ParFlow with other earth system compartment models such as land surface and atmospheric models using the OASIS-MCT coupler library, which handles the data exchange between the different models. The ParFlow GPU implementation is fully compatible with the coupled implementation with little changes to the source code. Moreover, coupled simulations offer interesting load-balancing opportunities for optimal usage of the existing resources. For example, running ParFlow on GPU nodes, and another application component on CPU-only nodes, or efficiently distributing the CPU and GPU resources of a single node between the different application components may result in the best usage of heterogeneous architectures.</p>||2021|10.5194/EGUSPHERE-EGU21-10831|D. Pleiter, J. Kraus, M. Hrywniak, S. Kollet, Jaro Hokkanen, A. Herten|0.0|0
2162|SU‐E‐T‐36: A GPU‐Accelerated Monte‐Carlo Dose Calculation Platform and Its Application Toward Validating a ViewRay Beam Model|Purpose: To build a fast, accurate and easily-deployable research platform for Monte-Carlo dose calculations. We port the dose calculation engine PENELOPE to C++, and accelerate calculations using GPU acceleration. Simulations of a Co-60 beam model provided by ViewRay demonstrate the capabilities of the platform. Methods: We built software that incorporates a beam model interface, CT-phantom model, GPU-accelerated PENELOPE engine, and GUI front-end. We rewrote the PENELOPE kernel in C++ (from Fortran) and accelerated the code on a GPU. We seamlessly integrated a Co-60 beam model (obtained from ViewRay) into our platform. Simulations of various field sizes and SSDs using a homogeneous water phantom generated PDDs, dose profiles, and output factors that were compared to experiment data. Results: With GPU acceleration using a dated graphics card (Nvidia Tesla C2050), a highly accurate simulation – including 100*100*100 grid, 3×3×3 mm3 voxels, <1% uncertainty, and 4.2×4.2 cm2 field size – runs 24 times faster (20 minutes versus 8 hours) than when parallelizing on 8 threads across a new CPU (Intel i7-4770). Simulated PDDs, profiles and output ratios for the commercial system agree well with experiment data measured using radiographic film or ionization chamber. Based on our analysis, this beam model is precise enough for general applications. Conclusions: Using a beam model for a Co-60 system provided by ViewRay, we evaluate a dose calculation platform that we developed. Comparison to measurements demonstrates the promise of our software for use as a research platform for dose calculations, with applications including quality assurance and treatment plan verification.||2015|10.1118/1.4924397|Y. Hu, Deshan Yang, Yuhe Wang, H. Wooten, S. Mutic, T. Zhao, T. Mazur, O. Green, Harold H. Li|0.0|0
2163|Granular Gym: High Performance Simulation for Robotic Tasks with Granular Materials|Granular materials are of critical interest to many robotic tasks in planetary science, construction, and manufacturing. However, the dynamics of granular materials are complex and often computationally very expensive to simulate. We propose a set of methodologies and a system for the fast simulation of granular materials on Graphics Processing Units (GPUs), and show that this simulation is fast enough for basic training with Reinforcement Learning algorithms, which currently require many dynamics samples to achieve acceptable performance. Our method models granular material dynamics using implicit timestepping methods for multibody rigid contacts, as well as algorithmic techniques for efficient parallel collision detection between pairs of particles and between particle and arbitrarily shaped rigid bodies, and programming techniques for minimizing warp divergence on Single-Instruction, Multiple-Thread (SIMT) chip architectures. We showcase our simulation system on several environments targeted toward robotic tasks, and release our simulator as an open-source tool.|Robotics: Science and Systems|2023|10.48550/arXiv.2306.01369|P. Backes, J. Bowkett, G. Sukhatme, Daniel Pastor, David Millard|0.0|0
2167|MEGsim: A Novel Methodology for Efficient Simulation of Graphics Workloads in GPUs|An important drawback of cycle-accurate microarchitectural simulators is that they are several orders of magnitude slower than the system they model. This becomes an important issue when simulations have to be repeated multiple times sweeping over the desired design space. In the specific context of graphics workloads, performing cycle-accurate simulations are even more demanding due to the high number of triangles that have to be shaded, lighted and textured to compose a single frame. As a result, simulating a few minutes of a video game sequence is extremely time-consuming.In this paper, we make the observation that collecting information about the vertices and primitives that are processed, along with the times that shader programs are invoked, allows us to characterize the activity performed on a given frame. Based on that, we propose a novel methodology for the efficient simulation of graphics workloads called MEGsim, an approach that is capable of accurately characterizing entire video sequences by using a small subset of selected frames which substantially drops the simulation time. For a set of popular Android games, we show that MEGsim achieves an average simulation speedup of 126×, achieving remarkably accurate results for the estimated final statistics, e.g., with average relative errors of just 0.84% for the total number of cycles, 0.99% for the number of DRAM accesses, 1.2% for the number of L2 cache accesses, and 0.86% for the number of L1 (tile cache) accesses.|IEEE International Symposium on Performance Analysis of Systems and Software|2022|10.1109/ispass55109.2022.00007|David Corbalán-Navarro, Juan L. Aragón, Antonio González, Jorge L. Ortiz|0.0|0
2168|Performance comparison of CFD-DEM solver MFiX-Exa, on GPUs and CPUs|We present computational performance comparisons of gas-solid simulations performed on current CPU and GPU architectures using MFiX Exa, a CFD-DEM solver that leverages hybrid CPU+GPU parallelism. A representative fluidized bed simulation with varying particle numbers from 2 to 67 million is used to compare serial and parallel performance. A single GPU was observed to be about 10 times faster compared to a single CPU core. The use of 3 GPUs on a single compute node was observed to be 4x faster than using all 64 CPU cores. We also observed that using an error controlled adaptive time stepping scheme for particle advance provided a consistent 4x speed-up on both CPUs and GPUs. Weak scaling results indicate superior parallel efficiencies when using GPUs compared to CPUs for the problem sizes studied in this work.|arXiv.org|2021|10.2172/1847911|Deepthi Vaidhynathan, T. Hauser, H. Sitaraman, C. Hrenya, Aaron Holt, S. Lao|0.0|0
2169|UAV target detection algorithm based on improved YOLOv5s|Real-time UAV monitoring is an important means of battlefield reconnaissance, and machine interpretation of UAV images has become the main form of image interpretation, so the merit of the algorithm becomes an important factor limiting UAV reconnaissance. To address the problems of insufficient graphics card arithmetic power, low detection accuracy and difficult deployment of algorithm models at the embedded end of UAVs, this paper proposes an improved lightweight target detection algorithm based on YOLOv5s, adding K-means++ algorithm and CA attention mechanism module to the original algorithm, and training the improved YOLOv5s-CA network using tank dataset, and the simulation results show that: the improved YOLOv5s-CA has an mAP value of 97.50%, an F1 value of 0.96, and an FPS value of 74.8, which can be deployed on UAVs for real-time detection.|Other Conferences|2023|10.1117/12.2684583|Xihui Fan, Dongxu Chen, Tao Zhang, Fenmei Wang|0.0|0
2170|A GPU Variant of Mbtrack and Its Application in SLS-2|Mbtrack is a widely used multi-bunch tracking code for modeling collective instabilities in electron storage rings. It has been applied to the Swiss Light Source upgrade proposal (SLS-2) for the study of single bunch instabilities. However, an n-bunch simulation using mbtrack requires to run n+1 MPI processes. Therefore, a large scale computing cluster may be necessary to perform the simulation. In order to reduce the demands of computing resources for multi-bunch simulations, a CUDA version of mbtrack was developed, in which the computations of mbtrack are offloaded to a graphics processing unit (GPU). With the mbtrack-cuda variant, multi-bunch simulations can now run in a standalone workstation equipped with an Nvidia graphics card for scientific computing. The implementation and benchmark of the mbtrack-cuda code together with the applications in the study of longitudinal instabilities for SLS-2 will be presented.||2017|10.18429/JACOW-IPAC2017-THPAB051|Haisheng Xu, L. Stingelin, A. Adelmann, U. Locans|0.0|0
2175|Imitation challenges: From uniform random variables to complex systems|In stochastic simulation, we construct mathematical models to imitate the behavior of real systems, use computers to sample behavioral histories (sample paths) of these models, and exploit those samples to improve decision making with the real system. The imitation part can be very challenging, in particular for modeling uncertainty. Fitting univariate probability distribution to data is far from sufficient. Modeling the dependence is very important and much more challenging. It involves multivariate distributions, copulas, stochastic processes, and other complicated stochastic objects. Simulating the model on a computer also involves an imitation game, to simulate the realizations of random variables and stochastic processes with deterministic algorithms on a computer. Random number generation involves writing deterministic computer programs that can imitate simple probabilistic models such as independent uniform random variables uniformly distributed over the interval (0, 1). An “exact” algorithmic implantation of such models is theoretically impossible, so we settle for a reasonable fake. The talk will give snapshots and expose ideas collected from the author's journey thought stochastic simulation. The tour will start with random number generation and visit some challenging problems such as stochastic modeling, simulation-based optimization, rare events, simulation on parallel processors, and future challenges.|Online World Conference on Soft Computing in Industrial Applications|2015|10.1109/WSC.2015.7408303|P. L'Ecuyer|0.0|0
2179|Simulation based optimal design and sensitivity assessment of a vector-controlled induction motor drive using a multi-modal optimization algorithm|In this paper an optimized design of a vector-controlled induction motor drive system is presented using a novel parallel multi-modal optimization algorithm. The algorithm is highly parallelizable and is implemented on a 60-core parallel processor and yielded 16.4 times speedup over the sequential approach. The most robust optimal design of the vector-controlled drive system can be achieved using surrogate models that are obtained by the algorithm during the course of optimization. Using the surrogate models as a replacement of the detailed simulation model, sensitivity analysis can be conducted without any extra simulation runs.|Compel|2015|10.1109/COMPEL.2015.7236458|S. Filizadeh, A. Gole, R. Singh, A. Y. Goharrizi|0.0|0
2180|IMPLEMENTATION OF THE STAR-MACHINE ON GPU|In this paper, we present the simulation of an abstract model of SIMD type with vertical data processing (the STAR-machine) on GPU with CUDA framework. There is a number of algorithms developed for the STAR-machine. The research conducted recently shows that such a model is extremely efficient when used to solve graph problems. Associative operations are the key properties of this model. In particular, all of them take constant time. In this paper, we present an implementation of associative operations on GPU (Graphic Processing Units). This study is aimed at providing a bridge or a general manual instruction to convert the STAR algorithms to the GPU implementation. As the architecture of the STAR-machine in modern technologies has not been built yet, this provides a possible way to implement the STAR algorithms on an alternative platform to verify their correctness and efficiency, especially, for massive data input.||2016|10.31144/BNCC.CS.2542-1972.2016.N39.P51-62|T. Snytnikova|0.0|0
2192|An Approach to Accelerate Three-dimensional Cardiac Simulation on GPU|The simulation of electrophysiological cardiac models plays a crucial role in enabling researchers to explore the behavior of the heart under diverse circumstances. However, conducting such simulations often demands exceedingly high computational capabilities. To address this challenge, this paper proposes an approach to accelerate three-dimensional cardiac simulations using GPU. Through our investigations, we demonstrate that GPU-based computing constitutes an ideal solution for simulating electrophysiological cardiac models. In comparison to CPUs, GPUs possess the necessary computing prowess to meet the demanding requirements of cardiac simulations. Furthermore, GPUs exhibit remarkable acceleration performance, particularly when applied to large-scale simulations.|International Workshop on Computational Intelligence and Applications|2023|10.1109/IWCIA59471.2023.10335869|Wenfeng Shen, Xin Zhu, Qin Li|0.0|0
2198|A gas kinetic scheme approach for modeling and simulation of fire on massively parallel hardware|This work presents a simulation approach based on a Gas Kinetic Scheme (GKS) for the simulation of fire that is implemented on massively parallel hardware in terms of Graphics Processing Units (GPU) in the framework of General Purpose computing on Graphics Processing Units (GPGPU). \n \nGas kinetic schemes belong to the class of kinetic methods because their governing equation is the mesoscopic Boltzmann equation, rather than the macroscopic Navier-Stokes equations. \nFormally, kinetic methods have the advantage of a linear advection term which simplifies discretization. \nGKS inherently contains the full energy equation which is required for compressible flows. \nGKS provides a flux formulation derived from kinetic theory and is usually implemented as a finite volume method on cell-centered grids. \nIn this work, we consider an implementation on nested Cartesian grids. \nTo that end, a coupling algorithm for uniform grids with varying resolution was developed and is presented in this work. \nThe limitation to local uniform Cartesian grids allows an efficient implementation on GPUs, which belong to the class of many core processors, i.e. massively parallel hardware. \nMulti-GPU support is also implemented and efficiency is enhanced by communication hiding. \n \nThe fluid solver is validated for several two- and three-dimensional test cases including natural convection, turbulent natural convection and turbulent decay. \nIt is subsequently applied to a study of boundary layer stability of natural convection in a cavity with differentially heated walls and large temperature differences. \n \nThe fluid solver is further augmented by a simple combustion model for non-premixed flames. \nIt is validated by comparison to experimental data for two different fire plumes. \nThe results are further compared to the industry standard for fire simulation, i.e. the Fire Dynamics Simulator (FDS). \nWhile the accuracy of GKS appears slightly reduced as compared to FDS, a substantial speedup in terms of time to solution is found. \nFinally, GKS is applied to the simulation of a compartment fire. \n \nThis work shows that the GKS has a large potential for efficient high performance fire simulations.||2020|10.24355/DBBS.084-202005281033-0|Stephan Lenz|0.0|0
2200|Accelerating and simulating detected physical interations|The aim of this doctoral thesis is to present a body of work aimed at improving performance and developing new methods for animating physical interactions using simulation in virtual environments. To this end we develop a number of novel parallel collision detection and fracture simulation algorithms. Methods for traversing and constructing bounding volume hierarchies (BVH) on graphics processing units (GPU) have had a wide success. In particular, they have been adopted widely in simulators, libraries and benchmarks as they allow applications to reach new heights in terms of performance. Even with such a development however, a thorough adoption of techniques has not occurred in commercial and practical applications. Due to this, parallel collision detection on GPUs remains a relatively niche problem and a wide number of applications could benefit from a significant boost in proclaimed performance gains. In fracture simulations, explicit surface tracking methods have a good track record of success. In particular they have been adopted thoroughly in 3D modelling and animation software like Houdini [124] as they allow accurate simulation of intricate fracture patterns with complex interactions, which are generated using physical laws. Even so, existing methods can pose restrictions on the geometries of simulated objects. Further, they often have tight dependencies on implicit surfaces (e.g. level sets) for representing cracks and performing cutting to produce rigid-body fragments. Due to these restrictions, catering to various geometries can be a challenge and the memory cost of using implicit surfaces can be detrimental and without guarantee on the preservation of sharp features. We present our work in four main chapters. We first tackle the problem in the accelerating collision detection on the GPU via BVH traversal one of the most demanding components during collision detection. Secondly, we show the construction of a new representation of the BVH called the ostensibly implicit tree a layout of nodes in memory which is encoded using the bitwise representation of the number of enclosed objects in the tree (e.g. polygons). Thirdly, we shift paradigm to the task of simulating breaking objects after collision: we show how traditional finite elements can be extended as a way to prevent frequent re-meshing during fracture evolution problems. Finally, we show how the fracture surface–represented as an explicit (e.g. triangulated) surface mesh–is used to generate rigid body fragments using a novel approach to mesh cutting.||2020|10.7488/ERA/388|Floyd M. Chitalu|0.0|0
2203|Projection algorithms for large scale optimization and genomic data analysis|"The advent of the Big Data era has spawned intense interest in scalable mathematical optimization methods. Traditional approaches such as Newton’s method fall apart whenever the features outnumber the examples in a data set. Consequently, researchers have intensely developed first-order methods that rely only on gradients and subgradients of a cost function.In this dissertation we focus on projected gradient methods for large-scale con-strained optimization. We develop a particular case of a proximal gradient methodcalled the proximal distance algorithm. Proximal distance algorithms combine theclassical penalty method of constrained minimization with distance majorization. Tooptimize the loss function $f(x)$ over a constraint set $C$, the proximal distance principle mandates minimizing the penalized loss $f(x) + \rho \mathrm{dist} \; (x,C)^2$ and following the solution $x_{\rho}$ to its limit as $\rho \to \infty$. At each iteration the squared Euclidean distance $\mathrm{dist} \; (x, C)^2$ is majorized by $\| x − \Pi_{C}(x_k) \|_2^2$, where $\Pi_{C}(x_k)$ denotes the projection of the current iterate $x_k$ onto $C$. The minimum of the surrogate function $f(x) + \rho \| x − \Pi_{C} (x_k) \|_2^2$ is given by the proximal map $\mathrm{prox}_{ρ^{−1}} \; f [ \Pi_{C} (x_k )]$. The next iterate $x_{k+1}$ automatically decreases the original penalized loss for fixed $\rho$. Since many explicit projections and proximal maps are known in analytic or computable form, the proximal distance algorithm provides a scalable computational framework for a variety of constraints.For the particular case of sparse linear regression, we implement a projected gradient algorithm known as iterative hard thresholding for a particular large-scale genomics analysis known as a genome-wide association study. A genome-wide association study (GWAS) correlates marker variation with trait variation in a sample of individuals. Each study subject is genotyped at a multitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here we assume that subjects are unrelated and collected at random and that trait values are normally distributed or transformed to normality. Over the past decade, researchers have been remarkably successful in applying GWAS analysis to hundreds of traits. The massive amount of data produced in these studies present unique computational challenges. Penalized regression with LASSO or MCP penalties is capable of selecting a handful of associated SNPs from millions of potential SNPs. Unfortunately, model selection can be corrupted by false positives and false negatives, obscuring the genetic underpinning of a trait. Our parallel implementation of IHT accommodates SNP genotype compression and exploits multiple CPU cores and graphics processing units (GPUs). This allows statistical geneticists to leverage desktop workstations in GWAS analysis and to eschew expensive supercomputing resources. We evaluate IHT performance on both simulated and real GWAS data and conclude that it reduces false positive and false negative rates while remaining competitive in computational time with penalized regression."||2016|10.5465/ambpp.2016.14398abstract|Kevin L. Keys|0.0|0
2204|Performance analysis of a parallel Monte Carlo code for simulating solar radiative transfer in cloudy atmospheres using CUDA-enabled NVIDIA GPU|One tool to improve the performance of Monte Carlo methods for numerical simulation of light transport in the Earth’s atmosphere is the parallel technology. A new algorithm oriented to parallel execution on the CUDA-enabled NVIDIA graphics processor is discussed. The efficiency of parallelization is analyzed on the basis of calculating the upward and downward fluxes of solar radiation in both a vertically homogeneous and inhomogeneous models of the atmosphere. The results of testing the new code under various atmospheric conditions including continuous singlelayered and multilayered clouds, and selective molecular absorption are presented. The results of testing the code using video cards with different compute capability are analyzed. It is shown that the changeover of computing from conventional PCs to the architecture of graphics processors gives more than a hundredfold increase in performance and fully reveals the capabilities of the technology used.|Atmospheric and Ocean Optics|2017|10.1117/12.2286861|T. Russkova|0.0|0
2205|Development, implementation and application of a Stochastic Rotation Dynamics algorithm for granular matter|In this work we present an extension of the well-known particle based stochastic rotation dynamics method for the simulation of hydrodynamics of granular gases. We use an effective local coefficient of restitution to render energy dissipation dependent on local macroscopic observables, while locally conserving density and momentum. We derive the granular Boltzmann equation and demonstrate that our model obeys linear granular hydrodynamic equations. Furthermore, we derive a formula for the kinematic viscosity of the model fluid in two dimensions. We present results from simulations with a software implementation for general purpose graphics cards, that we successfully test and benchmarked with analytical predictions for standard stochastic rotation dynamics. For the granular system we observe that our prediction of the kinematic viscosity compares well with the results obtained from simulations. In this context we find that for low shear driving the fluid becomes unstable and develops shear bands. In the simulations of a freely cooling granular gas the temperature evolution follows the prediction of Haff’s law over several orders of magnitude in both time and temperature. Furthermore, we observe clustering for lower coefficients of restitution. The emergence and dynamics of the cluster compare well with expectations based on theory, experiments and simulations. The clustering sets in as the global Mach number exceeds one. Subsequently, density fluctuations grow while we observe a change in the power law of the temperature evolution. The clusters exhibit a higher cooling rate than dilute regions, hence, density and temperature become anti-correlated. This locally leads to supersonic flow. After their emergence, clusters move, collide and thus grow further. The velocity distribution function compares well with theoretical predictions. The shape of the reduced velocity distribution function changes with time as predicted, and the evolution of the second Sonine coefficient qualitative matches with analytical predictions. In our discussion we provide criteria for the selection of model parameters, and identify the effects of the finite system size.||2017||A. Zantop|0.0|0
2207|Lazy Event Prediction using Defining Trees and Schedule Bypass for Out-of-Order PDES|Out-of-order parallel discrete event simulation (PDES) has been shown to be very effective in speeding up system design by utilizing parallel processors on multi- and many-core hosts. As the number of threads in the design model grows larger, however, the original scheduling approach does not scale. In this work, we analyze the out-of-order scheduler and identify a bottleneck with quadratic complexity in event prediction. We propose a more efficient lazy strategy based on defining trees and a schedule bypass with O(m log2 m) complexity which shows sustained and improved performance gains in simulation of SystemC models with many processes. For models containing over 1000 processes, experimental results show simulation run time speedups of up to 90x using lazy event prediction against the original out-of-order PDES approach.|Design, Automation and Test in Europe|2020|10.23919/DATE48585.2020.9116512|R. Dömer, Daniel Mendoza, Zhongqi Cheng, E. Arasteh|0.0|0
2211|High Performance Simulation for Scalable Multi-Agent Reinforcement Learning|Multi-agent reinforcement learning experiments and open-source training environments are typically limited in scale, supporting tens or sometimes up to hundreds of interacting agents. In this paper we demonstrate the use of Vogue, a high performance agent based model (ABM) framework. Vogue serves as a multi-agent training environment, supporting thousands to tens of thousands of interacting agents while maintaining high training throughput by running both the environment and reinforcement learning (RL) agents on the GPU. High performance multi-agent environments at this scale have the potential to enable the learning of robust and flexible policies for use in ABMs and simulations of complex systems. We demonstrate training performance with two newly developed, large scale multi-agent training environments. Moreover, we show that these environments can train shared RL policies on time-scales of minutes and hours.|arXiv.org|2022|10.48550/arXiv.2207.03945|Sebastian M. Schmon, Jordan Langham-Lopez, P. Cannon|0.0|0
2212|Supercomputer Simulations in Design of Ultrasound Tomography Devices|The paper considers the use of supercomputers in design of medical ultrasound tomography devices. The mathematical models describing the wave propagation in ultrasound tomography should take into account such physical phenomena as diffraction, multiple scattering, and so on. The inverse problem of wave tomography is posed as a coefficient inverse problem with respect to the wave propagation velocity and the absorption factor. Numerous simulations made it possible to determine the optimal parameters of an ultrasound tomograph in order to obtain a spatial resolution of 1.5 mm suitable for early-stage breast cancer diagnosis. The developed methods were tested both on model problems and on real data obtained at the experimental test bench for tomographic studies. The computations were performed on GPU devices of Lomonosov-2 supercomputer at Lomonosov Moscow State University.|Supercomputing Frontiers and Innovations|2018|10.14529/jsfi180321|S. Seryozhnikov, A. Goncharsky|0.0|0
2213|Monte Carlo modelling of photon transport using Heterogeneous Computing|In this work the use of the OpenCL framework to implement the parallel processing of the Monte Carlo (MC) simulation of the transport of photons on matter is presented. Our platform, named OCL_MC, relies on the physical model for photons implemented in EGSnrc and adapts it for execution across heterogeneous platforms consisting of central processing units (CPUs) and graphics processing units (GPUs). Runtime and accuracy of OCL_MC was tested against a selection of phantoms and several photon beams in the kV range, using different heterogeneous platforms. We could demonstrate that the dose distributions calculated with OCL_MC are in good agreement with EGSnrc. Our implementation scales very well across the different processors available in the studied heterogeneous platforms. Therefore, OCL_MC is capable to exploit all available processing resources inside modern computing systems without sacrificing the accuracy and reliability of the MC method.|Journal of Physics: Conference Series|2018|10.1088/1742-6596/1043/1/012062|C. Rebolledo, E. Doerner, V. Gomez|0.0|0
2215|Iterative reconstruction and motion compensation in computed tomography on GPUs|Computed tomography (CT), and especially cone-beam computed tomography (CBCT) has a wide range of applications. This thesis focuses on CBCT for image-guided radiation therapy (IGRT), particularly for lung cancer treatment. In lung IGRT the tumour moves due to respiration, not only making it hard to target with the radiation beam, but also blurring the images acquired for daily treatment tuning. Generating high quality images without motion artefacts is essential for radiation and hadron therapy. In this thesis, motion modelling ideas from CERN’s phase space tomography are modified and adapted to lung CBCT. The CERN method includes a knowledge of the motion in the basic building blocks of the image reconstruction and uses all the acquired data to reconstruct a single static image at any chosen moment within the acquisition timespan. In order to use this method, and in general improve the reconstructed image quality of CBCT, iterative algorithms are explored with a focus on fast reconstruction using GPUs. The work presented here lead to the publication of the TIGRE Toolbox, a fast, easy-to-use MATLAB-CUDA toolbox for the reconstruction of CBCT images at state-of-the-art speeds with an extensive variety of iterative algorithms. This thesis presents the mathematics, GPU techniques and different applications of TIGRE and its algorithms, strengthening the idea already stated that iterative algorithms can significantly improve image quality in CBCT. A motion compensation method is developed together with a fast GPU implementation and its robustness is tested numerically by simulating the expected clinical errors in the data. The method is very robust and provides high-quality static images using data from disparate moments in time, offering the prospect of videos of patients breathing at no extra cost in radiation dose.||2018||A. Biguri|0.0|0
2221|Portable data‐parallel surface reconstruction on a uniform rectilinear grid|With the increasing heterogeneity and on‐node parallelism of high‐performance computing hardware, a major challenge is to develop portable and efficient algorithms and software. In this work, we present our implementation of a portable code to perform surface reconstruction using NVIDIA's Thrust library. Surface reconstruction is a technique commonly used in volume tracking methods for simulations of multimaterial flow with interfaces. We have designed a 3D mesh data structure that is easily mapped to the 1D vectors used by Thrust and at the same time is simple to use and uses familiar data structure terminology (such as cells, faces, vertices, and edges). With this new data structure in place, we have implemented a piecewise linear interface reconstruction algorithm in 3 dimensions that effectively exploits the symmetry present in a uniform rectilinear computational cell. Finally, we report performance results, which show that a single implementation of these algorithms can be compiled to multiple backends (specifically, multi‐core CPUs, NVIDIA GPUs, and Intel Xeon Phi processors), making efficient use of the available parallelism on each. We also compare performance of our implementation to a legacy FORTRAN implementation in Message Passing Interface (MPI) and show performance parity on single and multi‐core CPU and achieved good parallel speed‐ups on GPU. Our research demonstrates the advantage of performance portability of the underlying data‐parallel programming model.||2018|10.1002/fld.4410|J. Velechovský, Marianne Francois, Christopher M. Sewell, Li-Ta Lo|0.0|0
2222|Massively Parallel Large Scale Inundation Modelling|Over the last 20 years, flooding has been the most common natural disaster, accounting for 44.7% of all disasters, affecting about 1.65 billion people worldwide and causing roughly 105 thousand deaths†. In contrast to other natural disasters, the impact of floods is preventable through affordable structures such as dams, dykes and drainage systems. To be most effective, however, these structures have to be planned and evaluated using the highest precision data of the underlying terrain and current weather conditions. Modern laser scanning techniques provide very detailed and reliable terrain information that may be used for flood inundation modelling in planning and hazard warning systems. These warning systems become more important since flood hazards increase in recent years due to ongoing climate change. In contrast to simulations in planning, simulations in hazard warning systems are time critical due to potentially fast changing weather conditions and limited accuracy in forecasts. In this paper we present a highly optimized CUDA implementation of a numerical solver for the hydraulic equations. Our implementation maximizes the GPU’s memory throughput, achieving up to 80% utilization. A speedup of a factor of three is observed in comparison to previous work. Furthermore, we present a low-overhead, in-situ visualization of the simulated data running entirely on the GPU. With this, an area of 15 km2 with a resolution of 1 m can be visualized hundreds of times faster than real time on consumer grade hardware. Furthermore, the flow settings can be changed interactively during computation. CCS Concepts • Human-centered computing → Scientific visualization; Geographic visualization; • Computing methodologies → Realtime simulation; Massively parallel and high-performance simulations; Massively parallel algorithms;|EGPGV@EuroVis|2022|10.2312/pgv.20221063|S. Guthe, P. Mewis, Arne-Tobias Rak|0.0|0
2223|Closest distance searching by GPU-based massive parallel computation|A simulation system for real-time closest distance searching is developed using GPU-based massive parallel computation technology. The system can be used for minimum distance detection between two three-dimensional (3-D) objects in a virtual environment. First, two 3-D models are converted into vertex/mesh format; then, CUDA-based GPU parallel computation is performed for the nearest neighbor searching, in order to find the closest separation Euler-distance of two objects; at last, Unity3D game engine is used as a testing platform for visualization. Experimental results indicate that the system not only works for arbitrary complex objects (i.e., non-convex object), but also has good real-time capability and high accuracy. Compared with traditional brute force algorithm, our parallel searching is about 50 times faster.|Cyber ..|2015|10.1109/CYBER.2015.7288261|G. Sun, Yinhao Song, Yunfeng Fei|0.0|0
2224|Physically based radiative transfer framework for hyperspectral modelling of light interaction with volumetrically inhomogeneous scattering tissue-like media (Conference Presentation)|In the current report we present further developments of a unified Monte Carlo-based computational model and explore hyperspectral modelling of light interaction with volumetrically inhomogeneous scattering tissue-like media. The developed framework utilizes voxelized representation of the medium and considers spatial/volumetric variations in both structural e.g. surface roughness and wavelength-dependant optical properties. We present the detailed description of algorithms for modelling of light-medium interactions and schemes used for voxel-to-voxel photon packet transitions. The results of calculation of diffuse reflectance and Bidirectional Scattering-Surface Reflectance Distribution Function (BSSRDF) are presented. The results of simulations are compared with exact analytical solutions, phantom studies and measurements obtained by a low-cost experimental system developed in house for acquiring shape and subsurface scattering properties of objects by means of projection of temporal sequences of binary patterns. The computational solution is accelerated by the graphics processing units (GPUs) and compatible with most standard graphics/ and computer tomography file formats.|BiOS|2017|10.1117/12.2253120|A. Doronin, A. Bykov, I. Meglinski, H. Rushmeier|0.0|0
2236|Special issue on Performance modeling, benchmarking, and simulation of high performance computing systems|Polly loop optimizer on six benchmarks from the PolyBench suite. They show that their autotuning approach outperforms other approaches on five of the six benchmarks. Nguyen et al. assess the performance of reconfigurable architectures for numerical simulations. 19 They evaluate the performance of FPGAs from Intel and Xilinx, and compare them to the Xeon, Xeon Phi and NVIDIA V100 architectures. Their work shows that while FPGAs typically struggle to compete in absolute terms, they require significantly less power, and can deliver nearly equivalent energy efficiency. Our final paper, by Sai et al., describes several implementations of high-order stencil computations on GPUs. 20 They provide a detailed performance analysis using various metrics, across a range of GPUs from both NVIDIA and AMD (with hipify ). They compare their implementations to an alternative written in C using OpenACC directives, and show a six times speed-up in the best case.|Concurrency and Computation|2022|10.1002/cpe.7165|Steven A. Wright|0.0|0
2238|Researches on Mixing of Granular Materials with Discrete Element Method|The mixing of granular materials is an important unit operation in many industries. Due to the complex behaviors of granular flows, general laws and fundamental mechanisms of granular flows in industrial mixers are not completely understood yet. As a detailed numerical approach, the discrete element method (DEM) describes the forces and motions of granular materials at the particle scale, and thus has notable advantages over experimental approaches in the research of mixing mechanisms. With the rapid developments of its models and the computational technologies, this method becomes more and more popular in the simulations of various mixing processes. The effects of particle properties, mixer types, and operating parameters on mixing rate and mixing mechanisms could be investigated comprehensively through DEM, which would be quite valuable for the design and optimization of mixers as well as their optimal operations. Moreover, the high computational cost of industrial-scale simulations could be greatly alleviated by the fast developments of computer hardware, such as the advent of graphics processing unit (GPU). This review summarizes the recent progresses of DEM simulations on mixing, with emphasis on the treatments for non-cohesive particles in different kinds of mixers (rotary and fixed), cohesive particles (fine and wet), non-spherical particles (direct description of shape and multi-sphere method), and large-scale implementations. Finally, future development of the DEM method in mixing simulations is prospected.||2015|10.7536/PC140502|Yu Fuhai, G. Wei, Qin Huabiao, Z. Guangzheng, Li Jinghai|0.0|0
2244|Accelerating 3D Prestack Reverse Time Migration by the GPU-based Parallel Pseudospectral Method|Because of high accuracy,stability,wide dynamic range,and relatively low core memory costs for 3D wavefield computation,the pseudospectral (PS) method was an attractive alternative to other numerical modeling schemes,such as the finite-difference and the finite-element methods. But in recent years it is under disfavor due to its mediocre parallel performance. Fortunately, with the help of GPU we can perform PS to simulate wave propagation. The purpose of this paper is to develop a fast parallel PS based on GPU to speed up 3D Reverse Time Migration (RTM),which always requires large amount of calculation and storage. The new algorithm will be codes as a sequence of kernels that can be called from the host for each time step. In order to reduce the input/output interaction and disk storage, we adopt a hybrid strategy of source wavefield reconstruction method combined with checkpointing method. Application on the 3D SEG/EAGE Salt model demonstrates our method is effective and easy to implement.||2015|10.3997/2214-4609.201413290|L. J. Zhang, S. Sun, Y. M. Ma, L. Yu, W. Yang, X. Sun|0.0|0
2245|An integrated framework for accelerating reactive flow simulation using GPU and machine learning models|Recent progress in artificial intelligence (AI) and high-performance computing (HPC) have brought potentially game-changing opportunities in accelerating reactive flow simulations. In this study, we introduce an open-source computational fluid dynamics (CFD) framework that integrates the strengths of machine learning (ML) and graphics processing unit (GPU) to demonstrate their combined capability. Within this framework, all computational operations are solely executed on GPU, including ML-accelerated chemistry integration, fully-implicit solving of PDEs, and computation of thermal and transport properties, thereby eliminating the CPU-GPU memory copy overhead. Optimisations both within the kernel functions and during the kernel launch process are conducted to enhance computational performance. Strategies such as static data reorganisation and dynamic data allocation are adopted to reduce the GPU memory footprint. The computational performance is evaluated in two turbulent flame benchmarks using quasi-DNS and LES modelling, respectively. Remarkably, while maintaining a similar level of accuracy to the conventional CPU/CVODE-based solver, the GPU/ML-accelerated approach shows an overall speedup of over two orders of magnitude for both cases. This result highlights that high-fidelity turbulent combustion simulation with finite-rate chemistry that requires normally hundreds of CPUs can now be performed on portable devices such as laptops with a medium-end GPU.|arXiv.org|2023|10.48550/arXiv.2312.13513|Yan Zhang, Zhi X. Chen, Min Zhang, Jiayang Xu, Xinyu Dong, Yingrui Wang, Han Li, Runze Mao|0.0|0
2249|YOLO-DSRF: An Improved Small-Scale Pedestrian Detection Al-gorithm Based on Yolov4|Although existing object detection algorithms have achieved good results, it is still a challenge to effectively detect small-scale pedestrians in real time. Aiming at the problems of complex structure, large number of parameters and high missed detection rate of small targets in existing pedestrian detection algorithms, the YOLO-DSRF pedestrian detection algorithm is proposed. On the basis of YOLOv4, the depth separation convolution was first introduced to significantly reduce the amount of parameters and computation of the model, and the channel attention mechanism was introduced into the network to improve the influence of important channel features on the network, a feature fusion module was designed in the backbone network by merging deep and shallow features to effectively extract target semantic information and location information, and introduce a receptive field module in the detection head to simulate the human receptive field to enhance feature extraction capability for small targets. For training and verification on the Caltech dataset, compared with the original algorithm, the number of parameters is reduced by 65.2%, and the running speed on the GPU is increased by 20%. The AP is roughly the same. The algorithm proposed in this paper can effectively reduce the model complexity while ensuring accuracy. Thereby increasing the running speed.|ICBASE|2022|10.1145/3577065.3577100|Shuguang Li, Lei Shi, Runjie Liu, Y. Duan|0.0|0
2257|A Systematic Review and Research of Energy Efficient Evaluation in WSNs|This research provides a blueprint for the energy efficient assessment of sensor node architectures with a couple of low-end processors and radio as well as a couple of high-ended, high-end energy-performance processor and radio systems and includes the methodologies and techniques for the assessment of power efficiency in WSNs. The presentation of an interface for a coprocessing device dynamically reconfigurable for a wireless sensor node. The hardware accelerator is tailored to manage sensor data streams which cannot be managed effectively by micro-controllers with low power. The use of reconfigurable computing mechanisms achieves high energy efficiency. The design specifics involve many, specialized and reconfigurable processing steps. The size of the reconfiguration data is reduced using several reconfiguration levels for a fast and efficient dynamic reconfiguration. \nThe second document describes that such nodes may provide a highly dynamic range of applications from basic temperature measurement collections or motion sensing to advanced sensor data signal processing. Our model discusses trade-offs in energy efficiency in connection with which processor and radio to select for each mission. To do this we have a general Semi-Markov Decision model, one with dynamic and one with static interconnect, to optimize the asymptotic existence of two alternative designs. The resulting models are simulated and implemented in the measurements recorded on an existing two-radios platform and two processors. Our findings demonstrate how the benefits of such a system can be quantified for each component's energy consumption properties. Moreover, on the basis of our power budget estimate, we infer that, considering the energy overhead of such systems, the conception of a reconfigurable link between multiple processors and radios would result in efficiency gains. \nA definition of periodic dynamic reconfiguration of a limited heterogeneous data path may lead to suitable device solutions for the target domain. To respond, the effect of the overhead reconfiguration on the overall effectiveness of the device is closely observed. Our tests therefore demonstrate the low energy consumption obtained in processors and ASICs, the low reconfiguration overhead and the particular architectural area in the design space. \nThe case study from certain papers has demonstrated that the heterogeneous cluster architecture expands the conventional parallel processor cluster with an IPA accelerator. Although programmable processors ensure programming that is legacy for simple peripherals management, the download of the data-intense and control-intensive kernels to the IPA can lead to substantially higher device level performance and energy efficiency.||2021|10.1109/iscon52037.2021.9702367|Dr.N.Ramadass G.Venkatesan|0.0|0
2258|Accelerating DEM simulations on GPUs by reducing the impact of warp divergences|A way to accelerate DEM calculations on the GPUs is developed. We examined how warp divergences take place in the contact detection and the force calculations taking account of the GPU architecture. Then we showed a strategy to reduce the impact of the warp divergences on the runtime of the DEM force calculations.|arXiv.org|2015|10.1299/jsmemecj.2015._g1100104-|T. Washizawa, Yasuhiro Nakahara|0.0|0
2261|STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning|Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments. These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning. By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment. The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model. However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible. Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment. Introducing random noise into model-based reinforcement learning has been proven beneficial. In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders. STORM achieves a mean human performance of $126.7\%$ on the Atari $100$k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques. Moreover, training an agent with $1.85$ hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only $4.3$ hours, showcasing improved efficiency compared to previous methodologies.|arXiv.org|2023|10.48550/arXiv.2310.09615|Gang Wang, Yetian Yuan, Jian Sun, Weipu Zhang, Gao Huang|0.0|0
2262|Tumor Growth Modeling: State Estimation with Maximum Likelihood and Particle Filtering|In this study, we combined the Maximum Likelihood Estimator from our previous works with a Sequential Importance Resampling (SIR) particle filter to estimate the states of the stochastic Gompertz tumor growth model. We also implemented a parallel version in CUDA for the SIR filter in order to reduce its execution time. Extensive simulations with synthetic data were run to examine whether the SIR filter can provide more accurate state estimates in respect to the Normalized Mean Squared Deviation criterion compared to those provided by the deterministic Gompertz model. Moreover, we monitored and compared the execution time of the SIR's parallel and sequential implementations for different numbers of particles. The results showed that the SIR filter can estimate the system's states very accurately, even at the early tumor growth stages. Additionally, the parallel implementation that ran on the GPU was way more efficient than the implementation that ran on the CPU. By combining the Maximum Likelihood Estimator (MLE) with an SIR filter, we were able to obtain very accurate estimates of the tumors' volume. Furthermore, the execution time for the SIR filter was significantly decreased by taking advantage of the GPUs ability to perform a very large number of computations in parallel.|2020 28th Mediterranean Conference on Control and Automation (MED)|2020|10.1109/MED48518.2020.9183193|G. Papavassilopoulos, A. Charalampidis, Spyridon Patmanidis|0.0|0
2266|Computational Tools for Cardiac Simulation - GPU-Parallel Multiphysics|Cardiovascular disease affects millions of people worldwide and its social and economic cost clearly motivates scientific research. Computer simulation can lead to a better understanding of cardiac physiology, and for pathology presents opportunities for low-cost and low-risk design and testing of therapies, including surgical and pharmacological intervention as well as automated diagnosis and screening. Currently, the simulation of a whole heart model, including the interaction of electrophysiology, solid mechanics and fluid dynamics is the subject of ongoing research in computational science. Typically, the computation of a single heartbeat requires many processor hours on a supercomputer. The financial and ultimately environmental cost of such a computation prevents it from becoming a viable clinical or research solution. We re-formulate the standard mathematical models of continuum mechanics, such as the Bidomain Model, Finite Strain Theory and the Navier-Stokes Equations, specifically for parallel processing and show proof-of-concept of a computational approach that can generate a complete description of a human heartbeat on a single Graphics Processing Unit (GPU) within a few minutes. The approach is based on a Finite Volume Method (FVM) discretisation which is both matrix- and mesh-free, ideally suited to voxel-based medical imaging data. The solution of nonlinear ordinary and partial differential equations proceeds via the method of lines and operator-splitting. The resulting algorithm is implemented in the OpenCL standard and can run on almost any platform. It does not perform any CPU processing and has no dependence on third-party software libraries.|arXiv.org|2023|10.48550/arXiv.2302.12519|Toby Simpson|0.0|0
2270|A Three-dimensional Thermal Generation and Thermal Transfer Simulator for LSI Substrate by GPU|Recently, thermal design is important by the increased heat generated with the high integration of circuits. There is a thermal analysis simulation for the solutions, but the amount of calculation is huge and time consuming. We have realized a high-speed thermal transfer simulator by two-dimensional thermal network model which uses a GPU. But analysis accuracy of thermal distribution in the vertical direction is insufficient. In this study, we propose a three-dimensional thermal network model considering the thermal distribution in three-dimensional. And we tried the efficiency of analytical calculation of the three-dimensional circuit using GPU by extension of existing circuit matrix compression approach for the analysis of three-dimensional. As a result, we have achieved improving the accuracy of analyzing the three-dimensional thermal transfer. Furthermore, we have achieved about 14.6 times speed-up than a CPU program while accelerating the speed of calculation by using GPU parallel computing.||2015|10.1109/gcce.2015.7398609|Lin Meng, Yuuki Kitagawa, M. Fukui, S. Watanabe, T. Omura, Lai Lin|0.0|0
2273|GPU Acceleration of Swendson-Wang Dynamics|When simulating a lattice system near its critical temperature, local algorithms for modeling the system's evolution can introduce very large autocorrelation times into sampled data. This critical slowing down places restrictions on the analysis that can be completed in a timely manner of the behavior of systems around the critical point. Because it is often desirable to study such systems around this point, a new algorithm must be introduced. Therefore, we turn to cluster algorithms, such as the Swendsen-Wang algorithm and the Wolff clustering algorithm. They incorporate global updates which generate new lattice configurations with little correlation to previous states, even near the critical point. We look to accelerate the rate at which these algorithm are capable of running by implementing and benchmarking a parallel implementation of each algorithm designed to run on GPUs under NVIDIA's CUDA framework. A 17 and 90 fold increase in the computational rate was respectively experienced when measured against the equivalent algorithm implemented in serial code.|International Journal of Modern Physics C|2023|10.1142/s0129183124500098|T. Protzman, J. Giedt|0.0|0
2278|New Methods for Understanding and Controlling the Self-Assembly of Reacting Systems Using Coarse-Grained Molecular Dynamics|This research aims at developing new computational methods to understand the molecular self-assembly of reacting systems whose complex structures depend on the thermodynamics of mixing, reaction kinetics, and diffusion kinetics. The specific reacting system examined in this study is epoxy, cured with linear chain thermoplastic tougheners whose complex microstructure is known from experiments to affect mechanical properties and to be sensitive to processing conditions. Mesoscale simulation techniques have helped to bridge the length and time scales needed to predict the microstructures of cured epoxies, but the prohibitive computational cost of simulating experimentally relevant system sizes has limited their impact. In this work we develop an open-source plugin for the molecular dynamics code HOOMD-Blue that permits epoxy crosslinking simulations of millions of particles to be routinely performed on a single modern graphics card. Using these capabilities, we are able to use ensembles of epoxy processing pathways to obtain realistic bond kinetics and relaxation times that sensitively depend on stochastic bonding rates and a diffusive drag parameter respectively. This work also demonstrates the first implementation of fully customizable temperature-time curing profiles and the largest cross-linked structures obtained using molecular dynamics simulation. We evaluate coarse-grained models based on Dissipative Particle Dynamics (DPD) and compare with Lennard-Jones(LJ) models for their suitability to study glassy dynamics which is important for modeling epoxies or any other glassy material. We find that “hard” particle potentials such as the LJ potential are necessary to model glassy materials and characterize multiple||2018|10.18122/TD/1448/BOISESTATE|Stephen Thomas|0.0|0
2283|Many-Core Architectures: Hardware-Software Optimization and Modeling Techniques|During the last few decades an unprecedented technological growth has been at the center of the embedded systems design paramount, with Moore’s Law being the leading factor of this trend. Today in fact an ever increasing number of cores can be integrated on the same die, marking the transition from state-of-the-art multi-core chips to the new many-core design paradigm. Despite the extraordinarily high computing power, the complexity of many-core chips opens the door to several challenges. As a result of the increased silicon density of modern Systems-on-a-Chip (SoC), the design space exploration needed to find the best design has exploded and hardware designers are in fact facing the problem of a huge design space. Virtual Platforms have always been used to enable hardware-software co-design, but today they are facing with the huge complexity of both hardware and software systems. In this thesis two different research works on Virtual Platforms are presented: the first one is intended for the hardware developer, to easily allow complex cycle accurate simulations of many-core SoCs. The second work exploits the parallel computing power of off-the-shelf General Purpose Graphics Processing Units (GPGPUs), with the goal of an increased simulation speed. The term Virtualization can be used in the context of many-core systems not only to refer to the aforementioned hardware emulation tools (Virtual Platforms), but also for two other main purposes: 1) to help the programmer to achieve the maximum possible performance of an application, by hiding the complexity of the underlying hardware. 2) to efficiently exploit the high parallel hardware of many-core chips in environments with multiple active Virtual Machines. This thesis is focused on virtualization techniques with the goal to mitigate, and overtake when possible, some of the challenges introduced by the many-core design paradigm.||2015|10.6092/unibo/amsdottorato/6824|Christian Pinto|0.0|0
2285|A hybrid mesh and voxel based Monte Carlo algorithm for accurate and efficient photon transport modeling in complex bio-tissues|Over the past decade, an increasing body of evidence has suggested that threedimensional (3-D) Monte Carlo (MC) light transport simulations are affected by the inherent limitations and errors of voxel-based domain boundaries. In this work, we specifically address this challenge using a hybrid MC algorithm, namely split-voxel MC or SVMC, that combines both mesh and voxel domain information to greatly improve MC simulation accuracy while remaining highly flexible and efficient in parallel hardware, such as graphics processing units (GPU). We achieve this by applying a marching-cubes algorithm to a pre-segmented domain to extract and encode sub-voxel information of curved surfaces, which is then used to inform ray-tracing computation within boundary voxels. This preservation of curved boundaries in a voxel data structure demonstrates significantly improved accuracy in several benchmarks, including a human brain atlas. The accuracy of the SVMC algorithm is comparable to that of mesh-based MC (MMC), but runs 2x-6x faster and requires only a lightweight preprocessing step. The proposed algorithm has been implemented in our open-source software and is freely available at http://mcx.space.|bioRxiv|2020|10.1101/2020.10.01.322982|Q. Fang, Shijie Yan|0.0|0
2287|FRA-RIR: Fast Random Approximation of the Image-source Method|The training of modern speech processing systems often requires a large amount of simulated room impulse response (RIR) data in order to allow the systems to generalize well in real-world, reverberant environments. However, simulating realistic RIR data typically requires accurate physical modeling, and the acceleration of such simulation process typically requires certain computational platforms such as a graphics processing unit (GPU). In this paper, we propose FRA-RIR, a fast random approximation method of the widely-used image-source method (ISM), to efficiently generate realistic RIR data without specific computational devices. FRA-RIR replaces the physical simulation in the standard ISM by a series of random approximations, which significantly speeds up the simulation process and enables its application in on-the-fly data generation pipelines. Experiments show that FRA-RIR can not only be significantly faster than other existing ISM-based RIR simulation tools on standard computational platforms, but also improves the performance of speech denoising systems evaluated on real-world RIR when trained with simulated RIR. A Python implementation of FRA-RIR is available online\footnote{\url{https://github.com/yluo42/FRA-RIR}}.|Interspeech|2022|10.48550/arXiv.2208.04101|Yi Luo, Jianwei Yu|0.0|0
2288|Monte Carlo Option Pricing in Tensorflow|We look at pricing financial derivatives on gpus. After some general comparison of frameworks, we focus on pricing via Monte Carlo simulations and compare numpy, tensorflow cpu and tensorflow gpu using the Black Scholes model, the Heston model and the Heston model with local volatility.||2018|10.2139/ssrn.3214058|K. Detlefsen|0.0|0
2290|Effect Of Impurities On Stability Of The Skyrmion Phase In A Frustrated Heisenberg Antiferromagnet|We employ a hybrid Monte Carlo simulation implemented on GPU to study the effect of nonmagnetic impurities in a frustrated Heisenberg antiferromagnetic (AFM) model on a triangular lattice with Dzyaloshinskii-Moriya interaction in the presence of the external magnetic field. We focus on the skyrmion lattice phase (SkX), which in the pure model is known to be stabilized in a quite wide temperature-field window. We aim to confront the effect of impurities on the SkX phase in the present frustrated AFM model with that in the nonfrustrated ferromagnetic counterpart as well as to consider more realistic conditions in the proposed experimental realizations of the present model. We show, that up to a fairly large concentration of the impurities, p ~35%, the SkX phase can survive albeit in somewhat distorted form. Distortion of the SkX phase due to formation of bimerons, reported in the ferromagnetic model, was not observed in the present case.|European Conference on Modelling and Simulation|2022|10.7148/2022-0331|M. Zukovic, M. Mohylna|0.0|0
2292|Toward GPU on CUDA for Simulating Non-hydrostatic Wave Model|The goal of this paper is to calculate the performance of the parallel scheme of non-hydrostatic Gravitational Surface Wave using GPU based on CUDA. Here, Navier-Stokes equations used as a model which is simplified by removing non-linear and friction terms. In this paper, simple finite different method is used as a numerical method. The discretization of pressure, vertical and horizontal velocity variables uses Arakawa C-Grid which leads to solve Poisson equation. The numerical simulation of propagating waves generated by surface pressure is given. The parallel computing is shown satisfied. Using large number of grids (Nx=1024,Ny =512), serial and parallel CPU time are observed 20197.66 and 6521.85 seconds respectively. Therefore the usage of parallel computing in this simulation produces speedup approximately 3 times of serial computing.|International Conference on Information and Communicatiaon Technology|2019|10.1109/ICoICT.2019.8835376|P. H. Gunawan, Muhammad Khadafi|0.0|0
2296|ИМИТАЦИОННАЯ МОДЕЛЬ ГРАФИЧЕСКОГО МУЛЬТИПРОЦЕССОРА НА ОСНОВЕ ТЕОРИИ СЕТЕЙ ПЕТРИ|This work addresses the problem of creating the simulation models of a graphical multiprocessor for carrying out computational experiments to determine the efficiency of using the parallel computing based on GPGPU (General-purpose computing for graphics processing units, non-specialized computing on graphics processors) in tasks of structural-parametric synthesis of big discrete systems based on evolutionary procedures. We propose to use the Petri netstheory as a mathematical tool.It has the parallelism property and allows describing discrete processes occurring both in the genetic algorithm and in the computing system itself.The development of a simulation model is carried out on the basis of the graphic multiprocessor module memory architecture taking into account the specifics of its work related to the ability to read, write and transmit data.In addition, we describe the feature of the arithmetic logic devices work, which are able to simultaneously execute one command over a set of data.When building the model we take into account a feature of graphic multiprocessors which allows to get a greater effect from usingthe parallel computing avoiding the branching and control blocks operation that slow down the multiprocessor (since their number is less than the calculators number), thereby forming the “narrow” places.The proposed simulation model of a multiprocessor unit based on the selected tool is implemented using the specialized software for simulation based on the Petri nets theory – PIPE 5.This software is distributed free of charge and has a wide range of instrumental and analytical tools, which greatly simplifies both the modeling process and the process of analyzing the obtainedmodels.The resulting model will provide an opportunity to evaluate the efficiency of using parallel computing based on GPGPU technology in solving the task of improving the performance of intelligent information decision support systems based on genetic algorithms adapted to the subject area.||2019||А. Г. Бажанов, Д. А. Петросов, Н. В. Петросова, О. И. Бажанова|0.0|0
2298|Parallel Crowd Simulation Based on Power Law|Crowd simulation technology is increasingly used in the fields of film, animation, games, military training, and public safety. In this paper, we propose a parallel crowd simulation model based on Power Law. This model use CUDA architecture to parallelize the Power Law model on GPU, so that each agent's behavior simulation is synchronized on different threads. This model considers the fine effects of the microscopic model while significantly improving the model simulation efficiency, making it possible to simulate large-scale crowds in real time and can accurately obtain the status information of each agent in each frame.|International Conference on Virtual Reality and Visualization|2018|10.1109/ICVRV.2018.00023|Tianlu Mao, Xiyuan Song, Hao Jiang, Zhaoqi Wang, Shaohua Liu, Ji Wang|0.0|0
2300|Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons|The adaptive leaky integrate-and-fire (ALIF) model is fundamental within computational neuroscience and has been instrumental in studying our brains $\textit{in silico}$. Due to the sequential nature of simulating these neural models, a commonly faced issue is the speed-accuracy trade-off: either accurately simulate a neuron using a small discretisation time-step (DT), which is slow, or more quickly simulate a neuron using a larger DT and incur a loss in simulation accuracy. Here we provide a solution to this dilemma, by algorithmically reinterpreting the ALIF model, reducing the sequential simulation complexity and permitting a more efficient parallelisation on GPUs. We computationally validate our implementation to obtain over a $50\times$ training speedup using small DTs on synthetic benchmarks. We also obtained a comparable performance to the standard ALIF implementation on different supervised classification tasks - yet in a fraction of the training time. Lastly, we showcase how our model makes it possible to quickly and accurately fit real electrophysiological recordings of cortical neurons, where very fine sub-millisecond DTs are crucial for capturing exact spike timing.|arXiv.org|2023|10.48550/arXiv.2311.11390|Andrew J King, N. Harper, Luke C L Taylor|0.0|0
2308|A simulation model of changing state of blood based on SPH method|Expression related to blood such as bloodshed and splashes is often treated in the CG field such as games and movie. Along with the improvement of technology and equipment performance, expressions such as blood adhering to objects became realistic and high speed, but on the other hand state change due to time course such as coagulation and discoloration has not been expressed yet. We proposed a method to expand the SPH method, which is a fluid simulation method, to simulate changes in the state of blood over time and aim for more realistic expression of blood. However, our method has a problem of increasing in calculation load. In this research, we accelerated process by voxel division of simulation space and GPU computing.|Other Conferences|2019|10.1117/12.2521568|Tomoaki Moriya, Reo Kumaki, Tokiichiro Takahashi|0.0|0
2310|A Massively Parallel Multi-Scale FE2 Framework for Multi-Trillion Degrees of Freedom Simulations|The advent of hybrid CPU and accelerator supercomputers opens the door to extremely large multi-scale simulations. An example of such a multi-scale technique, the FE2 approach, has been designed to simulate material deformations, by getting a better estimation of the material properties, which, in effect, reduces the need to introduce physical modelling at macro-scale level, such as constitutive laws, for instance. Both macro- and micro-scales are solved using the Finite Element method, the micro-scale being resolved at the Gauss points of the macro-scale mesh. As the micro-scale simulations do not require any information from each other, and are thus run concurrently, the stated problem is embarrassingly parallel. The FE2 method therefore directly benefits from hybrid machines, the macro-scale being solved on CPU whereas the micro-scale is offloaded to accelerators. The case of a flat plate, made of different materials is used to illustrate the potential of the method. In order to ensure good load balance on distributed memory machines, weighting based on the type of materials the plate is made of is applied by means of a Space Filling Curve technique. Simulations have been carried out for over 5 trillions of degrees of freedom on up to 2,048 nodes (49,152 CPUs and 12,288 GPUs) of the US DOE Oak Ridge National Laboratory high-end machine, Summit, showing an excellent speed-up for the assembly part of the framework, where the micro-scale is computed on GPU using CUDA.|Platform for Advanced Scientific Computing Conference|2023|10.1145/3592979.3593415|C. Moulinec, G. Houzeaux, G. Oyarzun, G. Giuntoli, Judicael Grasset, R. Borrell, M. Vázquez, Adria Quintanas Corominas|0.0|0
2318|Parallel implementation of spectral element method for Lamb wave propagation modeling|The proposed spectral element method implementation is based on sparse matrix storage of local shape function derivatives calculated at Gauss–Lobatto–Legendre points. The algorithm utilizes two basic operations: multiplication of sparse matrix by vector and element‐by‐element vectors multiplication. Compute‐intensive operations are performed for a part of equation of motion derived at the degree of freedom level of 3D isoparametric spectral elements. The assembly is performed at the force vector in such a way that atomic operations are minimized. This is achieved by a new mesh coloring technique The proposed parallel implementation of spectral element method on GPU is applied for the first time for Lamb wave simulations. It has been found that computation on multicore GPU is up to 14 times faster than on single CPU. Copyright © 2015 John Wiley & Sons, Ltd.||2016|10.1002/nme.5119|P. Kudela|3.888888888888889|-2
2317|Layered Models for Large Scale Time-Evolving Landscapes. (Modèles à couches pour simuler l'évolution de paysages à grande échelle)|The development of new technologies and algorithms allows the interactive visualization of virtual worlds showing an increasing amount of details and spatial extent. The production of plausible landscapes within these worlds becomes a major challenge, not only because the important part that terrain features and ecosystems play in the quality and realism of 3D sceneries, but also from the editing complexity of large landforms at mountain range scales. Interactive authoring is often achieved by coupling editing techniques with computationally and time demanding numerical simulation, whose calibration is harder as the number of non-intuitive parameters increases. \n \nThis thesis develops new methods for the simulation of large-scale landscapes. Our goal is to improve both the control and the realism of the synthetic scenes. Our strategy to increase the plausibility consists of building our methods on physically and geomorphologically-inspired laws: we develop new numerical methods, which, combined with intuitive control tools, improve user experience. \n \nBy observing phenomena triggered by compression areas within the Earth's crust, we propose a method for the intuitive control of the uplift based on a metaphor on the sculpting of the tectonic plates. Combined with new efficient methods for fluvial and glacial erosion, this allows for the fast sculpting of large mountain ranges. In order to visualize the resulting landscapes withing human sight, we demonstrate the need of combining the simulation of various phenomena with different time spans, and we propose a stochastic simulation technique to solve this complex cohabitation. This methodology is applied to the simulation of geological processes such as erosion interleaved with ecosystems formation. This method is then implemented on the GPU, combining long term effects (snowfall, phase changes of water) with highly dynamics ones (avalanches, skiers impact). \n \nOur methods allow the simulation of the evolution of large scale, visually plausible landscapes, while accounting for user control. These results were validated by user studies as well as comparisons with data obtained from real landscapes.||2018|10.2312/2632858|Guillaume Cordonnier|0.0|-2
