#,title,abstract,venue,year,doi,authors,relative_citation_index,relevance_score
1,Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling,"In computer architecture, significant innovation frequently comes from industry. However, the simulation tools used by industry are often not released for open use, and even when they are, the exact details of industrial designs are not disclosed. As a result, research in the architecture space must ensure that assumptions about contemporary processor design remain true.To help bridge the gap between opaque industrial innovation and public research, we introduce three mechanisms that make it much easier for GPU simulators to keep up with industry. First, we introduce a new GPU simulator frontend that minimizes the effort required to simulate different machine ISAs through trace-driven simulation of NVIDIA’s native machine ISA, while still supporting execution-driven simulation of the virtual ISA. Second, we extensively update GPGPU-Sim’s performance model to increase its level of detail, configurability and accuracy. Finally, surrounding the new frontend and flexible performance model is an infrastructure that enables quick, detailed validation. A comprehensive set of microbenchmarks and automated correlation plotting ease the modeling process.We use these three new mechanisms to build Accel-Sim, a detailed simulation framework that decreases cycle error 79 percentage points, over a wide range of 80 workloads, consisting of 1,945 kernel instances. We further demonstrate that Accel-Sim is able to simulate benchmark suites that no other open-source simulator can. In particular, we use Accel-sim to simulate an additional 60 workloads, comprised of 11,440 kernel instances, from the machine learning benchmark suite Deepbench. Deepbench makes use of closed-source, hand-tuned kernels with no virtual ISA implementation. Using a rigorous counter-by-counter analysis, we validate Accel-Sim against contemporary GPUs.Finally, to highlight the effects of falling behind industry, this paper presents two case-studies that demonstrate how incorrect baseline assumptions can hide new areas of opportunity and lead to potentially incorrect design decisions.",International Symposium on Computer Architecture,2018,10.1109/ISCA45697.2020.00047,"Mahmoud Khairy, Timothy G. Rogers, Tor M. Aamodt, Zhesheng Shen",21.285713999999999,11
2,PPT-GPU: Scalable GPU Performance Modeling,"Performance modeling is a challenging problem due to the complexities of hardware architectures. In this paper, we present PPT-GPU, a scalable and accurate simulation framework that enables GPU code developers and architects to predict the performance of applications in a fast, and accurate manner on different GPU architectures. PPT-GPU is part of the open source project, Performance Prediction Toolkit (PPT) developed at the Los Alamos National Laboratory. We extend the old GPU model in PPT that predict the runtimes of computational physics codes to offer better prediction accuracy, for which, we add models for different memory hierarchies found in GPUs and latencies for different instructions. To further show the utility of PPT-GPU, we compare our model against real GPU device(s) and the widely used cycle-accurate simulator, GPGPU-Sim using different workloads from RODINIA and Parboil benchmarks. The results indicate that the predicted performance of PPT-GPU is within a 10 percent error compared to the real device(s). In addition, PPT-GPU is highly scalable, where it is up to 450x faster than GPGPU-Sim with more accurate results.",IEEE computer architecture letters,2019,10.1109/LCA.2019.2904497,"Yehia Arafa, S. Eidenbenz, Gopinath Chennupati, N. Santhi, Abdel-Hameed A. Badawy",4.333333000000000,8
12,MGPUSim: Enabling Multi-GPU Performance Modeling and Optimization,"The rapidly growing popularity and scale of data-parallel workloads demand a corresponding increase in raw computational power of Graphics Processing Units (GPUs). As single-GPU platforms struggle to satisfy these performance demands, multi-GPU platforms have started to dominate the high-performance computing world. The advent of such systems raises a number of design challenges, including the GPU microarchitecture, multi-GPU interconnect fabric, runtime libraries, and associated programming models. The research community currently lacks a publicly available and comprehensive multi-GPU simulation framework to evaluate next- generation multi-GPU system designs. In this work, we present MGPUSim, a cycle-accurate, extensively validated, multi-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set architecture. MGPUSim comes with in-built support for multi-threaded execution to enable fast, parallelized, and accurate simulation. In terms of performance accuracy, MGPUSim differs by only 5.5% on average from the actual GPU hardware. We also achieve a 3.5x and a 2.5x average speedup running functional emulation and detailed timing simulation, respectively, on a 4-core CPU, while delivering the same accuracy as serial simulation. We illustrate the flexibility and capability of the simulator through two concrete design studies. In the first, we propose the Locality API, an API extension that allows the GPU programmer to both avoid the complexity of multi-GPU programming, while precisely controlling data placement in the multi-GPU memory. In the second design study, we propose Progressive Page Splitting Migration (PASI), a customized multi-GPU memory management system enabling the hardware to progressively improve data placement. For a discrete 4-GPU system, we observe that the Locality API can speed up the system by 1.6x (geometric mean), and PASI can improve the system performance by 2.6x (geometric mean) across all benchmarks, compared to a unified 4-GPU platform.",International Symposium on Computer Architecture,2019,10.1145/3307650.3322230,"Shane Treadway, Shi Dong, Yifan Sun, Trinayan Baruah, A. Joshi, Yuhui Bao, Harrison Barclay, Vincent Zhao, R. Ubal, Xiang Gong, Spencer Hance, Saiful A. Mojumder, Carter McCardwell, Zhongliang Chen, D. Kaeli, José L. Abellán, Amir Kavyan Ziabari, John Kim",9.666667000000000,6
9,Demystifying the Nvidia Ampere Architecture through Microbenchmarking and Instruction-level Analysis,"Graphics Processing Units (GPUs) are now considered the leading hardware to accelerate general-purpose workloads such as AI, data analytics, and HPC. Over the last decade, researchers have focused on demystifying and evaluating the microarchitecture features of various GPU architectures beyond what vendors reveal. This line of work is necessary to understand the hardware better and build more efficient workloads and applications. Many works have studied the recent Nvidia architectures, such as Volta and Turing, comparing them to their successor, Ampere. However, some microarchitecture features, such as the clock cycles for the different instructions, have not been extensively studied for the Ampere architecture. In this paper, we study the clock cycles per instruction with various data types found in the instruction-set architecture (ISA) of Nvidia GPUs. We measure the clock cycles for PTX ISA instructions and their SASS ISA counterparts using microbench-marks. We further calculate the clock cycles needed to access each memory unit. Moreover, we demystify the new version of the tensor core unit found in the Ampere architecture using the WMMA API and measuring its clock cycles per instruction and throughput for the different data types and input shapes. Our results should guide software developers and hardware architects. Furthermore, the clock cycles per instructions are widely used by performance modeling simulators and tools to model and predict the performance of the hardware.",IEEE Conference on High Performance Extreme Computing,2022,10.1109/HPEC55821.2022.9926299,"Yehia Arafa, N. Santhi, Abdel-Hameed A. Badawy, H. Abdelkhalik",2.0,6
5,NaviSim: A Highly Accurate GPU Simulator for AMD RDNA GPUs,"As GPUs continue to grow in popularity for accelerating demanding applications, such as high-performance computing and machine learning, GPU architects need to deliver more powerful devices with updated instruction set architectures (ISAs) and new microarchitectural features. The introduction of the AMD RDNA architecture is one example where the GPU architecture was dramatically changed, modifying the underlying programming model, the core architecture, and the cache hierarchy. To date, no publicly-available simulator infrastructure can model the AMD RDNA GPU, preventing researchers from exploring new GPU designs based on the state-of-the-art RDNA architecture. In this paper, we present the NaviSim simulator, the first cycle-level GPU simulator framework that models AMD RDNA GPUs. NaviSim faithfully emulates the new RDNA ISA. We extensively tune and validate NaviSim using several microbenchmarks and 10 full workloads. Our evaluation shows that NaviSim can accurately model the GPU's kernel execution time, achieving similar performance to hardware execution within 9.92% (on average), as measured on an AMD RX 5500 XT GPU and an AMD Radeon Pro W6800 GPU. To demonstrate the full utility of the NaviSim simulator, we carry out a performance study of the impact of individual RDNA features, attempting to understand better the design decisions behind these features. We carry out a number of experiments to isolate each RDNA feature and evaluate its impact on overall performance, as well as demonstrate the usability and flexibility of NaviSim.",International Conference on Parallel Architectures and Compilation Techniques,2022,10.1145/3559009.3569666,"D. Kaeli, Yifan Sun, Trinayan Baruah, Yuhui Bao, M. Shen, Micah Weston, Ajay Joshi, José L. Abellán, Z. Feric, John Kim",1.3333330000000001,6
6,A Detailed Model for Contemporary GPU Memory Systems,"This paper explores the impact of simulator accuracy on architecture design decisions in the general-purpose graphics processing unit (GPGPU) space. We enhance the most popular publicly available GPU simulator, GPGPU-Sim, by performing a rigorous correlation of the simulator with a contemporary GPU. Our enhanced GPU model is able to describe the NVIDIA Volta architecture in sufficient detail to reduce error in memory system counters by as much as 66×. The reduced error in the memory system further reduces execution time error by 2.5×. To demonstrate the accuracy of our enhanced model against a real machine, we perform a counter-by-counter validation against an NVIDIA TITAN V Volta GPU, demonstrating the relative accuracy of the new simulator versus the previous model. We go on to demonstrate that the simpler model discounts the importance of advanced memory system designs such as out-of-order memory access scheduling. Our results demonstrate that it is important for the academic community to enhance the level of detail in architecture simulators as system complexity continues to grow.",IEEE International Symposium on Performance Analysis of Systems and Software,2019,10.1109/ISPASS.2019.00023,"Akshay Jain, Mahmoud Khairy, Timothy G. Rogers, Tor M. Aamodt",0.6666670000000000,6
8,Exploring Modern GPU Memory System Design Challenges through Accurate Modeling,"This paper explores the impact of simulator accuracy on architecture design decisions in the general-purpose graphics processing unit (GPGPU) space. We perform a detailed, quantitative analysis of the most popular publicly available GPU simulator, GPGPU-Sim, against our enhanced version of the simulator, updated to model the memory system of modern GPUs in more detail. Our enhanced GPU model is able to describe the NVIDIA Volta architecture in sufficient detail to reduce error in memory system even counters by as much as 66X. The reduced error in the memory system further reduces execution time error versus real hardware by 2.5X. To demonstrate the accuracy of our enhanced model against a real machine, we perform a counter-by-counter validation against an NVIDIA TITAN V Volta GPU, demonstrating the relative accuracy of the new simulator versus the publicly available model. We go on to demonstrate that the simpler model discounts the importance of advanced memory system designs such as out-of-order memory access scheduling, while overstating the impact of more heavily researched areas like L1 cache bypassing. Our results demonstrate that it is important for the academic community to enhance the level of detail in architecture simulators as system complexity continues to grow. As part of this detailed correlation and modeling effort, we developed a new Correlator toolset that includes a consolidation of applications from a variety of popular GPGPU benchmark suites, designed to run in reasonable simulation times. The Correlator also includes a database of hardware profiling results for all these applications on NVIDIA cards ranging from Fermi to Volta and a toolchain that enables users to gather correlation statistics and create detailed counter-by-counter hardware correlation plots with minimal effort.",arXiv.org,2018,10.1145/3224430,"Mahmoud Khairy, Timothy G. Rogers, Tor M. Aamodt, Akshay Jain",0.5714290000000000,6
3,Path Forward Beyond Simulators: Fast and Accurate GPU Execution Time Prediction for DNN Workloads,"Today, DNNs’ high computational complexity and sub-optimal device utilization present a major roadblock to democratizing DNNs. To reduce the execution time and improve device utilization, researchers have been proposing new system design solutions, which require performance models (especially GPU models) to help them with pre-product concept validation. Currently, researchers have been utilizing simulators to predict execution time, which provides high flexibility and acceptable accuracy, but at the cost of a long simulation time. Simulators are becoming increasingly impractical to model today’s large-scale systems and DNNs, urging us to find alternative lightweight solutions. To solve this problem, we propose using a data-driven method for modeling DNNs system performance. We first build a dataset that includes the execution time of numerous networks/layers/kernels. After identifying the relationships of directly known information (e.g., network structure, hardware theoretical computing capabilities), we discuss how to build a simple, yet accurate, performance model for DNNs execution time. Our observations on the dataset demonstrate prevalent linear relationships between the GPU kernel execution times, operation counts, and input/output parameters of DNNs layers. Guided by our observations, we develop a fast, linear-regression-based DNNs execution time predictor. Our evaluation using various image classification models suggests our method can predict new DNNs performance with a 7% error and new GPU performance with a 15.2% error. Our case studies also demonstrate how the performance model can facilitate future DNNs system research.",Micro,2023,10.1145/3613424.3614277,"Ying Li, Yifan Sun, Adwait Jog",0.5,6
11,Parallel GPU Architecture Simulation Framework Exploiting Architectural-Level Parallelism with Timing Error Prediction,"The performance analysis and study of large-scale many-core processor architectures require fast and highly accurate simulation techniques in order to reduce time consumption. State-of-the-art graphics processing units (GPUs), which are used extensively as coprocessors in the high-performance-computing area, also require fast simulation techniques because they have massively complex microarchitectures with thousands of processing elements. At present, however, GPU simulators do not have sufficient simulation speed for advanced software and architecture studies. In this study, we propose a new parallel simulation framework and a new parallel simulation technique for improving the simulation speed of GPUs. The proposed framework facilitates multithreaded simulation by exploiting the architectural-level parallelism and execution model parallelism of GPUs. In addition, an error predictive synchronization scheme based on a timing error prediction mechanism is used to minimize the cycle errors and simulator slowdown during parallel simulations. The experimental results obtained using a simulator with the proposed framework showed that the proposed technique provided a speedup of up to 8.9 times compared with an existing single-thread-based GPU simulator on a 16-core machine.",IEEE transactions on computers,2016,10.1109/TC.2015.2444848,"Sangpil Lee, W. Ro",0.4444440000000000,6
4,Balar: A SST GPU Component for Performance Modeling and Profiling.,"Programmable accelerators have become commonplace in modern computing systems. Advances in programming models and the availability of unprecedented amounts of data have created a space for massively parallel accelerators capable of maintaining context for thousands of concurrent threads resident on-chip. These threads are grouped and interleaved on a cycle-by-cycle basis among several massively parallel computing cores. One path for the design of future supercomputers relies on an ability to model the performance of these massively parallel cores at scale. The SST framework has been proven to scale up to run simulations containing tens of thousands of nodes. A previous report described the initial integration of the open-source, execution-driven GPU simulator, GPGPU-Sim, into the SST framework. This report discusses the results of the integration and how to use the new GPU component in SST. It also provides examples of what it can be used to analyze and a correlation study showing how closely the execution matches that of a Nvidia V100 GPU when running kernels and mini-apps.",,2019,10.2172/1560919,"S. Hammond, Mengchi Zhang, C. Hughes, Mahmoud Khairy, R. Hoekstra, Roland N. Green, Timothy G. Rogers",0.0,6
7,Efficient L2 Cache Management to Boost GPGPU Performance,"In recent years, the growing need for computing capacity has become a challenge that has led the industry to look for alternative architectures to conventional out-of-order superscalar processors, with the goal of enabling an increase of computing power while achieving higher energy efficiency. GPU architectures, which just a decade ago were applied to accelerate computer graphics exclusively, have been one of the most employed alternatives for several years to reach the mentioned goal. A particular characteristic of GPUs is their high main memory bandwidth, which allows executing a large number of threads in a very efficient way. This feature, as well as their high computational power regarding floating-point operations, have caused the emergence of the GPGPU computing paradigm, where GPU architectures perform general purpose computations. The aforementioned characteristics make GPU devices very appropriate for the execution of massively parallel applications that have been traditionally executed in conventional high-performance processors. The work performed in this thesis aims to help improve the performance of GPUs in the execution of GPGPU applications. To this end, as a first step, a characterization study is carried out. In this study, the most important features of GPGPU applications, with respect to the memory hierarchy and its impact on performance, are identified. For this purpose, a detailed cycle-accurate simulator is used to model the architecture of a recent GPU. The study reveals that it is necessary to model with more detail some critical components of the GPU memory hierarchy in order to obtain accurate results. In addition, it shows that the achieved benefits can vary up to a factor of 3× depending on how these critical components are modeled. Due to this reason, as a second step before realizing a novel proposal, the work in this thesis focuses on determining which components of the GPU memory hierarchy must be modeled with more detail to increase the accuracy of simulator results and improving the existing simulator models of these components. Moreover, a validation study is performed comparing the results obtained with the improved GPU models against those from a real commercial GPU. The implemented simulator improvements reduce the deviation of the results obtained with the simulator from results obtained with the real GPU by about 96%. Finally, once simulation accuracy is increased, this thesis proposes a novel approach, called FRC (Fetch and Replacement Cache), which highly improves the GPU computational power by enhancing main memory-level parallelism. The proposal increases the number of parallel accesses to main memory by accelerating the management of fetch and replacement actions corresponding to those cache accesses that miss in the cache. The FRC approach is based on a small auxiliary cache structure that efficiently unclogs the memory subsystem, enhancing the GPU performance up to 118% on average compared to the studied baseline. In addition, the FRC approach reduces the energy consumption of the memory hierarchy by a 57%.",,2019,10.4995/thesis/10251/125477,Francisco Candel Margaix,0.0,6
10,Analyzing and Improving Hardware Modeling of Accel-Sim,"GPU architectures have become popular for executing general-purpose programs. Their many-core architecture supports a large number of threads that run concurrently to hide the latency among dependent instructions. In modern GPU architectures, each SM/core is typically composed of several sub-cores, where each sub-core has its own independent pipeline. Simulators are a key tool for investigating novel concepts in computer architecture. They must be performance-accurate and have a proper model related to the target hardware to explore the different bottlenecks properly. This paper presents a wide analysis of different parts of Accel-sim, a popular GPGPU simulator, and some improvements of its model. First, we focus on the front-end and developed a more realistic model. Then, we analyze the way the result bus works and develop a more realistic one. Next, we describe the current memory pipeline model and propose a model for a more cost-effective design. Finally, we discuss other areas of improvement of the simulator.",,2024,10.11606/issn.2316-9141.rh.2020.217365,"Mojtaba Abaie Shoushtary, Rodrigo Huerta, Antonio Gonz'alez",0.0,6
20,Understanding GPU Power,"Modern graphics processing units (GPUs) have complex architectures that admit exceptional performance and energy efficiency for high-throughput applications. Although GPUs consume large amounts of power, their use for high-throughput applications facilitate state-of-the-art energy efficiency and performance. Consequently, continued development relies on understanding their power consumption. This work is a survey of GPU power modeling and profiling methods with increased detail on noteworthy efforts. As direct measurement of GPU power is necessary for model evaluation and parameter initiation, internal and external power sensors are discussed. Hardware counters, which are low-level tallies of hardware events, share strong correlation to power use and performance. Statistical correlation between power and performance counters has yielded worthwhile GPU power models, yet the complexity inherent to GPU architectures presents new hurdles for power modeling. Developments and challenges of counter-based GPU power modeling are discussed. Often building on the counter-based models, research efforts for GPU power simulation, which make power predictions from input code and hardware knowledge, provide opportunities for optimization in programming or architectural design. Noteworthy strides in power simulations for GPUs are included along with their performance or functional simulator counterparts when appropriate. Last, possible directions for future research are discussed.",ACM Computing Surveys,2016,10.1145/2962131,"N. Imam, T. Mintz, R. A. Bridges",9.444444000000001,5
13,GPGPU-MiniBench: Accelerating GPGPU Micro-Architecture Simulation,"Graphics processing units (GPU), due to their massive computational power with up to thousands of concurrent threads and general-purpose GPU (GPGPU) programming models such as CUDA and OpenCL, have opened up new opportunities for speeding up general-purpose parallel applications. Unfortunately, pre-silicon architectural simulation of modern-day GPGPU architectures and workloads is extremely time-consuming. This paper addresses the GPGPU simulation challenge by proposing a framework, called GPGPU-MiniBench, for generating miniature, yet representative GPGPU workloads. GPGPU-MiniBench first summarizes the inherent execution behavior of existing GPGPU workloads in a profile. The central component in the profile is the Divergence Flow Statistics Graph (DFSG), which characterizes the dynamic control flow behavior including loops and branches of a GPGPU kernel. GPGPU-MiniBench generates a synthetic miniature GPGPU kernel that exhibits similar execution characteristics as the original workload, yet its execution time is much shorter thereby dramatically speeding up architectural simulation. Our experimental results show that GPGPU-MiniBench can speed up GPGPU architectural simulation by a factor of 49× on average and up to 589×, with an average IPC error of 4.7 percent across a broad set of GPGPU benchmarks from the CUDA SDK, Rodinia and Parboil benchmark suites. We also demonstrate the usefulness of GPGPU-MiniBench for driving GPU architecture exploration.",IEEE transactions on computers,2015,10.1109/TC.2015.2395427,"Junmin Wu, L. Eeckhout, Hai Jin, Zhibin Yu, L. John, Chengzhong Xu, Nilanjan Goswami, Tao Li",1.9,5
18,A Quantitative Evaluation of Contemporary GPU Simulation Methodology,"Contemporary Graphics Processing Units (GPUs) are used to accelerate highly parallel compute workloads. For the last decade, researchers in academia and industry have used cycle-level GPU architecture simulators to evaluate future designs. This paper performs an in-depth analysis of commonly accepted GPU simulation methodology, examining the effect both the workload and the choice of instruction set architecture have on the accuracy of a widely-used simulation infrastructure, GPGPU-Sim. We analyze numerous aspects of the architecture, validating the simulation results against real hardware. Based on a characterized set of over 1700 GPU kernels, we demonstrate that while the relative accuracy of compute-intensive workloads is high, inaccuracies in modeling the memory system result in much higher error when memory performance is critical. We then perform a case study using a recently proposed GPU architecture modification, demonstrating that the cross-product of workload characteristics and instruction set architecture choice can have an affect on the predicted efficacy of the technique.",Measurement and Modeling of Computer Systems,2018,10.1145/3219617.3219658,"Akshay Jain, Mahmoud Khairy, Timothy G. Rogers",1.1428570000000000,5
14,ParaCells: A GPU Architecture for Cell-Centered Models in Computational Biology,"In computational biology, the hierarchy of biological systems requires the development of flexible and powerful computational tools. Graphics processing unit (GPU) architecture has been a suitable device for parallel computing in simulating multi-cellular systems. However, in modeling complex biological systems, scientists often face two tasks, mathematical formulation and skillful programming. In particular, specific programming skills are needed for GPU programming. Therefore, the development of an easy-to-use computational architecture, which utilizes GPU for parallel computing and provides intuitive interfaces for simple implementation, is needed so that general scientists can perform GPU simulations without knowing much about the GPU architecture. Here, we introduce ParaCells, a cell-centered GPU simulation architecture for NVIDIA compute unified device architecture (CUDA). ParaCells was designed as a versatile architecture that connects the user logic (in C++) with NVIDIA CUDA runtime and is specific to the modeling of multi-cellular systems. An advantage of ParaCells is its object-oriented model declaration, which allows it to be widely applied to many biological systems through the combination of basic biological concepts. We test ParaCells with two applications. Both applications are significantly faster when compared with sequential as well as parallel OpenMP and OpenACC implementations. Moreover, the simulation programs based on ParaCells are cleaner and more readable than other versions.",IEEE/ACM Transactions on Computational Biology & Bioinformatics,2019,10.1109/TCBB.2018.2814570,"You Song, Siyu Yang, J. Lei",0.8333330000000000,5
16,Optimization of Heterogeneous NoC for Fused CPU-GPU Architecture,"Author(s): Alhubail, Lulwah | Advisor(s): Bagherzadeh, Nader | Abstract: Heterogeneous computing architectures that utilize both CPU and GPU have been the trend nowadays. Several products from AMD, Intel, and NVIDIA have emerged that fused CPU and GPU on the same chip. In such architectures, different processing elements (PEs), including many CPU cores, GPU cores, memory controllers (MCs), and caches, are connected through a common interconnection. CPU and GPU exhibit different network behaviors; CPU tends to be latency-sensitive and GPU, with its high thread level parallelism (TLP), tends to be throughput hungry. Using homogeneous interconnect for such heterogeneous processors can result in performance degradation and power increase. This dissertation focused on designing a heterogeneous mesh-style network-on-chip (NoC) to connect heterogeneous CPU-GPU processors while considering their diametric network demands. There are many aspects to consider when designing a 2D mesh NoC. Firstly, the placement of the PEs within the mesh. Secondly, setting the NoC parameters: the size of the router's buffer, the number of virtual channels, and the bandwidth of the links. This dissertation tackled all these problems simultaneously. Moreover, to design a heterogeneous NoC, heterogeneity was explored at the router's port and link level, where each port of each router can have different buffer size and number of virtual channels, and each link can have different bandwidth. This explodes the design space and makes exploring all possible design combinations using simulation very difficult. In this dissertation, heuristic-based optimization methods were proposed to obtain a near-optimal heterogeneous NoC design. Firstly, a method based on Genetic Algorithm (GA) to get a design with optimal performance in terms of the average network latency. An analytical model based on queueing theory that supports virtual channels was proposed to get a performance measure of the design. Secondly, a multi-objective method based on the Strength Pareto Evolutionary Algorithm 2 (SPEA2) to get an optimal design in terms of the performance and the power of NoC. Also, an activity based power model was proposed to get the power of the design. The optimal designs were validated using a full-system simulator.",,2019,10.23919/date.2019.8714769,Lulwah Alhubail,0.8333330000000000,5
19,A Model-Based Software Solution for Simultaneous Multiple Kernels on GPUs,"As a critical computing resource in multiuser systems such as supercomputers, data centers, and cloud services, a GPU contains multiple compute units (CUs). GPU Multitasking is an intuitive solution to underutilization in GPGPU computing. Recently proposed solutions of multitasking GPUs can be classified into two categories: (1) spatially partitioned sharing (SPS), which coexecutes different kernels on disjointed sets of compute units (CU), and (2) simultaneous multikernel (SMK), which runs multiple kernels simultaneously within a CU. Compared to SPS, SMK can improve resource utilization even further due to the interleaving of instructions from kernels with low dynamic resource contentions. However, it is hard to implement SMK on current GPU architecture, because (1) techniques for applying SMK on top of GPU hardware scheduling policy are scarce and (2) finding an efficient SMK scheme is difficult due to the complex interferences of concurrently executed kernels. In this article, we propose a lightweight and effective performance model to evaluate the complex interferences of SMK. Based on the probability of independent events, our performance model is built from a totally new angle and contains limited parameters. Then, we propose a metric, symbiotic factor, which can evaluate an SMK scheme so that kernels with complementary resource utilization can corun within a CU. Also, we analyze the advantages and disadvantages of kernel slicing and kernel stretching techniques and integrate them to apply SMK on GPUs instead of simulators. We validate our model on 18 benchmarks. Compared to the optimized hardware-based concurrent kernel execution whose kernel launching order brings fast execution time, the results of corunning kernel pairs show 11%, 18%, and 12% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average. Compared to the Warped-Slicer, the results show 29%, 18%, and 51% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average.",ACM Transactions on Architecture and Code Optimization (TACO),2020,10.1145/3377138,"Weizhi Liu, Hao Wu, Huanxin Lin, Cho-Li Wang",0.6,5
17,A Quantitative Evaluation of Contemporary GPU Simulation Methodology,"Contemporary Graphics Processing Units (GPUs) are used to accelerate highly parallel compute workloads. For the last decade, researchers in academia and industry have used cycle-level GPU architecture simulators to evaluate future designs. This paper performs an in-depth analysis of commonly accepted GPU simulation methodology, examining the effect both the workload and the choice of instruction set architecture have on the accuracy of a widely-used simulation infrastructure, GPGPU-Sim. We analyze numerous aspects of the architecture, validating the simulation results against real hardware. Based on a characterized set of over 1700 GPU kernels, we demonstrate that while the relative accuracy of compute-intensive workloads is high, inaccuracies in modeling the memory system result in much higher error when memory performance is critical. We then perform a case study using a recently proposed GPU architecture modification, Cache-Conscious Wavefront Scheduling. The case study demonstrates that the cross-product of workload characteristics and instruction set architecture choice can affect the predicted efficacy of the technique.",Proceedings of the ACM on Measurement and Analysis of Computing Systems,2018,10.1145/3224430,"Akshay Jain, Mahmoud Khairy, Timothy G. Rogers",0.5714290000000000,5
15,Modeling the Energy Efficiency of GEMM using Optical Random Access Memory,"General matrix-matrix multiplication (GEMM) is the key computation kernel in many applications. GEMM has been supported on various hardware platforms, including CPU, GPU, FPGA. To optimize the performance of GEMM, developers use on-chip electrical static random access memory (E-SRAM) to exploit the data locality of GEMM. However, intensively accessing E-SRAM for GEMM can lead to significant energy consumption, which is not energy-efficient for commercial data centers. In this paper, we evaluate the optical static random access memory (O-SRAM) for GEMM. O-SRAM is a promising tech-nology that has extremely low access latency and low energy consumption compared with the traditional E-SRAM. First, we propose an O-SRAM based wafer-scale system for GEMM and a baseline E-SRAM based system. Second, we build the theoretical performance models of the two systems to analyze their energy consumption of on-chip memory accesses. Then, we conduct simulation-based experiments to evaluate the energy consumption of the two system. The evaluation results show that O-SRAM based system is 7 x more energy efficient than the baseline E-SRAM based system.",IEEE Conference on High Performance Extreme Computing,2022,10.1109/HPEC55821.2022.9926291,"R. T. Lakkireddy, Bingyi Zhang, Ajey P. Jacob, Akhilesh R. Jaiswal, V. Prasanna, Clynn Mathew, Sasindu Wijeratne",0.3333330000000000,5
21,A Survey of Power Consumption Modeling for GPU Architecture,"GPUs are of increasing interests in the multi-core era due to their high computing power. However, the power consumption caused by the rising performance of GPUs has been a general concern. As a consequence, it is becoming an imperative demand to optimize the GPU power consumption, among which the power consumption estimation is one of the important and useful solutions. In this work, we give a survey of the power modeling for GPU. We first introduce the current development of heterogeneous architectures and then summarize the existing modeling techniques for GPU power consumption. The main two types of power modeling could be classified as simulator-based methods and real machine-based methods.",,2016,10.23977/CPCS.2016.11006,"Ning Li, Li Shen, Qiong Wang, Zhiying Wang",0.0,5
24,gem5-gpu: A Heterogeneous CPU-GPU Simulator,"gem5-gpu is a new simulator that models tightly integrated CPU-GPU systems. It builds on gem5, a modular full-system CPU simulator, and GPGPUSim, a detailed GPGPU simulator. gem5-gpu routes most memory accesses through Ruby, which is a highly configurable memory system in gem5. By doing this, it is able to simulate many system configurations, ranging from a system with coherent caches and a single virtual address space across the CPU and GPU to a system that maintains separate GPU and CPU physical address spaces. gem5gpu can run most unmodified CUDA 3.2 source code. Applications can launch non-blocking kernels, allowing the CPU and GPU to execute simultaneously. We present gem5-gpu's software architecture and a brief performance validation. We also discuss possible extensions to the simulator. gem5-gpu is open source and available at gem5-gpu.cs.wisc.edu.",IEEE computer architecture letters,2015,10.1109/LCA.2014.2299539,"Marc S. Orr, M. Hill, Joel Hestness, Jason Power, D. Wood",15.2,4
114,Automatic Selection of Sparse Matrix Representation on GPUs,"Sparse matrix-vector multiplication (SpMV) is a core kernel in numerous applications, ranging from physics simulation and large-scale solvers to data analytics. Many GPU implementations of SpMV have been proposed, targeting several sparse representations and aiming at maximizing overall performance. No single sparse matrix representation is uniformly superior, and the best performing representation varies for sparse matrices with different sparsity patterns. In this paper, we study the inter-relation between GPU architecture, sparse matrix representation and the sparse dataset. We perform extensive characterization of pertinent sparsity features of around 700 sparse matrices, and their SpMV performance with a number of sparse representations implemented in the NVIDIA CUSP and cuSPARSE libraries. We then build a decision model using machine learning to automatically select the best representation to use for a given sparse matrix on a given target platform, based on the sparse matrix features. Experimental results on three GPUs demonstrate that the approach is very effective in selecting the best representation.",International Conference on Supercomputing,2015,10.1145/2751205.2751244,"S. Parthasarathy, P. Sadayappan, L. Pouchet, N. Sedaghati, Te Mu",9.7,4
69,Neural acceleration for GPU throughput processors,"Graphics Processing Units (GPUs) can accelerate diverse classes of applications, such as recognition, gaming, data analytics, weather prediction, and multimedia. Many of these applications are amenable to approximate execution. This application characteristic provides an opportunity to improve GPU performance and efficiency. Among approximation techniques, neural accelerators have been shown to provide significant performance and efficiency gains when augmenting CPU processors. However, the integration of neural accelerators within a GPU processor has remained unexplored. GPUs are, in a sense, many-core accelerators that exploit large degrees of data-level parallelism in the applications through the SIMT execution model. This paper aims to harmoniously bring neural and GPU accelerators together without hindering SIMT execution or adding excessive hardware overhead. We introduce a low overhead neurally accelerated architecture for GPUs, called NGPU, that enables scalable integration of neural accelerators for large number of GPU cores. This work also devises a mechanism that controls the tradeoff between the quality of results and the benefits from neural acceleration. Compared to the baseline GPU architecture, cycle-accurate simulation results for NGPU show a 2.4× average speedup and a 2.8× average energy reduction within 10% quality loss margin across a diverse set of benchmarks. The proposed quality control mechanism retains a 1.9 ×s average speedup and a 2.1 × energy reduction while reducing the degradation in the quality of results to 2.5%. These benefits are achieved by less than 1% area overhead.",Micro,2015,10.1145/2830772.2830810,"Hardik Sharma, H. Esmaeilzadeh, Jongse Park, P. Lotfi-Kamran, A. Yazdanbakhsh",7.6,4
63,Anatomy of GPU Memory System for Multi-Application Execution,"As GPUs make headway in the computing landscape spanning mobile platforms, supercomputers, cloud and virtual desktop platforms, supporting concurrent execution of multiple applications in GPUs becomes essential for unlocking their full potential. However, unlike CPUs, multi-application execution in GPUs is little explored. In this paper, we study the memory system of GPUs in a concurrently executing multi-application environment. We first present an analytical performance model for many-threaded architectures and show that the common use of misses-per-kilo-instruction (MPKI) as a proxy for performance is not accurate without considering the bandwidth usage of applications. We characterize the memory interference of applications and discuss the limitations of existing memory schedulers in mitigating this interference. We extend the analytical model to multiple applications and identify the key metrics to control various performance metrics. We conduct extensive simulations using an enhanced version of GPGPU-Sim targeted for concurrently executing multiple applications, and show that memory scheduling decisions based on MPKI and bandwidth information are more effective in enhancing throughput compared to the traditional FR-FCFS and the recently proposed RR FR-FCFS policies.",International Symposium on Memory Systems,2015,10.1145/2818950.2818979,"Tuba Kesten, Niladrish Chatterjee, C. Das, Evgeny Bolotin, M. Kandemir, Ashutosh Pattnaik, Onur Kayiran, Adwait Jog, S. Keckler",7.2,4
96,A large-scale study of soft-errors on GPUs in the field,"Parallelism provided by the GPU architecture has enabled domain scientists to simulate physical phenomena at a much faster rate and finer granularity than what was previously possible by CPU-based large-scale clusters. Architecture researchers have been investigating reliability characteristics of GPUs and innovating techniques to increase the reliability of these emerging computing devices. Such efforts are often guided by technology projections and simplistic scientific kernels, and performed using architectural simulators and modeling tools. Lack of large-scale field data impedes the effectiveness of such efforts. This study attempts to bridge this gap by presenting a large-scale field data analysis of GPU reliability. We characterize and quantify different kinds of soft-errors on the Titan supercomputer's GPU nodes. Our study uncovers several interesting and previously unknown insights about the characteristics and impact of soft-errors.",International Symposium on High-Performance Computer Architecture,2016,10.1109/HPCA.2016.7446091,"James H. Rogers, Saurabh Gupta, E. Smirni, Bin Nie, Devesh Tiwari",6.333333000000000,4
128,GPGPU Performance Estimation With Core and Memory Frequency Scaling,"Contemporary graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, accurate and straightforward performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is essential to determine the best frequency configuration for energy saving. In this article, we reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Compared to the cycle-level simulators, which are too slow to apply on real hardware, our model only needs simple and one-off micro-benchmarks to extract a set of hardware parameters and kernel performance counters without any source code analysis. Our experimental results show that the proposed performance model can capture the kernel performance scaling behaviors under different frequency settings and achieve decent accuracy (average errors of 3.85, 8.6, 8.82, and 8.83 percent on a set of 20 GPU kernels with four modern Nvidia GPUs).",IEEE Transactions on Parallel and Distributed Systems,2020,10.1109/TPDS.2020.3004623,"X. Chu, Qiang Wang",6.2,4
137,GPU-Accelerated Sparse LU Factorization for Circuit Simulation with Performance Modeling,"The sparse matrix solver by LU factorization is a serious bottleneck in Simulation Program with Integrated Circuit Emphasis (SPICE)-based circuit simulators. The state-of-the-art Graphics Processing Units (GPU) have numerous cores sharing the same memory, provide attractive memory bandwidth and compute capability, and support massive thread-level parallelism, so GPUs can potentially accelerate the sparse solver in circuit simulators. In this paper, an efficient GPU-based sparse solver for circuit problems is proposed. We develop a hybrid parallel LU factorization approach combining task-level and data-level parallelism on GPUs. Work partitioning, number of active thread groups, and memory access patterns are optimized based on the GPU architecture. Experiments show that the proposed LU factorization approach on NVIDIA GTX580 attains an average speedup of 7.02× (geometric mean) compared with sequential PARDISO, and 1.55× compared with 16-threaded PARDISO. We also investigate bottlenecks of the proposed approach by a parametric performance model. The performance of the sparse LU factorization on GPUs is constrained by the global memory bandwidth, so the performance can be further improved by future GPUs with larger memory bandwidth.",IEEE Transactions on Parallel and Distributed Systems,2015,10.1109/TPDS.2014.2312199,"Xiaoming Chen, Ling Ren, Huazhong Yang, Yu Wang",5.4,4
25,"Performance analysis of CUDA, OpenACC and OpenMP programming models on TESLA V100 GPU","Graphics processors are widely utilized in modern supercomputers as accelerators. Ability to perform efficient parallelization and low-level allow scientists to greatly boost performance of their codes. Modern Nvidia GPUs feature low-level approaches, such as CUDA, along with high-level approaches: OpenACC and OpenMP. While the low-level approach aims to explore all possible abilities of SIMT GPU architecture by writing low-level C/C++ code, it takes significant effort from programmer. OpenACC and OpenMP programming models are opposite to CUDA. Using these models the programmer only have to identify the blocks of code to be parallelized using pragmas. We compare the performance of CUDA, OpenMP and OpenACC on state-of-the-art Nvidia Tesla V100 GPU in various typical scenarios that arise in scientific programming, such as matrix multiplication, regular memory access patterns and evaluate performance of physical simulation codes implemented using these programming models. Moreover, we study the performance matrix multiplication implemented in vendor-optimized BLAS libraries for Nvidia Tesla V100 GPU and modern Intel Xeon processor.",,2021,10.1088/1742-6596/1740/1/012056,"Alexey Timoveev, M. Khalilov",5.25,4
71,A GPU-accelerated Monte Carlo dose calculation platform and its application toward validating an MRI-guided radiation therapy beam model.,"PURPOSE\nThe clinical commissioning of IMRT subject to a magnetic field is challenging. The purpose of this work is to develop a GPU-accelerated Monte Carlo dose calculation platform based on penelope and then use the platform to validate a vendor-provided MRIdian head model toward quality assurance of clinical IMRT treatment plans subject to a 0.35 T magnetic field.\n\n\nMETHODS\npenelope was first translated from fortran to c++ and the result was confirmed to produce equivalent results to the original code. The c++ code was then adapted to cuda in a workflow optimized for GPU architecture. The original code was expanded to include voxelized transport with Woodcock tracking, faster electron/positron propagation in a magnetic field, and several features that make gpenelope highly user-friendly. Moreover, the vendor-provided MRIdian head model was incorporated into the code in an effort to apply gpenelope as both an accurate and rapid dose validation system. A set of experimental measurements were performed on the MRIdian system to examine the accuracy of both the head model and gpenelope. Ultimately, gpenelope was applied toward independent validation of patient doses calculated by MRIdian's kmc.\n\n\nRESULTS\nAn acceleration factor of 152 was achieved in comparison to the original single-thread fortran implementation with the original accuracy being preserved. For 16 treatment plans including stomach (4), lung (2), liver (3), adrenal gland (2), pancreas (2), spleen(1), mediastinum (1), and breast (1), the MRIdian dose calculation engine agrees with gpenelope with a mean gamma passing rate of 99.1% ± 0.6% (2%/2 mm).\n\n\nCONCLUSIONS\nA Monte Carlo simulation platform was developed based on a GPU- accelerated version of penelope. This platform was used to validate that both the vendor-provided head model and fast Monte Carlo engine used by the MRIdian system are accurate in modeling radiation transport in a patient using 2%/2 mm gamma criteria. Future applications of this platform will include dose validation and accumulation, IMRT optimization, and dosimetry system modeling for next generation MR-IGRT systems.",Medical Physics (Lancaster),2016,10.1118/1.4953198,"T. Zhao, Hua Li, T. Mazur, O. Green, Deshan Yang, V. Rodriguez, Yuhe Wang, Yanle Hu, H. Wooten, H. Li, S. Mutic",5.0,4
78,Parallelized combined finite‐discrete element (FDEM) procedure using multi‐GPU with CUDA,"This paper focuses on the efficiency of finite discrete element method (FDEM) algorithmic procedures in massive computers and analyzes the time‐consuming part of contact detection and interaction computations in the numerical solution. A detailed operable GPU parallel procedure was designed for the element node force calculation, contact detection, and contact interaction with thread allocation and data access based on the CUDA computing. The emphasis is on the parallel optimization of time‐consuming contact detection based on load balance and GPU architecture. A CUDA FDEM parallel program was developed with the overall speedup ratio over 53 times after the fracture from the efficiency and fidelity performance test of models of in situ stress, UCS, and BD simulations in Intel i7‐7700K CPU and the NVIDIA TITAN Z GPU. The CUDA FDEM parallel computing improves the computational efficiency significantly compared with the CPU‐based ones with the same reliability, providing conditions for achieving larger‐scale simulations of fracture.",International journal for numerical and analytical methods in geomechanics (Print),2019,10.1002/nag.3011,"Weiqin Wang, Quansheng Liu, Hao Ma",4.666667000000000,4
27,Need for Speed: Experiences Building a Trustworthy System-Level GPU Simulator,"The demands of high-performance computing (HPC) and machine learning (ML) workloads have resulted in the rapid architectural evolution of GPUs over the last decade. The growing memory footprint and diversity of data types in these workloads has required GPUs to embrace micro-architectural heterogeneity and increased memory system sophistication to scale performance. Effective simulation of new architectural features early in the design cycle enables quick and effective exploration of design trade-offs across this increasingly diverse set of workloads. This work provides a retrospective on the design and development of NVArchSim (NVAS), an architectural simulator used within NVIDIA to design and evaluate features that are difficult to appraise using other methodologies due to workload type, size, complexity, or lack of modeling flexibility. We argue that overly precise and/or overly slow architectural models hamper an architect’s ability to evaluate new features within a reasonable time frame, hurting productivity. Because of its speed, NVAS is being used to trace and evaluate hundreds of HPC and state-of-the-art ML workloads on single-GPU or multi-GPU systems. By adding component fidelity only when necessary to improve system-level modeling accuracy, NVAS delivers simulation speed orders of magnitude higher than most publicly available GPU simulators while retaining high levels of accuracy and simulation flexibility. Building trustworthy high-level simulation platforms is a difficult exercise in balance and compromise; we share our experiences to help and encourage those in academia who take on the challenge of building GPU simulation platforms.",International Symposium on High-Performance Computer Architecture,2021,10.1109/HPCA51647.2021.00077,"Niladrish Chatterjee, Evgeny Bolotin, D. Nellans, Zi Yan, Yaosheng Fu, Nan Jiang, Daniel Lustig, Oreste Villa",4.5,4
59,Bridging Data Center AI Systems with Edge Computing for Actionable Information Retrieval,"Extremely high data rates at modern synchrotron and X-ray free-electron laser light source beamlines motivate the use of machine learning methods for data reduction, feature detection, and other purposes. Regardless of the application, the basic concept is the same: data collected in early stages of an experiment, data from past similar experiments, and/or data simulated for the upcoming experiment are used to train machine learning models that, in effect, learn specific characteristics of those data; these models are then used to process subsequent data more efficiently than would general-purpose models that lack knowledge of the specific dataset or data class. Thus, a key challenge is to be able to train models with sufficient rapidity that they can be deployed and used within useful timescales. We describe here how specialized data center AI (DCAI) systems can be used for this purpose through a geographically distributed workflow. Experiments show that although there are data movement cost and service overhead to use remote DCAI systems for DNN training, the turnaround time is still less than 1/30 of using a locally deploy-able GPU.",Annual Workshop on Large-scale Experiment-in-the-Loop Computing,2021,10.1109/xloop54565.2021.00008,"Dennis Trujillo, Ian T. Foster, Hemant Sharma, Chun Hong Yoon, Zhengchun Liu, Jana Thayer, P. Kenesei, N. Schwarz, Ahsan Ali, H. Yoo, R. Coffee, R. Herbst, A. Miceli",4.25,4
149,"Fast , Interactive Origami Simulation using GPU Computation","We present an explicit method for simulating origami that can be rapidly computed on a Graphics Processing Unit (GPU). Previous work on origami simulation methods model the geometric or structural behaviors of origami with a focus on physical realism; in this paper we introduce a compliant, explicit numerical simulation method that emphasizes computational speed and interactivity. We do this by reformulating existing techniques for simulating origami so that they can be computed on highly parallel GPU architectures. We implement this method in an open-source, GPU-accelerated WebGL app that runs in any modern browser. We evaluate our method’s performance, stability, and scalability to existing methods (Freeform Origami, MERLIN) and demonstrate its capacity for real-time interaction through a traditional GUI and immersive virtual reality.",,2018,10.1109/tro.2018.2862882,"N. Gershenfeld, E. Demaine, Amanda Ghassaei",4.142857000000000,4
67,A Survey of Performance Modeling and Simulation Techniques for Accelerator-Based Computing,"The high performance computing landscape is shifting from collections of homogeneous nodes towards heterogeneous systems, in which nodes consist of a combination of traditional out-of-order execution cores and accelerator devices. Accelerators, built around GPUs, many-core chips, FPGAs or DSPs, are used to offload compute-intensive tasks. The advent of this type of systems has brought about a wide and diverse ecosystem of development platforms, optimization tools and performance analysis frameworks. This is a review of the state-of-the-art in performance tools for heterogeneous computing, focusing on the most popular families of accelerators: GPUs and Intel's Xeon Phi. We describe current heterogeneous systems and the development frameworks and tools that can be used for developing for them. The core of this survey is a review of the performance models and tools, including simulators, proposed in the literature for these platforms.",IEEE Transactions on Parallel and Distributed Systems,2015,10.1109/TPDS.2014.2308216,"J. Miguel-Alonso, A. Mendiburu, Unai López-Novoa",4.1,4
141,A Survey of GPU Multitasking Methods Supported by Hardware Architecture,"The ability to support multitasking becomes more and more important in the development of graphic processing unit (GPU). GPU multitasking methods are classified into three types: temporal multitasking, spatial multitasking, and simultaneous multitasking (SMK). This article first introduces the features of some commercial GPU architectures to support multitasking and the common metrics used for evaluating the performance of GPU multitasking methods, and then reviews the GPU multitasking methods supported by hardware architecture (i.e., hardware GPU multitasking methods). The main problems of each type of hardware GPU multitasking methods to be solved are illustrated. Meanwhile, the key idea of each previous hardware GPU multitasking method is introduced. In addition, the characteristics of hardware GPU multitasking methods belonging to the same type are compared. This article also gives some valuable suggestions for the future research. An enhanced GPU simulator is needed to bridge the gap between academia and industry. In addition, it is promising to expand the research space with machine learning technologies, advanced GPU architectural innovations, 3D stacked memory, etc. Because most previous GPU multitasking methods are based on NVIDIA GPUs, this article focuses on NVIDIA GPU architecture, and uses NVIDIA's terminology. To our knowledge, this article is the first survey about hardware GPU multitasking methods. We believe that our survey can help the readers gain insights into the research field of hardware GPU multitasking methods.",IEEE Transactions on Parallel and Distributed Systems,2022,10.1109/TPDS.2021.3115630,"F. Nie, W. Gao, Huiyang Zhou, Chen Zhao",4.0,4
144,MGSim + MGMark: A Framework for Multi-GPU System Research,"The rapidly growing popularity and scale of data-parallel workloads demand a corresponding increase in raw computational power of GPUs (Graphics Processing Units). As single-GPU systems struggle to satisfy the performance demands, multi-GPU systems have begun to dominate the high-performance computing world. The advent of such systems raises a number of design challenges, including the GPU microarchitecture, multi-GPU interconnect fabrics, runtime libraries and associated programming models. The research community currently lacks a publically available and comprehensive multi-GPU simulation framework and benchmark suite to evaluate multi-GPU system design solutions. \nIn this work, we present MGSim, a cycle-accurate, extensively validated, multi-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set architecture. We complement MGSim with MGMark, a suite of multi-GPU workloads that explores multi-GPU collaborative execution patterns. Our simulator is scalable and comes with in-built support for multi-threaded execution to enable fast and efficient simulations. In terms of performance accuracy, MGSim differs $5.5\%$ on average when compared against actual GPU hardware. We also achieve a $3.5\times$ and a $2.5\times$ average speedup in function emulation and architectural simulation with 4 CPU cores, while delivering the same accuracy as the serial simulation. \nWe illustrate the novel simulation capabilities provided by our simulator through a case study exploring programming models based on a unified multi-GPU system~(U-MGPU) and a discrete multi-GPU system~(D-MGPU) that both utilize unified memory space and cross-GPU memory access. We evaluate the design implications from our case study, suggesting that D-MGPU is an attractive programming model for future multi-GPU systems.",arXiv.org,2018,10.1109/iiswc.2018.8573521,"R. Ubal, Xiang Gong, Saiful A. Mojumder, D. Kaeli, José L. Abellán, John Kim, Shane Treadway, Shi Dong, Yifan Sun, Trinayan Baruah, A. Joshi, Yuhui Bao, Vincent Zhao",3.857143000000000,4
108,CUDAAdvisor: LLVM-based runtime profiling for modern GPUs,"General-purpose GPUs have been widely utilized to accelerate parallel applications. Given a relatively complex programming model and fast architecture evolution, producing efficient GPU code is nontrivial. A variety of simulation and profiling tools have been developed to aid GPU application optimization and architecture design. However, existing tools are either limited by insufficient insights or lacking in support across different GPU architectures, runtime and driver versions. This paper presents CUDAAdvisor, a profiling framework to guide code optimization in modern NVIDIA GPUs. CUDAAdvisor performs various fine-grained analyses based on the profiling results from GPU kernels, such as memory-level analysis (e.g., reuse distance and memory divergence), control flow analysis (e.g., branch divergence) and code-/data-centric debugging. Unlike prior tools, CUDAAdvisor supports GPU profiling across different CUDA versions and architectures, including CUDA 8.0 and Pascal architecture. We demonstrate several case studies that derive significant insights to guide GPU code optimization for performance improvement.",IEEE/ACM International Symposium on Code Generation and Optimization,2018,10.1145/3168831,"Ang Li, Du Shen, Xu Liu, S. Song",3.7142860000000000,4
38,DIRECT: Distributed Cross-Domain Resource Orchestration in Cellular Edge Computing,"Network slicing and edge computing are key technologies to enable compute-intensive applications for vertical industries in 5G. We define cellular networks with edge computing capabilities as cellular edge computing. In this paper, we study the cross-domain resource orchestration solution for dynamic network slicing in cellular edge computing. The fundamental research challenge is from the difficulty in modeling the relationship between the slice performance and resources from multiple technical domains across the network with many base stations and distributed edge servers. To address this challenge, we develop a distributed cross-domain resource orchestration (DIRECT) protocol which optimizes the cross-domain resource orchestration while providing the performance and functional isolations among network slices. The main component of DIRECT is a distributed cross-domain resource orchestration algorithm which is designed by integrating the ADMM method and a new learning-assisted optimization approach. The proposed resource orchestration algorithm efficiently orchestrates multi-domain resources without requiring the performance model of the network slices. We develop and implement the DIRECT protocol in a small-scale prototype of cellular edge computing which is designed based on OpenAirInterface LTE and CUDA GPU computing platforms. The performance of DIRECT is validated through both experiments and network simulations.",ACM Interational Symposium on Mobile Ad Hoc Networking and Computing,2019,10.1145/3323679.3326516,"T. Han, Qiang Liu",3.3333330000000001,4
87,FPGA-Based Scalable and Power-Efficient Fluid Simulation using Floating-Point DSP Blocks,"High-performance and low-power computation is required for large-scale fluid dynamics simulation. Due to the inefficient architecture and structure of CPUs and GPUs, they now have a difficulty in improving power efficiency for the target application. Although FPGAs become promising alternatives for power-efficient and high-performance computation due to their new architecture having floating-point (FP) DSP blocks, their relatively narrow memory bandwidth requires an appropriate way to fully exploit the advantage. This paper presents an architecture and design for scalable fluid simulation based on data-flow computing with a state-of-the-art FPGA. To exploit available hardware resources including FP DSPs, we introduce spatial and temporal parallelism to further scale the performance by adding more stream processing elements (SPEs) in an array. Performance modeling and prototype implementation allow us to explore the design space for both the existing Altera Arria10 and the upcoming Intel Stratix10 FPGAs. We demonstrate that Arria10 10AX115 FPGA achieves 519 GFlops at 9.67 GFlops/W only with a stream bandwidth of 9.0 GB/s, which is 97.9 percent of the peak performance of 18 implemented SPEs. We also estimate that Stratix10 FPGA can scale up to 6844 GFlops by combining spatial and temporal parallelism adequately.",IEEE Transactions on Parallel and Distributed Systems,2017,10.1109/TPDS.2017.2691770,"S. Yamamoto, K. Sano",3.25,4
53,Energy Efficient Architecture for Graph Analytics Accelerators,"Specialized hardware accelerators can significantly improve the performance and power efficiency of compute systems. In this paper, we focus on hardware accelerators for graph analytics applications and propose a configurable architecture template that is specifically optimized for iterative vertex-centric graph applications with irregular access patterns and asymmetric convergence. The proposed architecture addresses the limitations of the existing multi-core CPU and GPU architectures for these types of applications. The SystemC-based template we provide can be customized easily for different vertex-centric applications by inserting application-level data structures and functions. After that, a cycle-accurate simulator and RTL can be generated to model the target hardware accelerators. In our experiments, we study several graph-parallel applications, and show that the hardware accelerators generated by our template can outperform a 24 core high end server CPU system by up to 3x in terms of performance. We also estimate the area requirement and power consumption of these hardware accelerators through physical-aware logic synthesis, and show up to 65x better power consumption with significantly smaller area.",International Symposium on Computer Architecture,2016,10.1145/3007787.3001155,"John Greth, S. Burns, Serif Yesil, Özcan Özturk, A. Ayupov, Taemin Kim, Muhammet Mustafa Ozdal",3.111111000000000,4
152,"Hybrid, Scalable, Trace-Driven Performance Modeling of GPGPUs","In this paper, we present PPT-GPU, a scalable performance prediction toolkit for GPUs. PPT-GPU achieves scalability through a hybrid high-level modeling approach where some computations are extrapolated and multiple parts of the model are parallelized. The tool primary prediction models use pre-collected memory and instructions traces of the workloads to accurately capture the dynamic behavior of the kernels. PPT-GPU reports an extensive array of GPU performance metrics accurately while being easily extensible. We use a broad set of benchmarks to verify predictions accuracy. We compare the results against hardware metrics collected using vendor profiling tools and cycle-accurate simulators. The results show that the performance predictions are highly correlated to the actual hardware (MAPE: < 16% and Correlation: > 0.98). Moreover, PPT-GPU is orders of magnitude faster than cycle-accurate simulators. This comprehensiveness of the collected metrics can guide architects and developers to perform design space explorations. Moreover, the scalability of the tool enables conducting efficient and fast sensitivity analyses for performance-critical applications.","International Conference for High Performance Computing, Networking, Storage and Analysis",2021,10.1145/3458817.3476221,"Yehia Arafa, Atanu Barai, Ali Eker, S. Eidenbenz, Gopinath Chennupati, N. Santhi, Abdel-Hameed A. Badawy, Ammar Elwazir",3.0,4
90,Preprint High-Throughput Logic Timing Simulation on GPGPUs,"This article may be used for research, teaching and private study purposes. Any substantial or systematic reproduction, redistribution , reselling , loan or sub-licensing, systematic supply or distribution in any form to anyone is expressly forbidden. granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Many EDA tasks like test set characterization or the precise estimation of power consumption, power droop and temperature development, require a very large number of time-aware gate-level logic simulations. Until now, such characterizations have been feasible only for rather small designs or with reduced precision due to the high computational demands. The new simulation system presented here is able to accelerate such tasks by more than two orders of magnitude and provides for the first time fast and comprehensive timing simulations for industrial-sized designs. Hazards, pulse-filtering, and pin-to-pin delay are supported for the first time in a GPGPU accelerated simulator, and the system can easily be extended to even more realistic delay models and further applications. A sophisticated mapping with efficient memory utilization and access patterns as well as minimal synchro-nizations and control flow divergence is able to use the full potential of GPGPU architectures. To provide such a mapping, we combine for the first time the versatility of event-based timing simulation and multi-dimensional parallelism used in GPU-based gate-level simulators. The result is a throughput-optimized timing simulation algorithm, which runs many simulation instances in parallel and at the same time fully exploits gate-parallelism within the circuit. A preliminary version of this work has been published at the Asian Test Symposium (ATS) 2012. The manuscript at hand has a more general focus and contains significantly more material as detailed in the supporting documents. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this …",,2015,10.1145/2714564,"M. Imhof, S. Holst, H. Wunderlich",2.7,4
95,GPU advancements reduce simulation times for 25 GHz automotive radar models,"The high frequencies utilized by automotive radar sensors, coupled with the electrically large fascia in front of the sensors, have posed a challenge for simulation software in the form of long simulation times. Advances in NVIDIA graphics processing units (GPUs) have alleviated this problem, increasing the productivity of RF engineers who need high fidelity simulations of a sensor behind fascia. Using a 25 GHz short-range radar (SRR) model, this paper compares multiple GPU architectures released in 2007 through 2013 to look at the downward trend in simulation times. A larger than 65% decrease in simulation times is observed, which allows a fully detailed sensor simulation to run in less than 4 hours on modern GPUs.",European Conference on Antennas and Propagation,2015,10.1140/epjc/s10052-015-3533-3,M. Barney,2.7,4
112,High-Throughput Logic Timing Simulation on GPGPUs,"Many EDA tasks such as test set characterization or the precise estimation of power consumption, power droop and temperature development, require a very large number of time-aware gate-level logic simulations. Until now, such characterizations have been feasible only for rather small designs or with reduced precision due to the high computational demands.\n The new simulation system presented here is able to accelerate such tasks by more than two orders of magnitude and provides for the first time fast and comprehensive timing simulations for industrial-sized designs. Hazards, pulse-filtering, and pin-to-pin delay are supported for the first time in a GPGPU accelerated simulator, and the system can easily be extended to even more realistic delay models and further applications.\n A sophisticated mapping with efficient memory utilization and access patterns as well as minimal synchronizations and control flow divergence is able to use the full potential of GPGPU architectures. To provide such a mapping, we combine for the first time the versatility of event-based timing simulation and multi-dimensional parallelism used in GPU-based gate-level simulators. The result is a throughput-optimized timing simulation algorithm, which runs many simulation instances in parallel and at the same time fully exploits gate-parallelism within the circuit.",TODE,2015,10.1145/2714564,"M. Imhof, S. Holst, H. Wunderlich",2.7,4
36,Porting the COSMO Weather Model to Manycore CPUs,"Weather and climate simulations are a major application driver in high-performance computing (HPC). With the end of Dennard scaling and Moore's law, the HPC industry increasingly employs specialized computation accelerators to increase computational throughput. Manycore architectures, such as Intel's Knights Landing (KNL), are a representative example of future processing devices. However, software has to be modified to use these devices efficiently. In this work, we demonstrate how an existing domain-specific language that has been designed for CPUs and GPUs can be extended to Manycore architectures such as KNL. We achieve comparable performance to the NVIDIA Tesla P100 GPU architecture on hand-tuned representative stencils of the dynamical core of the COSMO weather model and its radiation code. Further, we present performance within a factor of two of the P100 of the full DSL-based GPU-optimized COSMO dycore code. We find that optimizing code to full performance on modern manycore architectures requires similar effort and hardware knowledge as for GPUs. Further, we show limitations of the present approaches, and outline our lessons learned and possible principles for design of future DSLs for accelerators in the weather and climate domain.",Platform for Advanced Scientific Computing Conference,2019,10.1145/3324989.3325723,"L. Mosimann, H. Vogt, T. Hoefler, C. Osuna, Felix Thaler, O. Fuhrer, A. Afanasyev, Stefan Moosbrugger, T. Schulthess, Mauro Bianco",2.6666669999999999,4
37,gpuFI-4: A Microarchitecture-Level Framework for Assessing the Cross-Layer Resilience of Nvidia GPUs,"Pre-silicon reliability evaluation of processors is usually performed at the microarchitecture or at the software level. Recent studies on CPUs have, however, shown that software level approaches can mislead the soft error vulnerability assessment process and drive designers towards wrong error protection decisions. To avoid such pitfalls in the GPUs domain, the availability of microarchitecture level reliability assessment tools is of paramount importance. Although there are several publicly available frameworks for the reliability assessment of GPUs, they only operate at the software level, and do not consider the microarchitecture. This paper aims at accurate microarchitecture level GPU soft error vulnerability assessment. We introduce gpuFI-4: a detailed microarchitecture-level fault injection framework to assess the cross-layer vulnerability of hardware structures and entire GPU chips for single and multiple bit faults, built on top of the state-of-the-art simulator GPGPU-Sim 4.0. We employ gpuFI-4 for fault injection of soft errors on CUDA-enabled Nvidia GPU architectures. The target hardware structures that our framework analyzes are the register file, the shared memory, the LI data and texture caches and the L2 cache, altogether accounting for tens of MBs of on-chip GPU storage. We showcase the features of the tool reporting the vulnerability of three Nvidia GPU chip models: two different modem GPU architectures – RTX 2060 (Turing) and Quadro GV100 (Volta) – and an older generation – GTX Titan (Kepler), for both single-bit and triple-bit fault injections and for twelve different CUDA benchmarks that are simulated on the actual physical instruction set (SASS). Our experiments report the Architectural Vulnerability Factor (AVF) of the GPU chips (which can be only measured at the microarchitecture level) as well as their predicted Failures in Time (FIT) rate when technology information is incorporated in the assessment.",IEEE International Symposium on Performance Analysis of Systems and Software,2022,10.1109/ISPASS55109.2022.00004,"D. Gizopoulos, Dimitris Sartzetakis, G. Papadimitriou",2.6666669999999999,4
89,Vulkan-Sim: A GPU Architecture Simulator for Ray Tracing,"Ray tracing can generate photorealistic images with more convincing visual effects compared to rasterization. Recent hardware advances have enabled ray tracing to be applied in real-time. Current GPUs feature a dedicated ray tracing acceleration unit, and game developers have started to make use of ray tracing APIs to bring more realistic graphics to their players. Industry cooperatively contributed to Vulkan, which recently introduced an open-standard API for ray tracing. However, little has been disclosed about the mapping of this API to hardware. In this paper, we introduce Vulkan-Sim, a detailed cycle-level simulator for enabling architecture research for ray tracing. We extend GPGPU-Sim, integrating it with Mesa, an open-source graphics library to support the Vulkan API, and add dedicated ray traversal and intersection units. We also demonstrate an explicit mapping of the Vulkan ray tracing pipeline to a modern GPU using a technique we call delayed intersection and any-hit execution. Additionally we evaluate several ray tracing workloads with Vulkan-Sim, identifying bottlenecks and inefficiencies of the ray tracing hardware we model. To demonstrate the utility of Vulkan-Sim we conduct two case studies evaluating techniques recently proposed or deployed by industry targeting enhanced ray tracing performance.",Micro,2022,10.1109/MICRO56248.2022.00027,"Tor M. Aamodt, Tyler Nowicki, Mohammadreza Saed, Yuan Chou, Lufei Liu",2.6666669999999999,4
127,GPU-I-TASSER: a GPU accelerated I-TASSER protein structure prediction tool,"MOTIVATION\nAccurate and efficient predictions of protein structures play an important role in understanding their functions. I-TASSER (Iterative Threading Assembly Refinement) is one of the most successful and widely used protein structure prediction methods in the recent community-wide CASP experiments. Yet, the computational efficiency of I-TASSER is one of the limiting factors that prevent its application for large-scale structure modelling.\n\n\nRESULTS\nWe present GPU-I-TASSER, a GPU accelerated I-TASSER protein structure prediction tool for fast and accurate protein structure prediction. Our implementation is based on OpenACC parallelization of the replica-exchange Monte Carlo simulations to enhance the speed of I-TASSER by extending its capabilities to the GPU architecture. On a benchmark dataset of 71 protein structures, GPU-I-TASSER achieves on average a 10x speedup with comparable structure prediction accuracy compared to the CPU version of the I-TASSER.\n\n\nAVAILABILITY\nThe complete source code for GPU-I-TASSER can be downloaded and used without restriction from https://zhanggroup.org/GPU-I-TASSER/.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.",Bioinform.,2022,10.1093/bioinformatics/btab871,"Chengxin Zhang, Yang Zhang, Elijah MacCarthy, K. Dukka",2.6666669999999999,4
48,A Compaction Method for STLs for GPU in-field test,"Nowadays, Graphics Processing Units (GPUs) are effective platforms for implementing complex algorithms (e.g., for Artificial Intelligence) in different domains (e.g., automotive and robotics), where massive parallelism and high computational effort are required. In some domains, strict safety-critical requirements exist, mandating the adoption of mechanisms to detect faults during the operational phases of a device. An effective test solution is based on Self-Test Libraries (STLs) aiming at testing devices functionally. This solution is frequently adopted for CPUs, but can also be used with GPUs. Nevertheless, the in-field constraints restrict the size and duration of acceptable STLs. This work proposes a method to automatically compact the test programs of a given STL targeting GPUs. The proposed method combines a multi-level abstraction analysis resorting to logic simulation to extract the microarchitectural operations triggered by the test program and the information about the thread-level activity of each instruction and to fault simulation to know its ability to propagate faults to an observable point. The main advantage of the proposed method is that it requires a single fault simulation to perform the compaction. The effectiveness of the proposed approach was evaluated, resorting to several test programs developed for an open-source GPU model (FlexGripPlus) compatible with NVIDIA GPUs. The results show that the method can compact test programs by up to 98.64% in code size and by up to 98.42% in terms of duration, with minimum effects on the achieved fault coverage.","Design, Automation and Test in Europe",2022,10.23919/DATE54114.2022.9774597,"Juan-David Guerrero-Balaguera, M. Reorda, J. E. R. Condia",2.3333330000000001,4
61,Titan: A Parallel Asynchronous Library for Multi-Agent and Soft-Body Robotics using NVIDIA CUDA,"While most robotics simulation libraries are built for low-dimensional and intrinsically serial tasks, soft-body and multi-agent robotics have created a demand for simulation environments that can model many interacting bodies in parallel. Despite the increasing interest in these fields, no existing simulation library addresses the challenge of providing a unified, highly-parallelized, GPU-accelerated interface for simulating large robotic systems. Titan is a versatile CUDA-based C++ robotics simulation library that employs a novel asynchronous computing model for GPU-accelerated simulations of robotics primitives. The innovative GPU architecture design permits simultaneous optimization and control on the CPU while the GPU runs asynchronously, enabling rapid topology optimization and reinforcement learning iterations. Kinematics are solved with a massively parallel integration scheme that incorporates constraints and environmental forces. We report dramatically improved performance over CPU baselines, simulating as many as 300 million primitive updates per second, while allowing flexibility for a wide range of research applications. We present several applications of Titan to high-performance simulations of soft-body and multi-agent robots.",IEEE International Conference on Robotics and Automation,2019,10.1109/ICRA40945.2020.9196808,"H. Lipson, J. Austin, S. Wyetzner, Rafael Corrales-Fatou",2.3333330000000001,4
110,Core tunneling: Variation-aware voltage noise mitigation in GPUs,"Voltage noise and manufacturing process variation represent significant reliability challenges for modern microprocessors. Voltage noise is caused by rapid changes in processor activity that can lead to timing violations and errors. Process variation is caused by manufacturing challenges in low-nanometer technologies and can lead to significant heterogeneity in performance and reliability across the chip. To ensure correct execution under worst-case conditions, chip designers generally add operating margins that are often unnecessarily conservative for most use cases, which results in wasted energy. This paper investigates the combined effects of process variation and voltage noise on modern GPU architectures. A distributed power delivery and process variation model at functional unit granularity was developed and used to simulate supply voltage behavior in a multicore GPU system. We observed that, just like in CPUs, large changes in power demand can lead to significant voltage droops. We also note that process variation makes some cores much more vulnerable to noise than others in the same GPU. Therefore, protecting the chip against large voltage droops by using fixed and uniform voltage guardbands is costly and inefficient. This paper presents core tunneling, a variation-aware solution for dynamically reducing voltage margins. The system relies on hardware critical path monitors to detect voltage noise conditions and quickly reacts by clock-gating vulnerable cores to prevent timing violations. This allows a substantial reduction in voltage margins. Since clock gating is enabled infrequently and only on the most vulnerable cores, the performance impact of core tunneling is very low. On average, core tunneling reduces energy consumption by 15%.",International Symposium on High-Performance Computer Architecture,2016,10.1109/HPCA.2016.7446061,"Renji Thomas, R. Teodorescu, N. Sedaghati, Li Zhou, Kristin Barber",2.2222219999999999,4
92,Advanced load balancing for SPH simulations on multi-GPU architectures,"Smoothed Particle Hydrodynamics (SPH) is a numerical method for fluid flow modeling, in which the fluid is discretized by a set of particles. SPH allows to model complex scenarios, which are difficult or costly to measure in the real world. This method has several advantages compared to other approaches, but suffers from a huge numerical complexity. In order to simulate real life phenomena, up to several hundred millions of particles have to be considered. Hence, HPC methods need to be leveraged to make SPH applicable for industrial applications. Distributing the respective computations among different GPUs to exploit massive parallelism is thereby particularly suited. However, certain characteristics of SPH make it a non-trivial task to properly distribute the respective workload. In this work, we present a load balancing method for a CUDA-based industrial SPH implementation on multi-GPU architectures. To that end, dedicated memory handling schemes are introduced, which reduce the synchronization overhead. Experimental evaluations confirm the scalability and efficiency of the proposed methods.",IEEE Conference on High Performance Extreme Computing,2017,10.1109/HPEC.2017.8091093,"K. Szewc, R. Wille, Kevin Verma",2.125,4
33,A Cloud Gaming System Based on User-Level Virtualization and Its Resource Scheduling,"Many believe the future of gaming lies in the cloud, namely Cloud Gaming, which renders an interactive gaming application in the cloud and streams the scenes as a video sequence to the player over Internet. This paper proposes GCloud, a GPU/CPU hybrid cluster for cloud gaming based on the user-level virtualization technology. Specially, we present a performance model to analyze the server-capacity and games' resource-consumptions, which categorizes games into two types: CPU-critical and memory-of-critical. Consequently, several scheduling strategies have been proposed to improve the resource-utilization and compared with others. Simulation tests show that both of the First-Fit-like and the Best-Fit-like strategies outperform the other(s); especially they are near optimal in the batch processing mode. Other test results indicate that GCloud is efficient: An off-the-shelf PC can support five high-end video-games run at the same time. In addition, the average per-frame processing delay is 8~19 ms under different image-resolutions, which outperforms other similar solutions.",IEEE Transactions on Parallel and Distributed Systems,2016,10.1109/TPDS.2015.2433916,"Cihang Jiang, Weimin Zheng, Youhui Zhang, Peng Qu",2.0,4
130,Parallel Application Performance Prediction Using Analysis Based Models and HPC Simulations,"Parallel application performance models provide valuable insight about the performance in real systems. Capable tools providing fast, accurate, and comprehensive prediction and evaluation of high-performance computing (HPC) applications and system architectures have important value. This paper presents PyPassT, an analysis based modeling framework built on static program analysis and integrated simulation of the target HPC architectures. More specifically, the framework analyzes application source code written in C with OpenACC directives and transforms it into an application model describing its computation and communication behavior (including CPU and GPU workloads, memory accesses, and message-passing transactions). The application model is then executed on a simulated HPC architecture for performance analysis. Preliminary experiments demonstrate that the proposed framework can represent the runtime behavior of benchmark applications with good accuracy.",SIGSIM Principles of Advanced Discrete Simulation,2018,10.1145/3200921.3200937,"S. Eidenbenz, Gopinath Chennupati, N. Santhi, Jason Liu, M. Obaida",2.0,4
116,FLAME GPU: Complex System Simulation Framework,"FLAME GPU is an agent based simulation frame-work that utilises the parallel architecture of Graphic Processing Unit (GPU) to enable real time model interaction and visualisation. In this paper, we provide an overview of the features of FLAME GPU and demonstrate its efficiency as a parallel agent based simulation platform. FLAME GPU abstracts the complexity of the GPU architecture from the users by offering a high level modelling syntax based on a formal state machine representation. A flocking model is presented showing how a simple multi-agent system is modelled.",International Symposium on High Performance Computing Systems and Applications,2017,10.1109/HPCS.2017.12,"M. Chimeh, P. Richmond",1.875,4
129,A Massively Parallel Reservoir Simulator on the GPU Architecture,"\n Reservoir simulation computational costs have been continuously growing due to high-resolution reservoir characterization, increasing model complexity, and uncertainty analysis workflows. Reducing simulation costs by upscaling is often necessary for operational requirements. Fast evolving HPC technologies offer opportunities to reduce cost without compromising fidelity.\n This work presents a novel in-house massively parallel full-physics reservoir simulator running on the emerging GPU architecture. Almost all the simulation kernels have been designed and implemented to honor the GPU SIMD programming paradigm. These kernels include physical property calculations, phase equilibrium computations, Jacobian construction, linear and nonlinear solvers, and wells. Novel techniques are devised in various kernels to expose enough parallelism to ensure that the control and data-flow patterns are well suited for the GPU environment. Mixed-precision computation is also employed when appropriate (e.g., in derivative calculation) to reduce computational costs without compromising the solution accuracy.\n The GPU implementation of the simulator is tested and benchmarked using various reservoir models, ranging from the synthetic SPE10 Benchmark (Christie & Blunt, 2001) to several industrial-scale models. These real field models range in size from tens of millions of cells to more than billion cells with black-oil and multicomponent compositional fluid. The GPU simulator is benchmarked on the IBM AC922 massively parallel architecture having tens of NVidia Volta V100 GPUs. To compare performance with CPU architectures, an optimized CPU implementation of the simulator is benchmarked on the IBM AC922 CPUs and on a cluster consisting of thousands of Intel's Haswell-EP Xeon® CPU E5-2680 v3. Detailed analysis of several numerical experiments comparing the simulator performance on the GPU and the CPU architectures is presented. In almost all of the cases, the analysis shows that the use of hardware acceleration offers substantial benefits in terms of wall time and power consumption.\n This novel in-house full-physics, black-oil and compositional reservoir simulator employs several novel techniques in various simulation kernels to ensure full utilization of the GPU resources. Detailed analysis is presented to highlight the simulator performance in terms of runtime reduction, parallel scalability and power savings.","Day 1 Tue, October 26, 2021",2021,10.2118/203918-ms,"A. Dogru, Alhubail Maitham Makki, Todd R. Ferguson, Usuf Middya, T. Byer, A. Manea",1.75,4
43,SnuHPL: high performance LINPACK for heterogeneous GPUs,"These days, it is typical for a large-scale cluster system to have different kinds of GPUs. However, HPL (High-Performance LINPACK), the de-facto standard LINPACK implementation for evaluating the performance of a cluster system, is originally designed to work only for homogeneous CPU-only systems. In this paper, we develop SnuHPL, an optimized HPL for clusters of modern heterogeneous GPUs. To optimize SnuHPL for the heterogeneous GPUs, we design a performance model, a SnuHPL simulator based on the model, and a greedy heuristic algorithm based on the simulator. The algorithm generates the best data distribution for a given cluster configuration by considering computing power, memory capacity, and network performance altogether. We also present a simple technique to increase the energy efficiency of HPL by adjusting the core clock frequency of the GPUs. The evaluation of the data distribution algorithm on small clusters of different GPU combinations shows that it outperforms well-known other data distribution strategies. We show the effectiveness of SnuHPL on a cluster of 1,760 NVIDIA A100-80GB GPUs and 440 A100-40GB GPUs. We also show the effectiveness of the proposed energy optimization technique on a cluster of 144 A100-80GB GPUs.",International Conference on Supercomputing,2022,10.1145/3524059.3532370,"Jihwan Park, Jinpyo Kim, Seungwook Lee, Jintaek Kang, Hyungdal Kwon, Jaejin Lee",1.6666669999999999,4
52,A GPU-Based Solution for Ray Tracing 3-D Radiative Transfer Model for Optical and Thermal Images,"Three-dimensional (3-D) radiative transfer (RT) models are frequently recognized as a prerequisite when using high spatial resolution remote sensing (RS) data in heterogeneous surfaces. However, most studies of 3-D RT models have been restricted to limited applications due to the low computational efficiency. Therefore, this study proposed a graphic processing unit (GPU)-based solution for the ray tracing 3-D RT model. A state-of-the-art graphics and compute application programming interface (API), Vulkan, were introduced to implement the RT process. A bounding box method was adopted for the computation acceleration. By comparison with a central processing unit (CPU)-based solution, the performance efficiency of the proposed solution is significantly better: the simulation time of a GPU model is significantly reduced by more than 99% when facing a large-scale simulation mission. The simulation accuracy of the two solutions is similar, with root mean squared errors (RMSEs) lower than 0.005, 0.032, and 0.31 K for the red, near-infrared (NIR), and brightness temperature (BT) images, respectively. An evaluation based on airborne multiangle measurements also indicated that the accuracy of the proposed solution was satisfactory for simulating the red and NIR bidirectional reflectance factor (BRF) and BT directional anisotropies (DAs), with RMSEs lower than 0.003, 0.020, and 0.20 K, respectively, when treating the whole scene as a pixel. Considering the simulation accuracy and efficiency, a GPU-based model will be an important supplement to the CPU model.",IEEE Geoscience and Remote Sensing Letters,2022,10.1109/LGRS.2022.3206312,"Yongming Du, J. Gastellu-Etchegorry, J. Roujean, Qinhuo Liu, Qing Xiao, Jianbo Qi, Biao Cao, Z. Bian, Hua Li",1.6666669999999999,4
81,Performance Characterisation and Simulation of Intel's Integrated GPU Architecture,"Integrated GPUs (iGPUs) are ubiquitous in today's client devices such as laptops and desktops. Examples include Intel's HD or Iris Graphics and AMD's APUs. An iGPU resides on the same chip as the CPU, which is in contrast to a conventional discrete GPU that would typically be connected over the PCI-E bus. Much like discrete GPUs, iGPUs are also capable of general purpose computation in addition to traditional graphics roles. Further, iGPUs have some interesting differences compared to traditional GPUs such as a cache-coherent memory hierarchy and a shared last level cache with the CPU. Despite their wide spread use, they are not studied very extensively. To the best of our knowledge, this paper introduces the first open source trace generation and microarchitectural simulation framework for Intel's integrated GPUs. We characterise the performance of Intel's Skylake and Kabylake GPUs through detailed microbenchmarks, and use the performance evaluations to guide our models and validate the simulator.",IEEE International Symposium on Performance Analysis of Systems and Software,2018,10.1109/ISPASS.2018.00027,"Sunpyo Hong, Prasun Gera, Hyesoon Kim, Hyojong Kim, V. George, C. Luk",1.5714290000000000,4
146,EmerGPU: Understanding and mitigating resonance-induced voltage noise in GPU architectures,"This paper characterizes voltage noise in GPU architectures running general purpose workloads. In particular, it focuses on resonance-induced voltage noise, which is caused by workload-induced fluctuations in power demand that occur at the resonance frequency of the chip's power delivery network. A distributed power delivery model at functional unit granularity was developed and used to simulate supply voltage behavior in a GPU system. We observe that resonance noise can lead to very large voltage droops and protecting against these droops by using voltage guardbands is costly and inefficient. We propose EmerGPU, a solution that detects and mitigates resonance noise in GPUs. EmerGPU monitors workload activity levels and detects oscillations in power demand that approach resonance frequencies. When such conditions are detected, EmerGPU deploys a mitigation mechanism implemented in the warp scheduler that disrupts the resonance activity pattern. EmerGPU has no impact on performance and a small power cost. Reducing voltage noise improves system reliability and allows for smaller voltage margins to be used, reducing overall energy consumption by an average of 21%.",IEEE International Symposium on Performance Analysis of Systems and Software,2016,10.1109/ISPASS.2016.7482076,"Renji Thomas, R. Teodorescu, N. Sedaghati",1.5555559999999999,4
155,Assessment of Efficiency and Performance in Tsunami Numerical Modeling with GPU,"Non-linear shallow water equations (NSWE) are used to solve the propagation and coastal amplification of long waves and tsunamis. Leap Frog scheme of finite difference technique is one of the satisfactory numerical methods which is widely used in these problems. Tsunami numerical models are necessary for not only academic but also operational purposes which need faster and accurate solutions. Recent developments in information technology provide considerably faster numerical solutions in this respect and are becoming one of the crucial requirements. Tsunami numerical code NAMI DANCE uses finite difference numerical method to solve linear and non-linear forms of shallow water equations for long wave problems, specifically for tsunamis. In this study, the new code is structured for Graphical Processing Unit (GPU) using CUDA API. The new code is applied to different (analytical, experimental and field) benchmark problems of tsunamis for tests. One of those applications is 2011 Great East Japan tsunami which was instrumentally recorded on various types of gauges including tide and wave gauges and offshore GPS buoys cabled Ocean Bottom Pressure (OBP) gauges and DART buoys. The accuracy of the results are compared with the measurements and fairly well agreements are obtained. The efficiency and performance of the code is also compared with the version using multi-core Central Processing Unit (CPU). Dependence of simulation speed with GPU on linear or non-linear solutions is also investigated. One of the results is that the simulation speed is increased up to ∼75 times comparing to the process time in the computer using single 4/8 thread multi-core CPU. The results are presented with comparisons and discussions. Furthermore how multi-dimensional finite difference problems fits towards GPU architecture is also discussed.",,2017,10.1007/s11069-017-3082-1,"B. Yalçiner, A. Zaytsev",1.5,4
45,A PCIe Congestion-Aware Performance Model for Densely Populated Accelerator Servers,"MeteoSwiss, the Swiss national weather forecast institute, has selected densely populated accelerator servers as their primary system to compute weather forecast simulation. Servers with multiple accelerator devices that are primarily connected by a PCI-Express (PCIe) network achieve a significantly higher energy efficiency. Memory transfers between accelerators in such a system are subjected to PCIe arbitration policies. In this paper, we study the impact of PCIe topology and develop a congestion-aware performance model for PCIe communication. We present an algorithm for computing congestion factors of every communication in a congestion graph that characterizes the dynamic usage of network resources by an application. Our model applies to any PCIe tree topology. Our validation results on two different topologies of 8 GPU devices demonstrate that our model achieves an accuracy of over 97% within the PCIe network. We demonstrate the model on a weather forecast application to identify the best algorithms for its communication patterns among GPUs.","International Conference for High Performance Computing, Networking, Storage and Analysis",2016,10.1109/SC.2016.62,"Maxime Martinasso, T. Schulthess, T. Hoefler, S. Alam, Grzegorz Kwasniewski",1.4444440000000001,4
145,End-to-end performance modeling of distributed GPU applications,"With the growing number of GPU-based supercomputing platforms and GPU-enabled applications, the ability to accurately model the performance of such applications is becoming increasingly important. Most current performance models for GPU-enabled applications are limited to single node performance. In this work, we propose a methodology for end-to-end performance modeling of distributed GPU applications. Our work strives to create performance models that are both accurate and easily applicable to any distributed GPU application. We combine trace-driven simulation of MPI communication using the TraceR-CODES framework with a profiling-based roofline model for GPU kernels. We make substantial modifications to these models to capture the complex effects of both on-node and off-node networks in today's multi-GPU supercomputers. We validate our model against empirical data from GPU platforms and also vary tunable parameters of our model to observe how they might affect application performance.",International Conference on Supercomputing,2020,10.1145/3392717.3392737,"A. Bhatele, D. Richards, Jaemin Choi, L. Kalé",1.4,4
23,Performance Portability of Sparse Block Diagonal Matrix Multiple Vector Multiplications on GPUs,"The emergence of accelerator-based computer architectures and programming models makes it challenging to achieve performance portability for large-scale scientific simulation software. In this paper, we focus on a sparse block diagonal matrix multiple vector (SpMM) computational kernel and discuss techniques that can be used to achieve performance portability on NVIDIA and AMD based accelerators using CUDA, HIP, OpenACC, Kokkos. We show that performance portability can vary significantly across programming models, GPU architectures, and problem settings, by up to 52× in the explored problems. Our study visits the performance portability aggregation techniques to guide the development and the selection of performance portable algorithmic variants.","International Workshop on Performance, Portability and Productivity in HPC",2022,10.1109/P3HPC56579.2022.00011,"Pieter Maris, K. Ibrahim, Chao Yang",1.3333330000000001,4
100,optimizations in CuSNP Simulator for Spiking Neural P Systems on CUDA GPUs,"Spiking Neural P systems (in short, SNP systems) are computing models based on living neurons. SNP systems are non-deterministic and parallel, hence making use of a parallel processor such as a graphics processing unit (in short, GPU) is a natural candidate for simulations. Matrix representations and algorithms were previously developed for simulating SNP systems. In this work, our two results extend previous works in simulating SNP systems in the GPU: (a) the number of neurons the simulator can handle is now arbitrary; (b) SNP systems are now represented in a dense instead of sparse way. The impact in terms of time and space of these extensions to the GPU simulator are analysed. As expected, SNP systems with more neurons need more simulation time, although the simulator performance can scale (i.e. perform better) with larger GPUs. The dense representation helps in the simulation of larger systems.",International Symposium on High Performance Computing Systems and Applications,2019,10.1109/HPCS48598.2019.9188174,"J. P. Carandang, F. Cabarle, H. Adorna, R. T. Cruz, Miguel A. Martínez-del-Amor, Edward James A. Bariring, Blaine Corwyn D. Aboy",1.3333330000000001,4
30,OO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future NUMA-Based Multi-GPU Systems,"With the strong computation capability, NUMA-based multi-GPU system is a promising candidate to provide sustainable and scalable performance for Virtual Reality (VR) applications and deliver the excellent user experience. However, the entire multi-GPU system is viewed as a single GPU under the single programming model which greatly ignores the data locality among VR rendering tasks during the workload distribution, leading to tremendous remote memory accesses among GPU models (GPMs). The limited inter- GPM link bandwidth (e.g., 64GB/s for NVlink) becomes the major obstacle when executing VR applications in the multi-GPU system. By conducting comprehensive characterizations on different kinds of parallel rendering frameworks, we observe that distributing the rendering object along with its required data per GPM can reduce the inter-GPM memory accesses. However, this object-level rendering still faces two major challenges in NUMA-based multi- GPU system: (1) the large data locality between the left and right views of the same object and the data sharing among different objects and (2) the unbalanced workloads induced by the software- level distribution and composition mechanisms. To tackle these challenges, we propose object-oriented VR rendering framework (OO-VR) that conducts the software and hardware co-optimization to provide a NUMA friendly solution for VR multi-view rendering in NUMA-based multi-GPU systems. We first propose an object-oriented VR programming model to exploit the data sharing between two views of the same object and group objects into batches based on their texture sharing levels. Then, we design an object aware runtime batch distribution engine and distributed hardware composition unit to achieve the balanced workloads among GPMs and further improve the performance of VR rendering. Finally, evaluations on our VR featured simulator show that OO-VR provides 1.58x overall performance improvement and 76% inter-GPM memory traffic reduction over the state-of- the-art multi-GPU systems. In addition, OO-VR provides NUMA friendly performance scalability for the future larger multi-GPU scenarios with ever increasing asymmetric bandwidth between local and remote memory.",International Symposium on Computer Architecture,2019,10.1145/3307650.3322247,"Xin Fu, S. Song, Chenhao Xie, Mingsong Chen",1.1666669999999999,4
98,Real-time end-to-end AO simulations at ELT scale on multiple GPUs with the COMPASS platform,"The COMPASS platform was designed to meet the need for high-performance for the simulation of AO systems. Taking advantage of the specific hardware architecture of the GPU, the COMPASS tool allows the AO scientist to obtain adequate execution speeds and to conduct large simulation campaigns scaled to the E-ELT dimensioning for a variety of AO flavors from SCAO to MOAO. On the latest GPU architecture (NVIDA Volta), execution speeds of several hundreds of short exposure PSF per second can be achieved for a SCAO system on the E-ELT, making the COMPASS platform a real-time end-to-end simulation tool. In this paper, we provide a full description of the critical physical models used in the simulation pipeline, review a range AO system configurations that can be addressed with COMPASS and report on the time to solution obtained for this systems. Scalability over multiple GPUs and multiple generations of GPUs is also discussed.",Astronomical Telescopes + Instrumentation,2018,10.1117/12.2312593,"F. Vidal, N. Doucet, E. Gendron, F. Ferreira, D. Gratadour, A. Sevin, V. Deo",1.1428570000000000,4
26,Characterization of Transmission Lines in Microelectronic Circuits Using the ARTEMIS Solver,"Modeling and characterization of electromagnetic wave interactions with microelectronic devices to derive network parameters has been a widely used practice in the electronic industry. However, as these devices become increasingly miniaturized with finer-scale geometric features, computational tools must make use of manycore/GPU architectures to efficiently resolve length and time scales of interest. This has been the focus of our open-source solver, ARTEMIS (Adaptive mesh Refinement Time-domain ElectrodynaMIcs Solver), which is performant on modern GPU-based supercomputing architectures while being amenable to additional physics coupling. This work demonstrates its use for characterizing network parameters of transmission lines using established techniques. A rigorous verification and validation of the workflow is carried out, followed by its application for analyzing a transmission line on a CMOS chip designed for a photon-detector application. Simulations are performed for millions of timesteps on state-of-the-art GPU resources to resolve nanoscale features at gigahertz frequencies. The network parameters are used to obtain phase delay and characteristic impedance that serve as inputs to SPICE models. The code is demonstrated to exhibit ideal weak scaling efficiency up to 1024 GPUs and 84% efficiency for 2048 GPUs, which underscores its use for network analysis of larger, more complex circuit devices in the future.",IEEE Journal on Multiscale and Multiphysics Computational Techniques,2022,10.1109/JMMCT.2022.3228281,"A. Nonaka, Saurabh S. Sawant, R. Jambunathan, Z. Yao",1.0,4
74,Probabilistically Certified Management of Data Centers Using Predictive Control,"Data centers are facilities with large number of servers providing cloud services. The increasing number of data centers in the last years has generated environmental concern due to the large amount of energy consumed by them. This also includes some auxiliary services such as the cooling equipment which is known to be very costly. For that reason, efficient data center strategies are needed in order to provide an acceptable quality of service (QoS) and suitable temperature for every server while using the least amount of resources possible. This article presents some strategies to deal with the unified workload and temperature problem that appears in the data center. As the system is modeled as a queue and the control variables have an hybrid nature, some highly parallelizable particle-based optimization algorithms are proposed to solve the optimization problem. Numerical simulations are provided in order to illustrate the effectiveness of the strategy. These simulations also show the improvements obtained from the GPU computing. Finally, a probabilistic evaluation approach is developed in order to provide certificates on the probability of constraint satisfaction without increasing the computational burden of the online problem. Note to Practitioners—This article addresses the problem of deciding in real-time the number of active servers in a data center that is required to meet the quality of service (QoS) demands while keeping energy consumption at a minimum. The temperature set point of the cooling equipment must also be taken into account, as it is advisable to use the minimum cooling that keeps the servers running in safe conditions. The management strategy proposed is based on predictive control. In this way, the number of active servers and temperature set point will be chosen so that the future energy consumption is minimized while guaranteeing that QoS and safety demands are met under different possible operating conditions. Furthermore, the proposed management strategy can be tuned depending on the QoS that it is desirable to provide. The proposed strategy will lead to energy-consumption improvements while having guarantees on the data center performance.",IEEE Transactions on Automation Science and Engineering,2022,10.1109/TASE.2021.3093699,"A. D. Carnerero, D. Ramírez, T. Alamo, D. Limón",1.0,4
88,Parallelizing Hines Matrix Solver in Neuron Simulations on GPU,"Hines matrices arise in the simulations of mathematical models describing initiation and propagation of action potentials in a neuron. In this work, we exploit the structural properties of Hines matrices and design a scalable, linear work, recursive parallel algorithm for solving a system of linear equations where the underlying matrix is a Hines matrix, using the Exact Domain Decomposition Method (EDD). We give a general form for representing a Hines matrix and use the general form to prove that the intermediate matrix obtained via the EDD has the same structural properties as that of a Hines matrix. Using the above observation, we propose a novel decomposition strategy called fine decomposition which is suitable for a GPU architecture. Our algorithmic approach R-FINE-TPT based on fine decomposition outperforms the previously known approach in all the cases and gives a speedup of 2.5x on average for a variety of input neuron morphologies. We further perform experiments to understand the behaviour of R-FINE-TPT approach and show its robustness. We also employ a machine learning technique called linear regression to effectively guide recursion in our algorithm.",International Conference on High Performance Computing,2017,10.1109/HiPC.2017.00051,"Kishore Kothapalli, U. Bhalla, Dharma Teja Vooturi",1.0,4
143,Computing and Compressing Electron Repulsion Integrals on FPGAs,"The computation of electron repulsion integrals (ERIs) over Gaussian-type orbitals (GTOs) is a challenging problem in quantum-mechanics-based atomistic simulations. In practical simulations, several trillions of ERIs may have to be computed for every time step. In this work, we investigate FPGAs as accelerators for the ERI computation. We use template parameters, here within the Intel oneAPI tool flow, to create customized designs for 256 different ERI quartet classes, based on their orbitals. To maximize data reuse, all intermediates are buffered in FPGA on-chip memory with customized layouts. The pre-calculation of intermediates also helps to overcome data dependencies caused by multi-dimensional recurrence relations. The involved loop structures are partially or even fully unrolled for high throughput of FPGA kernels. Furthermore, a lossy compression algorithm utilizing arbitrary bitwidth integers is integrated in the FPGA kernels. To our best knowledge, this is the first work on ERI computation on FPGAs that supports more than just the single most basic quartet class. Also, the integration of ERI computation and compression is a novelty that is not even covered by CPU or GPU libraries so far. Our evaluation shows that using 16-bit integer for the ERI compression, the fastest FPGA kernels exceed the performance of 10 GERIS ($10\times 10^{9}$ ERIs per second) on one Intel Stratix 10 GX 2800 FPGA, with maximum absolute errors around 10−7 - 10−5 Hartree. The measured throughput can be accurately explained by a performance model. The FPGA kernels deployed on 2 FPGAs outperform similar computations using the widely used libint reference on a two-socket server with 40 Xeon Gold 6148 CPU cores of the same process technology by factors up to 6.0x and on a new two-socket server with 128 EPYC 7713 CPU cores by up to 1.9x.",IEEE Symposium on Field-Programmable Custom Computing Machines,2023,10.1109/FCCM57271.2023.00026,"Tobias Kenter, Robert Schade, Xin-Chuan Wu, Christian Plessl, T. Kühne",1.0,4
76,CPU/GPU Heterogeneous Parallel CFD Solver and Optimizations,"Graphics Processing Units (GPU) has been widely used in the area of general computing. Nowadays, CFD has dramatically benefited from the strong abilities of floating-point operation and memory bandwidth of GPU architecture. Lots of GPU-based CFD solvers have already shown that GPU has capacities in accelerating numerical simulations. Compute unified device architecture (CUDA), a general parallel computing platform and programming model, reduces program complication, brings the great opportunities to CFD, and has been successfully used to the parallel solution of compressible Navier-Stokes equations. In this paper, we present a CPU/GPU heterogeneous parallel CFD solver which established on NVIDIA GTX 1070 GPU, and the serial code executes on the host, the parallel code performs on the device. Three optimization methods are discussed: maximize utilization, reducing global memory access and minimize data transfer between host and device, which further improve the performance of the solver. Two cases, including flow over sphere at Reynolds number 118 and flow over double ellipsoid, are presented to demonstrate the solver's capacities for compressible flow. Numerical results agree well with experimental data, which illustrate that the solver has a high computational precision for compressible flow.",International Conference Service Robotics Technologies,2018,10.1145/3208833.3208847,"Zhengyu Tian, Hua Li, Jianqi Lai",0.8571430000000000,4
151,AMulti-GPU PCISPH Implementation with Efficient Memory Transfers,"Smoothed Particle Hydrodynamics (SPH) is a particle-based method for fluid flow modeling. One promising variant of SPH is Predictive-Corrective Incompressible SPH (PCISPH), which employs a dedicate prediction-correction scheme and, by this, outperforms other SPH variants by almost one order of magnitude. However, similar to other particle-based methods, it suffers from a huge numerical complexity. In order to simulate real world phenomena, several millions of particles need to be considered. To make SPH applicable to real world engineering problems, it is hence common to exploit massive parallelism of multi-GPU architectures. However, certain algorithmic characteristics of PCISPH make it a non-trivial task to efficiently parallelize this method on multi-GPUs. In this work, we are, for the first time, proposing a multi-GPU implementation for PCISPH. To this end, we are proposing a scheme which allows to overlap the memory transfers between GPUs by actual computations and, by this, avoids the drawbacks caused by the mentioned algorithmic characteristics of PCISPH. Experimental evaluations confirm the efficiency of the proposed methods.",IEEE Conference on High Performance Extreme Computing,2018,10.1109/HPEC.2018.8547542,"R. Wille, Chong Peng, Kevin Verma, K. Szewc",0.8571430000000000,4
39,Power and Performance Optimal NoC Design for CPU-GPU Architecture Using Formal Models,"Heterogeneous computing architectures that fuse both CPU and GPU on the same chip are common nowadays. Using homogeneous interconnect for such heterogeneous processors each with different network demands can result in performance degradation. In this paper, we focused on designing a heterogeneous mesh-style network-on-chip (NoC) to connect heterogeneous CPU-GPU processors. We tackled three problems at once; mapping Processing Elements (PEs) to the routers of the mesh, assigning the number of virtual channels (VC), and assigning the buffer size (BS) for each port of each router in the NoC. By relying on formal models, we developed a method based on Strength Pareto Evolutionary Algorithm2 (SPEA2) to obtain the Pareto optimal set that optimizes communication performance and power consumption of the NoC. By validating our method on a full-system simulator, results show that the NoC performance can be improved by 17% while minimizing the power consumption by at least 2.3x and maintaining the overall system performance.","Design, Automation and Test in Europe",2019,10.23919/DATE.2019.8714769,"N. Bagherzadeh, Lulwah Alhubail",0.8333330000000000,4
70,Modeling Emerging Memory-Divergent GPU Applications,"Analytical performance models yield valuable architectural insight without incurring the excessive runtime overheads of simulation. In this work, we study contemporary GPU applications and find that the key performance-related behavior of such applications is distinct from traditional GPU applications. The key issue is that these GPU applications are memory-intensive and have poor spatial locality, which implies that the loads of different threads commonly access different cache blocks. Such memory-divergent applications quickly exhaust the number of misses the L1 cache can process concurrently, and thereby cripple the GPU's ability to use Memory-Level Parallelism (MLP) and Thread-Level Parallelism (TLP) to hide memory latencies. Our Memory Divergence Model (MDM) is able to accurately represent this behavior and thereby reduces average performance prediction error by 14× compared to the state-of-the-art GPUMech approach across our memory-divergent applications.",IEEE computer architecture letters,2019,10.1109/LCA.2019.2923618,"Lu Wang, L. Eeckhout, Almutaz Adileh, Zhiying Wang, Magnus Jahre",0.8333330000000000,4
60,A Performance Model for GPU-Accelerated FDTD Applications,"In this work we develop, validate and use a performance model for a Finite-Difference Time-Domain (FDTD) application which is parallelized on multiple GPUs. FDTD is a method for simulating electrodynamic interaction and is applied in a number of research and engineering areas. In this work we focus on a particular implementation called B-CALM (Belgium-California Light Machine). We adopt a simple, semi-empirical modelling approach to design a model which we validate for different hardware architectures. Using the model allows making implementation decisions and exploring the architectural design space with the goal of optimizing HPC systems for this application.",International Conference on High Performance Computing,2015,10.1109/HiPC.2015.24,"D. Pleiter, P. Wahl, J. Kraus, T. Hater, P. Baumeister",0.8,4
117,Task-Based Crowd Simulation for Heterogeneous Architectures,"Industry trends in the coming years imply the availability of cluster computing with hundreds to thousands of cores per chip, as well as the use of accelerators. Programming presents a challenge due to this heterogeneous architecture; thus, using novel programming models that facilitate this process is necessary. In this chapter, the case of simulation and visualization of crowds is presented. The authors analyze and compare the use of two programming models: OmpSs and CUDA. OmpSs allows to take advantage of all the resources available per node by combining the CPU and GPU while automatically taking care of memory management, scheduling, communications and synchronization. Experimental results obtained from Fermi, Kepler and Maxwell GPU architectures are presented, and the different modes used for visualizing the results are described, as well.",,2016,10.4018/978-1-5225-0287-6.CH008,"E. Ayguadé, Benjamín Hernández, Hugo Perez, Isaac Rudomín",0.7777780000000000,4
138,Parallel Massive-Thread Electromagnetic Transient Simulation on GPU,"The electromagnetic transient (EMT) simulation of a large-scale power system consumes so much computational power that parallel programming techniques are urgently needed in this area. For example, realistic-sized power systems include thousands of buses, generators, and transmission lines. Massive-thread computing is one of the key developments that can increase the EMT computational capabilities substantially when the processing unit has enough hardware cores. Compared to the traditional CPU, the graphic-processing unit (GPU) has many more cores with distributed memory which can offer higher data throughput. This paper proposes a massive-thread EMT program (MT-EMTP) and develops massive-thread parallel modules for linear passive elements, the universal line model, and the universal machine model for offline EMT simulation. An efficient node-mapping structure is proposed to transform the original power system admittance matrix into a block-node diagonal sparse format to exploit the massive-thread parallel GPU architecture. The developed MT-EMTP program has been tested on large-scale power systems of up to 2458 three-phase buses with detailed component modeling. The simulation results and execution times are compared with mainstream commercial software, EMTP-RV, to show the improvement in performance with equivalent accuracy.",IEEE Transactions on Power Delivery,2015,10.1109/PESGM.2015.7285591,"V. Dinavahi, Zhiyin Zhou",0.7,4
22,A Parallel Mode Optimized GPU Accelerated Monte Carlo Model for Light Propagation in 3-D Voxelized Bio-Tissues,"Monte Carlo simulation is a precise method to model light propagation in bio-tissues and has been considered the golden standard to estimate the result of other computation methods. But the huge computation burden limited the application. In this paper, we propose a parallel computing model using graphic card to accelerate the Monte Carlo simulation in 3-D voxelized media with the consideration of internal refraction. Optimization of the parallel mode is made by using segmentations and offered an extra boost of simulation speed. The acceleration efficiency affecting factors are investigated and the acceleration rate of the five segmented model is 32.6 times higher than non-GPU model and 1.66 times higher than non-optimized model for a real human head 3-D structure simulation.",IEEE Access,2019,10.1109/ACCESS.2019.2923320,"Xiang Fang, Ting Li, Yingxin Li, Weichao Liu, Hao Li",0.6666670000000000,4
86,NoMali: Simulating a realistic graphics driver stack using a stub GPU,"Since the advent of the smartphone, all high-end mobile devices have required graphics acceleration in the form of a GPU. Today, even low-power devices such as smartwatches use GPUs for rendering and composition. However, the computer architecture community has largely ignored these developments when evaluating new architecture proposals. A common approach when evaluating CPU designs for the mobile space has been to use software rendering instead of a GPU model. However, due to the ubiquity of GPUs in mobile devices, they are used in both 3D applications and 2D applications. For example, when running a 2D application such as the web browser in Android with a software renderer instead of a GPU, the CPU ends up executing twice as many instructions. Both the CPU characteristics and the memory system characteristics differ significantly between the browser and the software renderer. The software renderer typically executes tight loops of vector instructions, while the browser predominantly consists of integer instructions and complex control flow with hard-to-predict branches. Including software rendering results in unrepresentative benchmark performance. In this paper, we use gem5 to quantify the effects of software rendering on a set of common mobile workloads. We also introduce the NoMali stub GPU model that can be used as a drop-in replacement for a real Mali GPU model. This model behaves like a normal GPU, but does not render anything. Using this stub GPU, we demonstrate how most of the problems associated with software rendering can be avoided, while at the same time simulating a representative graphics stack.",IEEE International Symposium on Performance Analysis of Systems and Software,2016,10.1109/ISPASS.2016.7482100,"R. D. Jong, Andreas Sandberg",0.6666670000000000,4
94,Towards Virtual Certification of Gas Turbine Engines With Performance-Portable Simulations,"We present the large-scale, computational fluid dy-namics (CFD) simulation of a full gas-turbine engine compressor, demonstrating capability towards overcoming current limitations for virtual certification of aero-engine design. The simulation is carried out through a performance portable code-base on multi-core/many-core HPC clusters with a CFD-to-CFD coupled execution, combining an industrial CFD solver linked using custom coupler software. The application innovates in its design for performance portability through the OP2 domain specific library for the CFD components, allowing the automatic generation of highly optimized platform-specific parallelizations for both multi-core (CPU) and many-core (GPU) clusters from a single high-level source. The code is used for the simulation of a 4.58B node, full-annulus 10-row production-grade test compressor (DLR's Rig250), using a coupled sliding-plane setup on the ARCHER2 and Cirrus supercomputers at EPCC. The OP2 generated multiple parallelizations, together with optimized coupler configurations on heterogeneous/hybrid settings achieve, for the first time, execution of 1 revolution in less than 6 hours on 512 nodes of ARCHER2 (65k cores), with a parallel scaling efficiency of over 80 % compared to a 107 node run. Results indicate a speed up of the CFD suite by an order of a magnitude (≈30 x) relative to current production capability. Benchmarking and performance modelling project a time-to-solution of less than 5 hours on a cluster of 488xNVIDIA V100 GPUs, about 3x-4 x speedup over CPU clusters. The work demonstrates a step-change towards achieving virtual certification of aircraft engines with the requisite fidelity and tractable time-to-solution that was previously out of reach under production settings.",IEEE International Conference on Cluster Computing,2022,10.1109/CLUSTER51413.2022.00034,"D. Amirante, S. Jarvis, G. Mudalige, I. Reguly, L. Lapworth, A. Prabhakar",0.6666670000000000,4
73,Solving incompressible Navier-Stokes equations on heterogeneous parallel architectures. (Résolution des équations de Navier-Stokes incompressibles sur architectures parallèles hétérogènes),"In this PhD thesis, we present our research in the domain of high performance software for computational fluid dynamics (CFD). With the increasing demand of high-resolution simulations, there is a need of numerical solvers that can fully take advantage of current manycore accelerated parallel architectures. In this thesis we focus more specifically on developing an efficient parallel solver for 3D incompressible Navier-Stokes (NS) equations on heterogeneous CPU/GPU architectures. We first present an overview of the CFD domain along with the NS equations for incompressible fluid flows and existing numerical methods. We describe the mathematical model and the numerical method that we chose, based on an incremental prediction-projection method.A balanced distribution of the computational workload is obtained by using a domain decomposition method. A two-level parallelization combined with SIMD vectorization is used in our implementation to take advantage of the current distributed multicore machines. Numerical experiments on various parallel architectures show that this solver provides satisfying performance and good scalability.In order to further improve the performance of the NS solver, we integrate GPU computing to accelerate the most time-consuming tasks. The resulting solver can be configured for running on various heterogeneous architectures by specifying explicitly the numbers of MPI processes, threads and GPUs. This thesis manuscript also includes simulation results for two benchmarks designed from real physical cases. The computed solutions are compared with existing reference results. The code developed in this work will be the base for a future CFD library for parallel CPU/GPU computations.",,2015,10.1002/fld.4019,Yushan Wang,0.6,4
93,Benchmarking multi-GPU communication using the shallow water equations,The shallow water model equations provide a simple yet realistic benchmark problem in computational fluid dynamics (CFD) that can be implemented on a variety of computational platforms. Graphical processing units (GPUs) can be used to accelerate such applications on a single device using a data parallel decompositional scheme or with multiple devices using a domain decompositional approach. A shallow water equation simulation is implemented on a range of modern GPU architectures and multi-GPU systems. The typical performance of these systems using the two main device-device communication methods is reported on for single- and double-precision calculations.,International Journal of Big Data Intelligence,2015,10.1504/IJBDI.2015.070596,"D. Playne, K. Hawick",0.6,4
46,Multi-GPU hybrid programming accelerated three-dimensional phase-field model in binary alloy,"In the process of dendritic growth simulation, the computational efficiency and the problem scales have extremely important influence on simulation efficiency of three-dimensional phase-field model. Thus, seeking for high performance calculation method to improve the computational efficiency and to expand the problem scales has a great significance to the research of microstructure of the material. A high performance calculation method based on MPI+CUDA hybrid programming model is introduced. Multi-GPU is used to implement quantitative numerical simulations of three-dimensional phase-field model in binary alloy under the condition of multi-physical processes coupling. The acceleration effect of different GPU nodes on different calculation scales is explored. On the foundation of multi-GPU calculation model that has been introduced, two optimization schemes, Non-blocking communication optimization and overlap of MPI and GPU computing optimization, are proposed. The results of two optimization schemes and basic multi-GPU model are compared. The calculation results show that the use of multi-GPU calculation model can improve the computational efficiency of three-dimensional phase-field obviously, which is 13 times to single GPU, and the problem scales have been expanded to 8193. The feasibility of two optimization schemes is shown, and the overlap of MPI and GPU computing optimization has better performance, which is 1.7 times to basic multi-GPU model, when 21 GPUs are used.In the process of dendritic growth simulation, the computational efficiency and the problem scales have extremely important influence on simulation efficiency of three-dimensional phase-field model. Thus, seeking for high performance calculation method to improve the computational efficiency and to expand the problem scales has a great significance to the research of microstructure of the material. A high performance calculation method based on MPI+CUDA hybrid programming model is introduced. Multi-GPU is used to implement quantitative numerical simulations of three-dimensional phase-field model in binary alloy under the condition of multi-physical processes coupling. The acceleration effect of different GPU nodes on different calculation scales is explored. On the foundation of multi-GPU calculation model that has been introduced, two optimization schemes, Non-blocking communication optimization and overlap of MPI and GPU computing optimization, are proposed. The results of two optimization schemes and b...",,2018,10.1063/1.5021730,"M. Zhu, Li Feng, Changsheng Zhu, Jieqiong Liu",0.5714290000000000,4
40,TUFLOW GPU – Best Practice Advice for Hydrologic and Hydraulic Model Simulations,Graphics Processing Unit (GPU) computing represents a significant advancement in the continued evolution of flood modelling. BMT WBM the developers of TUFLOW are continually undertaking research work focusing on how best to use the software. This paper presents a range of GPU model validation results and summarises best practice recommendations on how to optimise execution to achieve fastest model simulation and most accurate results for hydrologic and hydraulic applications.,,2016,10.1038/537587a,"Bill Syme, C. Huxley",0.5555560000000000,4
85,GPU Performance Prediction Through Parallel Discrete Event Simulation and Common Sense,"We present the GPU Module of a Performance Prediction Toolkit developed at Los Alamos National Laboratory, which enables code developers to efficiently test novel algorithmic ideas particularly for large-scale computational physics codes. The GPU Module is a heavily-parameterized model of the GPU hardware that takes as input a sequence of abstracted instructions that the user provides as a representation of the application or can also be read in from the GPU intermediate representation PTX format. These instructions are then executed in a discrete event simulation framework of the entire computing infrastructure that can include multi-GPU and also multi-node components as typically found in high performance computing applications. Our GPU Module aims at a trade-off between the cycle-accuracy of GPU simulators and the fast execution times of analytical models. This trade-off is achieved by simulating at cycle level only a portion of the computations and using this partial runtime to analytically predict the total execution of the modeled application. We present GPU models that we validate against three different benchmark applications that cover the range from bandwidth- to cycle-limited. Our runtime predictions are within an error of 20%. We then predict performance of a next-generation GPU (Nvidia’s Pascal) for the same benchmark applications.",EAI Endorsed Transactions on Ubiquitous Environments,2016,10.4108/eai.14-12-2015.2262575,"S. Eidenbenz, N. Santhi, Guillaume Chapuis",0.5555560000000000,4
135,A review of load flow and network reconfiguration techniques with their enhancement for radial distribution network,Load flow analysis of electrical distribution networks either for providing household electricity or in integrated circuits has always been a topic of great interest for researchers from last few decades. Various novel methods and techniques have been proposed for load flow calculations in the network and simulation tools are being used to determine the various characteristics of the network. The paper mainly focusses on these techniques at a single site and the enhancements made to the already existing techniques throughout the use of GPU architecture using CUDA platform in lieu of enhancing the performance of already existing solution with respect to time and algorithmic complexity. The paper also explains the need of enhancing prevailing solutions for load flow analysis for future generation smart grids or real time systems by comparing the performance with serial version for different topologies of network. Further the paper also throws light on some of the network reconfiguration techniques used to remodel the RDN due to high power losses in the network and floats an idea how parallel processing can be beneficial in enhancing already existing genetic algorithm based network reconfiguration technique to support real time load flow calculations and topology construction in smart grids and future generation systems.,"2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC)",2016,10.1109/PDGC.2016.7913188,"Rahul Saxena, Monika Jain, Ankit Mundra, D. P. Sharma",0.5555560000000000,4
29,Porting Batched Iterative Solvers onto Intel GPUs with SYCL,"Batched linear solvers play a vital role in computational sciences, especially in the fields of plasma physics and combustion simulations. With the imminent deployment of the Aurora Supercomputer and other upcoming systems equipped with Intel GPUs, there is a compelling demand to expand the capabilities of these solvers for Intel GPU architectures. In this paper, we present our efforts in porting and optimizing the batched iterative solvers on Intel GPUs using the SYCL programming model. These new solvers achieve impressive performance on the Intel GPU Max 1550s (Ponte Vecchio GPUs) which surpass our previous CUDA implementation on NVIDIA H100 GPUs by an average of 2.4x for the PeleLM application inputs. The batched solvers are ready for production use in real-world scientific applications through the Ginkgo library, complementing the performance portability of the batched functionality of Ginkgo.",SC Workshops,2023,10.1145/3624062.3624181,"Phuong Nguyen, Pratik Nayak, H. Anzt",0.5,4
58,Attribute-Aware RBFs: Interactive Visualization of Time Series Particle Volumes Using RT Core Range Queries,"Smoothed-particle hydrodynamics (SPH) is a mesh-free method used to simulate volumetric media in fluids, astrophysics, and solid mechanics. Visualizing these simulations is problematic because these datasets often contain millions, if not billions of particles carrying physical attributes and moving over time. Radial basis functions (RBFs) are used to model particles, and overlapping particles are interpolated to reconstruct a high-quality volumetric field; however, this interpolation process is expensive and makes interactive visualization difficult. Existing RBF interpolation schemes do not account for color-mapped attributes and are instead constrained to visualizing just the density field. To address these challenges, we exploit ray tracing cores in modern GPU architectures to accelerate scalar field reconstruction. We use a novel RBF interpolation scheme to integrate per-particle colors and densities, and leverage GPU-parallel tree construction and refitting to quickly update the tree as the simulation animates over time or when the user manipulates particle radii. We also propose a Hilbert reordering scheme to cluster particles together at the leaves of the tree to reduce tree memory consumption. Finally, we reduce the noise of volumetric shadows by adopting a spatially temporal blue noise sampling scheme. Our method can provide a more detailed and interactive view of these large, volumetric, time-series particle datasets than traditional methods, leading to new insights into these physics simulations.",IEEE Transactions on Visualization and Computer Graphics,2023,10.1109/TVCG.2023.3327366,"Patrick Shriwise, Valerio Pascucci, Stefan Zellmann, Alper Sahistan, N. Morrical",0.5,4
84,GATSim: Abstract timing simulation of GPUs,"General-Purpose Graphic Processing Units (GPUs) have become an integral part of heterogeneous system architectures. Ever increasing complexities have made rapid, early performance evaluation of GPU-based architectures and applications a primary design concern. Traditional cycle-accurate GPU simulators are too slow, while existing analytical or source-level estimation approaches are often inaccurate. This paper proposes a novel abstract GPU performance simulation approach that is based on flexible separation of functional and timing models, combining a fast functional execution either on existing simulators or native GPU hardware with a light, fast and accurate abstract timing model. Micro-architecture timing of individual GPU cores is abstracted through static, one-time pre-characterization of code, and only the dynamic scheduling effects are simulated. Using a native GPU for functional execution and excluding pre-characterization, our GPU simulation achieves a throughput of more than 80 MIPS. This is on average 400x faster with 4% error compared to a cycle-accurate GPU simulator for standard GPU benchmarks. Moreover, our simple timing model provides flexibility to target different GPU configurations with little or no extra effort.","Design, Automation and Test in Europe",2017,10.23919/DATE.2017.7926956,"Behzad Boroujerdian, A. Gerstlauer, Kishore Punniyamurthy",0.5,4
109,Characterizing and Optimizing Irregular Applications on Graphics Processing Units,"In recent years, GPGPUs have experienced tremendous growth as general-purpose and high-throughput computing devices. Applications from various domains achieve significant speedups using GPGPUs. However, irregular applications do not perform well due to the mismatches between irregular problem structures and SIMD-like GPU architectures. The lack of in-depth characterization and quantifying the ways in which irregular applications differ from regular ones on GPGPUs has prevented users from effectively making use of the hardware resource. To characterize the performance aspects and analyze the bottlenecks, a suite of representative irregular applications are examined on a cycle-accurate GPU simulator as well as a real GPU. The experimental results identify control-flow divergences,",,2015,10.1016/j.compfluid.2014.11.017,Tao Zhang,0.5,4
154,Efficient parallelization of SPH algorithm on modern multi-core CPUs and massively parallel GPUs,"Smoothed Particle Hydrodynamics (SPH) is fast emerging as a practically useful computational simulation tool for a wide variety of engineering problems. SPH is also gaining popularity as the back bone for fast and realistic animations in graphics and video games. The Lagrangian and mesh-free nature of the method facilitates fast and accurate simulation of material deformation, interface capture, etc. Typically, particle-based methods would necessitate particle search and locate algorithms to be implemented efficiently, as continuous creation of neighbor particle lists is a computationally expensive step. Hence, it is advantageous to implement SPH, on modern multi-core platforms with the help of High-Performance Computing (HPC) tools. In this work, the computational performance of an SPH algorithm is assessed on multi-core Central Processing Unit (CPU) as well as massively parallel General Purpose Graphical Processing Units (GP-GPU). Parallelizing SPH faces several challenges such as, scalability of the neighbor search process, force calculations, minimizing thread divergence, achieving coalesced memory access patterns, balancing workload, ensuring optimum use of computational resources, etc. While addressing some of these challenges, detailed analysis of performance metrics such as speedup, global load efficiency, global store efficiency, warp execution efficiency, occupancy, etc. is evaluated. The OpenMP and Compute Unified Device Architecture[Formula: see text] parallel programming models have been used for parallel computing on Intel Xeon[Formula: see text] E5-[Formula: see text] multi-core CPU and NVIDIA Quadro M[Formula: see text] and NVIDIA Tesla p[Formula: see text] massively parallel GPU architectures. Standard benchmark problems from the Computational Fluid Dynamics (CFD) literature are chosen for the validation. The key concern of how to identify a suitable architecture for mesh-less methods which essentially require heavy workload of neighbor search and evaluation of local force fields from neighbor interactions is addressed.",Advances in Complex Systems,2021,10.1142/s1793962321500549,"V. Sanapala, R. Nasre, B. Patnaik, Pravin Jagtap",0.5,4
107,Using colored petri nets for GPGPU performance modeling,"Performance analysis and modeling of applications running on GPUs is still a challenge for most designers and developers. State-of-the-art solutions are dominated by two classic approaches: statistical models that require a lot of training and profiling on existing hardware, and analytical models that require in-depth knowledge of the hardware platform and significant calibration. Both these classes separate the application from the hardware and attempt a high-level combination of the two models for performance prediction. In this work, we propose an orthogonal approach, based on high-level simulation. Specifically, we use Colored Petri Nets (CPN) to model both the hardware and the application. Using this model, the execution of the application is a simulation of the CPN model using warps as tokens. Our prototype implementation of this modeling approach demonstrates promising results on a few case studies on two different GPU architectures: both reasonably accurate predictions and detailed execution information are obtained. We conclude that CPN-based GPU performance modeling is an elegant solution for systematic performance prediction, and we focus further on optimizing the models to improve the execution time of the symbolic simulation.",Conf. Computing Frontiers,2016,10.1145/2903150.2903167,"A. Varbanescu, C. D. Laat, S. Madougou",0.4444440000000000,4
125,Simulating PCI-Express Interconnect for Future System Exploration,"The PCI-Express interconnect is the dominant interconnection technology within a single computer node that is used for connecting off-chip devices such as network interface cards (NICs) and GPUs to the processor chip. The PCI-Express bandwidth and latency are often the bottleneck in the processor, memory and device interactions and impacts the overall performance of the connected devices. Architecture simulators often focus on modeling the performance of processor and memory and lack a performance model for the I/O devices and interconnections. In this work, we implement a flexible and detailed model for the PCI-Express interconnect in a widely known architecture simulator. We also implement a PCI-Express device model that is configured by a PCI-Express device driver. We validate our PCI-Express interconnect performance against a physical Gen 2 PCI-Express link. Our evaluation results show that the PCI-Express model bandwidth is within 19.0% of the physical setup. We use our model to evaluate different PCI-Express link widths and latency and show its impact on the overall I/O performance of an I/O intensive application.",IEEE International Symposium on Workload Characterization,2018,10.1109/IISWC.2018.8573496,"Mohammad Alian, N. Kim, K. Srinivasan",0.42857099999999998,4
80,Developing Efficient Discrete Simulations on Multicore and GPU Architectures,"In this paper we show how to efficiently implement parallel discrete simulations on multicore and GPU architectures through a real example of an application: a cellular automata model of laser dynamics. We describe the techniques employed to build and optimize the implementations using OpenMP and CUDA frameworks. We have evaluated the performance on two different hardware platforms that represent different target market segments: high-end platforms for scientific computing, using an Intel Xeon Platinum 8259CL server with 48 cores, and also an NVIDIA Tesla V100 GPU, both running on Amazon Web Server (AWS) Cloud; and on a consumer-oriented platform, using an Intel Core i9 9900k CPU and an NVIDIA GeForce GTX 1050 TI GPU. Performance results were compared and analyzed in detail. We show that excellent performance and scalability can be obtained in both platforms, and we extract some important issues that imply a performance degradation for them. We also found that current multicore CPUs with large core numbers can bring a performance very near to that of GPUs, and even identical in some cases.",,2020,10.3390/electronics9010189,"M. López-Torres, D. Cagigas-Muñiz, J. Guisado, F. Díaz-del-Río, F. Jiménez-Morales",0.4,4
82,LISFLOOD-FP 8.0: the new discontinuous Galerkin shallow-water solver for multi-core CPUs and GPUs,"Abstract. LISFLOOD-FP 8.0 includes second-order discontinuous Galerkin (DG2) and first-order finite-volume (FV1) solvers of the two-dimensional shallow-water equations for modelling a wide range of flows, including rapidly propagating, supercritical flows, shock waves or flows over very smooth surfaces.\nThe solvers are parallelised on multi-core CPU and Nvidia GPU architectures and run existing LISFLOOD-FP modelling scenarios without modification.\nThese new, fully two-dimensional solvers are available alongside the existing local inertia solver (called ACC), which is optimised for multi-core CPUs and integrates with the LISFLOOD-FP sub-grid channel model.\nThe predictive capabilities and computational scalability of the new DG2 and FV1 solvers are studied for two Environment Agency benchmark tests and a real-world fluvial flood simulation driven by rainfall across a 2500 km2 catchment.\nDG2's second-order-accurate, piecewise-planar representation of topography and flow variables enables predictions on coarse grids that are competitive with FV1 and ACC predictions on 2–4 times finer grids, particularly where river channels are wider than half the grid spacing.\nDespite the simplified formulation of the local inertia solver, ACC is shown to be spatially second-order-accurate and yields predictions that are close to DG2. The DG2-CPU and FV1-CPU solvers achieve near-optimal scalability up to 16 CPU cores and achieve greater efficiency on grids with fewer than 0.1 million elements. The DG2-GPU and FV1-GPU solvers are most efficient on grids with more than 1 million elements, where the GPU solvers are 2.5–4 times faster than the corresponding 16-core CPU solvers. LISFLOOD-FP 8.0 therefore marks a new step towards operational DG2 flood inundation modelling at the catchment scale.\nLISFLOOD-FP 8.0 is freely available under the GPL v3 license, with additional documentation and case studies at https://www.seamlesswave.com/LISFLOOD8.0 (last access: 2 June 2021).\n",Geoscientific Model Development,2020,10.5194/gmd-2020-340,"M. Sharifian, G. Kesserwani, P. Bates, James Shaw, J. Neal",0.4,4
119,SLATE: Managing Heterogeneous Cloud Functions,"This paper presents SLATE, a fully-managed, heterogeneous Function-as-a-Service (FaaS) system for deploying serverless functions onto heterogeneous cloud infrastructures. We extend the traditional homogeneous FaaS execution model to support heterogeneous functions, automating and abstracting runtime management of heterogeneous compute resources in order to improve cloud tenant accessibility to specialised, accelerator resources, such as FPGAs and GPUs. In particular, we focus on the mechanisms required for heterogeneous scaling of deployed function instances to guarantee latency objectives while minimising cost. We develop a simulator to validate and evaluate our approach, considering case-study functions in three application domains: machine learning, bio-informatics, and physics. We incorporate empirically derived performance models for each function implementation targeting a hardware platform with combined computational capacity of 24 FPGAs and 12 CPU cores. Compared to homogeneous CPU and homogeneous FPGA functions, simulation results achieve respectively a cost improvement for non-uniform task traffic of up to 8.7 times and 1.7 times, while maintaining specified latency objectives.","IEEE International Conference on Application-Specific Systems, Architectures, and Processors",2020,10.1109/ASAP49362.2020.00032,"Eriko Nurvitadhi, Mishali Naik, W. Luk, J. Coutinho, Jessica Vandebon",0.4,4
142,PAQSIM: Fast Performance Model for Graphics Workload on Mobile GPUs,"As the popularity of GPU in embedded systems keeps increasing, there is a growing demand for performance models for rapid estimation and tuning. One major challenge of developing a GPU performance model is the balance between accuracy and speed. The analytical model and the architectural model, two prevailing performance models, both have their weaknesses. The analytical model is fast to execute and simple to implement but usually suffers from low simulation accuracy. On the other hand, the cycle-level architectural model can offer high accuracy, but often at the expense of the execution time. In this work, we present a hybrid performance model for core-level performance studies. Our model takes advantage of the speed of the analytical model and the accuracy of the cycle-level architectural model. We model the resource contention as in traditional architectural models but reduce the pipeline stages when no contention is expected. The graphics workloads have shown uniform characteristics, which allows us to replace some detailed simulation with analytical models for latency estimation in key events such as memory accesses, texture fetches, and synchronizations. Such design greatly reduces the simulation time while maintains decent simulation accuracy. We evaluate our performance model against commercial mobile GPUs. The experiments using graphics workloads from popular games show great simulation speed and high accuracy in predicting the GPU performance. For simulations using the aggressive mode, the simulator can achieve an average 4.1x slowdown, with an average error rate at 6% and the peak error rate at 27.9%.","ACM SIGPLAN Conference on Languages, Compilers, and Tools for Embedded Systems",2020,10.1145/3372799.3394359,"C. Lim, Xiang Gong, Chunling Hu",0.4,4
148,Massive Scaling of MASSIF: Algorithm Development and Analysis for Simulation on GPUs,"Micromechanical Analysis of Stress-Strain Inhomogeneities with Fourier transforms (MASSIF) is a large-scale Fortran-based differential equation solver used to study local stresses and strains in materials. Due to its prohibitive memory requirements, it is extremely difficult to port the code to GPUs with small on-device memory. In this work, we present an algorithm design that uses domain decomposition with approximate convolution, which reduces memory footprint to make the MASSIF simulation feasible on distributed GPU systems. A first-order performance model of our method estimates that compression and multi-resolution sampling strategies can enable domain computation within GPU memory constraints for 3D grids larger than those simulated by the current state-of-the-art Fortran MPI implementation. The model analysis also provides an insight into design requirements for further scalability. Lastly, we discuss the extension of our method to irregular domain decomposition and challenges to be tackled in the future.",Platform for Advanced Scientific Computing Conference,2020,10.1145/3394277.3401857,"Jelena Kovacevic, Anuva Kulkarni, F. Franchetti",0.4,4
62,Real-Time Simulation and Optimization of Elastic Aircraft Vehicle Based on Multi-GPU Workstation,"Modern aircraft such as missile and rocket, due to the large slenderness ratio of slender body vehicles, the influence of elastic deformation and vibration on navigation, guidance, and engine modules in simulation can not be ignored. For the problems of slow calculation speed and incapability of real-time simulation for time-domain simulation, by analyzing the time proportion of each calculation step under different computing scale, the dynamic parallel construction of octree is used to represent the aerodynamic parameter table under the environment of single and multi GPU. Meanwhile, an innovative parallel algorithm of element stiffness matrix based on finite element model is designed in GPU architecture. Accordingly, the optimized performance is enhanced through the adaptive hardware resources and rational use of shared memory. Furthermore, A multi-threaded asynchronous framework based on task queue and thread pool is proposed to realize the parallel task calculation with different granularities. The numerical result shows that the acceleration ratio of about 20 times in the single GPU condition can be obtained, and the acceleration ratio of at least 30 times can be obtained by the parallel computing of dual GPUs, enabling the real-time simulation of the flexible aircraft with 1200 elements within 20ms.",IEEE Access,2019,10.1109/ACCESS.2019.2946684,"Liu Xingguo, Binxing Hu",0.3333330000000000,4
104,GPU accelerated Monte-Carlo simulation of SEM images for metrology,"In this work we address the computation times of numerical studies in dimensional metrology. In particular, full Monte-Carlo simulation programs for scanning electron microscopy (SEM) image acquisition are known to be notoriously slow. Our quest in reducing the computation time of SEM image simulation has led us to investigate the use of graphics processing units (GPUs) for metrology. We have succeeded in creating a full Monte-Carlo simulation program for SEM images, which runs entirely on a GPU. The physical scattering models of this GPU simulator are identical to a previous CPU-based simulator, which includes the dielectric function model for inelastic scattering and also refinements for low-voltage SEM applications. As a case study for the performance, we considered the simulated exposure of a complex feature: an isolated silicon line with rough sidewalls located on a at silicon substrate. The surface of the rough feature is decomposed into 408 012 triangles. We have used an exposure dose of 6 mC/cm2, which corresponds to 6 553 600 primary electrons on average (Poisson distributed). We repeat the simulation for various primary electron energies, 300 eV, 500 eV, 800 eV, 1 keV, 3 keV and 5 keV. At first we run the simulation on a GeForce GTX480 from NVIDIA. The very same simulation is duplicated on our CPU-based program, for which we have used an Intel Xeon X5650. Apart from statistics in the simulation, no difference is found between the CPU and GPU simulated results. The GTX480 generates the images (depending on the primary electron energy) 350 to 425 times faster than a single threaded Intel X5650 CPU. Although this is a tremendous speedup, we actually have not reached the maximum throughput because of the limited amount of available memory on the GTX480. Nevertheless, the speedup enables the fast acquisition of simulated SEM images for metrology. We now have the potential to investigate case studies in CD-SEM metrology, which otherwise would take unreasonable amounts of computation time.",SPIE Advanced Lithography,2016,10.1117/12.2219160,"T. Verduin, S. Lokhorst, C. W. Hagen",0.3333330000000000,4
122,Validated Thermal Air Management Simulations of Data Centers Using Remote Graphics Processing Units,"Simulation tools for thermal management of data centers help to improve layout of new builds or analyse thermal problems in existing data centers. The development of LBM on remote GPUs as an approach for such simulations is discussed making use of VirtualGL and prioritised multi-threaded implementations of an existing LBM code. The simulation is configured to model an existing and highly monitored test data center. Steady-state root mean square averages of measured and simulated temperatures are compared showing good agreement. The full capability of this simulation approach is demonstrated when comparing rack temperatures against a time varying workload, which employs time-dependent boundary conditions.",Annual Conference of the IEEE Industrial Electronics Society,2018,10.1109/IECON.2018.8591192,"N. Delbosc, Mattias Vesterlund, J. Summers, Johannes Sjolund, Amirul Khan",0.2857140000000000,4
72,VLAG: A very fast locality approximation model for GPU kernels with regular access patterns,"Performance modeling plays an important role for optimal hardware design and optimized application implementation. This paper presents a very low overhead performance model, called VLAG, to approximate the data localities exploited by GPU kernels. VLAG receives source code-level information to estimate per memory-access instruction, per data array, and per kernel localities within GPU kernels. VLAG is only applicable to kernels with regular memory access patterns. VLAG was experimentally evaluated using an NVIDIA Maxwell GPU. For two different Matrix Multiplication kernels, the average errors of 7.68% and 6.29%, was resulted, respectively. The slowdown of VLAG for MM was measured 1.4X which, comparing with other approaches such as trace-driven simulation, is negligible.",International Conference on Computer and Knowledge Engineering,2017,10.1109/ICCKE.2017.8167887,"Mohsen Kiani, Amir Rajabzadeh",0.25,4
123,M2S-CGM: A Detailed Architectural Simulator for Coherent CPU-GPU Systems,"We introduce M2S-CGM a detailed architectural simulator that models the interactions between CPUs and GPUs operating in coherent heterogeneous compute environments. M2S-CGM extends an existing and established x86 CPU model and Southern Islands GPU model, adds a new custom-built memory system model and switching fabric called CGM, and incorporates a well-known SDRAM model. The CGM memory system simulator provides configurable entire system simulation and can support a range of non-coherent and coherent CPU-GPU configurations. M2S-CGM supports the runtime for OpenCL-based benchmarks in addition to traditional multithreaded CPU benchmarks and can run benchmarks from established heterogeneous benchmark collections. This allows us to experiment with different coherent CPU-GPU configurations and propose effective future improvements in these systems. We present the makeup of M2S-CGM's software architectural design, provide a validation of the simulator, and provide coherent CPU-GPU execution results. Our validation results show average differences between our physical test system and M2S-CGM, of 10.4%, 22%, and 6.4% for 2 threaded, 4 threaded, and heterogeneous benchmark runs respectively. Our coherent CPU-GPU experimental results show an average speedup of 2.8 for our benchmarks over the baseline noncoherent system.",ICCD,2017,10.1109/ICCD.2017.84,"Christopher E. Giles, Mark A. Heinrich",0.25,4
103,Implementation of a pre-calculated database approach for scatter correction in SPECT,"In single photon emission computed tomography (SPECT), attenuation and scatter introduce important artefacts in the reconstructed images leading to mis diagnosis for patient’s follow-up. Furthermore, by using Monte Carlo simulation (MCS), physical effects undergone by photons during the SPECT exam can be precisely modeled and accounted for during iterative reconstruction, which improves the quality of the image. However, MCS are time consuming and therefore inappropriate for the rate of daily exams performed in clinical routine. Our work is based on the assumption that patients are composed of identical biological tissues and that photon propagation in an element volume of a given tissue is similar and reproducible from one subject to another. We hence propose to accelerate modeling of the physical effects occurring in emission tomography making it adequate for daily exam by using the approach of scatter pre-calculated database. The developed efficient patient-dependent attenuation and scatter correction were implemented on a GPU architecture on a state-of-art single-processor workstation and yielded to a speed-up factor in time computing of four orders of magnitude. Results presented in this proof of concept study are in good agreement with full MCS.",,2016,10.1088/2057-1976/2/5/055014,"Benjamin Auer, Z. E. Bitar, Clement Rey, V. Bekaert, J. Gallone",0.2222220000000000,4
49,Green High Performance Simulation for AMB models of Aedes aegypti,"The increase in temperature caused by the climate change has resulted in the rapid dissemination of infectious diseases. Given the alert for the current situation, the World Health Organization (WHO) has declared a state of health emergency, highlighting the severity of the situation in some countries. For this reason, coming up with knowledge and tools that can help control and eradicate the vectors propagating these diseases is of the utmost importance. High-performance modeling and simulation can be used to produce knowledge and strategies that allow predicting infections, guiding actions and/or training health/civil protection agents. The model developed as part of this research work is aimed at assisting the decision-making process for disease prevention and control, as well as evaluating the reproduction and predicting the evolution of the Aedes aegypti mosquito, which is the transmitting vector of the dengue, Zika and chikungunya diseases. Since a large number of simulation runs are required to achieve results with statistical variability, GPU has been used. This platform has enough computational power to reduce execution time while maintaining a lower energy consumption. Different scenarios and experiments are proposed to corroborate the benefits of the architecture proposed.",Journal of Computational Science and Technology,2020,10.24215/16666038.20.e02,"R. Suppi, Laura Cristina De Gisuti, M. Naiouf, Erica Soledad Montes de Oca",0.2,4
106,Efficient simulations of spiking neurons on parallel and distributed platforms: Towards large-scale modeling in computational neuroscience,"Human brain communicates information by means of electro-chemical reactions and processes it in a parallel, distributed manner. Computational models of neurons at different levels of details are used in order to make predictions for physiological dysfunctions. Advances in the field of brain simulations and brain computer interfaces have increased the complexity of this modeling process. With a focus to build large-scale detailed networks, we used high performance computing techniques to model and simulate the granular layer of the cerebellum. Neuronal firing patterns of cerebellar granule neurons were modeled using two mathematical models Hodgkin-Huxley (HH) and Adaptive Exponential Leaky Integrate and Fire (AdEx). The performance efficiency of these modeled neurons was tested against a detailed multi-compartmental model of the granule cell. We compared different schemes suitable for large scale simulations of cerebellar networks. Large networks of neurons were constructed and simulated. Graphic Processing Units (GPU) was employed in the pleasantly parallel implementation while Message Passing Interface (MPI) was used in the distributed computing approach. This allowed to explore constraints of different parallel architectures and to efficiently load balance the tasks by maximally utilizing the available resources. For small scale networks, the observed absolute speedup was 6X in an MPI based approach with 32 processors while GPUs gave 10X performance gain compared to a single CPU implementation. In large networks, GPUs gave approximately 5X performance gain in processing time compared to the MPI implementation. The results enabled us to choose parallelization schemes suitable for large-scale simulations of cerebellar circuits. We are currently extending the network model based on large scale simulations evaluated in this paper and using a hybrid - heterogeneous MPI based multi-GPU architecture for incorporating millions of cerebellar neurons for assessing physiological disorders in such circuits.",IEEE Recent Advances in Intelligent Computational Systems,2015,10.1109/RAICS.2015.7488425,"Revathy S. Kumar, Manjusha Nair, Shyam Diwakar, B. Nair, S. Surya",0.2,4
124,Improving accuracy of source level timing simulation for GPUs using a probabilistic resource model,"After their success in the high performance and desktop market, Graphic Processing Units (GPUs), that can be used for general purpose computing are introduced for embedded systems on a chip (SOCs). Due to some advanced architectural features, like massive simultaneous multithreading, static performance analysis and high-level timing simulation are difficult to apply to code running on these systems. This paper extends a method for performance simulation of GPUs. The method uses automated performance annotations in the application's OpenCL C source code, and an extended performance model for derivation of a kernels runtime from metrics produced by the execution of annotated kernels. The final results are then generated using a probabilistic resource conflict model. The model reaches an accuracy of 90% on most test cases and delivers a higher average accuracy than previous methods.","International Conference / Workshop on Embedded Computer Systems: Architectures, Modeling and Simulation",2015,10.1109/SAMOS.2015.7363655,"Christoph Gerum, W. Rosenstiel, O. Bringmann",0.2,4
56,Parallel computing algorithm for real-time mapping between large-scale networks,"In this paper, we propose a scalable massively-parallel algorithm to solve the general mapping problem in large-scale networks in real-time. The proposed parallel algorithm takes advantage of GPU architecture and launches millions of workers to calculate values on a target network simultaneously. Threads are managed through the SIMT execution model and target values are updated through atomic operations. Our experiments show the proposed algorithm can accomplish network mapping (find importance weights for links in a real-world large-scale shared-mobility network) with more than 2 million weights within 1.82 µs (microsecond-level), which is truly real-time. The algorithm performance suggests that mapping computations may no longer be the bottleneck in highly dynamic network-centered problems, as the computations can be completed faster than the solid state drive (SSD) read access latency. Compared to serial algorithms, the speedup is more than 12,000 times. The proposed algorithm is also scalable. Results on simulated data show that even when the network size grows exponentially, microsecond-level computing performance can still be obtained, and even more than 190,000 times speedup can be achieved. The proposed algorithm can serve as a cornerstone for ultra-fast processing of highly dynamic large-scale networks.",International Conference on Intelligent Transportation Systems,2019,10.1109/ITSC.2019.8917463,"Ethan Zhang, Amirmahdi Tafreshian, Neda Masoud",0.16666700000000001,4
68,"Projections of achievable performance for Weather & Climate Dwarfs, and for entire NWP applications, on hybrid architectures","This document is one of the deliverable reports created for the ESCAPE project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather Prediction at Exascale. The project develops world-class, extreme-scale computing capabilities for European operational numerical weather prediction and future climate models. This is done by identifying Weather & Climate dwarfs which are key patterns in terms of computation and communication (in the spirit of the Berkeley dwarfs). These dwarfs are then optimised for different hardware architectures (single and multi-node) and alternative algorithms are explored. Performance portability is addressed through the use of domain specific languages. \nThis deliverable contains the description of the performance and energy models for the selected Weather & Climate dwarfs for different hardware architectures, multinode with GPU accelerators in particular. Presented performance models are extension to model provided in Deliverable 3.2. With some further enhancements, they are incorporated in the DCworms simulator. In particular, extended models allow to predict computational and energy performance on different architectures: single and multinodes, equipped with CPUs and GPUs accelerators. This allows to provide feasible performance projection at system scale.",arXiv.org,2019,10.2139/ssrn.3432809,"Marek Blazewicz, Sebastian Ciesielski, M. Kulczewski",0.16666700000000001,4
140,Gravitational Octree Code Performance Evaluation on Volta GPU,"In this study, the gravitational octree code originally optimized for the Fermi, Kepler, and Maxwell GPU architectures is adapted to the Volta architecture. The Volta architecture introduces independent thread scheduling requiring either the insertion of the explicit synchronizations at appropriate locations or the enforcement of the same implicit synchronizations as do the Pascal or earlier architectures by specifying -gencode arch=compute_60,code=sm_70. The performance measurements on Tesla V100, the current flagship GPU by NVIDIA, revealed that the N-body simulations of the Andromeda galaxy model with 223 = 8 388 608 particles took 3.8 × 10-2 s or 3.3 × 10-2 s per step for cases without or with the implicit synchronizations, respectively. Tesla V100 achieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the flagship GPU in the previous generation. The observed speed-up of 2.2 is greater than 1.5, which is the ratio of the theoretical peak performance of the two GPUs. The independence of the units for integer operations from those for floating-point number operations enables the overlapped execution of integer and floating-point number operations. It hides the execution time of the integer operations leading to the speed-up rate above the theoretical peak performance ratio. Tesla V100 can execute N-body simulation with up to 25 × 220 = 26 214 400 particles, and it took 2.0 × 10-1 s per step. It corresponds to 3.5 TFlop/s, which is 22% of the single-precision theoretical peak performance.",International Conference on Parallel Processing,2018,10.1145/3337821.3337845,Yohei Miki,0.14285700000000001,4
32,Algorithms and Software for High-Performance Fracture Simulation on GPU Architectures,"Author(s): Lim, Rone Kwei | Advisor(s): Petzold, Linda R | Abstract: Computer simulation of fracture in materials with nonlinear mechanical response can be computationally expensive. These simulations often require a large number of degrees of freedom, and the nonlinearity in the problem can pose difficulties when computing solutions. This work focuses on two material models. The first model consists of rigid bricks interacting through nonlinear cohesive springs. Fracture in the material occurs through the rupture of the cohesive springs. The second, more complicated, model consists of deformable elements interacting through nonlinear cohesive springs. In the first model, we assume the bricks are under a quasi-static loading scenario. With this assumption, the problem can be solved using a global Monte Carlo minimization algorithm to minimize the energy of the system. The energy in the system comes from the deformation and rupture of the nonlinear cohesive springs. Since these simulations have a high computational cost, we have developed a GPU-based (Graphics Processing Unit) Monte Carlo minimization algorithm that offers a significant speedup compared to a conventional multithreaded CPU-based algorithm. With the second model, we have dynamic simulations with explicit time discretization. In this case we compute the force, acceleration, velocity, and position explicitly. The force in the system comes from both the deformation of the elements as well as the deformation of the nonlinear cohesive springs. We have developed explicit, CPU-based methods and implicit-explict methods on both CPUs and GPUs. Our implicit-explict GPU-based method achieves substantial performance improvement compared to the explicit, CPU-based method. We present our GPU-based implementation of AES (Advanced Encryption Standard), which is used in the Monte Carlo minimization algorithm to generate random numbers. Our implementation is substantially faster than CPU-based implementation of AES. It is also faster than previous GPU implementations of AES.",,2017,10.5977/jkasne.2017.23.3.330,R. K. Lim,0.125,4
41,An Out-of-Core Method for Physical Simulations on a Multi-GPU Architecture Using Lattice Boltzmann Method,"Simulating complex physical phenomena implies the manipulation of an important amount of data. In order to simulate very large simulation domains on a limited computing architecture, such as industrial infrastructures, solutions have to be proposed. In this paper, a new out-of-core method is introduced in order to perform fast physical simulations using a complex Lattice Boltzmann model (LBM) on a single-node multi-GPU (CUDA) architecture. GPU global memory generally is far lower than the CPU main memory, can be problematic for a large simulation domain. The objective of this paper is to propose an efficient method of data exchanges between GPUs, the CPU main memory, which allows to perform fast complex simulations on large installations. The combination of this method with the massive parallelism of GPUs allows to keep good simulation performance. A complex simulation involving two physical components (water + air) is used in order to validate this method.","2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)",2016,10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0099,"J. Duchateau, C. Renaud, G. Roussel, N. Maquignon, F. Rousselle",0.1111110000000000,4
120,Energy and Performance Prediction of CUDA Applications using Dynamic Regression Models,"Many emerging supercomputers and future exa-scale computing machines require accelerator-based GPU computing architectures for boosting their computing performances. CUDA is one of the widely applied GPGPU parallel computing platform for those architectures owing to its better performance for certain scientific applications. However, the emerging rise in the development of CUDA applications from various scientific domains, such as, bioinformatics, HEP, and so forth, has urged the need for tools that identify optimal application parameters and the other GPGPU architecture metrics, including work group size, work item, memory utilization, and so forth. In fact, the tuning process might end up with several executions of various possible code variants. This paper proposed Dynamic Regression models, namely, Dynamic Random Forests (DynRFM), Dynamic Support Vector Machines (DynSVM), and Dynamic Linear Regression Models (Dyn LRM) for the energy/performance prediction of the code variants of CUDA applications. The prediction was based on the application parameters and the performance metrics of applications, such as, number of instructions, memory issues, and so forth. In order to obtain energy/performance measurements for CUDA applications, EACudaLib (a monitoring library implemented in EnergyAnalyzer tool) was developed. In addition, the proposed Dynamic Regression models were compared to the classical regression models, such as, RFM, SVM, and LRM. The validation results of the proposed dynamic regression models, when tested with the different problem sizes of Nbody and Particle CUDA simulations, manifested the energy/performance prediction improvement of over 50.26 to 61.23 percentages.",International Symposium on Electronic Commerce,2016,10.1145/2856636.2856643,"S. Benedict, R. Rejitha, Suja A. Alex",0.1111110000000000,4
147,Fast Race Detection and Profiling Framework for Heterogeneous System,"Heterogeneous computing is a growing trend in recent computer architecture design and is often used to improve the performance and power efficiency for computing applications by utilizing the special-purpose processors or accelerators, such as the Graphic Computing Unit (GPU), Field Programmable Gate Array (FPGA) and Digital Signal Processor (DSP). With the increase of complexity, the interaction among accelerators and processors could be deadfall if a race condition happens. However, the existing tools for such task are either too slow or hard to extend the race condition detection mechanism. Therefore, tools for application profiling with approximate timing model are important to the design of such heterogeneous systems in a timing manner. In this paper, we proposed a pluggable GPU interface on an existing timing approximate CPU simulator based on QEMU for analyzing the memory behavior of heterogeneous systems. Monitoring the memory behavior, the pluggable interface could be extended to any kinds of accelerators, such as GPU, DSP and FPGA, for race condition detection. Taking the GPU as an example, we integrated the detailed GPU simulator from Multi2Sim with the existing timing approximate CPU simulator, VPA, to showcase the efficiency of the proposed work. The experimental results showed that the emulation speed of the proposed framework reached at most 9x faster than Multi2Sim with acceptable timing results accuracy which is less than 20% error rate from our previous work. In addition, the race condition detection mechanism further indicates the problematic memory accesses to user.",International Conference on Supercomputing,2016,10.1109/ICS.2016.0110,"Chenggang Lai, Shih-Hao Hung, C. Yeh",0.1111110000000000,4
83,Communication efficient work distributions in stencil operation based applications,"In recent years, the use of accelerators in conjunction with CPUs, known as heterogeneous computing, has brought about significant performance increases for scientific applications. One of the best examples of this is lattice quantum chromodynamics (QCD), a stencil operation based simulation. These simulations have a large memory footprint necessitating the use of many graphics processing units (GPUs) in parallel. This requires the use of a heterogeneous cluster with one or more GPUs per node. In order to obtain optimal performance, it is necessary to determine an efficient communication pattern between GPUs on the same node and between nodes. In this paper, we present a performance model based method for minimizing the communication time of applications with stencil operations, such as lattice QCD, on heterogeneous computing systems with a non‐blocking InfiniBand interconnection network. The proposed method is able to increase the performance of the most computationally intensive kernel of lattice QCD by 25% due to improved overlapping of communication and computation. We also demonstrate that the aforementioned performance model and efficient communication patterns can be used to determine a cost efficient heterogeneous system design for stencil operation based applications. Copyright © 2014 John Wiley & Sons, Ltd.",Concurrency and Computation,2015,10.1002/cpe.3210,"T. El-Ghazawi, L. Ríha, J. Schneible, A. Alexandru, Maria Malik",0.1,4
139,Asynchronous OpenCL/MPI numerical simulations of conservation laws,"Hyperbolic conservation laws are important mathematical models for describing many phenomena in physics or engineering. The Finite Volume (FV) method and the Discontinuous Galerkin (DG) methods are two popular methods for solving conservation laws on computers. Those two methods are good candidates for parallel computing: • they require a large amount of uniform and simple computations, • they rely on explicit time-integration • they present regular and local data access pattern. In this paper, we present several FV and DG numerical sim- ulations that we have realized with the OpenCL and MPI paradigms. First, we compare two optimized implementations of the FV method on a regular grid: an OpenCL implementation and a more traditional OpenMP implementation. We compare the efficiency of the approach on several CPU and GPU architectures of different brands. Then we give a short presentation of the DG method. Finally, we present how we have implemented this DG method in the OpenCL/MPI framework in order to achieve high efficiency. The implementation relies on a splitting of the DG mesh into sub-domains and sub-zones. Different kernels are compiled according to the zones properties. In addition, we rely on the OpenCL asynchronous task graph in order to overlap OpenCL computations, memory transfers and MPI communications.",Software for Exascale Computing,2015,10.1145/2791321.2791325,"T. Strub, M. Roberts, P. Helluy, M. Massaro",0.1,4
150,Finite element numerical integration for first order approximations on multi-core architectures,"The paper presents investigations on the implementation and performance of the finite element numerical integration algorithm for first order approximations and three processor architectures, popular in scientific computing, classical CPU, Intel Xeon Phi and NVIDIA Kepler GPU. A unifying programming model and portable OpenCL implementation is considered for all architectures. Variations of the algorithm due to different problems solved and different element types are investigated and several optimizations aimed at proper optimization and mapping of the algorithm to computer architectures are demonstrated. Performance models of execution are developed for different processors and tested in practical experiments. The results show the varying levels of performance for different architectures, but indicate that the algorithm can be effectively ported to all of them. The general conclusion is that the finite element numerical integration can achieve sufficient performance on different multiand many-core architectures and should not become a performance bottleneck for finite element simulation codes.",arXiv.org,2015,10.1115/omae2015-42101,"K. Banas, J. Bielanski, Filip Kruzel",0.1,4
28,Transient-simulation guided graph sparsification approach to scalable Harmonic Balance (HB) analysis of post-layout RF circuits leveraging heterogeneous CPU-GPU computing systems,"Harmonic Balance (HB) analysis is key to efficient verification of large post-layout RF and microwave integrated circuits (ICs). This paper introduces a novel transient-simulation guided graph sparsification technique, as well as an efficient runtime performance modeling approach tailored for heterogeneous manycore CPU-GPU computing system to build nearly-optimal subgraph preconditioners that can lead to minimum HB simulation runtime. Additionally, we propose a novel heterogeneous parallel sparse block matrix algorithm by taking advantages of the structure of HB Jacobian matrices as well as GPU's streaming multiprocessors to achieve optimal work load balancing during the preconditioning phase of HB analysis. We also show how the proposed preconditioned iterative algorithm can efficiently adapt to heterogeneous computing systems with different CPU and GPU computing capabilities. Extensive experimental results show that our HB solver can achieve up to 20X speedups and 5X memory reduction when compared with the state-of-the-art direct solver highly optimized for eight-core CPUs.",Design Automation Conference,2015,10.1145/2744769.2744920,"Zhuo Feng, Lengfei Han",0.0,4
31,A GPU-Accelerated Hydrodynamics Solver For Atmosphere-Fire Interactions,"A fundamental process to understand fire spread is the atmospheric flow. Building computational tools to simulate this complex flow has several challenges including boundary layer effects, resolving vegetation and the forest canopies, conserving fluid mass, and incorporating fire-induced flows. We develop a two-dimensional hydrodynamic solver that models fire-induced flow as a convective sink that converts the two-dimensional horizontal flow into a vertical flow through the buoyant plume. The resulting equations are the two-dimensional Navier-Stokes equations, but with point source delta functions appearing in the conservation of mass equation. We develop a projection method to solve these equations and implement them on a GPU architecture. The ultimate goalis to simulate wildfire spread faster than real-time, and with the ability for users to introduce real-time updates in an augmented reality sandbox.",SIGGRAPH Posters,2022,10.1145/3532719.3543263,"B. Quaife, K. Speer, Jhamieka Greenwood",0.0,4
34,Reliability issues in GPGPUs,"The present work discusses approaches for implementing software redundancy schemes using the open source GPGPU model FlexGrip to increase the reliability of a GPGPU. ?? ??Most GPUs do not feature hardware support for error detection, and a device such as GPGPU a corrupt result could be unacceptable, as applications such as machine vision rely on the correctness of the processed image. A fault could occur at any time during the operation of the device, and it's critical that it is either masked or detected. Therefore improving the fidelity of GPGPU using software redundancy seems to be the only way to avoid errors. ?? ??In this work of thesis several approaches for matrix multiplication were produced, recording the performance of each; The three approaches differ in the method by which they guarantee the correct result. The first case is double comparison (DWC) which implies repeatedly performing operations and comparing the results, in case they are equal the correct result is stored in memory. ??The second method is the TMR. It is based on the triplication of resources and a voter who establishes by a majority which element is the correct one. The last method studied is ABFT which through comparisons identifies in which cell the error occurred and corrects it. ?? ??Each code was tested on the FlexGrip model after the injection of static faults inside the register file of each streaming multiprocessor. The expected result of each program obtained in simulation - the ""golden output"" - was compared to the same result in presence of injected static faults. ?? ??Results were finally collected and the fault coverage analysed, along with the time required and memory space. Future tests may be performed with different fault models, such as transient or delay faults, since the behaviour of the circuit would vary unpredictably.",,2019,10.4324/9780429276316-4,Chiara Penaglia,0.0,4
35,BcBench: Exploring Throughput Processor Designs based on Blockchain Benchmarking,"Benchmark suites have become the most important wheel to drive the research of GPU architecture designs. The most popular GPU benchmark suites, such as Rodinia and Parboil, cover a wide range of real-world applications, which provide a good reference for the researchers to estimate the performance behaviors of the GPU devices in production. However, the existing GPU benchmarks were initially proposed a decade ago, which unfortunately cannot reflect the unique features of the latest GPU applications accurately. Therefore, it is inappropriate to employ the stale benchmarks as the reference to evaluate the state-of-the-art GPU architectures. Tackling this challenge, we propose BcBench, a new set of GPU workloads collected from the emerging blockchain applications. BcBench is designed for estimating the performance behaviors of the future GPU architectures in accelerating the execution of the emerging applications. To this end, we have ported BcBench to the popular GPU simulators (e.g., GPGPU-Sim) for in-depth performance analysis. We first characterize the blockchain workloads at the micro-architecture level of GPUs fully by leveraging the adequate statistics generated by the GPU simulator. We then conclude five key observations from the workload characterization. We further explore five future GPU architecture designs, which target for the blockchain applications.",ACM Symposium on Applied Computing,2023,10.1145/3555776.3577701,"Yue Chen, Xiurui Pan, Shushu Yi, Jie Zhang",0.0,4
42,TH-AB-BRA-07: PENELOPE-Based GPU-Accelerated Dose Calculation System Applied to MRI-Guided Radiation Therapy.,"PURPOSE\nThe clinical commissioning of IMRT subject to a magnetic field is challenging. The purpose of this work is to develop a GPU-accelerated Monte Carlo dose calculation platform based on PENELOPE and then use the platform to validate a vendor-provided MRIdian head model toward quality assurance of clinical IMRT treatment plans subject to a 0.35 T magnetic field.\n\n\nMETHODS\nWe first translated PENELOPE from FORTRAN to C++ and validated that the translation produced equivalent results. Then we adapted the C++ code to CUDA in a workflow optimized for GPU architecture. We expanded upon the original code to include voxelized transport boosted by Woodcock tracking, faster electron/positron propagation in a magnetic field, and several features that make gPENELOPE highly user-friendly. Moreover, we incorporated the vendor-provided MRIdian head model into the code. We performed a set of experimental measurements on MRIdian to examine the accuracy of both the head model and gPENELOPE, and then applied gPENELOPE toward independent validation of patient doses calculated by MRIdian's KMC.\n\n\nRESULTS\nWe achieve an average acceleration factor of 152 compared to the original single-thread FORTRAN implementation with the original accuracy preserved. For 16 treatment plans including stomach (4), lung (2), liver (3), adrenal gland (2), pancreas (2), spleen (1), mediastinum (1) and breast (1), the MRIdian dose calculation engine agrees with gPENELOPE with a mean gamma passing rate of 99.1% ± 0.6% (2%/2 mm).\n\n\nCONCLUSIONS\nWe developed a Monte Carlo simulation platform based on a GPU-accelerated version of PENELOPE. We validated that both the vendor provided head model and fast Monte Carlo engine used by the MRIdian system are accurate in modeling radiation transport in a patient using 2%/2 mm gamma criteria. Future applications of this platform will include dose validation and accumulation, IMRT optimization, and dosimetry system modeling for next generation MR-IGRT systems.",Medical Physics (Lancaster),2016,10.1118/1.4958058,"Y. Hu, V. Rodriguez, Y. Wang, H. Wooten, H. Li, S. Mutic, T. Zhao, D. Yang, T. Mazur, O. Green",0.0,4
44,Exploration of the Human Purkinje Network in Virtual Populations,"This thesis investigates the Purkinje network (PN) and its dependency on the heart shape (HS) through cardiac simulation on virtual populations (VPs). The heart is a complex organ and essential to the wellbeing of humans; its dysfunction is responsible for more than 27% of all deaths in the UK. The PN delivers the activation impulse to the ventricles of the heart and ensures their synchronous activation. Thus, the morphology of the PN is important, but it varies between species and in vivo imaging is not feasible. However, computer simulation could provide an alternative experimental tool. \nIn simulation of the cardiac electrophysiology, the PN is often replaced by stimulus points on the HS that are ﬁtted to physiological measurements (heart activation times, ECG). Thus, not allowing the study of the PN morphology, nor studies of arrhythmia involving re-entry into the PN. In this thesis, three studies involving explicit models of PNs have been conducted. \nFirst, an eﬃcient algorithm for solving electrophysiology models for the PN is introduced. These allow performing simulations of physiological activations. To minimise the time for simulations, parallelisation with CPU and GPU architectures are investigated, which is of interest for VP studies. \nIn the second study, false tendons (FTs) are studied, which provide an additional connection from the left bundle branch (LBB) and are potentially beneﬁcial in case of LBB block. Therefore, the reduction in activation times by FT is studied as a function of the HS. \nIn the third study, an automatically generated VP is used to explore uncertainty in the PN morphology. The conjecture is that the PN structure adapts to the HS. The coverage of the septum and the minimum distance of the PN to the base are varied. The features of the resulting ECG are used to ﬁnd the PN that gives maximally synchronised contraction.",,2016,10.1007/978-3-319-28712-6_10,M. Lange,0.0,4
47,A study of recent contributions on performance and simulation techniques for accelerator devices,"High performance computing platform is moving from homogeneous individual unites to heterogeneous systems. Where each unit is a combination of homogeneous cores and accelerator devices. Accelerator s uch as GPUs, FPGAs, DSPs, these devices usually designed for the specific and intensive type of computing tasks. The presence of these devices have created fresh and attractive development platforms for developers and designers, brand new performance analysis frameworks and optimization tools. This is the cutting edge in the performance of some accelerator devices like GPUs and Intel's Xeon Phi. We outline some of the existing heterogeneous systems and their development frameworks. The core of this study is a review of performance modeling of these devices. In this paper, we address the emerging issues that affect the performance of these devices and associated techniques employed for simulation and evaluation.",International Conference on E-Learning and E-Technologies in Education,2017,10.1109/ICEEE2.2017.7935829,"Hussein Ajam, Michael Opoku Agyeman",0.0,4
50,Techniques for Managing Irregular Control Flow on GPUs,"GPGPU is a highly multithreaded throughput architecture that can deliver high speed-up for regular applications while remaining energy efficient. In recent years, there has been much focus on tuning irregular applications and/or the GPU architecture to achieve similar benefits for irregular applications as well as efforts to extract data parallelism from task parallel applications. In this work we tackle both problems.The first part of this work tackles the problem of Control divergence in GPUs. GPGPUs’ SIMT execution model is ineffective for workloads with irregular control-flow because GPGPUs serialize the execution of divergent paths which lead to thread-level parallelism (TLP) loss. Previous works focused on creating new warps based on the control path threads follow, or created different warps for the different paths, or ran multiple narrower warps in parallel. While all previous solutions showed speedup for irregular workloads, they imposed some performance loss on regular workloads. In this work we propose a more fine-grained approach to exploit intra-warpconvergence: rather than threads executing the same code path, opcode-convergent threadsexecute the same instruction, but with potentially different operands. Based on this new definition we find that divergent control blocks within a warp exhibit substantial opcode convergence. We build a compiler that analyzes divergent blocks and identifies the common streams of opcodes. We modify the GPU architecture so that these common instructions are executed as convergent instructions. Using software simulation, we achieve a 17% speedup over baseline GPGPU for irregular workloads and do not incur any performance loss on regular workloads.In the second part we suggest techniques for extracting data parallelism from irregular, task parallel applications in order to take advantage of the massive parallelism provided by the GPU. Our technique involves dividing each task into multiple sub-tasks each performing less work and touching a smaller memory footprint. Our framework performs a locality-aware scheduling that works on minimizing the memory footprint of each warp (a set of threads performing in lock-step). We evaluate our framework with 3 task-parallel benchmarks and show that we can achieve significant speedups over optimized GPU code.",,2020,10.25394/PGS.8041376.V1,Jad Hbeika,0.0,4
51,Multi-agent System with Multiple Group Modelling for Bird Flocking on GPU,"Birds flocking is an interesting natural phenomenon to study as proven by numerous papers in this field. In this paper, we present a GPGPU model for birds flocking simulation using NVIDIA's CUDA framework. This technology has been widely adopted in computational science and have dramatically increased computation performances. Using the autonomous agent approach with multi-agents and multiple groups for birds flocking modeling, we present the ACIADDRI model both aggregate motion of a large number of birds in virtual environment and other species or predators avoidance in the plane as well. From these experiments we gained significant performance improvements in the terms of speedup. In conclusion, the work shows that the use of the CUDA technology can be effective to cut computational costs also in multi-agent modeling.","International Euromicro Conference on Parallel, Distributed and Network-Based Processing",2016,10.1109/PDP.2016.112,"D. D'Ambrosio, W. Spataro, Rahmat Hidayat, E. D. Giorgio, D. Spataro",0.0,4
54,X-Aevol: GPU implementation of an evolutionary experimentation simulator,"X-Aevol is the GPU port of the Aevol model, a bio-inspired genetic algorithm designed to study the evolution of micro-organisms and its effects on their genome structure. This model is used for in-silico experimental evolution that requires the computation of populations of thousands of individuals during tens of millions of generations. As the model is extended with new features and experiments are conducted with larger populations, computational time becomes prohibitive. X-Aevol is a response to the need of more computational power. It was designed to leverage the massive parallelization capabilities of GPU. As Aevol exposes an irregular and dynamic computational pattern, it was not a straightforward process to adapt it for massively parallel architectures. In this paper, we present how we have adapted the Aevol underlying algorithms to GPU architectures. We implement our new algorithms with CUDA programming language and test them on a representative benchmark of Aevol workloads. To conclude, we present our performance evaluation on NVIDIA Tesla V100 and A100. We show how we reach a speed-up of 1,000 over a sequential execution on a CPU and the speed-up gain up to 50% from using the newer Ampere micro-architecture in comparison with Volta one.",GECCO Companion,2021,10.1145/3449726.3463195,"T. Gautier, Jonathan Rouzaud-Cornabas, L. Turpin",0.0,4
55,Towards Detailed Real-Time Simulations of Cardiac Arrhythmia,"Recent advances in personalized arrhythmia risk prediction show that computational models can provide not only safer but also more accurate results than invasive procedures. However, biophysically accurate simulations require solving linear systems over fine meshes and time resolutions, which can take hours or even days. This limits the use of such simulations in the clinic where diagnosis and treatment planning can be time sensitive, even if it is just for the reason of operation schedules. Furthermore, the non-interactive, non-intuitive way of accessing simulations and their results makes it hard to study these collaboratively. Overcoming these limitations requires speeding up computations from hours to seconds, which requires a massive increase in computational capabilities.Fortunately, the cost of computing has fallen dramatically in the past decade. A prominent reason for this is the recent introduction of manycore processors such as GPUs, which by now power the majority of the world’s leading supercomputers. These devices owe their success to the fact that they are optimized for massively parallel workloads, such as applying similar ODE kernel computations to millions of mesh elements in scientific computing applications. Unlike CPUs, which are typically optimized for sequential performance, this allows GPU architectures to dedicate more transistors to performing computations, thereby increasing parallel speed and energy efficiency.",2019 Computing in Cardiology (CinC),2019,10.23919/CinC49843.2019.9005849,"Xing Cai, Kristian Gregorius Hustad, H. Arevalo, J. Langguth",0.0,4
57,A comparison of Algebraic Multigrid Bidomain solvers on hybrid CPU-GPU architectures,"The numerical simulation of cardiac electrophysiology is a highly challenging problem in scientific computing. The Bidomain system is the most complete mathematical model of cardiac bioelectrical activity. It consists of an elliptic and a parabolic partial differential equation (PDE), of reaction-diffusion type, describing the spread of electrical excitation in the cardiac tissue. The two PDEs are coupled with a stiff system of ordinary differential equations (ODEs), representing ionic currents through the cardiac membrane. Developing efficient and scalable preconditioners for the linear systems arising from the discretization of such computationally challenging model is crucial in order to reduce the computational costs required by the numerical simulations of cardiac electrophysiology. In this work, focusing on the Bidomain system as a model problem, we have benchmarked two popular implementations of the Algebraic Multigrid (AMG) preconditioner embedded in the PETSc library and we have studied the performance on the calibration of specific parameters. We have conducted our analysis on modern HPC architectures, performing scalability tests on multi-core and multi-GPUs setttings. The results have shown that, for our problem, although scalability is verified on CPUs, GPUs are the optimal choice, since they yield the best performance in terms of solution time.",arXiv.org,2023,10.48550/arXiv.2311.13914,"Simone Scacchi, Edoardo Centofanti",0.0,4
64,LAWS: Large-Scale Accelerated Wave Simulations on FPGAs,"Computing numerical solution to large-scale scientific computing problems described by partial differential equations is a common task in high-performance computing. Improving their performance and efficiency is critical to exa-scale computing. Application-specific hardware design is a well-known solution, but the wide range of kernels makes it infeasible to provision supercomputers with accelerators for all applications. This makes reconfigurable platforms a promising direction. In this work, we focus on wave simulations using discontinuous Galerkin solvers, as an important class of applications. Existing work using FPGAs is limited to accelerating specific kernels or small problems that fit into FPGA BRAM. We present LAWS, a generic and configurable architecture for large-scale accelerated wave simulation problems running on FPGAs out of DRAM. LAWS exploits fine- and coarse-grain parallelism using a scalable array of application-specific cores, and incorporates novel dataflow optimizations, including prefetching, kernel fusion, and memory layout optimizations to minimize data transfers and maximize DRAM bandwidth utilization. We further accompany LAWS with an analytical performance model that allows for scaling across technology trends and architecture configurations. We demonstrate LAWS on the simulation of elastic wave equations. Results show that a single FPGA core achieves 69% higher performance than 24 Xeon cores with 13.27x better energy efficiency, when given 1.94x less peak DRAM bandwidth. Scaling to the same peak DRAM bandwidth shows that an FPGA is 3.27x and 1.5x faster than 24 CPU cores and an Nvidia P100 GPU, with 22.3x and 4.53x better efficiency, respectively.",Symposium on Field Programmable Gate Arrays,2023,10.1145/3543622.3573160,"Dimitrios Gourounas, A. Fathi, Dimitar Trenev, L. John, Bagus Hanindhito, A. Gerstlauer",0.0,4
65,Exploration of GPU Cache Architectures Targeting Machine Learning Applications,"The computation power from graphics processing units (GPUs) has become prevalent in many fields of computer engineering. Massively parallel workloads and large data set capabilities make GPUs an essential asset in tackling today’s computationally intensive problems. One field that benefited greatly with the introduction of GPUs is machine learning. Many applications of machine learning use algorithms that show a significant speedup on a GPU compared to other processors due to the massively parallel nature of the problem set. The existing cache architecture, however, may not be ideal for these applications. The goal of this thesis is to determine if a cache architecture for the GPU can be redesigned to better fit the needs of this increasingly popular field of computer engineering. This work uses a cycle accurate GPU simulator, Multi2Sim, to analyze NVIDIA GPU architectures. The architectures are based on the Kepler series, but the flexibility of the simulator allows for emulation of newer features. Changes have been made to source code to expand on the metrics recorded to further the understanding of the cache architecture. Two suites of benchmarks were used: one for general purpose algorithms and another for machine learning. Running the benchmarks with various cache configurations led to insight into the effects the cache architecture had on each of them. Analysis of the results shows that the cache architecture, while beneficial to the general purpose algorithms, does not need to be as complex for machine learning algorithms. A large contributor to the complexity is the cache coherence protocol used by GPUs. Due to the high spacial locality associated with machine learning problems, the overhead needed by implementing the coherence protocol has little benefit, and simplifying the architecture can lead to smaller, cheaper, and more efficient designs.",,2019,10.1145/3343031.3350545,Gerald Kotas,0.0,4
66,Photon: A Fine-grained Sampled Simulation Methodology for GPU Workloads,"GPUs, due to their massively-parallel computing architectures, provide high performance for data-parallel applications. However, existing GPU simulators are too slow to enable architects to quickly evaluate their hardware designs and software analysis studies. Sampled simulation methodologies are one common way to speed up CPU simulation. However, GPUs apply drastically different execution models that challenge the sampled simulation methods designed for CPU simulations. Recent GPU sampled simulation methodologies do not fully take advantage of the GPU’s special architecture features, such as limited types of basic blocks or warps. Moreover, these methods depend on up-front analysis via profiling tools or functional simulation, making them difficult to use. To address this, we extensively studied the execution patterns of a variety of GPU workloads and propose Photon, a sampled simulation methodology tailored to GPUs. Photon incorporates methodologies that automatically consider different levels of GPU execution, such as kernels, warps, and basic blocks. Photon does not require up-front profiling of GPU workloads and utilizes a light-weight online analysis method based on the identification of highly repetitive software behavior. We evaluate Photon using a variety of GPU workloads, including real-world applications like VGG and ResNet. The final result shows that Photon reduces the simulation time needed to perform one inference of ResNet-152 with batch size 1 from 7.05 days to just 1.7 hours with a low sampling error of 10.7%.",Micro,2023,10.1145/3613424.3623773,"Yifan Sun, Trevor E. Carlson, Changxi Liu",0.0,4
75,Characterizing the Performance Bottlenecks of Irregular GPU Kernels,"Graphics processing units (GPUs) are increasingly being used to accelerate general-purpose applications, including applications with data-dependent, irregular memory access patterns and control flow. However, relatively little is known about the behavior of irregular GPU codes, and there has been minimal effort to quantify the ways in which they differ from regular general-purpose GPU applications. I examine the behavior of a suite of optimized irregular GPU applications written in CUDA on a cycle-level GPU simulator. I characterize the performance bottlenecks in each program and connect source code to microarchitectural performance characteristics. I also assess the performance impact of modifying hardware parameters such as the cache and DRAM bandwidths and latencies, data cache sizes, coalescing behavior, and warp scheduling policy, and I discuss the implications for future GPU architecture design. I find that, while irregular graph codes exhibit significantly more underutilized execution cycles due to branch divergence, load imbalance, and synchronization overhead than regular programs, overall, code optimizations are often able to effectively address these performance hurdles. Insufficient bandwidth, long memory latency, and poor cache effectiveness are the biggest limiters of performance. Applications with irregular memory access patterns are more sensitive to changes in L2 latency and bandwidth than DRAM latency and bandwidth. Additionally, greedy-then-oldest scheduling is the best simple warp scheduler for irregular codes, and two-level scheduling does not significantly improve the performance of such codes.",,2015,10.1109/sspd.2015.7288505,M. A. O'Neil,0.0,4
77,Optimizing Complex Spatially-Variant Coefficient Stencils for Seismic Modeling on GPU,"The Explicit Time Evolution (ETE) method is an innovative Finite-Difference (FD) type method to simulate the wave propagation in acoustic media with higher spatial and temporal accuracy. However, different from FD, it is difficult to achieve an efficient GPU design because of the poor memory access patterns caused by the off-axis points and spatially-variant coefficients. In this paper, we present a set of new optimization strategies for ETE stencils according to the memory hierarchy of NVIDIA GPU. To handle the problem caused by the complexity of the stencil shapes, we design a one-to-multi updating scheme for shared memory usage. To alleviate the performance damage resulted from the poor memory access pattern of reading spatially-variant coefficients, we propose a stencil decomposition method to reduce un-coalesced global memory access. Based on the state-of-the-art GPU architecture, combining with existing spatial and temporal stencil blocking schemes, we manage to achieve 9.6x and 9.9x speedups compared with a well-tuned 12-core CPUs version for 37-point and 73-point ETE stencils, respectively. Compared with a well-tuned MIC version, the best speedups for the 2 type stencils are 3.7x and 4.7x. Our designs leads to an ETE method that is 31.2x faster than conventional CPU-FD method and make it a practical seismic imaging technology.",International Conference on Parallel and Distributed Systems,2015,10.1109/ICPADS.2015.86,"L. Gan, Guangwen Yang, H. Fu, N. Dai, Wei Wu, He Zhang, Jiarui Fang",0.0,4
79,Performance evaluation using GEM 5-GPU simulator,With introduction of heterogeneous computing for developing and manufacturing next generation of computer it is necessary to have a simulator which would evaluate and simulate designed heterogeneous system showing how it would behave in actual application. The heterogeneous system refers to a system which uses more than one type of processor or cores. For the purpose of analyzing heterogeneous systems (CPU-GPU) behavior Gem5-gpu simulator was developed. The Gem5-gpu simulator was build combining two different simulators Gem5 simulator and GPGPU-sim simulator. The GPGPU-sim is used for simulating GPUs while Gem5 simulator is used for modelling CPUs. The Gem5-gpu combines best characteristics of both simulators.,International Conference Computing Methodologies and Communication,2017,10.1109/ICCMC.2017.8282713,"Sagar Namdev Sawal, Nitesh B. Guinde",0.0,4
91,Suitability of Shallow Water Solving Methods for GPU Acceleration,"In the past 15 years the field of general purpose computing on graphics processing units, or GPUs, has become well developed and the practice is becoming more mainstream. Computational demands of simulation software are continuously increasing. As such for many applications traditionally computed on the central processing unit the question arises of whether moving to GPU computing is a possible cost effective way of meeting these demands. The fundamental nature of GPU architecture that makes it so cost effective at doing bulk computation also poses restrictions on which applications are suitable for it. The shallow water equations are a simplified form of the Navier-Stokes equations and describe water levels and flow currents in suitably shallow water such as rivers, estuaries and the North sea. The main research goal of this thesis project was to determine whether the shallow water equations are suitable for implementation on a GPU. Two options exist, the equations may be solved with either an explicit or implicit time integration method. First, a literature study was conducted to familiarize with the tools required to build explicit and implicit shallow water models on a GPU. Then both an explicit and implicit shallow water solver were developed first in the MATLAB programming language and later in CUDA C++ on both CPU and GPU. The main findings are that both explicit and implicit methods are well suited for GPU implementation. Both methods proved to be compatible with a wetting and drying mechanism of numerical cells. The Cuda C++ implementation was in the order of 10 times as fast as a MATLAB implementation for both CPU and GPU. For the benchmark cases tested, the Cuda C++ GPU implementation was in the order of 50 times faster than the equivalent multithreaded CPU implementation. The implicit implementation was benchmarked using the conjugate gradient method to solve the linear system. Various preconditioners were tested and a Repeated Red Black preconditioner was found to be the most effective. The computation time of the RRB preconditioned implicit method was compared with the explicit method and it was found that the two methods reached parity in computation time when the implicit time step was taken roughly 50 times as large as the explicit time step. For implicit time steps smaller than that the explicit method was faster and when the implicit time step was larger the implicit method was faster. For the benchmark cases tested, the implicit method using a time step 50 times larger than the explicit method was found to be less accurate and less stable than the explicit method. The conclusion is that for cases similar to the benchmark cases an explicit method is the fastest, most stable and most accurate method and thus the preferred choice.",,2020,10.21125/edulearn.2020.1231,Floris Buwalda,0.0,4
97,UISS-GPU: Accelerated In-Silico Tuberculosis Vaccine Trials Using FLAME GPU,"The Universal Immune System Simulator (UISS) is a computational framework based on agent-based modelling (ABM) paradigm that has been specifically developed for simulating the immune system behaviour in presence of diseases and treatments. It has a long history of development, ranging from its initial applications into the field of tumor immunology and then moving towards wide disease modelling scenarios such as influenza, Multiple Sclerosis and atherosclerosis. Recently, inside the STriTuVaD H2020 EU project, it has been specialized to simulate tuberculosis dynamics and its interaction with the immune system, including the efficacy of the combined action of various treatments such as isoniazid and novel vaccines. TB simulation entitles large scale (e.g., tissue to organ scale) simulations over a wide digital population cohort. The computational costs of running large scale simulations are prohibitive using traditional forms of CPU simulation. This paper considers the use of parallel to gpu-based computing approaches via an agent-based domain independent complex systems simulator, FLAME GPU. Integration of FLAME GPU with UISS enables the simulation of larger, more complex problem domains. The combined UISS-FLAMEGPU simulator provides vastly increased performance characteristics for large problems, with a speedup of 4.22x for a typical tuberculosis model simulating 128 microlitres. FLAME GPU abstracts away a significant portion of the normal programming that would be required to effectively parallelise a model of this complexity. Adaptations were made to increase performance, such as message mutation and parallelisation of certain algorithms.",IEEE International Conference on Bioinformatics and Biomedicine,2022,10.1109/BIBM55620.2022.9995159,"P. Richmond, G. Russo, M. Pennisi, F. Pappalardo, M. Leach, Peter Heywood",0.0,4
99,Chrono DEM-Engine: A Discrete Element Method dual-GPU simulator with customizable contact forces and element shape,"This paper introduces DEM-Engine, a new submodule of Project Chrono, that is designed to carry out Discrete Element Method (DEM) simulations. Based on spherical primitive shapes, DEM-Engine can simulate polydisperse granular materials and handle complex shapes generated as assemblies of primitives, referred to as clumps. DEM-Engine has a multi-tier parallelized structure that is optimized to operate simultaneously on two GPUs. The code uses custom-defined data types to reduce memory footprint and increase bandwidth. A novel""delayed contact detection""algorithm allows the decoupling of the contact detection and force computation, thus splitting the workload into two asynchronous GPU streams. DEM-Engine uses just-in-time compilation to support user-defined contact force models. This paper discusses its C++ and Python interfaces and presents a variety of numerical tests, in which impact forces, complex-shaped particle flows, and a custom force model are validated considering well-known benchmark cases. Additionally, the full potential of the simulator is demonstrated for the investigation of extraterrestrial rover mobility on granular terrain. The chosen case study demonstrates that large-scale co-simulations (comprising 11 million elements) spanning 15 seconds, in conjunction with an external multi-body dynamics system, can be efficiently executed within a day. Lastly, a performance test suggests that DEM-Engine displays linear scaling up to 150 million elements on two NVIDIA A100 GPUs.",arXiv.org,2023,10.48550/arXiv.2311.04648,"D. Negrut, B. Tagliafierro, Shlok Sabarwal, R. Serban, Ruochun Zhang, Yulong Yue, Luning Bakke, Xin Wei, Colin Vanden Heuvel",0.0,4
101,Optimizations for energy efficiency in GPGPU architectures,"Author(s): Sankaranarayanan, Alamelu | Advisor(s): Renau, Jose; Briz, Jose L | Abstract: It is commonplace for graphics processing units or GPUs today to render extremely complex 3D scenes and textures, in real time, both in the traditional and mobile computing spaces. The computational power required to do this makes them a valuable resource to exploit for general purpose computation. In order to map programs originally designed for sequential CPUs onto massively parallel GPU architectures, it would be necessary to justify the transition with huge performance benefits. Over the last couple of years, there have been numerous proposals to improve the performance of GPUs used for general purpose computing (GPGPUs), but without much consideration for energy efficiency. In my dissertation, I evaluate the feasibility of GPGPUs from an energy perspective and propose some optimizations based on the unique programming model used by GPGPUs. First, I describe the simulation infrastructure, one of the few available to model GPGPUs today, both individually and as part of a heterogeneous system. Next, I propose a design using a shared translation lookaside buffer (TLB) to eliminate chronic memory copies between the CPU and GPU addressing spaces, making heterogeneous CPU-GPU designs energy efficient. Furthermore, to improve the energy efficiency of the on-chip memory hierarchy, I propose adding tiny incoherent caches per processing element, which can filter out frequent accesses to large shared and energy-inefficient cache structures. Finally, I evaluate a design which moves away from the underlying SIMD architecture of GPUs towards a more MIMD-like architecture, enabling the execution of both CPU and GPGPU workloads without negatively affecting the energy efficiency availed by traditional workloads on GPGPUs.",,2016,10.15740/has/tajh/11.1/176-179,Alamelu Sankaranarayanan,0.0,4
102,Parallel Simulation of PDP Systems: Updates and Roadmap,"PDP systems are a type of multienvironment P systems, which serve as a formal modeling framework for Population Dynamics. The accurate simulation of these probabilistic models entails large run times. Hence, parallel platforms such as GPUs has been employed to speedup the simulation. In 2012 [14], the first GPU simulator of PDP systems was presented. In this paper, we present current updates made on this simulator, and future developments to consider.",,2015,10.1007/978-3-319-28475-0_12,"M. J. P. Jiménez, L. F. Ramos, M. A. Amor",0.0,4
105,Programming Perspectives for Pre-exascale Systems,"Reservoir simulation of large scale projects is becoming increasingly complex, requiring more than simple black oil models and vertical well models to capture the behaviour of unconventional, fractured and highly heterogeneous production zones. Nvidia provides an array of accelerated linear algebra libraries to deal with the equations that must be solved in these situations. Accelerating sparse linear algebra on the latest GPU architectures has real potential for performance gains of hundreds of percent over carefully tuned multi-core CPU-only implementations, but at what cost in complexity? This talk will address the programming approaches needed to utilize GPUs at scale for today’s most challenging problems, and give a glimpse of the path forward to pre-exascale applications.",International Conference on High Performance Computing,2015,10.3997/2214-4609.201414023,"F. Courteille, J. Eaton",0.0,4
111,KPU-SQL: Kernel Processing Unit for High-Performance SQL Acceleration,"Application-specific accelerator is a prominent way for analytic query processing. To achieve a substantial improvement over the state-of-the-art in performance while maintaining programmability, we propose a kernel processing unit (KPU) framework and apply it to SQL acceleration. Kernel customization and data transmission are two critical bottlenecks, we separately optimize them in the key core and shadow core with a self-designed data management system. A software stack named RACE with a performance model and function simulator is also introduced. The experiments demonstrate that KPU-SQL outperforms the CPU and GPU by 24.5x and 8.75x on average, respectively.",ACM Great Lakes Symposium on VLSI,2023,10.1145/3583781.3590268,"Wenyan Lu, Guihai Yan, Hao Kong, Liyun Cheng, Jingya Wu, Haishuang Fan, Xiaowei Li, Yan Chen",0.0,4
113,A Low-Latency Communication Design for Brain Simulations,"Brain simulation, as one of the latest advances in artificial intelligence, facilitates better understanding about how information is represented and processed in the brain. The extreme complexity of the human brain makes brain simulations only feasible on high-performance computing platforms. Supercomputers with a large number of interconnected graphical processing units (GPUs) are currently employed for supporting brain simulations. Therefore, high-throughput low-latency inter-GPU communications in super-computers play a crucial role in meeting the performance requirements of brain simulation as a highly time-sensitive application. In this article, we first provide an overview of the current parallelizing technologies for brain simulations using multi-GPU architectures. Then we analyze the challenges to communications for brain simulation and summarize guidelines for communication design to address such challenges. Furthermore, we propose a partitioning algorithm and a two-level routing method to achieve efficient low-latency communications in multi-GPU architecture for brain simulation. We report experiment results obtained on a supercomputer with 2000 GPUs for simulating a brain model with 10 billion neurons (digital twin brain, DTB) to show that our approach can significantly improve communication performance. We also discuss open issues and identify some research directions for low-latency communication design for brain simulations.",IEEE Network,2022,10.48550/arXiv.2205.07125,Xin Du,0.0,4
115,Performance Portable Solid Mechanics via Matrix-Free p-Multigrid,"—Finite element analysis of solid mechanics is a foundational tool of modern engineering, with low-order ﬁnite element methods and assembled sparse matrices representing the industry standard for implicit analysis. We use performance models and numerical experiments to demonstrate that high- order methods greatly reduce the costs to reach engineering tolerances while enabling effective use of GPUs. We demonstrate the reliability, efﬁciency, and scalability of matrix-free p -multigrid methods with algebraic multigrid coarse solvers through large deformation hyperelastic simulations of multiscale structures. We investigate accuracy, cost, and execution time on multi-node CPU and GPU systems for moderate to large models using AMD MI250X (OLCF Crusher), NVIDIA A100 (NERSC Perlmutter), and V100 (LLNL Lassen and OLCF Summit), resulting in order of magnitude efﬁciency improvements over a broad range of model properties and scales. We discuss efﬁcient matrix-free representation of Jacobians and demonstrate how automatic differentiation enables rapid development of nonlinear material models without impacting debuggability and workﬂows targeting GPUs.",arXiv.org,2022,10.48550/arXiv.2204.01722,"W. Moses, J. Thompson, M. Knepley, Rezgar Shakeri, Jed Brown, Leila Ghaffari, Junchao Zhang, V. Barra, Karen Stengel, Natalie N. Beams",0.0,4
118,Modelling and simulation of GPU processing in the MERPSYS environment,"In this work, we evaluate an analytical GPU performance model based on Little's law, that expresses the kernel execution time in terms of latency bound, throughput bound, and achieved occupancy.We then combine it with the results of several research papers, introduce equations for data transfer time estimation, and finally incorporate it into the MERPSYS framework, which is a general-purpose simulator for parallel and distributed systems.The resulting solution enables the user to express a CUDA application in a MERPSYS editor using an extended Java language and then conveniently evaluate its performance for various launch configurations using different hardware units.We also provide a systematic methodology for extracting kernel characteristics, that are used as input parameters of the model.The model was evaluated using kernels representing different traits and for a large variety of launch configurations.We found it to be very accurate for computation bound kernels and realistic workloads, whilst for memory throughput bound kernels and uncommon scenarios the results were still within acceptable limits.We have also proven its portability between two devices of the same hardware architecture but different processing power.Consequently, MERPSYS with the theoretical models embedded in it can be used for evaluationof application performance on various GPUs and used for performance prediction and e.g. purchase decision making.",Scalable Computing : Practice and Experience,2018,10.12694/scpe.v19i4.1439,"P. Czarnul, Tomasz Gajger",0.0,4
121,Predictive Analysis of Large-Scale Coupled CFD Simulations with the CPX Mini-App,"As the complexity of multi-physics simulations increases, there is a need for efficient flow of information between components. Discrete ‘coupler’ codes can abstract away this process, improving solver interoperability. One such multi-physics problem is modelling the high pressure compressor of turbofan engines, where instances of rotor/stator CFD simulations are coupled. Configuring couplers and allocating resources correctly can be challenging for such problems due to the sliding interfaces between codes. In this research, we present CPX, a mini-coupler designed to model the performance behaviour of a production coupler framework at Rolls-Royce plc., used for coupling rotor/stator simulations. CPX, the first mini-coupler framework of its kind, is combined with a CFD mini-app to predict the run-time and scaling behaviour of large scale coupled CFD simulations. We demonstrate high qualitative and quantitative predictive accuracy with a less than 17 % mean error. A performance model is developed to predict the ‘optimum’ configuration of resources, and is tested to show the high accuracy of these predictions. The model is also used to project the ‘optimum’ configuration for a 6 Billion cell test case, a problem size representative of current leading-edge production workloads, on a 100,000 core cluster and a 400 GPU cluster. Further testing reveals that the ‘optimum’ configuration is unstable if not set up correctly, and therefore a trade-off needs to be made with a marginally less-than-optimal setup to ensure stability. The work illustrates the significant utility of CPX to carry out such rapid design space and run-time setup exploration studies to obtain the best performance from production CFD coupled simulations.",International Conference on High Performance Computing,2021,10.1109/HiPC53243.2021.00028,"I. Reguly, K. Choudry, S. Jarvis, Arvind Prabhakar, D. Amirante, A. Powell, G. Mudalige",0.0,4
126,Energy efficiency and performance modeling of stencil applications on manycore and GPU computing resources,"One of the most critical application areas for many operational High-Performance Computing systems worldwide is a numerical weather prediction. Such complex predictions and corresponding computing models have to solve a large number of Partial Differential Equations using stencil computations on structured grids within tight production schedules. A stencil kernel within simulations is often the most demanding computing part and naturally may impact the energy consumption of the whole HPC system. In this paper, we introduce new functional extensions developed for DCworms and GSSIM simulators to predict the energy efficiency of extreme-scale stencil applications on heterogeneous computing resources, in particular consisting of a large number of many-core and GPUs. New energy-efficiency metrics for modeling of heterogeneous computing resources have been successfully added to the simulators thanks to their pluggable and extensible architectures. Our recent improvements related to application-specific performance models in simulators help users to take into account many relevant application parameters, in particular, those related to detailed energy consumption on heterogeneous HPC resources. We show in this paper how to extract stencil application-specific parameters based on real experiments. Moreover, we demonstrate how those parameters can be applied for modeling and simulation experiments to evaluate the overall performance and energy consumption of stencil computations on manycore CPUs and GPUs. Finally, we discuss new DCworms simulator capabilities and demonstrate added-values of performance analysis tools.","IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing",2020,10.1109/CCGrid49817.2020.00-70,"M. Ciznicki, J. Weglarz, K. Kurowski",0.0,4
131,Cache Memory Access Patterns in the GPU Architecture,"Data exchange between a Central Processing Unit (CPU) and a Graphic Processing Unit (GPU) can be very expensive in terms of performance. The characterization of data and cache memory access patterns differ between a CPU and a GPU. The motivation of this research is to analyze the cache memory access patterns of GPU architectures and to potentially improve data exchange between a CPU and GPU. The methodology of this work uses Multi2Sim GPU simulator for AMD Radeon and NVIDIA Kepler GPU architectures. This simulator, used to emulate the GPU architecture in software, enables certain code modifications for the L1 and L2 cache memory blocks. Multi2Sim was configured to run multiple benchmarks to analyze and record how the benchmarks access GPU cache memory. The recorded results were used to study three main metrics: (1) Most Recently Used (MRU) and Least Recently Used (LRU) accesses for L1 and L2 caches, (2) Inter-warp and Intra-warp cache memory accesses in the GPU architecture for different sets of workloads, and (3) To record and compare the GPU cache access patterns for certain machine learning benchmarks with its general purpose counterparts.",,2018,10.1109/igcc.2018.8752137,Yash Nimkar,0.0,4
132,ANT-MOC: Scalable Neutral Particle Transport Using 3D Method of Characteristics on Multi-GPU Systems,"The Method Of Characteristic (MOC) to solve the Neutron Transport Equation (NTE) is the core of full-core simulation for reactors. High resolution is enabled by discretizing the NTE through massive tracks to traverse the 3D reactor geometry. However, the 3D full-core simulation is prohibitively expensive because of the high memory consumption and the severe load imbalance. To deal with these challenges, we develop ANT-MOC1. Specifically, we build a performance model for memory footprint, computation and communication, based on which a track management strategy is proposed to overcome the resolution bottlenecks caused by limited GPU memory. Furthermore, we implement a novel multi-level load mapping strategy to ensure load balancing among nodes, GPUs, and CUs. ANT-MOC enables a 3D full-core reactor simulation with 100 billion tracks on 16,000 GPUs, with 70.69% and 89.38% parallel efficiency for strong scalability and weak scalability, respectively.",International Conference on Software Composition,2023,10.1145/3581784.3607063,"X. Chi, Zhikuang Xin, Shigang Li, Yangang Wang, Shunde Li, Jue Wang, Zongguo Wang, Yangde Feng, Lingkun Bu, Peng Shi, Yun Hu",0.0,4
133,CPU-GPU Heterogeneous Code Acceleration of a Finite Volume Computational Fluid Dynamics Solver,"This work deals with the CPU-GPU heterogeneous code acceleration of a finite-volume CFD solver utilizing multiple CPUs and GPUs at the same time. First, a high-level description of the CFD solver called SENSEI, the discretization of SENSEI, and the CPU-GPU heterogeneous computing workflow in SENSEI leveraging MPI and OpenACC are given. Then, a performance model for CPU-GPU heterogeneous computing requiring ghost cell exchange is proposed to help estimate the performance of the heterogeneous implementation. The scaling performance of the CPU-GPU heterogeneous computing and its comparison with the pure multi-CPU/GPU performance for a supersonic inlet test case is presented to display the advantages of leveraging the computational power of both the CPU and the GPU. Using CPUs and GPUs as workers together, the performance can be improved further compared to using pure CPUs or GPUs, and the advantages can be fairly estimated by the performance model proposed in this work. Finally, conclusions are drawn to provide 1) suggestions for application users who have an interest to leverage the computational power of the CPU and GPU to accelerate their own scientific computing simulations and 2) feedback for hardware architects who have an interest to design a better CPU-GPU heterogeneous system for heterogeneous computing.",arXiv.org,2023,10.48550/arXiv.2305.18057,"Hongyu Wang, Weicheng Xue, Christopher J. Roy",0.0,4
134,Exploiting Nested Parallelism on Heterogeneous Processors,"Title of Thesis: EXPLOTING NESTED PARALLELISM ON HETEROGENEOUS PROCESSORS Michael Jeffrey Zuzak, Master of Science, 2016 Thesis Directed By: Associate Professor and Director of Computer Engineering Education, Doctor Donald Yeung, Department of Electrical and Computer Engineering Heterogeneous computing systems have become common in modern processor architectures. These systems, such as those released by AMD, Intel, and Nvidia, include both CPU and GPU cores on a single die available with reduced communication overhead compared to their discrete predecessors. Currently, discrete CPU/GPU systems are limited, requiring larger, regular, highly-parallel workloads to overcome the communication costs of the system. Without the traditional communication delay assumed between GPUs and CPUs, we believe non-traditional workloads could be targeted for GPU execution. Specifically, this thesis focuses on the execution model of nested parallel workloads on heterogeneous systems. We have designed a simulation flow which utilizes widely used CPU and GPU simulators to model heterogeneous computing architectures. We then applied this simulator to nontraditional GPU workloads using different execution models. We also have proposed a new execution model for nested parallelism allowing users to exploit these heterogeneous systems to reduce execution time. EXPLOITING NESTED PARALLELISM IN HETEROGENEOUS COMPUTING SYSTEMS",,2016,10.13016/M28B6V,Michael Zuzak,0.0,4
136,Parallel Q-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation,"Reinforcement learning is time-consuming for complex tasks due to the need for large amounts of training data. Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works have used on-policy methods like PPO due to their simplicity and easy-to-scale nature. Off-policy meth-ods are more sample-efficient, but challenging to scale, resulting in a longer wall-clock training time. This paper presents a novel Parallel Q -Learning (PQL) scheme that outperforms PPO in terms of wall-clock time and maintains superior sample efficiency. The driving force lies in the parallelization of data collection, policy function learning, and value function learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate the capability of scaling up Q -learning methods to tens of thousands of parallel environments and investigate important factors that can affect learning speed, including the number of parallel environments, exploration strategies, batch size, GPU models, etc. The code is available at https://github.com/Improbable-AI/pql.",International Conference on Machine Learning,2023,10.48550/arXiv.2307.12983,"Pulkit Agrawal, Tao Chen, Zechu Li, Zhang-Wei Hong, Anurag Ajay",0.0,4
153,SCALABLE INTEGRATED CIRCUIT SIMULATION ALGORITHMS FOR ENERGY-EFFICIENT TERAFLOP HETEROGENEOUS PARALLEL COMPUTING PLATFORMS,"Integrated circuit technology has gone through several decades of aggressive scaling. It is increasingly challenging to analyze growing design complexity. Post-layout SPICE simulation can be computationally prohibitive due to the huge amount of parasitic elements, which can easily boost the computation and memory cost. As the decrease in device size, the circuits become more vulnerable to process variations. Designers need to statistically simulate the probability that a circuit does not meet the performance metric, which requires millions times of simulations to capture rare failure events. Recent, multiprocessors with heterogeneous architecture have emerged as mainstream computing platforms. The heterogeneous computing platform can achieve highthroughput energy efficient computing. However, the application of such platform is not trivial and needs to reinvent existing algorithms to fully utilize the computing resources. This dissertation presents several new algorithms to address those aforementioned two significant and challenging issues on the heterogeneous platform. Harmonic Balance (HB) analysis is essential for efficient verification of large postlayout RF and microwave integrated circuits (ICs). However, existing methods either suffer from excessively long simulation time and prohibitively large memory consumption or exhibit poor stability. This dissertation introduces a novel transient-simulation guided graph sparsification technique, as well as an efficient runtime performance modeling approach tailored for heterogeneous manycore CPU-GPU computing system to build nearly-optimal subgraph preconditioners that can lead to minimum HB simulation runtime. Additionally, we propose a novel heterogeneous parallel sparse block matrix algorithm by taking advantages of the structure of HB Jacobian matrices as well as GPU’s streaming multiprocessors to achieve optimal workload balancing",,2016,10.37099/mtu.dc.etdr/86,Lengfei Han,0.0,4
162,Understanding the Future of Energy Efficiency in Multi-Module GPUs,"As Moore’s law slows down, GPUs must pivot towards multi-module designs to continue scaling performance at historical rates. Prior work on multi-module GPUs has focused on performance, while largely ignoring the issue of energy efficiency. In this work, we propose a new metric for GPU efficiency called EDP Scaling Efficiency that quantifies the effects of both strong performance scaling and overall energy efficiency in these designs. To enable this analysis, we develop a novel top-down GPU energy estimation framework that is accurate within 10% of a recent GPU design. Being decoupled from granular GPU microarchitectural details, the framework is appropriate for energy efficiency studies in future GPUs. Using this model in conjunction with performance simulation, we show that the dominating factor influencing the energy efficiency of GPUs over the next decade is GPUmodule (GPM) idle time. Furthermore, neither inter-module interconnect energy, nor GPM microarchitectural design is expected to play a key role in this regard. We demonstrate that multi-module GPUs are on a trajectory to become 2⇥ less energy efficient than current monolithic designs; a significant issue for data centers which are already energy constrained. Finally, we show that architects must be willing to spend more (not less) energy to enable higher bandwidth inter-GPM connections, because counter-intuitively, this additional energy expenditure can reduce total GPU energy consumption by as much as 45%, providing a path to energy efficient strong scaling in the future.",International Symposium on High-Performance Computer Architecture,2019,10.1109/HPCA.2019.00063,"Evgeny Bolotin, D. Nellans, A. Arunkumar, Carole-Jean Wu",4.0,3
177,Blind Identification of Thermal Models and Power Sources From Thermal Measurements,"The ability to sense the temperatures and power consumption of various key components of a chip is central to the operation of modern integrated circuits, such as processors. While modern chips often include a number of embedded thermal sensors, they lack the ability to sense power at fine granularity. This paper proposes a new direction to simultaneously identify the thermal models and the fine-grain power consumption of a chip from just the measurements of the thermal sensors and the total power consumption. Our identification technique is blind as it does not require design knowledge of the thermal-power model to identify the power sources. We investigate the main challenges in blind identification, which are the permutation and scaling ambiguities, and propose novel techniques to resolve these ambiguities. We implement our technique and apply it in three contexts. First, we implement it within a controlled simulation environment, which enables us to verify its accuracy and analyze its sensitivity to relevant issues, such as measurement noise and number of available training samples. Second, we apply it on a real multi-core CPU + GPU processor-based system, where we show the ability to identify the runtime power consumption of the individual cores using just the total power measurement and the measurements of the embedded thermal sensors under different workloads. Third, we apply it for non-invasive power sensing of chips by inverting the temperatures measured using an external infrared imaging camera. We show that our technique consistently improves the modeling and sensing accuracy of integrated circuits.",IEEE Sensors Journal,2018,10.1109/JSEN.2017.2774704,"S. Reda, K. Dev, A. Belouchrani",2.4285709999999998,3
172,Graphics-Processing-Unit-Based Acceleration of Electromagnetic Transients Simulation,"This paper presents a novel approach to speed up electromagnetic-transients (EMT) simulation, using graphics-processing-unit (GPU)-based computing. This paper extends earlier published works in the area, by exploiting additional parallelism inside EMT simulation. A 2D-parallel matrix-vector multiplication is used that is faster than previous 1D-methods. Also, this paper implements a GPU-specific sparsity technique to further speed up the simulations, as the available CPU-based sparsity techniques are not suitable for GPUs. In addition, as an extension to previous works, this paper demonstrates modelling a power-electronic subsystem. The efficacy of the approach is demonstrated using two different scalable test systems. A low granularity system, that is, one with a large cluster of buses connected to others with a few transmission lines is considered, as is also a high granularity where a small cluster of buses is connected to other clusters, thereby requiring more interconnecting transmission lines. Computation times for GPU-based computing are compared with the computation times for sequential implementations on the CPU. This paper shows two surprising differences of GPU simulation in comparison with CPU simulation. First, the inclusion of sparsity only makes minor reductions in the GPU-based simulation time. Second, excessive granularity, even though it appears to increase the number of parallel-computable subsystems, significantly slows down the GPU-based simulation.",IEEE Transactions on Power Delivery,2016,10.1109/TPWRD.2015.2492983,"W. Fung, A. Gole, J. Debnath",2.3333330000000001,3
175,Machine Learning for Performance and Power Modeling of Heterogeneous Systems,"Modern processing systems with heterogeneous components (e.g., CPUs, GPUs) have numerous configuration and design options such as the number and types of cores, frequency, and memory bandwidth. Hardware architects must perform design space explorations in order to accurately target markets of interest under tight time-to-market constraints. This need highlights the importance of rapid performance and power estimation mechanisms. This work describes the use of machine learning (ML) techniques within a methodology for the estimating performance and power of heterogeneous systems. In particular, we measure the power and performance of a large collection of test applications running on real hardware across numerous hardware configurations. We use these measurements to train a ML model; the model learns how the applications scale with the system's key design parameters. Later, new applications of interest are executed on a single configuration, and we gather hardware performance counter values which describe how the application used the hardware. These values are fed into our ML model's inference algorithm, which quickly identify how this application will scale across various design points. In this way, we can rapidly predict the performance and power of the new application across a wide range of system configurations. Once the initial run of the program is complete, our ML algorithm can predict the application's performance and power at many hardware points faster than running it at each of those points and with a level of accuracy comparable to cycle-level simulators.",2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),2018,10.1145/3240765.3243484,"J. Greathouse, G. Loh",1.8571430000000000,3
167,Granular layEr Simulator: Design and Multi-GPU Simulation of the Cerebellar Granular Layer,"In modern computational modeling, neuroscientists need to reproduce long-lasting activity of large-scale networks, where neurons are described by highly complex mathematical models. These aspects strongly increase the computational load of the simulations, which can be efficiently performed by exploiting parallel systems to reduce the processing times. Graphics Processing Unit (GPU) devices meet this need providing on desktop High Performance Computing. In this work, authors describe a novel Granular layEr Simulator development implemented on a multi-GPU system capable of reconstructing the cerebellar granular layer in a 3D space and reproducing its neuronal activity. The reconstruction is characterized by a high level of novelty and realism considering axonal/dendritic field geometries, oriented in the 3D space, and following convergence/divergence rates provided in literature. Neurons are modeled using Hodgkin and Huxley representations. The network is validated by reproducing typical behaviors which are well-documented in the literature, such as the center-surround organization. The reconstruction of a network, whose volume is 600 × 150 × 1,200 μm3 with 432,000 granules, 972 Golgi cells, 32,399 glomeruli, and 4,051 mossy fibers, takes 235 s on an Intel i9 processor. The 10 s activity reproduction takes only 4.34 and 3.37 h exploiting a single and multi-GPU desktop system (with one or two NVIDIA RTX 2080 GPU, respectively). Moreover, the code takes only 3.52 and 2.44 h if run on one or two NVIDIA V100 GPU, respectively. The relevant speedups reached (up to ~38× in the single-GPU version, and ~55× in the multi-GPU) clearly demonstrate that the GPU technology is highly suitable for realistic large network simulations.",Frontiers in Computational Neuroscience,2021,10.3389/fncom.2021.630795,"F. Leporati, Stefano Masoli, Giordana Florimbi, E. D’Angelo, E. Torti",1.5,3
173,Online Power Estimation of Graphics Processing Units,"Accurate power estimation at runtime is essential for the efficient functioning of a power management system. While years of research have yielded accurate power models for the online prediction of instantaneous power for CPUs, such power models for graphics processing units (GPUs) are lacking. GPUs rely on low-resolution power meters that only nominally support basic power management. To address this, we propose an instantaneous power model, and in turn, a power estimator, that uses performance counters in a novel way so as to deliver accurate power estimation at runtime. Our power estimator runs on two real NVIDIA GPUs to show that accurate runtime estimation is possible without the need for the high-fidelity details that are assumed on simulation-based power models. To construct our power model, we first use correlation analysis to identify a concise set of performance counters that work well despite GPU device limitations. Next, we explore several statistical regression techniques and identify the best one. Then, to improve the prediction accuracy, we propose a novel application-dependent modeling technique, where the model is constructed online at runtime, based on the readings from a low-resolution, built-in GPU power meter. Our quantitative results show that a multi-linear model, which produces a mean absolute error of 6%, works the best in practice. An application-specific quadratic model reduces the error to nearly 1%. We show that this model can be constructed with low overhead and high accuracy at runtime. To the best of our knowledge, this is the first work attempting to model the instantaneous power of a real GPU system, earlier related work focused on average power.","IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing",2016,10.1109/CCGrid.2016.93,"Vignesh Adhinarayanan, Balaji Subramaniam, Wu-chun Feng",1.2222219999999999,3
156,Parallel High-Fidelity Electromagnetic Transient Simulation of Large-Scale Multi-Terminal DC Grids,"Electromagnetic transient (EMT) simulation of power electronics conducted on the CPU slows down as the system scales up. Thus, the massively parallelism of the graphics processing unit (GPU) is utilized to expedite the simulation of the multi-terminal DC (MTDC) grid, where detailed models of the semiconductor switches are adopted to provide comprehensive device-level information. As the large number of nodes leads to an inefficient solution of the DC grid, three levels of circuit partitioning are applied, i.e., the transmission line-based natural separation of converter stations, splitting of the apparatus inside the station, and the coupled voltage-current sources for fine-grained partitioning. Components of similar attributes are written as one CUDA C function and computed in massive parallelism by means of single-instruction multi-threading. The GPU’s potential as a new EMT simulation platform for the analysis of large-scale MTDC grids is demonstrated by a remarkable speedup of up to 270 times for the Greater CIGRÉ DC grid with time-steps of 50 ns and  $1~\mu \text{s}$  for device-level and system-level simulation over the CPU implementation. Finally, the accuracy of GPU simulation is validated by the commercial tools SaberRD and PSCAD/EMTDC.",IEEE Power and Energy Technology Systems Journal,2019,10.1109/JPETS.2018.2881483,"Ning Lin, V. Dinavahi",1.1666669999999999,3
157,Know Your Enemy To Save Cloud Energy: Energy-Performance Characterization of Machine Learning Serving,"The proportion of machine learning (ML) inference in modern cloud workloads is rapidly increasing, and graphic processing units (GPUs) are the most preferred computational accelerators for it. The massively parallel computing capability of GPUs is well-suited to the inference workloads but consumes more power than conventional CPUs. Therefore, GPU servers contribute significantly to the total power consumption of a data center. However, despite their heavy power consumption, GPU power management in cloud-scale has not yet been actively researched. In this paper, we reveal three findings about energy efficiency of ML inference clusters in the cloud. ❶ GPUs of different architectures have comparative advantages in energy efficiency to each other for a set of ML models. ❷ The energy efficiency of a GPU set may significantly vary depending on the number of active GPUs and their clock frequencies even when producing the same level of throughput. ❸ The service level objective(SLO)-blind dynamic voltage and frequency scaling (DVFS) driver of commercial GPUs maintain an immoderately high clock frequency. Based on these implications, we propose a hierarchical GPU resource management approach for cloud-scale inference services. The proposed approach consists of energy-aware cluster allocation, intra-cluster node scaling, intra-node GPU scaling and GPU clock scaling schemes considering the inference service architecture hierarchy. We evaluated our approach with its prototype implementation and cloud-scale simulation. The evaluation with real-world traces showed that the proposed schemes can save up to 28.3% of the cloud-scale energy consumption when serving five ML models with 105 servers having three different kinds of GPUs.",International Symposium on High-Performance Computer Architecture,2023,10.1109/HPCA56546.2023.10070943,"Junyeol Yu, Jongseok Kim, Euiseong Seo",1.0,3
158,Runtime Construction of Large-Scale Spiking Neuronal Network Models on GPU Devices,"Simulation speed matters for neuroscientific research: this includes not only how quickly the simulated model time of a large-scale spiking neuronal network progresses but also how long it takes to instantiate the network model in computer memory. On the hardware side, acceleration via highly parallel GPUs is being increasingly utilized. On the software side, code generation approaches ensure highly optimized code at the expense of repeated code regeneration and recompilation after modifications to the network model. Aiming for a greater flexibility with respect to iterative model changes, here we propose a new method for creating network connections interactively, dynamically, and directly in GPU memory through a set of commonly used high-level connection rules. We validate the simulation performance with both consumer and data center GPUs on two neuroscientifically relevant models: a cortical microcircuit of about 77,000 leaky-integrate-and-fire neuron models and 300 million static synapses, and a two-population network recurrently connected using a variety of connection rules. With our proposed ad hoc network instantiation, both network construction and simulation times are comparable or shorter than those obtained with other state-of-the-art simulation technologies while still meeting the flexibility demands of explorative network modeling.",Applied Sciences,2023,10.3390/app13179598,"B. Golosio, Johanna Senk, E. Pastorelli, A. Morrison, V. Fanti, P. Paolucci, Gianmarco Tiddia, Jose Villamar, J. Stapmanns",1.0,3
171,Getting the Ball Rolling: Learning a Dexterous Policy for a Biomimetic Tendon-Driven Hand with Rolling Contact Joints,"Biomimetic, dexterous robotic hands have the potential to replicate much of the tasks that a human can do, and to achieve status as a general manipulation platform. Recent advances in reinforcement learning (RL) frameworks have achieved remarkable performance in quadrupedal locomotion and dexterous manipulation tasks. Combined with GPU-based highly parallelized simulations capable of simulating thousands of robots in parallel, RL-based controllers have become more scalable and approachable. However, in order to bring RL-trained policies to the real world, we require training frameworks that output policies that can work with physical actuators and sensors as well as a hardware platform that can be manufactured with accessible materials yet is robust enough to run interactive policies. This work introduces the biomimetic tendon-driven Faive Hand and its system architecture, which uses tendon-driven rolling contact joints to achieve a 3D printable, robust high-DoF hand design. We model each element of the hand and integrate it into a GPU simulation environment to train a policy with RL, and achieve zero-shot transfer of a dexterous in-hand sphere rotation skill to the physical robot hand.11https://srl-ethz.github.io/get-ball-rolling/video:https://youtu.be/YahsMhqNU8o",IEEE-RAS International Conference on Humanoid Robots,2023,10.1109/Humanoids57100.2023.10375231,"U. Steger, Barnabas Gavin Cangan, Manuel Knecht, Stefan Weirich, B. Forrai, Robert K. Katzschmann, Yasunori Toshimitsu",1.0,3
178,Efficient Performance Estimation and Work-Group Size Pruning for OpenCL Kernels on GPUs,"Graphic Processing Units (GPUs) play a vital role in state-of-the-art high-performance scientific computing realm and research work towards its performance analysis is crucial but nontrivial. Extant GPU performance models are far from practical use, while fine-grained GPU simulation requires a considerably large time cost. Moreover, massive amounts of designs with various program inputs and parameter settings pose a challenge for efficient performance estimation and tuning of parallel GPU applications. To this end, this article presents a hybrid framework for the efficient performance estimation and work-group size pruning of OpenCL workloads on GPUs. The framework contains a static module used to extract the kernel execution trace from the high-level source code and a dynamical module used to mimic the kernel execution flow to estimate the runtime performance. For the design space pruning, an extra analysis is performed to filter out the redundant work-group sizes with duplicated execution traces and inferior pipelines. The proposed framework does not require any program runs to estimate the performance and find the optimal or near-optimal designs. Experiments on four Commercial Off-The-Shelf (COTS) Nvidia GPUs show that the framework can predict the runtime performance with an average error of 17.04 percent and reduce the program design space by an average of 78.47 percent.",IEEE Transactions on Parallel and Distributed Systems,2020,10.1109/TPDS.2019.2958343,"Kai Huang, Xuehai Qian, A. Knoll, Xiebing Wang",0.6,3
168,Fast GPU simulations of the FRAP experiment,"In this thesis we study several models of the Fluorescence Recovery After Photobleaching (FRAP) experiment. FRAP is a technique used to estimate the diﬀusion coefficient of ﬂuids based on Confocal Laser Scanning Microscopy (CLSM). A ﬂuorescent sample is ﬁrst photobleached on a user deﬁned region. Then, by studying the recovery of ﬂuorescence in the bleaching region, one can retrieve important parameters of the ﬂuid such as the diffusion coefficient and binding constants by ﬁtting a model to the data. We implemented and compared three models of the FRAP experiment. The ﬁrst model assumes bleaching and image acquisition is an instantaneous process. The second model, based on the ﬁrst one, introduces multiple bleach frames. The ﬁnal model takes into account the scanning movement of the CLSM and is computationally much more complex. For the instantaneous models, two schemes are introduced and compared against each other to ensure correct implementation of the algorithms. The ﬁrst scheme uses the spectral method to solve the diﬀusion-reaction equations and the second uses a stochastic formulation of the problem. The last model, due to its complexity, has only been implemented stochasticaly. All three models have been implemented on Graphical Processing Units (GPUs) using the OpenCL API in C++. The GPU has a massively parallel architecture that can be exploited for scientiﬁc computing. These schemes are ”embarrassingly parallel” and thus suitable for a GPU implementation. By comparing the diﬀerent models, we see that a good compromise between precision and computing resource is given by the instantaneous bleaching with multiple bleach frames model. Because of the scanning nature of the CLSM, we would expect the last model to reveal some asymmetry in the results. These were only found for extreme and unrealistic parameters and it is thus not necessary to simulate the FRAP experiment with such complexity.",,2018,10.23919/eusipco.2018.8553615,Leander Lacroix,0.5714290000000000,3
179,Synchronous speculative simulation of tightly coupled agents in continuous time on CPUs and GPUs,"Traditionally, parallel discrete-event simulations of agent-based models in continuous time are organized around logical processes exchanging time-stamped events, which clashes with the properties of models in which tightly coupled agents frequently and instantaneously access each other’s states. To illustrate the challenges of such models and to derive a solution, we consider the domain-specific modeling language ML3, which allows modelers to succinctly express transitions and interactions of linked agents based on a continuous-time Markov chain (CTMC) semantics. We propose synchronous optimistic synchronization algorithms tailored toward simulations of fine-grained interactions among tightly coupled agents in highly dynamic topologies and present implementations targeting multicore central processing units (CPUs) as well as many-core graphics processing units (GPUs). By dynamically restricting the temporal progress per round to ensure that at most one transition or state access per agent, the synchronization algorithms enable efficient direct agent interaction and limit the required agent state history to only a single current and projected state. To maintain concurrency given actions that depend on dynamically updated macro-level properties, we introduce a simple relaxation scheme with guaranteed error bounds. Using an extended variant of the classical susceptible-infected-recovered network model, we benchmark and profile the performance of the different algorithms running on CPUs and on a data center GPU.",International Conference on Advances in System Simulation,2023,10.1177/00375497231158930,"Philipp Andelfinger, A. Uhrmacher",0.5,3
159,ALUPower: Data Dependent Power Consumption in GPUs,"Existing architectural power models for GPUs count activities such as executing floating point or integer instructions, but do not consider the data values processed. While data value dependent power consumption can often be neglected when performing architectural simulations of high performance Out-of-Order (OoO) CPUs, we show that this approach is invalid for estimating the power consumption of GPUs. The throughput processing approach of GPUs reduces the amount of control logic and shifts the area and power budget towards functional units and register files. This makes accurate estimations of the power consumption of functional units even more crucial than in OoO CPUs. Using measurements from actual GPUs, we show that the processed data values influence the energy consumption of GPUs significantly. For example, the power consumption of one kernel varies between 155 and 257 Watt depending on the processed values. Existing architectural simulators are not able to model the influence of the data values on power consumption. RTL and gate level simulators usually consider data values in their power estimates but require detailed modeling of the employed units and are extremely slow. We first describe how the power consumption of GPU functional units can be measured and characterized using microbenchmarks. Then measurement results are presented and several opportunities for energy reduction by software developers or compilers are described. Finally, we demonstrate a simple and fast power macro model to estimate the power consumption of functional units and provide a significant improvement in accuracy compared to previously used constant energy per instruction models.","IEEE/ACM International Symposium on Modeling, Analysis, and Simulation On Computer and Telecommunication Systems",2016,10.1109/MASCOTS.2016.21,"J. Lucas, B. Juurlink",0.4444440000000000,3
163,"Special Issue on Recent Trends and Future of Fog and Edge Computing, Services and Enabling Technologies Editorial File","Recent Trends and Future of Fog and Edge Computing, Services, and Enabling Technologies \nCloud computing has been established as the most popular as well as suitable computing infrastructure providing on-demand, scalable and pay-as-you-go computing resources and services for the state-of-the-art ICT applications which generate a massive amount of data. Though Cloud is certainly the most fitting solution for most of the applications with respect to processing capability and storage, it may not be so for the real-time applications. The main problem with Cloud is the latency as the Cloud data centres typically are very far from the data sources as well as the data consumers. This latency is ok with the application domains such as enterprise or web applications, but not for the modern Internet of Things (IoT)-based pervasive and ubiquitous application domains such as autonomous vehicle, smart and pervasive healthcare, real-time traffic monitoring, unmanned aerial vehicles, smart building, smart city, smart manufacturing, cognitive IoT, and so on. The prerequisite for these types of application is that the latency between the data generation and consumption should be minimal. For that, the generated data need to be processed locally, instead of sending to the Cloud. This approach is known as Edge computing where the data processing is done at the network edge in the edge devices such as set-top boxes, access points, routers, switches, base stations etc. which are typically located at the edge of the network. These devices are increasingly being incorporated with significant computing and storage capacity to cater to the need for local Big Data processing. The enabling of Edge computing can be attributed to the Emerging network technologies, such as 4G and cognitive radios, high-speed wireless networks, and energy-efficient sophisticated sensors. \nDifferent Edge computing architectures are proposed (e.g., Fog computing, mobile edge computing (MEC), cloudlets, etc.). All of these enable the IoT and sensor data to be processed closer to the data sources. But, among them, Fog computing, a Cisco initiative, has attracted the most attention of people from both academia and corporate and has been emerged as a new computing-infrastructural paradigm in recent years. Though Fog computing has been proposed as a different computing architecture than Cloud, it is not meant to replace the Cloud. Rather, Fog computing extends the Cloud services to network edges for providing computation, networking, and storage services between end devices and data centres. Ideally, Fog nodes (edge devices) are supposed to pre-process the data, serve the need of the associated applications preliminarily, and forward the data to the Cloud if the data are needed to be stored and analysed further. \nFog computing enhances the benefits from smart devices operational not only in network perimeter but also under cloud servers. Fog-enabled services can be deployed anywhere in the network, and with these services provisioning and management, huge potential can be visualized to enhance intelligence within computing networks to realize context-awareness, high response time, and network traffic offloading. Several possibilities of Fog computing are already established. For example, sustainable smart cities, smart grid, smart logistics, environment monitoring, video surveillance, etc. \nTo design and implementation of Fog computing systems, various challenges concerning system design and implementation, computing and communication, system architecture and integration, application-based implementations, fault tolerance, designing efficient algorithms and protocols, availability and reliability, security and privacy, energy-efficiency and sustainability, etc. are needed to be addressed. Also, to make Fog compatible with Cloud several factors such as Fog and Cloud system integration, service collaboration between Fog and Cloud, workload balance between Fog and Cloud, and so on need to be taken care of. \nIt is our great privilege to present before you Volume 20, Issue 2 of the Scalable Computing: Practice and Experience. We had received 20 Research Papers and out of which 14 Papers are selected for Publication. The aim of this special issue is to highlight Recent Trends and Future of Fog and Edge Computing, Services and Enabling technologies. The special issue will present new dimensions of research to researchers and industry professionals with regard to Fog Computing, Cloud Computing and Edge Computing. \nSujata Dash et al. contributed a paper titled “Edge and Fog Computing in Healthcare- A Review” in which an in-depth review of fog and mist computing in the area of health care informatics is analysed, classified and discussed. The review presented in this paper is primarily focussed on three main aspects: The requirements of IoT based healthcare model and the description of services provided by fog computing to address then. The architecture of an IoT based health care system embedding fog computing layer and implementation of fog computing layer services along with performance and advantages. In addition to this, the researchers have highlighted the trade-off when allocating computational task to the level of network and also elaborated various challenges and security issues of fog and edge computing related to healthcare applications. \nParminder Singh et al. in the paper titled “Triangulation Resource Provisioning for Web Applications in Cloud Computing: A Profit-Aware” proposed a novel triangulation resource provisioning (TRP) technique with a profit-aware surplus VM selection policy to ensure fair resource utilization in hourly billing cycle while giving the quality of service to end-users. The proposed technique use time series workload forecasting, CPU utilization and response time in the analysis phase. The proposed technique is tested using CloudSim simulator and R language is used to implement prediction model on ClarkNet weblog. The proposed approach is compared with two baseline approaches i.e. Cost-aware (LRM) and (ARMA). The response time, CPU utilization and predicted request are applied in the analysis and planning phase for scaling decisions. The profit-aware surplus VM selection policy used in the execution phase for select the appropriate VM for scale-down. The result shows that the proposed model for web applications provides fair utilization of resources with minimum cost, thus provides maximum profit to application provider and QoE to the end users. \n  \nAkshi kumar and Abhilasha Sharma in the paper titled “Ontology driven Social Big Data Analytics for Fog enabled Sentic-Social Governance” utilized a semantic knowledge model for investigating public opinion towards adaption of fog enabled services for governance and comprehending the significance of two s-components (sentic and social) in aforesaid structure that specifically visualize fog enabled Sentic-Social Governance. The results using conventional TF-IDF (Term Frequency-Inverse Document Frequency) feature extraction are empirically compared with ontology driven TF-IDF feature extraction to find the best opinion mining model with optimal accuracy. The results concluded that implementation of ontology driven opinion mining for feature extraction in polarity classification outperforms the traditional TF-IDF method validated over baseline supervised learning algorithms with an average of 7.3% improvement in accuracy and approximately 38% reduction in features has been reported. \n  \n  \nAvinash Kaur and Pooja Gupta in the paper titled “Hybrid Balanced Task Clustering Algorithm for Scientific workflows in Cloud Computing” proposed novel hybrid balanced task clustering algorithm using the parameter of impact factor of workflows along with the structure of workflow and using this technique, tasks can be considered for clustering either vertically or horizontally based on value of impact factor. The testing of the algorithm proposed is done on Workflowsim- an extension of CloudSim and DAG model of workflow was executed. The Algorithm was tested on variables- Execution time of workflow and Performance Gain and compared with four clustering methods: Horizontal Runtime Balancing (HRB), Horizontal Clustering (HC), Horizontal Distance Balancing (HDB) and Horizontal Impact Factor Balancing (HIFB) and results stated that proposed algorithm is almost 5-10% better in makespan time of workflow depending on the workflow used. \nPijush Kanti Dutta Pramanik et al. in the paper titled “Green and Sustainable High-Performance Computing with Smartphone Crowd Computing: Benefits, Enablers and Challenges” presented a comprehensive statistical survey of the various commercial CPUs, GPUs, SoCs for smartphones confirming the capability of the SCC as an alternative to HPC. An exhaustive survey is presented on the present and optimistic future of the continuous improvement and research on different aspects of smartphone battery and other alternative power sources which will allow users to use their smartphones for SCC without worrying about the battery running out. \nDhanapal and P. Nithyanandam in the paper titled “The Slow HTTP Distributed Denial of Service (DDOS) Attack Detection in Cloud” proposed a novel method to detect slow HTTP DDoS attacks in cloud to overcome the issue of consuming all available server resources and making it unavailable to the real users. The proposed method is implemented using OpenStack cloud platform with slowHTTPTest tool. The results stated that proposed technique detects the attack in efficient manner. \nMandeep Kaur and Rajni Mohana in the paper titled “Static Load Balancing Technique for Geographically partitioned Public Cloud” proposed a novel approach focused upon load balancing in the partitioned public cloud by combining centralized and decentralized approaches, assuming the presence of fog layer. A load balancer entity is used for decentralized load balancing at partitions and a co",Scalable Computing : Practice and Experience,2019,10.12694/SCPE.V20I2.1558,"P. K. Pramanik, A. Nayyar, Rudra Rameshwar",0.3333330000000000,3
169,Mobile AP GPU power distribution network simulation and analysis based on chip power model,"These days, mobile devices require low-power consumption. To meet these requirements, IC's operating voltage is continuously lowered. As a result, power noise margin decreases which require more precise Power Distribution Network (PDN) design. In this work, we simulated and analyzed the mobile Application Processor (AP) GPU system based on Chip Power Model (CPM). We applied GPU's current model to simulate simultaneous switching noise and power noise in the chip PDN. To verify the model and simulation set-up, we measured voltage ripple and compared with simulation. We concluded that our simulation setup is reliable and conducted power integrity case studies for the future PDN design.",2016 IEEE International Symposium on Electromagnetic Compatibility (EMC),2016,10.1109/ISEMC.2016.7571668,"Taisik Yang, Heegon Kim, Jonghyun Cho, Youngwoo Kim, Joungho Kim, Yun Ra, Kibum Kang, Woohyun Paik",0.3333330000000000,3
160,Surrogate Modelling for Efficient Discovery of Emergent Population Dynamics,"Outcomes of simulating complex systems models, such as emergent properties and desirable system level behaviours, can be discovered via heuristic techniques such as Genetic Algorithms (GAs). Using simulation as the cost function evaluation for a GA (i.e. simulation guided search) is computationally expensive. Additionally the GA search process may require many generations before high quality solutions can be discovered. As such, simulation guided search can be considered high latency with respect to discovery of a range of high quality solutions. In this paper we experimentally demonstrate that the time to discovery of high quality solutions can be reduced through a low latency, hybrid GA search using a machine learning surrogate model trained to approximate simulation via large amounts of batched parallel simulation data generated in a HPC environment. Using a common population dynamics model optimised for GPU simulation by the FLAME GPU framework, we directly compare the hybrid approach with simulation guided search to understand the relationship between computational cost and quality of prediction. Our results indicate that given equivalent levels of simulation investment, results of equivalent quality can be obtained. The hybrid approach is however able to reduce the latency of the GA search process by shifting the computational cost of simulation to a highly parallel pre-search step used to train surrogate models.",International Symposium on High Performance Computing Systems and Applications,2019,10.1109/HPCS48598.2019.9188208,"M. Chimeh, P. Richmond, James Pyle",0.16666700000000001,3
161,Accelerating Large Scale Artificial Society Simulation with CPU/GPU Based Heterogeneous Parallel Method,"Artificial society is an effective way for social science research. However, in order to meet real-time and super real-time requirement of computational experiment, the execution efficiency of large-scale artificial society then becomes the burning question. The emergence of heterogeneous parallel system offers opportunities and challenges for accelerating large scale artificial society simulation. How to fully utilize heterogeneous computational resources in large scale agent based simulation becomes the key issue. The paper proposes a CPU/GPU-based accelerating computational method, in which GPU is fully utilized in two different ways at the same time. Firstly GPU is treated as host processor, and a GPU based simulation kernel is designed to execute the models collaboratively with CPU simulation kernel. Secondly, in order to accelerate the domain-specific models, a specific domain-oriented GPU simulation computational service component is proposed, and GPU is used as a co-processor to offer domain-specific parallel optimization. A SPMT (Single Process Multi Threads) based conservative parallel simulation framework is proposed to integrate the GPU simulation kernel and computational service component. At last, an experiment is designed to test the efficiency of GPU based simulation kernel, and argues about the application mode of GPU.",IEEE International Symposium on Distributed Simulation and Real-Time Applications,2015,10.1109/DS-RT.2015.11,"Chen Bin, Qiu Xiaogang, Guo Gang, Li Zhen",0.1,3
164,FAcET: Fast and accurate power/energy estimation tool for CPU-GPU platforms at architectural-level,"This paper proposes a novel fast and accurate architectural-level tool to estimate power and energy (FAcET) for heterogeneous (CPU-GPU) system architecture based platforms. FAcET consists of two components. The first is a set of generic parametrizable power models generated by characterizing the functional-level activities for different blocks of the chosen platforms. The second is a simulation-based architectural-level prototype that uses SystemC (JIT) simulators to accurately evaluate the parameters of the corresponding power models of the first component. The combination of the two components leads to a novel power and energy estimation methodology at the architectural level that provides a better balance between speed and accuracy. The efficacy of the FAcET tool is verified against measurements taken on real board platforms, which consist of low-power ARM quad-core processors (Cortex-A7, -A9 and -A15), NVIDIA GPUs (Quadro 1000M, Quadro FX5600, Tegra K1, and GTX480) and heterogeneous platforms (NVIDIA Tegra3 and NVIDIA Jetson TK1). Power and energy estimation results obtained with FAcET deviate in less than 3.6% for quad-core processors, 6.5% for GPU, 10% for heterogeneous multiprocessor based systems from the measurements and estimation is 15x faster than state-of-the-art tools.",ACM Symposium on Cloud Computing,2015,10.1109/SOCC.2015.7406947,"A. Cristal, J. Moreno, Oscar Palomar, S. Rethinagiri, O. Unsal",0.0,3
165,Accelerated GPU simulation of the gaseous detonation cell structure,The aim of the present paper is to report on our recent results for GPU accelerated simulations of the gaseous detonation structure. Reactive Euler equations with a one-step Arrhenius chemistry model have been used for numerical simulation. And the NND space discretization scheme combined with Steger-Warming split method has been used. For time discretization we have applied the explicit third order Runge-Kutta method. We have obtained a speedup of 8 times ( in comparison to 30 threads openmp program) for the gaseous detonation simulation on a structure grid of 320 million points.,Journal of Physics: Conference Series,2021,10.1088/1742-6596/2012/1/012079,"Rui Q. Yang, Chun Wang",0.0,3
166,Decentralized Training of Foundation Models in Heterogeneous Environments,"Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational""tasklets""in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8X faster than prior state-of-the-art training systems (Megatron).",Neural Information Processing Systems,2022,10.48550/arXiv.2206.01288,"Yongjun He, Tianyi Zhang, Beidi Chen, Percy Liang, Tri Dao, Jared Davis, Ce Zhang, Christopher Ré, Binhang Yuan",0.0,3
170,Fundamentals of a numerical cloud computing for applied sciences,"This paper is supposed to be used as a contribution for the `Consultation on Cloud Computing Research Innovation Challenges for WP 2018-2020 ́ as called for by the European Commission (DG CONNECT, unit `Cloud and software ́). We propose to encourage and support fundamental interdisciplinary research for making the benefits generated by cloud computing accessible to the applied science community. Introduction: Why cloud computing and high performance computing are contradicting The basic idea of cloud computing (CC) is to abstract from an IT infrastructure including compute-, memory-, networkingand software resources by virtualization. These resources are made accessible to the user in a dynamic and adaptive way. The major resulting advantages compared to a specially tailored `in-house solution ́ can be found in a transparent and simple usage, enhanced flexibility due to scalability and adaptivity to a specific need and finally in the increased efficiency due to savings in energy and money spent. The latter is due to scaling effects, operational efficiency, consolidation of resources and reduction of risks. The application is literally independent from any (local) data and compute resources as these can be concentrated effectively. All together, these advantages may some day supersede the traditional local / regional data center approach which can be found on the level of modern universities and research centers. From the point of view of data center management and operations, CC leads to a higher occupancy and therefore efficiency: The inevitable granularity effects that occur with medium or large workloads can be tackled with a backfilling of many small jobs. In addition, due to the fact that a specific application run's need for resources may vary from time to time, left-over capacities can be provided in a profitable `pay per use ́ style. In High Performance Computing (HPC) on the other hand, virtualization and abstraction concepts contradict the usual approaches especially in the simulation of technical processes: Here, the focus is put on enhancing the performance of an application by explicitly optimize for a certain type of hardware. This requires an a priori knowledge of the hardware which usually is given by the fact, that universities and regional research facilities have their own local or regional compute centers with comparatively static hardware components. This point of view can in some cases generate several orders of magnitude of performance gains and we call this concept hardware-oriented numerics. This paradigm comprises the simultaneous optimization for hardware, numerical and energy efficiency on all levels of application development [1,2,3,4]. One effort in hardware-oriented numerics is to optimize code and develop or choose numerical methods with respect to a heterogeneous hardware ecosystem: Multicore CPUs are as straight-forward as hardware accelerators like GPUs, FPGAs, Xeon Phi processors and system on a chip designs such as ARM-based CPUs with integrated GPUs. In addition, there are non-uniform memory architectures on the device level as well as heterogeneous communication infrastructures on the cluster level. The usual design pattern however is to optimize code for a (single) given hardware configuration where the simulation code is then optimized in a comparatively expensive way due to this proximity to hardware details. This development process is therefore the complete opposite of relying on a virtualization approach. Today's scientific cloud computing is not feasible for numerical simulation Up to today all efforts to make use of CC techniques in the science community can be characterized by what we call scientific cloud computing (SCC), which basically has been very successful for a specific type of application: In the scope of Big Data often a direct projection of a problem to a bag of tasks programming model can be found. Also other problems that are constituted by smaller independent tasks, where the coupling and therefore communication is minimal or zero can be coped with easily in a cloud environment. In numerical simulation on the other hand a strong coupling of the very computationally intense subproblems is the standard case. This induces a comparatively high synchronization need, requiring low communication latencies. The execution models of CC are literally blind for this type of strong coupling because the virtualization shuts down any attempt to optimize inter process communication. We believe, that the development of numerical simulation software should be characterized by the synthesis of hardware, numerical, and energy efficiency. Hence for this type of application a CC concept which takes into account the heterogeneity of compute hardware would be most feasible: According to our vision in future scenarios the user of such codes might want to choose for run time optimization in different metrics: Flexibility in the selection in which way a specific run should be allocated to a certain type(s) of compute node(s) are required. This flexibility has not been accounted for in the development of numerical code frameworks yet. A direct result of the service providers internalizing the concept of hardware-oriented numerics would be that the user of the service would be able to make an a priori choice for the core requirements for the run. For instance it could be decided whether an allocation of hardware should be made in order to minimize wall clock time or minimize energy to solution. Other hardware specifics could be made allocateable such as the type and properties of the communication links between nodes. The service would then return a number of allocations based upon available hardware. After selection, a complex optimization problem then has to be solved: The simulation software has to be able to select numerical algorithmic components that fit to this allocation and finally, a load balancing has to be performed for the individual problem to be solved. Towards a numerical cloud computing In order to realize this vision, there are two fundamental problems to solve: (1) Specially tailored numerics as well as load balancing strategies as well as (2) mapping, scheduling and operation strategies for numerical simulation have to be developed. In (1) numerical components in a code framework have to be revisited or developed from scratch with respect to (2) by adjusting them to the respective strategies. Such numerical alternatives range from preconditioners in linear solvers to whole discretization approaches on the model level. Different hardware specific implementations have to be provided and tuned in order to enable the optimizer in (2) to succeed, which is closely related to performance engineering. This has to be undergone with respect to all levels of parallelism in modern hardware architectures and on all levels of an application. On the other hand, the systems / strategies developed in (2) have to be sensitive for the effects of specific numerics on specific hardware. This problem is often closely related to numerical scaling, convergence and complexity theories. These theories and related skills are usually not addressed as an integral part of the training in computer science or service providers / operators. Here an automatic tuning system has to be developed that is capable of deciding what type of numerics is to be used for a given hardware allocation and which parts of the data are distributed to which part of the hardware by a static or even dynamic load balancing. The latter is an even more complex problem keeping in mind the heterogeneity even within one specific allocation, where CPUs are for instance to be saturated alongside GPUs. This optimization problem is very similar to how compilers schedule instructions on the processor level. It is also multi-dimensional, as not only raw performance has to be optimized for but also energy to solution as stated in the previous section. Hence we emphasize that these two components, (1) and (2), cannot be brought up independently: Specialists from the domain of applied mathematics, performance engineers and application specialists are required for the former, whereas the latter is to be coped with by computer sciences and service providers / specialists.",,2017,10.17877/DE290R-17825,"M. Geveler, S. Turek",0.0,3
174,GPU-Parallelized Simulation of Optical Forces on Nanoparticles in a Fluid Medium,"Experimental research in physics can be a costly and time-consuming venture, requiring simulation-based approaches to effectively narrow down the scope of experiments to only the most promising cases. Our multidisciplinary research in this paper demonstrates how the simulation of light-driven nanoparticles can substantially benefit from GPU-based parallelism. We develop a novel ray-tracing strategy and we implement it in C++/CUDA and extend it with a parallel differential equation solver. Our implementation relies on a custom memory layout optimization to tackle the computational challenges in the field and provide accurate solutions in near real time. We evaluate our approach on a variety of popular GPU architectures, including advanced data-center GPUs like the Nvidia V100, as well as consumer-grade hardware like the Nvidia RTX 2080 Ti and Nvidia GTX 1080. Our GPU-based approach achieves a speedup of up to $20\times$ compared to a parallel CPU-based prototype implementation.","IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",2023,10.1109/IPDPSW59300.2023.00114,"Florian Fey, Alexander Gerwing, S. Gorlatch",0.0,3
176,Power modeling and architectural techniques for energy-efficient GPUs,"Graphics Processing Units (GPUs) have evolved from fixed function graphics processors to programmable general-purpose compute accelerators in a short time. The high theoretical performance and energy efficiency of GPUs compared to CPUs have made them indispensable for mainstream computing. However, their high power consumption and limited energy efficiency under low utilization is a challenge that still needs to be tackled. This thesis investigates bottlenecks that cause low performance and low energy efficiency in GPUs and proposes architectural techniques to address them. To conduct energy efficiency research for GPUs, we first develop a flexible and accurate power simulator called GPUSimPow. We use a hybrid approach for power modeling that improves flexibility and accuracy compared to previous approaches. Our evaluation shows an average relative error of 11.7% and 10.8% between simulated and measured power for the NVIDIA GT240 and GTX580, respectively. We then use GPUSimPow to study the energy efficiency of a wide range of kernels and categorize them into high performance and low performance. We further investigate the bottlenecks of low-performance kernels by analyzing their occupancy. We quantify the gain in performance and energy efficiency when occupancy is increased. For instance, the average increase in instructions per cycle, the average reduction in energy consumption and energy-delay-product is 11%, 9%, and 23%, respectively, when occupancy is increased for a sub-category of low occupancy kernels. The full occupancy kernels have low performance despite having the maximum number of threads. Further investigation shows that several of these kernels are memory-bound and can gain significantly from an increase in memory bandwidth. The traditional ways of increasing memory bandwidth by widening interfaces and increasing frequency have issues such as high power consumption, large form factor, and difficulty in the scaling of pin count. Memory compression is a promising alternative to increase the effective memory bandwidth of GPUs, however, we find that the existing memory compression techniques for GPUs exploit simple patterns for compression and trade low compression ratios for low decompression latency. Based on the evidence that GPUs are less sensitive to latency than CPUs, we propose the more complex Entropy Encoding Based Memory Compression (E2MC) technique for GPUs. On average, E2MC delivers 53% higher compression ratio and 8% higher speedup than the state of the art. E2MC reduces energy consumption and energy-delay-product by 13% and 27%, respectively. While designing E2MC, we observe that lossless memory compression techniques including E2MC often have a low effective compression ratio due to the large memory access granularity (MAG) exhibited by GPUs. Our study of the distribution of compressed blocks reveals that a significant percentage of compressed blocks have only a few",,2019,10.14279/DEPOSITONCE-9156,S. Lal,0.0,3
523,DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,"We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever ""stale""), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. \n\nThis massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially ""solves"" the task -- near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ""ImageNet pre-training + task-specific fine-tuning"" for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",International Conference on Learning Representations,2019,10.1007/s11263-019-01228-7,"Ari S. Morcos, Devi Parikh, Stefan Lee, Erik Wijmans, M. Savva, Irfan Essa, Dhruv Batra, Abhishek Kadian",507.6666670000000,2
566,New ways to boost molecular dynamics simulations,"We describe a set of algorithms that allow to simulate dihydrofolate reductase (DHFR, a common benchmark) with the AMBER all‐atom force field at 160 nanoseconds/day on a single Intel Core i7 5960X CPU (no graphics processing unit (GPU), 23,786 atoms, particle mesh Ewald (PME), 8.0 Å cutoff, correct atom masses, reproducible trajectory, CPU with 3.6 GHz, no turbo boost, 8 AVX registers). The new features include a mixed multiple time‐step algorithm (reaching 5 fs), a tuned version of LINCS to constrain bond angles, the fusion of pair list creation and force calculation, pressure coupling with a “densostat,” and exploitation of new CPU instruction sets like AVX2. The impact of Intel's new transactional memory, atomic instructions, and sloppy pair lists is also analyzed. The algorithms map well to GPUs and can automatically handle most Protein Data Bank (PDB) files including ligands. An implementation is available as part of the YASARA molecular modeling and simulation program from www.YASARA.org. © 2015 The Authors Journal of Computational Chemistry Published by Wiley Periodicals, Inc.",Journal of Computational Chemistry,2015,10.1002/jcc.23899,"E. Krieger, G. Vriend",78.1,2
220,Assessing the Current State of Amber Force Field Modifications for DNA,"The utility of molecular dynamics (MD) simulations to model biomolecular structure, dynamics, and interactions has witnessed enormous advances in recent years due to the availability of optimized MD software and access to significant computational power, including GPU multicore computing engines and other specialized hardware. This has led researchers to routinely extend conformational sampling times to the microsecond level and beyond. The extended sampling time has allowed the community not only to converge conformational ensembles through complete sampling but also to discover deficiencies and overcome problems with the force fields. Accuracy of the force fields is a key component, along with sampling, toward being able to generate accurate and stable structures of biopolymers. The Amber force field for nucleic acids has been used extensively since the 1990s, and multiple artifacts have been discovered, corrected, and reassessed by different research groups. We present a direct comparison of two of the most recent and state-of-the-art Amber force field modifications, bsc1 and OL15, that focus on accurate modeling of double-stranded DNA. After extensive MD simulations with five test cases and two different water models, we conclude that both modifications are a remarkable improvement over the previous bsc0 force field. Both force field modifications show better agreement when compared to experimental structures. To ensure convergence, the Drew–Dickerson dodecamer (DDD) system was simulated using 100 independent MD simulations, each extended to at least 10 μs, and the independent MD simulations were concatenated into a single 1 ms long trajectory for each combination of force field and water model. This is significantly beyond the time scale needed to converge the conformational ensemble of the internal portions of a DNA helix absent internal base pair opening. Considering all of the simulations discussed in the current work, the MD simulations performed to assess and validate the current force fields and water models aggregate over 14 ms of simulation time. The results suggest that both the bsc1 and OL15 force fields render average structures that deviate significantly less than 1 Å from the average experimental structures. This can be compared to similar but less exhaustive simulations with the CHARMM 36 force field that aggregate to the ∼90 μs time scale and also perform well but do not produce structures as close to the DDD NMR average structures (with root-mean-square deviations of 1.3 Å) as the newer Amber force fields. On the basis of these analyses, any future research involving double-stranded DNA simulations using the Amber force fields should employ the bsc1 or OL15 modification.",Journal of Chemical Theory and Computation,2016,10.1021/acs.jctc.6b00186,"P. Jurečka, M. Otyepka, T. Cheatham, J. Šponer, R. Galindo-Murillo, Marie Zgarbová, James C Robertson",45.888888999999999,2
892,GPUMD: A package for constructing accurate machine-learned potentials and performing highly efficient atomistic simulations.,"We present our latest advancements of machine-learned potentials (MLPs) based on the neuroevolution potential (NEP) framework introduced in Fan et al. [Phys. Rev. B 104, 104309 (2021)] and their implementation in the open-source package gpumd. We increase the accuracy of NEP models both by improving the radial functions in the atomic-environment descriptor using a linear combination of Chebyshev basis functions and by extending the angular descriptor with some four-body and five-body contributions as in the atomic cluster expansion approach. We also detail our efficient implementation of the NEP approach in graphics processing units as well as our workflow for the construction of NEP models and demonstrate their application in large-scale atomistic simulations. By comparing to state-of-the-art MLPs, we show that the NEP approach not only achieves above-average accuracy but also is far more computationally efficient. These results demonstrate that the gpumd package is a promising tool for solving challenging problems requiring highly accurate, large-scale atomistic simulations. To enable the construction of MLPs using a minimal training set, we propose an active-learning scheme based on the latent space of a pre-trained NEP model. Finally, we introduce three separate Python packages, viz., gpyumd, calorine, and pynep, that enable the integration of gpumd into Python workflows.",Journal of Chemical Physics,2022,10.1063/5.0106617,"Jianyang Wu, P. Erhart, Z. Fan, Haikuan Dong, Keke Song, E. Lindgren, Ke Xu, Junjie Wang, A. Gabourie, Jiahui Liu, J. Rahm, Zezhu Zeng, Yue Chen, T. Ala‐Nissila, Yanzhou Wang, Yong Wang, Penghua Ying, Y. Su, Zheng Zhong, Jian Sun",41.666666999999997,2
888,ASE,"The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.",ACM Transactions on Graphics,2022,10.1145/3528223.3530110,"Yunrong Guo, S. Fidler, S. Levine, L. Halper, X. B. Peng",31.0,2
446,A practical light transport system model for chemiluminescence distribution reconstruction,"Plenoptic cameras and other integral photography instruments capture richer angular information from a scene than traditional 2D cameras. This extra information is used to estimate depth, perform superresolution or reconstruct 3D information from the scene. Many of these applications involve solving a large-scale numerical optimization problem. Most published approaches model the camera(s) using pre-computed matrices that require large amounts of memory and are not well-suited to modern many-core processors. We propose a flexible camera model based on light transport and use it to model plenoptic and traditional cameras. We implement the proposed model on a GPU and use it to reconstruct simulated and real 3D chemiluminescence distributions (flames) from images taken by traditional and plenoptic cameras.",,2018,10.1080/09542299.2018.1520050,"Madison G. McGaffin, V. Sick, Hao Chen, J. Fessler",23.714286000000001,2
265,GPU-based simulation of cloth wrinkles at submillimeter levels,"In this paper, we study physics-based cloth simulation in a very high resolution setting, presumably at submillimeter levels with millions of vertices, to meet perceptual precision of our human eyes. State-of-the-art simulation techniques, mostly developed for unstructured triangular meshes, can hardly meet this demand due to their large computational costs and memory footprints. We argue that in a very high resolution, it is more plausible to use regular meshes with an underlying grid structure, which can be highly compatible with GPU acceleration like high-resolution images. Based on this idea, we formulate and solve the nonlinear optimization problem for simulating high-resolution wrinkles, by a fast block-based descent method with reduced memory accesses. We also investigate the development of the collision handling component in our system, whose performance benefits greatly from the grid structure. Finally, we explore various issues related to the applications of our system, including initialization for fast convergence and temporal coherence, gathering effects, inflation and stuffing models, and mesh simplification. We can treat our system as a quasistatic wrinkle synthesis tool, run it as a standalone dynamic simulator, or integrate it into a multi-resolution solver as an additional component. The experiment demonstrates the capability, efficiency and flexibility of our system in producing a variety of high-resolution wrinkles effects.",ACM Transactions on Graphics,2021,10.1145/3450626.3459787,Huamin Wang,22.5,2
1078,QoE Fairness Resource Allocation in Digital Twin-Enabled Wireless Virtual Reality Systems,"Wireless virtual reality (VR) is expected to be a technology that revolutionizes human interaction and perceived media, where the quality of experience (QoE) is an important indicator to measure user service perception. However, existing schemes only consider general and time-invariant QoE optimization, which may suffer performance degradation. Moreover, it is also necessary to ensure the fairness of the individual user’s performance in wireless VR. To address these challenges, we employ digital twin technology to investigate a max-min QoE-optimal problem for wireless VR systems in this paper. Specifically, we maximize the QoE of the worst-case head-mounted displays (HDMs) client, where the QoE model is the linear weighting combination of video quality, service delay, and energy efficiency. The formulated optimization problem is characterized by multidimensional control, which jointly optimizes model selection, transmit power, computation time, and GPU-cycle frequency. Due to the mixed combinatorial features of the optimization problem, we give a low-complexity algorithm design by decoupling the optimization variables. Notably, we first obtain the allocation of the transmit power by employing the generalized fractional programming theory and the Lagrangian dual decomposition, followed by attaining the optimal allocation of GPU-cycle frequency in VR mode is derived by the proposed adaptive modified harmony search algorithm, and finally achieve the computation time by the barrier method. Meanwhile, we devise a greedy-style heuristic algorithm for mode selection. In the simulation, three baseline schemes are established as comparisons to assess the effectiveness of the proposed scheme. Meanwhile, the simulation results manifest that the proposed algorithms have good convergence performance and better increase the QoE of the DT-enabled wireless VR system compared to benchmark solutions.",IEEE Journal on Selected Areas in Communications,2023,10.1109/JSAC.2023.3313195,"Celimuge Wu, Xiangwang Hou, Jie Feng, Qingqi Pei, Lei Liu",22.0,2
834,Limitations of Linear Cross-Entropy as a Measure for Quantum Advantage,"Demonstrating quantum advantage requires experimental implementation of a computational task that is hard to achieve using state-of-the-art classical systems. One approach is to perform sampling from a probability distribution associated with a class of highly entangled many-body wavefunctions. It has been suggested that this approach can be certified with the Linear Cross-Entropy Benchmark (XEB). We critically examine this notion. First, in a""benign""setting where an honest implementation of noisy quantum circuits is assumed, we characterize the conditions under which the XEB approximates the fidelity. Second, in an""adversarial""setting where all possible classical algorithms are considered for comparison, we show that achieving relatively high XEB values does not imply faithful simulation of quantum dynamics. We present an efficient classical algorithm that, with 1 GPU within 2s, yields high XEB values, namely 2-12% of those obtained in experiments. By identifying and exploiting several vulnerabilities of the XEB, we achieve high XEB values without full simulation of quantum circuits. Remarkably, our algorithm features better scaling with the system size than noisy quantum devices for commonly studied random circuit ensembles. To quantitatively explain the success of our algorithm and the limitations of the XEB, we use a theoretical framework in which the average XEB and fidelity are mapped to statistical models. We illustrate the relation between the XEB and the fidelity for quantum circuits in various architectures, with different gate choices, and in the presence of noise. Our results show that XEB's utility as a proxy for fidelity hinges on several conditions, which must be checked in the benign setting but cannot be assumed in the adversarial setting. Thus, the XEB alone has limited utility as a benchmark for quantum advantage. We discuss ways to overcome these limitations.",arXiv.org,2021,10.1016/j.aop.2021.168618,"B. Barak, M. Kalinowski, M. Lukin, Xun Gao, Chi-Ning Chou, Soonwon Choi",21.5,2
1491,Accelerated 3D visualization of mock galaxy catalogues for the Dark Energy Survey,"We present a novel implementation of the visualization code Splotch1, exploiting optimized atomic operations of modern GPU hardware found in today's heterogeneous supercomputers, and show its application to the visualization of mock galaxy catalogues for the Dark Energy Survey (DES)2. These mock catalogues are generated by numerical simulation codes producing results which can then be directly compared with observations or validated against other models. As use case we compare a catalogue based on the L-PICOLA3 code with the MICE4 catalogue for DES. We present and discuss the potentiality of our optimized visualization algorithm as a debugging tool for L-PICOLA, in particular focusing on gaining rapidly an insight into potential anomalies on large scales and aiding in planning of appropriate remediation measures.",,2015,10.1093/mnras/stu2693,"M. Krokos, M. Manera, C. Gheller, Timothy Dykes, M. Rivi",20.5,2
871,DUNE Software and High Performance Computing,"DUNE, like other HEP experiments, faces a challenge related to matching execution patterns of our production simulation and data processing software to the limitations imposed by modern high-performance computing facilities. In order to efficiently exploit these new architectures, particularly those with high CPU core counts and GPU accelerators, our existing software execution models require adaptation. In addition, the large size of individual units of raw data from the far detector modules pose an additional challenge somewhat unique to DUNE. Here we describe some of these problems and how we begin to solve them today with existing software frameworks and toolkits. We also describe ways we may leverage these existing software architectures to attack remaining problems going forward. This whitepaper is a contribution to the Computational Frontier of Snowmass21.",,2022,10.1103/physrevresearch.4.013231,"X. Qian, K. Knoepfel, Hanyu Wei, Yihui Ren, B. Viren, Meifeng Lin, Haiwang Yu, B. Fleming, Shinjae Yoo",19.666667000000000,2
733,OxDNA.org: a public webserver for coarse-grained simulations of DNA and RNA nanostructures,"Abstract OxDNA and oxRNA are popular coarse-grained models used by the DNA/RNA nanotechnology community to prototype, analyze and rationalize designed DNA and RNA nanostructures. Here, we present oxDNA.org, a graphical web interface for running, visualizing and analyzing oxDNA and oxRNA molecular dynamics simulations on a GPU-enabled high performance computing server. OxDNA.org automatically generates simulation files, including a multi-step relaxation protocol for structures exported in non-physical states from DNA/RNA design tools. Once the simulation is complete, oxDNA.org provides an interactive visualization and analysis interface using the browser-based visualizer oxView to facilitate the understanding of simulation results for a user’s specific structure. This online tool significantly lowers the entry barrier of integrating simulations in the nanostructure design pipeline for users who are not experts in the technical aspects of molecular simulation. The webserver is freely available at oxdna.org.",Nucleic Acids Res.,2021,10.1093/nar/gkab324,"P. Šulc, Erik Poppleton, R. Romero, Aatmik Mallya, L. Rovigatti",17.75,2
1477,nbody6++gpu: ready for the gravitational million-body problem,"Accurate direct N-body simulations help to obtain detailed information about the dynamical evolution of star clusters. They also enable comparisons with analytical models and Fokker-Planck or Monte-Carlo methods. NBODY6 is a well-known direct N-body code for star clusters, and NBODY6++ is the extended version designed for large particle number simulations by supercomputers. We present NBODY6++GPU, an optimized version of NBODY6++ with hybrid parallelization methods (MPI, GPU, OpenMP, and AVX/SSE) to accelerate large direct N-body simulations, and in particular to solve the million-body problem. We discuss the new features of the NBODY6++GPU code, benchmarks, as well as the first results from a simulation of a realistic",,2015,10.1093/mnras/stv817,"M. Kouwenhoven, S. Aarseth, T. Naab, P. Berczik, Keigo Nitadori, Long Wang, R. Spurzem",16.6,2
1575,Accelerating Peridynamics Program Using GPU with CUDA and OpenACC,"In order to model some materials that may naturally form discontinuities such as cracks as a result of deformation, the peridynamics theory is introduced by Silling (2000). In this theory, the formulation employs integral equations instead of partial differential equations and it is assumed that particles in a continuum body interact with each other at a distance. However, because it is a particle based method, the simulation of 3-D model requires a large number of particles and time steps. This leads to huge runtime requirements. Hence, to find ways to accelerate the peridynamics program becomes important to the research. In this paper, we used two kinds of parallel computing methods to accelerate the peridynamics problems: CUDA (Compute Unified Device Architecture) and OpenACC. CUDA is a parallel language extension and OpenACC is a compiler directive. A bond-based peridynamics model of crack propagation was developed and tested using the two parallel computing methods. The results show that both methods can reach the speedups from 12 to 100 compared to the corresponding sequential implementation of peridynamics code. We also compared the differences between the two methods in speed-up ratio, time-cost of the code modification, multi-platform portability and utilization of GPU. Finally we will provide some suggestions on how to choose these two parallel computing methods.",,2017,10.1016/j.ijheatmasstransfer.2017.02.032,"J. Li, Y. Liu, Fei Xu, J. M. Zhao",15.125,2
702,Tinker-HP: Accelerating Molecular Dynamics Simulations of Large Complex Systems with Advanced Point Dipole Polarizable Force Fields Using GPUs and Multi-GPU Systems,"We present the extension of the Tinker-HP package (Lagardère, Chem. Sci.2018, 9, 956−97229732110) to the use of Graphics Processing Unit (GPU) cards to accelerate molecular dynamics simulations using polarizable many-body force fields. The new high-performance module allows for an efficient use of single- and multiple-GPU architectures ranging from research laboratories to modern supercomputer centers. After detailing an analysis of our general scalable strategy that relies on OpenACC and CUDA, we discuss the various capabilities of the package. Among them, the multiprecision possibilities of the code are discussed. If an efficient double precision implementation is provided to preserve the possibility of fast reference computations, we show that a lower precision arithmetic is preferred providing a similar accuracy for molecular dynamics while exhibiting superior performances. As Tinker-HP is mainly dedicated to accelerate simulations using new generation point dipole polarizable force field, we focus our study on the implementation of the AMOEBA model. Testing various NVIDIA platforms including 2080Ti, 3090, V100, and A100 cards, we provide illustrative benchmarks of the code for single- and multicards simulations on large biosystems encompassing up to millions of atoms. The new code strongly reduces time to solution and offers the best performances to date obtained using the AMOEBA polarizable force field. Perspectives toward the strong-scaling performance of our multinode massive parallelization strategy, unsupervised adaptive sampling and large scale applicability of the Tinker-HP code in biophysics are discussed. The present software has been released in phase advance on GitHub in link with the High Performance Computing community COVID-19 research efforts and is free for Academics (see https://github.com/TinkerTools/tinker-hp).",Journal of Chemical Theory and Computation,2021,10.1021/acs.jctc.0c01164,"I. Dupays, Théo Jaffrelot Inizan, Frédéric Célerse, Luc-Henri Jolly, Louis Lagardère, Zhi Wang, Jean‐Philip Piquemal, J. Ponder, O. Adjoua, A. Durocher, Pengyu Y. Ren, Thibaut Very",14.5,2
603,petar: a high-performance N-body code for modelling massive collisional stellar systems,"The numerical simulations of massive collisional stellar systems, such as globular clusters (GCs), are very time-consuming. Until now, only a few realistic million-body simulations of GCs with a small fraction of binaries (5%) have been performed by using the NBODY6++GPU code. Such models took half a year computational time on a GPU based super-computer. In this work, we develop a new N-body code, PeTar, by combining the methods of Barnes-Hut tree, Hermite integrator and slow-down algorithmic regularization (SDAR). The code can accurately handle an arbitrary fraction of multiple systems (e.g. binaries, triples) while keeping a high performance by using the hybrid parallelization methods with MPI, OpenMP, SIMD instructions and GPU. A few benchmarks indicate that PeTar and NBODY6++GPU have a very good agreement on the long-term evolution of the global structure, binary orbits and escapers. On a highly configured GPU desktop computer, the performance of a million-body simulation with all stars in binaries by using PeTar is 11 times faster than that of NBODY6++GPU. Moreover, on the Cray XC50 supercomputer, PeTar well scales when number of cores increase. The ten million-body problem, which covers the region of ultra compact dwarfs and nuclearstar clusters, becomes possible to be solved.",,2020,10.1093/mnras/staa1915,"Keigo Nitadori, Long Wang, J. Makino, M. Iwasawa",14.4,2
475,Data-driven fluid simulations using regression forests,"Traditional fluid simulations require large computational resources even for an average sized scene with the main bottleneck being a very small time step size, required to guarantee the stability of the solution. Despite a large progress in parallel computing and efficient algorithms for pressure computation in the recent years, realtime fluid simulations have been possible only under very restricted conditions. In this paper we propose a novel machine learning based approach, that formulates physics-based fluid simulation as a regression problem, estimating the acceleration of every particle for each frame. We designed a feature vector, directly modelling individual forces and constraints from the Navier-Stokes equations, giving the method strong generalization properties to reliably predict positions and velocities of particles in a large time step setting on yet unseen test videos. We used a regression forest to approximate the behaviour of particles observed in the large training set of simulations obtained using a traditional solver. Our GPU implementation led to a speed-up of one to three orders of magnitude compared to the state-of-the-art position-based fluid solver and runs in real-time for systems with up to 2 million particles.",ACM Transactions on Graphics,2015,10.1145/2816795.2818129,"Sohyeon Jeong, M. Pollefeys, B. Solenthaler, Lubor Ladicky, M. Gross",14.1,2
1142,"HELIOS-K: AN ULTRAFAST, OPEN-SOURCE OPACITY CALCULATOR FOR RADIATIVE TRANSFER","We present an ultrafast opacity calculator that we name HELIOS-K. It takes a line list as an input, computes the shape of each spectral line, and provides an option for grouping an enormous number of lines into a manageable number of bins. We implement a combination of Algorithm 916 and Gauss–Hermite quadrature to compute the Voigt profile, write the code in CUDA, and optimize the computation for graphics processing units (GPUs). We restate the theory of the k-distribution method and use it to reduce ∼ 10 5 ?> –108 lines to ∼10–104 wavenumber bins, which may then be used for radiative transfer, atmospheric retrieval and general circulation models. The choice of line-wing cutoff for the Voigt profile is a significant source of error and affects the value of the computed flux by ∼ 10 % ?> . This is an outstanding physical (rather than computational) problem, due to our incomplete knowledge of pressure broadening of spectral lines in the far line wings. We emphasize that this problem remains regardless of whether one performs line-by-line calculations or uses the k-distribution method and affects all calculations of exoplanetary atmospheres requiring the use of wavelength-dependent opacities. We elucidate the correlated-k approximation and demonstrate that it applies equally to inhomogeneous atmospheres with a single atomic/molecular species or homogeneous atmospheres with multiple species. Using a NVIDIA K20 GPU, HELIOS-K is capable of computing an opacity function with ∼ 10 5 ?> spectral lines in ∼1 s and is publicly available as part of the Exoclimes Simulation Platform (www.exoclime.org).",,2015,10.1088/0004-637X/808/2/182,"K. Heng, S. Grimm",13.9,2
555,Decentralized Distributed PPO: Mastering PointGoal Navigation,"We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever ""stale""), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially ""solves"" the task -- near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ""ImageNet pre-training + task-specific fine-tuning"" for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models + code will be publicly available).",International Conference on Learning Representations,2020,10.1007/978-3-030-58604-1_7,"Irfan Essa, Dhruv Batra, Erik Wijmans, Abhishek Kadian, Ari S. Morcos, Devi Parikh, Stefan Lee, M. Savva",13.4,2
939,DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality,"Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in diverse kinds of hardware and simulator setups, and in our case, with the Allegro Hand and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for researchers to achieve such results with commonly-available, affordable robot hands and cameras. Videos of the resulting policy and supplementary information, including experiments and demos, can be found on the website.",IEEE International Conference on Robotics and Automation,2022,10.1109/ICRA48891.2023.10160216,"Jean-Francois Lafleche, Jingzhou Liu, Yashraj S. Narang, Gavriel State, Denys Makoviichuk, Aleksei Petrenko, Karl Van Wyk, D. Fox, Arthur Allshire, Viktor Makoviychuk, Ankur Handa, Ritvik Singh, Balakumar Sundaralingam, Alexander Zhurkevich",13.333333000000000,2
842,GPU-accelerated Faster Mean Shift with euclidean distance metrics,"Handling clustering problems are important in data statistics, pattern recognition and image processing. The mean-shift algorithm, a common unsupervised algorithms, is widely used to solve clustering problems. However, the mean-shift algorithm is restricted by its huge computational resource cost. In previous research [1], we proposed a novel GPU-accelerated Faster Mean-shift algorithm, which greatly speed up the cosine-embedding clustering problem. In this study, we extend and improve the previous algorithm to handle Euclidean distance metrics. Different from conventional GPU-based mean-shift algorithms, our algorithm adopts novel Seed Selection & Early Stopping approaches, which greatly increase computing speed and reduce GPU memory consumption. In the simulation testing, when processing a 200K points clustering problem, our algorithm achieved around 3 times speedup compared to the state-of-the-art GPU-based mean-shift algorithms with optimized GPU memory consumption. Moreover, in this study, we implemented a plug-and-play model for faster mean-shift algorithm, which can be easily deployed. (Plug-and-play model is available: https://github.com/masqm/Faster-Mean-Shift-Euc)",Annual International Computer Software and Applications Conference,2021,10.1109/COMPSAC54236.2022.00037,"Lingxi Chen, Mengyang Zhao, Han Jiang, Jinyong Hu, Xintong Cui, C. Chang",12.75,2

